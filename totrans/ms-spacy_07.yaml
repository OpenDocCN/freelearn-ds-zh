- en: 'Chapter 5: Working with Word Vectors and Semantic Similarity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Word vectors** are handy tools and have been the hot topic of NLP for almost
    a decade. A word vector is basically a dense representation of a word. What''s
    surprising about these vectors is that semantically similar words have similar
    word vectors. Word vectors are great for semantic similarity applications, such
    as calculating the similarity between words, phrases, sentences, and documents.
    At a word level, word vectors provide information about synonymity, semantic analogies,
    and more. We can build semantic similarity applications by using word vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors are produced by algorithms that make use of the fact that similar
    words appear in similar contexts. To capture the meaning of a word, a word vector
    algorithm collects information about the surrounding words that the target word
    appears with. This paradigm of capturing semantics for words by their surrounding
    words is called **distributional semantics**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the **distributional semantics paradigm**
    and its associated **semantic similarity methods**. We will start by taking a
    conceptual look at **text vectorization** so that you know what NLP problems word
    vectors solve.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will become familiar with word vector computations such as **distance
    calculation**, **analogy calculations**, and **visualization**. Then, we will
    learn how to benefit from spaCy's pretrained word vectors, as well as import and
    use third-party vectors. Finally, we will go through advanced semantic similarity
    methods using spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using spaCy's pretrained vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using third-party word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced semantic similarity methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have used some external Python libraries besides spaCy
    for code visualization purposes. If you want to generate word vector visualizations
    in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find this chapter''s code in this book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The invention of word vectors (or **word2vec**) has been one of the most thrilling
    advancements in the NLP world. Those of you who are practicing NLP have definitely
    heard of word vectors at some point. This chapter will help you understand the
    underlying idea that caused the invention of word2vec, what word vectors look
    like, and how to use them in NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: The statistical world works with numbers, and all statistical methods, including
    statistical NLP algorithms, work with vectors. As a result, while working with
    statistical methods, we need to represent every real-world quantity as a vector,
    including text. In this section, we will learn about the different ways we can
    represent text as vectors and discover how word vectors provide semantic representation
    for words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by discovering text vectorization by covering the simplest implementation
    possible: one-hot encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**One-hot encoding** is a simple and straightforward way to assign vectors
    to words: assign an index value to each word in the vocabulary and then encode
    this value into a **sparse vector**. Let''s look at an example. Here, we will
    consider the vocabulary of a pizza ordering application; we can assign an index
    to each word in the order they appear in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the vector of a vocabulary word will be 0, except for the position of
    the word''s corresponding index value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can represent a sentence as a matrix, where each row corresponds to
    one word. For example, the sentence *I want a pizza* can be represented by the
    following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the preceding vocabulary and indices, the length of the vectors
    is equal to the number of the words in the vocabulary. Each dimension is devoted
    to one word explicitly. When we apply one-hot encoding vectorization to our text,
    each word is replaced by its vector and the sentence is transformed into a `(N,
    V)` matrix, where `N` is the number of words in the sentence and `V` is the vocabulary's
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way of representing text is straightforward to compute, as well as easy
    to debug and understand. This looks good so far, but there are some potential
    problems here, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The vectors are sparse. Each vector contains many 0s but only one `1`. Obviously,
    this is a waste of space if we know that words with similar meanings can be grouped
    together and share some dimensions. Also, numerical algorithms don't really like
    high-dimensional and sparse vectors in general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, what if the vocabulary size is over 1 million words? Obviously, we
    would need to use 1 million dimensional vectors, which is not really feasible
    in terms of memory and computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another problem is that the vectors are not *meaningful* at all. Similar words
    are not assigned similar vectors somehow. In the preceding vocabulary, the words
    `cheese`, `topping`, `salami`, and `pizza` actually carry related meanings, but
    their vectors are not related in any way. These vectors are indeed assigned randomly,
    depending on the corresponding word's index in the vocabulary. The one-hot encoded
    vectors don't capture any semantic relationships at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word vectors were invented to answer the preceding list of concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Word vectors are the solution to the preceding problems. A word vector is a
    **fixed-size**, **dense**, **real-valued** vector. From a broader perspective,
    a word vector is a learned representation of the text where semantically similar
    words have similar vectors. The following is what a word vector looks like. This
    has been extracted from **Glove English vectors** (we''ll look at Glove in detail
    in the *How word vectors are produced* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a 50-dimensional vector for the word `the`. As you can see, the dimensions
    are floating points. But what do the dimensions represent? These individual dimensions
    typically don't have inherent meanings. Instead, they represent locations in the
    vector space, and the distance between these vectors indicates the similarity
    of the corresponding words' meanings. Hence, a word's meaning is distributed across
    the dimensions. This way of representing a word's meaning is called **distributional
    semantics**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already mentioned that semantically similar words have similar representations.
    Let''s look at the vectors of the different words and how they offer semantic
    representations. We can use the word vector visualizer for TensorFlow at [https://projector.tensorflow.org/](https://projector.tensorflow.org/)
    for this. On this website, Google offers word vectors for 10,000 words. Each vector
    is 200-dimensional and projected onto three dimensions for visualization. Let''s
    look at the representation of the word `cheese` from our humble pizza ordering
    vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The vector representation of the word “cheese” and semantically
    similar words'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – The vector representation of the word "cheese" and semantically
    similar words
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the word `cheese` is semantically grouped with the other words
    about food. These are the words that are used together with the word `cheese`
    quite often: sauce, cola, food, and so on. In the following screenshot, we can
    see the closest words sorted by their cosine distance (think of cosine distance
    as a way of calculating the distance between vectors):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Closest points to “cheese” in the three-dimensional space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Closest points to "cheese" in the three-dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: 'How about some proper nouns? Word vectors are trained on a huge corpus, such
    as Wikipedia, which is why the representations of some proper nouns are also learned.
    For example, the proper noun **elizabeth** is represented by the following vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Vector representation of elizabeth'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Vector representation of elizabeth
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Notice that all the words in the preceding screenshot are in lowercase. Most
    of the word vector algorithms make all the vocabulary input words lowercase to
    avoid there being two representations of the same word.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that **elizabeth** indeed points to Queen Elizabeth of England.
    The surrounding words are **monarch**, **empress**, **princess**, **royal**, **lord**,
    **lady**, **crown**, **England**, **Tudor**, **Buckingham**, her mother's name,
    **anne**, her father's name, **henry**, and even her mother's rival queen's name,
    **catherine**! Both ordinary words such as **crown** and proper nouns such as
    **henry** are grouped together with **elizabeth**. We can also see that the syntactic
    category of all the neighbor words is noun; verbs don't go together with nouns.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors can capture synonyms, antonyms, and semantic categories such as
    animals, places, plants, names, and abstract concepts. Next, we'll dive deep into
    semantics and explore a surprising feature provided by word vectors – **word analogies**.
  prefs: []
  type: TYPE_NORMAL
- en: Analogies and vector operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've already seen that learned representations can capture semantics. What's
    more, word vectors support vector operations, such as vector addition and subtraction,
    in a meaningful way. Indeed, adding and subtracting word vectors is one way to
    support analogies.
  prefs: []
  type: TYPE_NORMAL
- en: A word analogy is a semantic relationship between a pair of words. There are
    many types of relationship, such as synonymity, anonymity, and wholepart relation.
    Some example pairs are (King – man, Queen – woman), (airplane – air, ship - sea),
    (fish – sea, bird - air), (branch – tree, arm – human), (forward – backward, absent
    – present), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can represent gender mapping between the Queen and King as
    `Queen – Woman + Man = King`. Here, if we subtract *woman* from *Queen* and add
    *man* instead, we get *King*. Then, this analogy reads as, *queen is to king as
    woman is to man*. Embeddings can generate remarkable analogies such as gender,
    tense, and capital city. The following diagram shows these analogies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)
    ](img/B16570_5_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)'
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, word vectors provide great semantic capabilities for NLP developers,
    but how are they produced? We'll learn more about word vector generation algorithms
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How word vectors are produced
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is more than one way to produce word vectors. Let''s look at the most
    popular pretrained vectors and how they are trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '**word2vec** is the name of the statistical algorithm that was created by Google
    to produce word vectors. Word vectors are trained with a neural network architecture,
    which processes windows of words and predicts the vector for each word, depending
    on the surrounding words. These pretrained word vectors can be downloaded from
    [https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models).
    We won''t go into the details here, but you can read the excellent blog at [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)
    for more details about the algorithm and data preparation steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glove** vectors are trained in another way and were invented by the Stanford
    NLP group. This method depends on singular value decomposition, which is used
    on the word co-occurrences matrix. A comprehensive guide to the Glove algorithm
    is available at [https://www.youtube.com/watch?v=Fn_U2OG1uqI](https://www.youtube.com/watch?v=Fn_U2OG1uqI).
    The pretrained vectors are available at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fastText** was created by Facebook Research and is similar to word2vec, but
    offers more. word2vec predicts words based on their surrounding context, while
    fastText predicts subwords; that is, character n-grams. For example, the word
    *chair* generates the following subwords:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: fastText produces a vector for each subword, including misspelled words, numbers,
    partial words, and single characters. fastText is robust when it comes to misspelled
    words and rare words. It can compute a vector for the tokens that are not proper
    lexicon words.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook Research published pretrained fastText vectors for 157 languages. You
    can find these models at [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'All the preceding algorithms follow the same idea: similar words occur in a
    similar context. The context – the surrounding words around a word – is key to
    generating the word vector for a specific word in any case. All the pretrained
    word vectors that are generated with the preceding three algorithms are trained
    on a huge corpus such as Wikipedia, the news, or Twitter.'
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: 'When we say similar words, the first concept that comes to mind is **synonymity**.
    Synonym words occur in a similar context; for example, *freedom* and *liberty*
    both mean the same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: We want free healthcare, education, and liberty.
  prefs: []
  type: TYPE_NORMAL
- en: We want free healthcare, education, and freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about antonyms? Antonyms can be used in the same context. Take *love* and
    *hate*, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: I hate cats.
  prefs: []
  type: TYPE_NORMAL
- en: I love cats.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, antonyms also appear in similar contexts; hence, usually, their
    vectors are also similar. If your downstream NLP task is sensitive in this aspect,
    be careful while using word vectors. In this case, always either train your own
    vectors or refine your word vectors by training them in the downstream task as
    well. You can train your own word vectors with the Gensim package ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)).
    The Keras library allows word vectors to be trained on downstream tasks. We'll
    revisit this issue in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*,
    Text Classification with spaCy*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know more about word vectors, let's look at how to use spaCy's pretrained
    word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using spaCy's pretrained vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We installed a medium-sized English spaCy language model in [*Chapter 1*](B16570_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,
    Getting Started with spaCy*, so that we can directly use word vectors. Word vectors
    are part of many spaCy language models. For instance, the `en_core_web_md` model
    ships with 300-dimensional vectors for 20,000 words, while the `en_core_web_lg`
    model ships with 300-dimensional vectors with a 685,000 word vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, small models (those whose names end with `sm`) do not include any
    word vectors but include context-sensitive tensors. You can still make the following
    semantic similarity calculations, but the results won't be as accurate as word
    vector computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can reach a word''s vector via the `token.vector` method. Let''s look at
    this method in an example. The following code queries the word vector for banana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot was taken within the Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Word vector for the word “banana”'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Word vector for the word "banana"
  prefs: []
  type: TYPE_NORMAL
- en: '`token.vector` returns a NumPy `ndarray`. You can call `numpy` methods on the
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this code segment, first, we queried the Python type of the word vector.
    Then, we invoked the `shape()` method of the NumPy array on the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Doc` and `Span` objects also have vectors. The vector of a sentence or
    a span is the average of its words'' vectors. Run the following code and view
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Only the words in the model''s vocabulary have vectors; words that are not
    in the vocabulary are called `token.is_oov` and `token.has_vector` are two methods
    we can use to query whether a token is in the model''s vocabulary and has a word
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is basically how we use spaCy's pretrained word vectors. Next, we'll discover
    how to invoke spaCy's semantic similarity method on `Doc`, `Span`, and `Token`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: The similarity method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In spaCy, every container type object has a similarity method that allows us
    to calculate the semantic similarity of other container objects by comparing their
    word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the semantic similarity between two container objects, even
    though they are different types of containers. For instance, we can compare a
    `Token` object to a `Doc` object and a `Doc` object to a `Span` object. The following
    example computes how similar two `Span` objects are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compare the two `Token` objects, `London` and `England`, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The sentence''s similarity is computed by calling `similarity()` on the `Doc`
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code segment calculates the semantic similarity between the two
    sentences `I visited England.` and `I went to London.`. The similarity score is
    high enough that it considers both sentences are similar (the degree of similarity
    ranges from `0` to `1`, with `0` being unrelated and `1` being identical).
  prefs: []
  type: TYPE_NORMAL
- en: 'Not surprisingly, the `similarity()` method returns `1` when you compare an
    object to itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Judging the distance with numbers is difficult sometimes, but looking at the
    vectors on paper can also help us understand how our vocabulary words are grouped.
    The following code snippet visualizes a simple vocabulary of two semantic classes.
    The first class of words is for animals, while the second class is for food. We
    expect these two classes of words to become two groups on the graphics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet achieves a lot. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the matplotlib library for creating our graphic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next two imports are for calculating the vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We imported `spacy` and created an `nlp` object as usual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we created a `Doc` object from our vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we stacked our word vectors vertically by calling `np.vstack`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the vectors are 300-dimensional, we needed to project them into a two-dimensional
    space for visualization purposes. We made this projection by extracting the two
    principal components via **principal component analysis** (**PCA**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest of the code deals with matplotlib function calls to create a scatter
    plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting visual looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Two semantic classes grouped'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Two semantic classes grouped
  prefs: []
  type: TYPE_NORMAL
- en: Voil•! Our spaCy word vectors really worked! Here, we can see the two semantic
    classes that were grouped on the visualization. Notice that the distance between
    the animals is less and more uniformly distributed, while the food class formed
    groups inside the group.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we mentioned that we can create our own word vectors or refine them
    on our own corpus. Once we've done that, can we use them within spaCy? The answer
    is yes! In the next section, we'll learn how to load custom word vectors into
    spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: Using third-party word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can also use third-party word vectors within spaCy. In this section, we'll
    learn how to import a third-party word vector package into spaCy. We'll use fastText's
    subword-based pretrained vectors from the Facebook AI. You can view the list of
    all the available English pretrained vectors at [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html).
  prefs: []
  type: TYPE_NORMAL
- en: The name of the package identifies the vector's dimension, the vocabulary size,
    and the corpus genre that the vectors will be trained on. For instance, `wiki-news-300d-1M-subword.vec.zip`
    indicates that it contains 1 million 300-dimensional word vectors that have been
    trained on a Wikipedia corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start downloading the vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your terminal, type the following command. Alternatively, you can copy and
    paste the URL into your browser and the download should start:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding line will download the 300-dimensional word vectors onto your
    machine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will unzip the following `.zip` file. You can either unzip it by right-clicking
    or by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''re ready to use spaCy''s `init-model` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If everything goes well, you should see the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we''ve created the language model. Now, we can load it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create a `doc` object with this `nlp` object, just like we did
    with spaCy''s default language models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model we just created is an empty model that was initiated with the word
    vectors, so it does not contain any other pipeline components. For instance, making
    a call to `doc.ents` will fail with an error. So, be careful while working with
    third-party vectors and favor built-in spaCy vectors whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced semantic similarity methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discover advanced semantic similarity methods for word,
    phrase, and sentence similarity. We've already learned how to calculate semantic
    similarity with spaCy's **similarity** method and obtained some scores. But what
    do these scores mean? How are they calculated? Before we look at more advanced
    methods, first, we'll learn how semantic similarity is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding semantic similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we collect text data (any sort of data), we want to see how some examples
    are similar, different, or related. We want to measure how similar two pieces
    of text are by calculating their similarity scores. Here, the term *semantic similarity*
    comes into the picture; **semantic similarity** is a **metric** that's defined
    over texts, where the distance between two texts is based on their semantics.
  prefs: []
  type: TYPE_NORMAL
- en: A metric in mathematics is basically a distance function. Every metric induces
    a topology on the vector space. Word vectors are vectors, so we want to calculate
    the distance between them and use this as a similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll learn about two commonly used distance functions: **Euclidian distance**
    and **cosine distance**. Let''s start with Euclidian distance.'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidian distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Euclidian distance between two points in a k-dimensional space is the length
    of the path between them. The distance between two points is calculated by the
    Pythagorean theorem. We calculate this distance by summing the difference of each
    coordinate''s square and then taking the square root of this sum. The following
    diagram shows the Euclidian distance between two vectors, dog and cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Euclidian distance between two vectors, dog and cat'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_5_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Euclidian distance between two vectors, dog and cat
  prefs: []
  type: TYPE_NORMAL
- en: What does Euclidian distance mean for word vectors? First, Euclidian distance
    has no idea of **vector orientation**; what matters is the **vector magnitude**.
    If we take a pen and draw a vector from the origin to the **dog** point (let's
    call it **dog vector**) and do the same for the **cat** point (let's call it **cat
    vector**) and subtract one vector from and other, then the distance is basically
    the magnitude of this difference vector.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we add two more semantically similar words (*canine*, *terrier*)
    to **dog** and make it a text of three words? Obviously, the dog vector will now
    grow in magnitude, possibly in the same direction. This time, the distance will
    be much bigger due to geometry (as shown in the following diagram), although the
    semantics of the first piece of text (now **dog canine terrier**) remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the main drawback of using Euclidian distance for semantic similarity
    – the orientation of the two vectors in the space is not taken into account. The
    following diagram illustrates the distance between **dog** and **cat** and the
    distance between **dog** **canine terrier** and **cat**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Distance between “dog” and “cat,” as well as the distance between
    “dog canine terrier” and “cat”](img/B16570_5_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Distance between "dog" and "cat," as well as the distance between
    "dog canine terrier" and "cat"
  prefs: []
  type: TYPE_NORMAL
- en: How can we fix this problem? There's another way of calculating similarity that
    addresses this problem, called **cosine similarity**. Let's take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine distance and cosine similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Contrary to Euclidian distance, cosine distance is more concerned with the
    orientation of the two vectors in the space. The cosine similarity of two vectors
    is basically the cosine of the angle that''s created by these two vectors. The
    following diagram shows the angle between the **dog** and **cat** vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic
    similarity is calculated by cos(θ)](img/B16570_5_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic similarity
    is calculated by cos(θ)
  prefs: []
  type: TYPE_NORMAL
- en: The maximum similarity score that's allowed by cosine similarity is `1`. This
    is obtained when the angle between two vectors is 0 degrees (hence, the vectors
    coincide). The similarity between two vectors is 0 when the angle between them
    is 90 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity provides us with scalability when the vectors grow in magnitude.
    We will refer to *Figure 5.8* again here. If we grow one of the input vectors,
    the angle between them remains the same, so the cosine similarity score is the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Note that here, we are calculating the semantic similarity score, not the distance.
    The highest possible value is 1 when the vectors coincide, while the lowest score
    is 0 when two vectors are perpendicular. The cosine distance is 1 – cos(θ), which
    is a distance function.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy uses cosine similarity to calculate semantic similarity. Hence, calling
    the `similarity` method helps us make cosine similarity calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we''ve learned how to calculate similarity scores, but we still haven''t
    discovered words we should look for meaning in. Obviously, not all the words in
    a sentence have the same impact on the semantics of the sentence. The similarity
    method will calculate the semantic similarity score for us, but for the results
    of that calculation to be useful, we need to choose the right keywords to compare.
    To understand why, consider the following text snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If we're interested in finding the biggest mammals on the planet, the phrases
    `biggest mammals` and `in the world` will be the key words. Comparing these phrases
    with the search phrases *largest mammals* and *on the planet* should give us a
    high similarity score. But if we're interested in finding out about some places
    in the world, `California` will be the keyword. `California` is semantically similar
    to the word *geography* and, even better, the entity type is a geographical noun.
  prefs: []
  type: TYPE_NORMAL
- en: We have already learned *how* to calculate the similarity score. In the next
    section, we'll learn about *where* to look for the *meaning*. We'll extract the
    key phrases and named entities from the sentences and then use them in similarity
    score calculations. We'll start by covering a case study on text categorization
    before improving the task results via key phrase extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing text with semantic similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Determining two sentence''s semantic similarity can help you categorize texts
    into predefined categories or spot only the relevant texts. In this case study,
    we''ll filter all user comments in an e-commerce website related to the word *perfume*.
    Suppose you need to evaluate the following user comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that only the second sentence is related. This is because it
    contains the word `fragrance`, as well as the adjectives describing scents. To
    understand which sentences are related, we can try several comparison strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can compare `perfume` to each sentence. Recall that spaCy generates
    a word vector for a sentence by averaging the word vector of its tokens. The following
    code snippet compares the preceding sentences to the `perfume` search key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we performed the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we created a `Doc` object with the three preceding sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, for each sentence, we calculated the similarity score with `perfume`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we printed the score by invoking the `similarity()` method on the sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The degree of similarity between `perfume` and the first sentence is small,
    indicating that this sentence is not very relevant to our search key. The second
    sentence looks relevant, which means that we correctly spotted the semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: How about the third sentence? The script identified that the third sentence
    is relevant somehow, most probably because it includes the word `bottle` and perfumes
    are sold in bottles. The word `bottle` appears in similar contexts with the word
    `perfume`. For this reason, the similarity score of this sentence and the search
    key is not low enough; also, the scores of the second sentence and the third sentence
    are not far away enough to make the second sentence significant.
  prefs: []
  type: TYPE_NORMAL
- en: There's another potential problem with comparing the key to the whole sentence.
    In practice, we occasionally deal with quite long texts, such as web documents.
    Averaging over a very long text lowers the importance of key words.
  prefs: []
  type: TYPE_NORMAL
- en: To improve performance, we can extract the *important* words. Let's look at
    how we can spot the key phrases in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting key phrases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A better way to do semantic categorization is to extract the important words/phrases
    and compare only them to the search key. Instead of comparing the key to the different
    parts of speech, we can compare the key to just the noun phrases. Noun phrases
    are the subjects, direct objects, and indirect objects of the sentences and carry
    a big percentage of the sentence's semantics on their shoulders.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the sentence *Blue whales live in California.*, you'd probably
    like to focus on *blue whales*, *whales*, *California*, or *whales in California*.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the preceding sentence about perfume, we focused on picking out
    the noun, *fragrance*. In different semantic tasks, you might need other context
    words such as verbs to decide what the sentence is about, but for semantic similarity,
    noun phrases carry most weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is a noun phrase, then? A **noun phrase** (**NP**) is a group of words
    that consist of a noun and its modifiers. Modifiers are usually pronouns, adjectives,
    and determiners. The following phrases are noun phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'spaCy extracts noun phases by parsing the output of the dependency parser.
    We can see the noun phrases of a sentence by using the `doc.noun_chunks` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s modify the preceding code snippet a bit. Instead of comparing the search
    key *perfume* to the entire sentence, this time, we will only compare it with
    the sentence''s noun chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we iterated over the sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, for each sentence, we extracted the noun chunks and stored them in a Python
    list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we joined the noun chunks in the list into a Python string and converted
    it into a `Doc` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we compared this `Doc` object of noun chunks to the search key *perfume*
    to determine their semantic similarity score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we compare these scores to the previous scores, we will see that the first
    sentence is still irrelevant, so its score went down slightly. The second sentence's
    score increased significantly. Now, the second sentence's and the third sentence's
    scores look so far away from each other for us to confidently say that the second
    sentence is the most related sentence here.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and comparing named entities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some cases, instead of extracting every noun, we will only focus on the
    proper nouns; hence, we want to extract the named entities. Let''s say we want
    to compare the following paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our code should be able to recognize that the first two paragraphs are about
    large technology companies and their products, while the third paragraph is about
    a geographic location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing all the noun phrases in these sentences may not be very helpful because
    many of them, such as `volume`, aren''t relevant to the categorization. The topics
    of these paragraphs are determined by the phrases within them; that is, `Google
    Search`, `Google`, `Microsoft Bing`, `Microsoft`, `Windows`, `Dead Sea`, `Jordan
    Valley`, and `Israel`. spaCy can spot these entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have extracted the words we want to compare, let''s calculate the
    similarity scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Looking at these figures, we can see that the highest level of similarity exists
    between the first and the second paragraph, which are both about large tech companies.
    The third paragraph is not really similar to the other paragraphs. How did we
    get this calculation by just using word vectors? Probably because the words *Google*
    and *Microsoft* often appear together in news and other social media text corpuses,
    hence creating similar word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You've reached the end of the *Advanced semantic similarity
    methods* section! You explored different ways of combining word vectors with linguistic
    features such as key phrases and named entities. By finishing this section, we
    are now ready to conclude this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you worked with word vectors, which are floating-point vectors
    that represent word semantics. First, you learned about the different ways to
    perform text vectorization, as well as how to use word vectors and distributed
    semantics. Then, you explored the vector operations that word vectors allow and
    what semantics these operations bring.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to use spaCy's built-in word vectors and how to import
    third-party vectors into spaCy. Finally, you learned about vector-based semantic
    similarity and how to blend linguistic concepts with word vectors to get the best
    out of these semantics.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is full of surprises – we'll look at a real-word case-based
    study that allows you to blend what you've learned about in the past five chapters.
    Let's see what spaCy can do when it comes to real-world problems!
  prefs: []
  type: TYPE_NORMAL
