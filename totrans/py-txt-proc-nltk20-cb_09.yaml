- en: Chapter 9. Parsing Specific Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 解析特定数据
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Parsing dates and times with Dateutil
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dateutil解析日期和时间
- en: Time zone lookup and conversion
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时区查找和转换
- en: Tagging temporal expressions with Timex
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Timex标记时间表达式
- en: Extracting URLs from HTML with lxml
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lxml从HTML中提取URL
- en: Cleaning and stripping HTML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和剥离HTML
- en: Converting HTML entities with BeautifulSoup
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BeautifulSoup转换HTML实体
- en: Detecting and converting character encodings
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测和转换字符编码
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'This chapter covers parsing specific kinds of data, focusing primarily on dates,
    times, and HTML. Luckily, there are a number of useful libraries for accomplishing
    this, so we don''t have to delve into tricky and overly complicated regular expressions.
    These libraries can be great complements to the NLTK:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了解析特定类型的数据，主要关注日期、时间和HTML。幸运的是，有多个有用的库可以完成这项任务，所以我们不必深入研究复杂且过于复杂的正则表达式。这些库可以很好地补充NLTK：
- en: '`dateutil`: Provides date/time parsing and time zone conversion'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dateutil`：提供日期/时间解析和时区转换'
- en: '`timex`: Can identify time words in text'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timex`：可以在文本中识别时间词'
- en: '`lxml` and `BeautifulSoup`: Can parse, clean, and convert HTML'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lxml`和`BeautifulSoup`：可以解析、清理和转换HTML'
- en: '`chardet`: Detects the character encoding of text'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chardet`：检测文本的字符编码'
- en: The libraries can be useful for pre-processing text before passing it to an
    NLTK object, or post-processing text that has been processed and extracted using
    NLTK. Here's an example that ties many of these tools together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库在将文本传递给NLTK对象之前进行预处理，或者在NLTK处理和提取后的文本进行后处理时非常有用。以下是一个将许多这些工具结合在一起的示例。
- en: Let's say you need to parse a blog article about a restaurant. You can use `lxml`
    or `BeautifulSoup` to extract the article text, outbound links, and the date and
    time when the article was written. The date and time can then be parsed to a Python
    `datetime` object with `dateutil`. Once you have the article text, you can use
    `chardet` to ensure it's UTF-8 before cleaning out the HTML and running it through
    NLTK-based part-of-speech tagging, chunk extraction, and/or text classification,
    to create additional metadata about the article. If there's an event happening
    at the restaurant, you may be able to discover that by looking at the time words
    identified by `timex`. The point of this example is that real-world text processing
    often requires more than just NLTK-based natural language processing, and the
    functionality covered in this chapter can help with those additional requirements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要解析一篇关于餐厅的博客文章。你可以使用`lxml`或`BeautifulSoup`提取文章文本、外链以及文章写作的日期和时间。然后，可以使用`dateutil`将这些日期和时间解析为Python的`datetime`对象。一旦你有了文章文本，你可以使用`chardet`确保它是UTF-8编码，然后在清理HTML并通过基于NLTK的词性标注、分块提取和/或文本分类进行处理之前，创建关于文章的额外元数据。如果你在餐厅有活动，你可能可以通过查看`timex`识别的时间词来发现这一点。这个示例的目的是说明现实世界的文本处理往往需要比基于NLTK的自然语言处理更多，而本章涵盖的功能可以帮助满足这些额外需求。
- en: Parsing dates and times with Dateutil
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Dateutil解析日期和时间
- en: If you need to parse dates and times in Python, there is no better library than
    `dateutil`. The `parser` module can parse `datetime` strings in many more formats
    than can be shown here, while the `tz` module provides everything you need for
    looking up time zones. Combined, these modules make it quite easy to parse strings
    into time zone aware `datetime` objects.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在Python中解析日期和时间，没有比`dateutil`更好的库了。`parser`模块可以解析比这里展示的更多格式的`datetime`字符串，而`tz`模块提供了查找时区所需的一切。这两个模块结合起来，使得将字符串解析为时区感知的`datetime`对象变得相当容易。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can install `dateutil` using `pip` or `easy_install`, that is `sudo pip
    install dateutil` or `sudo easy_install dateutil`. Complete documentation can
    be found at [http://labix.org/python-dateutil](http://labix.org/python-dateutil).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pip`或`easy_install`安装`dateutil`，即`sudo pip install dateutil`或`sudo easy_install
    dateutil`。完整的文档可以在[http://labix.org/python-dateutil](http://labix.org/python-dateutil)找到。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s dive into a few parsing examples:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入几个解析示例：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, all it takes is importing the `parser` module and calling the
    `parse()` function with a `datetime` string. The parser will do its best to return
    a sensible `datetime` object, but if it cannot parse the string, it will raise
    a `ValueError`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，只需导入`parser`模块，并使用`datetime`字符串调用`parse()`函数即可。解析器将尽力返回一个合理的`datetime`对象，但如果它无法解析字符串，它将引发`ValueError`。
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The parser does not use regular expressions. Instead, it looks for recognizable
    tokens and does its best to guess what those tokens refer to. The order of these
    tokens matters, for example, some cultures use a date format that looks like *Month/Day/Year*
    (the default order) while others use a *Day/Month/Year* format. To deal with this,
    the `parse()` function takes an optional keyword argument `dayfirst`, which defaults
    to `False`. If you set it to `True`, it can correctly parse dates in the latter
    format.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器不使用正则表达式。相反，它寻找可识别的标记，并尽力猜测这些标记代表什么。这些标记的顺序很重要，例如，一些文化使用看起来像 *Month/Day/Year*（默认顺序）的日期格式，而其他文化使用
    *Day/Month/Year* 格式。为了处理这个问题，`parse()` 函数接受一个可选的关键字参数 `dayfirst`，默认为 `False`。如果你将其设置为
    `True`，它可以正确解析后者的日期格式。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Another ordering issue can occur with two-digit years. For example, `'10-9-25'`
    is ambiguous. Since `dateutil` defaults to the *Month-Day-Year* format, `'10-9-25'`
    is parsed to the year 2025\. But if you pass `yearfirst=True` into `parse()`,
    it will be parsed to the year 2010.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 还可能发生与两位数年份相关的排序问题。例如，`'10-9-25'` 是模糊的。由于 `dateutil` 默认为 *Month-Day-Year* 格式，`'10-9-25'`
    被解析为 2025 年。但如果你在 `parse()` 中传递 `yearfirst=True`，它将被解析为 2010 年。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There's more...
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `dateutil` parser can also do **fuzzy parsing**, which allows it to ignore
    extraneous characters in a `datetime` string. With the default value of `False`,
    `parse()` will raise a `ValueError` when it encounters unknown tokens. But if
    `fuzzy=True`, then a `datetime` object can usually be returned.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`dateutil` 解析器还可以进行 **模糊解析**，这允许它忽略 `datetime` 字符串中的无关字符。默认值为 `False` 时，`parse()`
    遇到未知标记时会引发 `ValueError`。但如果 `fuzzy=True`，则通常可以返回 `datetime` 对象。'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we'll use the `tz` module from `dateutil` to do time zone
    lookup and conversion.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将使用 `dateutil` 的 `tz` 模块来进行时区查找和转换。
- en: Time zone lookup and conversion
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时区查找和转换
- en: Most `datetime` objects returned from the `dateutil` parser are *naive*, meaning
    they don't have an explicit `tzinfo`, which specifies the time zone and UTC offset.
    In the previous recipe, only one of the examples had a `tzinfo`, and that's because
    it's in the standard ISO format for UTC date and time strings. **UTC** is the
    coordinated universal time, and is the same as GMT. **ISO** is the **International
    Standards Organization**, which among other things, specifies standard date and
    time formatting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `dateutil` 解析器返回的大多数 `datetime` 对象是 *naive*，这意味着它们没有显式的 `tzinfo`，它指定了时区和 UTC
    偏移量。在先前的配方中，只有一个示例有 `tzinfo`，这是因为它是 UTC 日期和时间字符串的标准 ISO 格式。**UTC** 是协调世界时，它与 GMT
    相同。**ISO** 是 **国际标准化组织**，它规定了标准日期和时间格式。
- en: Python `datetime` objects can either be *naive* or *aware*. If a `datetime`
    object has a `tzinfo`, then it is aware. Otherwise the `datetime` is naive. To
    make a naive `datetime` object time zone aware, you must give it an explicit `tzinfo`.
    However, the Python `datetime` library only defines an abstract base class for
    `tzinfo`, and leaves it up to the others to actually implement `tzinfo` creation.
    This is where the `tz` module of `dateutil` comes in—it provides everything you
    need to lookup time zones from your OS time zone data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的 `datetime` 对象可以是 *naive* 或 *aware*。如果一个 `datetime` 对象有 `tzinfo`，则它是
    aware 的。否则，`datetime` 是 naive 的。要使 naive `datetime` 对象时区感知，你必须给它一个显式的 `tzinfo`。然而，Python
    的 `datetime` 库只定义了一个 `tzinfo` 的抽象基类，并将其留给其他人来实现 `tzinfo` 的创建。这就是 `dateutil` 的
    `tz` 模块发挥作用的地方——它提供了从你的操作系统时区数据中查找时区所需的一切。
- en: Getting ready
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '`dateutil` should be installed using `pip` or `easy_install`. You should also
    make sure your operating system has time zone data. On Linux, this is usually
    found in `/usr/share/zoneinfo`, and the Ubuntu package is called `tzdata`. If
    you have a number of files and directories in `/usr/share/zoneinfo`, such as `America/`,
    `Europe/`, and so on, then you should be ready to proceed. The following examples
    show directory paths for Ubuntu Linux.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 应使用 `pip` 或 `easy_install` 安装 `dateutil`。你还应该确保你的操作系统有时区数据。在 Linux 上，这通常位于 `/usr/share/zoneinfo`，Ubuntu
    软件包称为 `tzdata`。如果你在 `/usr/share/zoneinfo` 中有多个文件和目录，例如 `America/`、`Europe/` 等，那么你应该准备好继续。以下示例显示了
    Ubuntu Linux 的目录路径。
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Let's start by getting a UTC `tzinfo` object. This can be done by calling `tz.tzutc()`,
    and you can check that the offset is **0** by calling the `utcoffset()` method
    with a UTC `datetime` object.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从获取一个 UTC `tzinfo` 对象开始。这可以通过调用 `tz.tzutc()` 来完成，你可以通过调用带有 UTC `datetime`
    对象的 `utcoffset()` 方法来检查偏移量是否为 **0**。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To get `tzinfo` objects for other time zones, you can pass in a time zone file
    path to the `gettz()` function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取其他时区的 `tzinfo` 对象，你可以将时区文件路径传递给 `gettz()` 函数。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see the UTC offsets are `timedelta` objects, where the first number
    is *days*, and the second number is *seconds*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到 UTC 偏移是 `timedelta` 对象，其中第一个数字是**天**，第二个数字是**秒**。
- en: Tip
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you're storing `datetimes` in a database, it's a good idea to store them
    all in UTC to eliminate any time zone ambiguity. Even if the database can recognize
    time zones, it's still a good practice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 `datetimes` 存储在数据库中，将它们全部存储在 UTC 中以消除任何时区歧义是个好主意。即使数据库可以识别时区，这也是一个好习惯。
- en: To convert a non-UTC `datetime` object to UTC, it must be made time zone aware.
    If you try to convert a naive `datetime` to UTC, you'll get a `ValueError` exception.
    To make a naive `datetime` time zone aware, you simply call the `replace()` method
    with the correct `tzinfo`. Once a `datetime` object has a `tzinfo`, then UTC conversion
    can be performed by calling the `astimezone()` method with `tz.tzutc()`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要将非 UTC 的 `datetime` 对象转换为 UTC，它必须成为时区感知的。如果你尝试将无知的 `datetime` 转换为 UTC，你会得到一个
    `ValueError` 异常。要使无知的 `datetime` 时区感知，你只需使用正确的 `tzinfo` 调用 `replace()` 方法。一旦 `datetime`
    对象有了 `tzinfo`，就可以通过调用 `astimezone()` 方法并传递 `tz.tzutc()` 来执行 UTC 转换。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `tzutc` and `tzfile` objects are both subclasses of `tzinfo`. As such, they
    know the correct UTC offset for time zone conversion (which is 0 for `tzutc`).
    A `tzfile` object knows how to read your operating system's `zoneinfo` files to
    get the necessary offset data. The `replace()` method of a `datetime` object does
    what its name implies—it replaces attributes. Once a `datetime` has a `tzinfo`,
    the `astimezone()` method will be able to convert the time using the UTC offsets,
    and then replace the current `tzinfo` with the new `tzinfo`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`tzutc` 和 `tzfile` 对象都是 `tzinfo` 的子类。因此，它们知道时区转换的正确 UTC 偏移（对于 `tzutc` 是 0）。`tzfile`
    对象知道如何读取操作系统的 `zoneinfo` 文件以获取必要的偏移数据。`datetime` 对象的 `replace()` 方法做的是它的名字所暗示的——替换属性。一旦
    `datetime` 有了一个 `tzinfo`，`astimezone()` 方法将能够使用 UTC 偏移转换时间，然后使用新的 `tzinfo` 替换当前的
    `tzinfo`。'
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that both `replace()` and `astimezone()` return **new** `datetime` objects.
    They do not modify the current object.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`replace()` 和 `astimezone()` 都返回**新**的 `datetime` 对象。它们不会修改当前对象。
- en: There's more...
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: You can pass a `tzinfos` keyword argument into the `dateutil` parser to detect
    otherwise unrecognized time zones.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 `tzinfos` 关键字参数传递给 `dateutil` 解析器以检测其他未被识别的时间区域。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the first instance, we get a naive `datetime` since the time zone is not
    recognized. However, when we pass in the `tzinfos` mapping, we get a time zone
    aware `datetime`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们得到一个无知的 `datetime`，因为时区没有被识别。然而，当我们传递 `tzinfos` 映射时，我们得到一个时区感知的 `datetime`。
- en: Local time zone
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地时区
- en: If you want to lookup your local time zone, you can call `tz.tzlocal()`, which
    will use whatever your operating system thinks is the local time zone. In Ubuntu
    Linux, this is usually specified in the `/etc/timezone` file.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要查找你的本地时区，你可以调用 `tz.tzlocal()`，这将使用操作系统认为的本地时区。在 Ubuntu Linux 中，这通常在 `/etc/timezone`
    文件中指定。
- en: Custom offsets
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义偏移
- en: 'You can create your own `tzinfo` object with a custom UTC offset using the
    `tzoffset` object. A custom offset of one hour can be created as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `tzoffset` 对象创建具有自定义 UTC 偏移的 `tzinfo` 对象。可以创建一个一小时的自定义偏移，如下所示：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You must provide a name as the first argument, and the offset time in seconds
    as the second argument.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须提供名称作为第一个参数，以及以秒为单位的偏移时间作为第二个参数。
- en: See also
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous recipe covers parsing `datetime` strings with `dateutil.parser`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方涵盖了使用 `dateutil.parser` 解析 `datetime` 字符串。
- en: Tagging temporal expressions with Timex
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Timex 标记时间表达式
- en: The NLTK project has a little known `contrib` repository that contains, among
    other things, a module called `timex.py` that can tag temporal expressions. A
    **temporal expression** is just one or more time words, such as "this week", or
    "next month". These are ambiguous expressions that are relative to some other
    point in time, like when the text was written. The `timex` module provides a way
    to annotate text so these expressions can be extracted for further analysis. More
    on TIMEX can be found at [http://timex2.mitre.org/](http://timex2.mitre.org/).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 项目有一个鲜为人知的 `contrib` 仓库，其中包含许多其他模块，包括一个名为 `timex.py` 的模块，它可以标记时间表达式。**时间表达式**只是一些时间词，如“本周”或“下个月”。这些是相对于某个其他时间点的模糊表达式，比如文本编写的时间。`timex`
    模块提供了一种注释文本的方法，以便可以从文本中提取这些表达式进行进一步分析。更多关于 TIMEX 的信息可以在 [http://timex2.mitre.org/](http://timex2.mitre.org/)
    找到。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The `timex.py` module is part of the `nltk_contrib` package, which is separate
    from the current version of NLTK. This means you need to install it yourself,
    or use the `timex.py` module that is included with the book's code download. You
    can also download `timex.py` directly from [http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py](http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`timex.py` 模块是 `nltk_contrib` 包的一部分，它独立于 NLTK 的当前版本。这意味着你需要自己安装它，或者使用书中代码下载中包含的
    `timex.py` 模块。你也可以直接从 [http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py](http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py)
    下载 `timex.py`。'
- en: If you want to install the entire `nltk_contrib` package, you can check out
    the source at [http://nltk.googlecode.com/svn/trunk/](http://nltk.googlecode.com/svn/trunk/)
    and do `sudo python setup.py install` from within the `nltk_contrib` folder. If
    you do this, you'll need to do `from nltk_contrib import timex` instead of just
    `import timex` as done in the following *How to do it...* section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要安装整个 `nltk_contrib` 包，你可以从 [http://nltk.googlecode.com/svn/trunk/](http://nltk.googlecode.com/svn/trunk/)
    检出源代码，并在 `nltk_contrib` 文件夹中执行 `sudo python setup.py install`。如果你这样做，你需要执行 `from
    nltk_contrib import timex` 而不是在下面的 *如何操作* 部分中直接执行 `import timex`。
- en: For this recipe, you have to download the `timex.py` module into the same folder
    as the rest of the code, so that `import timex` does not cause an `ImportError`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，你必须将 `timex.py` 模块下载到与代码其余部分相同的文件夹中，这样 `import timex` 就不会引发 `ImportError`。
- en: You'll also need to get the `egenix-mx-base` package installed. This is a C
    extension library for Python, so if you have all the correct Python development
    headers installed, you should be able to do `sudo pip install egenix-mx-base`
    or `sudo easy_install egenix-mx-base`. If you're running Ubuntu Linux, you can
    instead do `sudo apt-get install python-egenix-mxdatetime`. If none of those work,
    you can go to [http://www.egenix.com/products/python/mxBase/](http://www.egenix.com/products/python/mxBase/)
    to download the package and find installation instructions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装 `egenix-mx-base` 包。这是一个用于 Python 的 C 扩展库，所以如果你已经安装了所有正确的 Python 开发头文件，你应该能够执行
    `sudo pip install egenix-mx-base` 或 `sudo easy_install egenix-mx-base`。如果你正在运行
    Ubuntu Linux，你可以改为执行 `sudo apt-get install python-egenix-mxdatetime`。如果这些都不起作用，你可以访问
    [http://www.egenix.com/products/python/mxBase/](http://www.egenix.com/products/python/mxBase/)
    下载该包并找到安装说明。
- en: How to do it...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Using `timex` is very simple: pass a string into the `timex.tag()` function
    and get back an annotated string. The annotations will be XML `TIMEX` tags surrounding
    each temporal expression.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `timex` 非常简单：将一个字符串传递给 `timex.tag()` 函数，并返回一个带有注释的字符串。这些注释将是围绕每个时间表达式的 XML
    `TIMEX` 标签。
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The implementation of `timex.py` is essentially over 300 lines of conditional
    regular expression matches. When one of the known expressions match, it creates
    a `RelativeDateTime` object (from the `mx.DateTime` module). This `RelativeDateTime`
    is then converted back to a string with surrounding `TIMEX` tags and replaces
    the original matched string in the text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`timex.py` 的实现基本上是超过 300 行的条件正则表达式匹配。当其中一个已知表达式匹配时，它创建一个 `RelativeDateTime`
    对象（来自 `mx.DateTime` 模块）。然后，这个 `RelativeDateTime` 被转换回一个带有周围 `TIMEX` 标签的字符串，并替换文本中的原始匹配字符串。'
- en: There's more...
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`timex` is smart enough not to tag expressions that have already been tagged,
    so it''s ok to pass `TIMEX` tagged text into the `tag()` function.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`timex` 非常智能，不会对已经标记的表达式再次进行标记，因此可以将标记过的 `TIMEX` 文本传递给 `tag()` 函数。'
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: See also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: In the next recipe, we'll be extracting URLs from HTML, but the same modules
    and techniques can be used to extract the `TIMEX` tagged expressions for further
    processing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将从HTML中提取URL，但可以使用相同的模块和技术来提取用于进一步处理的`TIMEX`标记表达式。
- en: Extracting URLs from HTML with lxml
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用lxml从HTML中提取URL
- en: A common task when parsing HTML is extracting links. This is one of the core
    functions of every general web crawler. There are a number of Python libraries
    for parsing HTML, and `lxml` is one of the best. As you'll see, it comes with
    some great helper functions geared specifically towards link extraction.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解析HTML时的一项常见任务是提取链接。这是每个通用网络爬虫的核心功能之一。有多个Python库用于解析HTML，`lxml`是其中之一。正如你将看到的，它包含一些专门针对链接提取的出色辅助函数。
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '`lxml` is a Python binding for the C libraries `libxml2` and `libxslt`. This
    makes it a very fast XML and HTML parsing library, while still being *pythonic*.
    However, that also means you need to install the C libraries for it to work. Installation
    instructions are at [http://codespe](http://codespe) [ak.net/lxml/installation.html](http://ak.net/lxml/installation.html).
    However, if you''re running Ubuntu Linux, installation is as easy as `sudo apt-get
    install python-lxml`.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml`是C库`libxml2`和`libxslt`的Python绑定。这使得它成为一个非常快速的XML和HTML解析库，同时仍然保持*pythonic*。然而，这也意味着你需要安装C库才能使其工作。安装说明请参阅[http://codespe](http://codespe)
    [ak.net/lxml/installation.html](http://ak.net/lxml/installation.html)。然而，如果你正在运行Ubuntu
    Linux，安装就像`sudo apt-get install python-lxml`一样简单。'
- en: How to do it...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: '`lxml` comes with an `html` module designed specifically for parsing HTML.
    Using the `fromstring()` function, we can parse an HTML string, then get a list
    of all the links. The `iterlinks()` method generates four-tuples of the form `(element,
    attr, link, pos)`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml`包含一个专门用于解析HTML的`html`模块。使用`fromstring()`函数，我们可以解析一个HTML字符串，然后获取所有链接的列表。`iterlinks()`方法生成形式为`(element,
    attr, link, pos)`的四元组：'
- en: '`element`: This is the parsed node of the anchor tag from which the `link`
    is extracted. If you''re just interested in the `link`, you can ignore this.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`element`：这是从锚标签中提取的`link`的解析节点。如果你只对`link`感兴趣，可以忽略这个。'
- en: '`attr`: This is the attribute the `link` came from, which is usually `href`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attr`：这是`link`的来源属性，通常是`href`。'
- en: '`link`: This is the actual URL extracted from the anchor tag.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`link`：这是从锚标签中提取的实际URL。'
- en: '`pos`: This is the numeric index of the anchor tag in the document. The first
    tag has a `pos` of `0`, the second has a `pos` of `1`, and so on.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos`：这是文档中锚标签的数字索引。第一个标签的`pos`为`0`，第二个标签的`pos`为`1`，依此类推。'
- en: 'Following is some code to demonstrate:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些演示代码：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`lxml` parses the HTML into an `ElementTree`. This is a tree structure of parent
    nodes and child nodes, where each node represents an HTML tag, and contains all
    the corresponding attributes of that tag. Once the tree is created, it can be
    iterated on to find elements, such as the `a` or **anchor tag**. The core tree
    handling code is in the `lxml.etree` module, while the `lxml.html` module contains
    only HTML-specific functions for creating and iterating a tree. For complete documentation,
    see the lxml tutorial: [http://codespeak.net/lxml/tutorial.html](http://codespeak.net/lxml/tutorial.html).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml`将HTML解析为`ElementTree`。这是一个由父节点和子节点组成的树结构，其中每个节点代表一个HTML标签，并包含该标签的所有相应属性。一旦创建了树，就可以迭代以查找元素，例如`a`或**锚标签**。核心树处理代码位于`lxml.etree`模块中，而`lxml.html`模块只包含创建和迭代树的HTML特定函数。有关完整文档，请参阅lxml教程：[http://codespeak.net/lxml/tutorial.html](http://codespeak.net/lxml/tutorial.html)。'
- en: There's more...
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: You'll notice in the previous code that the link is **relative**, meaning it's
    not an absolute URL. We can make it **absolute** by calling the `make_links_absolute()`
    method with a base URL before extracting the links.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在之前的代码中，链接是**相对的**，这意味着它不是一个绝对URL。在提取链接之前，我们可以通过调用带有基本URL的`make_links_absolute()`方法来将其转换为**绝对URL**。
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Extracting links directly
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接提取链接
- en: If you don't want to do anything other than extract links, you can call the
    `iterlinks()` function with an HTML string.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想提取链接而不做其他任何事情，你可以使用HTML字符串调用`iterlinks()`函数。
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parsing HTML from URLs or files
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从URL或文件解析HTML
- en: Instead of parsing an HTML string using the `fromstring()` function, you can
    call the `parse()` function with a URL or file name. For example, `html.parse("http://my/url")`
    or `html.parse("/path/to/file")`. The result will be the same as if you loaded
    the URL or file into a string yourself, then called `fromstring()`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `parse()` 函数而不是使用 `fromstring()` 函数来解析 HTML 字符串，通过传递一个 URL 或文件名。例如，`html.parse("http://my/url")`
    或 `html.parse("/path/to/file")`。结果将与你自己将 URL 或文件加载到字符串中然后调用 `fromstring()` 一样。
- en: Extracting links with XPaths
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 XPaths 提取链接
- en: Instead of using the `iterlinks()` method, you can also get links using the
    `xpath()` method, which is a general way to extract whatever you want from HTML
    or XML parse trees.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 `xpath()` 方法来获取链接，而不是使用 `iterlinks()` 方法，这是一个从 HTML 或 XML 解析树中提取任何所需内容的一般方法。
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For more on XPath syntax, see [http://www.w3schools.com/XPath/xpath_syntax.asp](http://www.w3schools.com/XPath/xpath_syntax.asp).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 XPath 语法，请参阅 [http://www.w3schools.com/XPath/xpath_syntax.asp](http://www.w3schools.com/XPath/xpath_syntax.asp)。
- en: See also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we'll cover cleaning and stripping HTML.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将介绍清理和剥离 HTML。
- en: Cleaning and stripping HTML
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理和剥离 HTML
- en: Cleaning up text is one of the unfortunate but entirely necessary aspects of
    text processing. When it comes to parsing HTML, you probably don't want to deal
    with any embedded JavaScript or CSS, and are only interested in the tags and text.
    Or you may want to remove the HTML entirely, and process only the text. This recipe
    covers how to do both of these pre-processing actions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 清理文本是文本处理中不幸但完全必要的方面之一。当涉及到解析 HTML 时，你可能不想处理任何嵌入的 JavaScript 或 CSS，你只对标签和文本感兴趣。或者你可能想完全移除
    HTML，只处理文本。这个配方涵盖了如何进行这两种预处理操作。
- en: Getting ready
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You'll need to install `lxml`. See the previous recipe or [http://codespeak.net/lxml/installation.html](http://codespeak.net/lxml/installation.html)
    for installation instructions. You'll also need NLTK installed for stripping HTML.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装 `lxml`。请参阅前面的配方或 [http://codespeak.net/lxml/installation.html](http://codespeak.net/lxml/installation.html)
    以获取安装说明。你还需要安装 NLTK 以剥离 HTML。
- en: How to do it...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We can use the `clean_html()` function in the `lxml.html.clean` module to remove
    unnecessary HTML tags and embedded JavaScript from an HTML string.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `lxml.html.clean` 模块中的 `clean_html()` 函数来从 HTML 字符串中移除不必要的 HTML 标签和嵌入的
    JavaScript。
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result is much cleaner and easier to deal with. The full module path to
    the `clean_html()` function is used because there's also has a `clean_html()`
    function in the `nltk.util` module, but its purpose is different. The `nltk.util.clean_html()`
    function removes all HTML tags when you just want the text.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果会更加干净，更容易处理。使用 `clean_html()` 函数的完整模块路径是因为 `nltk.util` 模块中也有一个 `clean_html()`
    函数，但它的用途不同。当你只想得到文本时，`nltk.util.clean_html()` 函数会移除所有 HTML 标签。
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `lxml.html.clean_html()` function parses the HTML string into a tree, then
    iterates over and removes all nodes that should be removed. It also cleans nodes
    of unnecessary attributes (such as embedded JavaScript) using regular expression
    matching and substitution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml.html.clean_html()` 函数将 HTML 字符串解析成树，然后迭代并移除所有应该被移除的节点。它还使用正则表达式匹配和替换来清理节点的非必要属性（例如嵌入的
    JavaScript）。'
- en: The `nltk.util.clean_html()` function performs a bunch of regular expression
    substitutions to remove HTML tags. To be safe, it's best to strip the HTML after
    cleaning it to ensure the regular expressions will match.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.util.clean_html()` 函数执行一系列正则表达式替换来移除 HTML 标签。为了安全起见，最好在清理后剥离 HTML，以确保正则表达式能够匹配。'
- en: There's more...
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `lxml.html.clean` module defines a default `Cleaner` class that's used when
    you call `clean_html()`. You can customize the behavior of this class by creating
    your own instance and calling its `clean_html()` method. For more details on this
    class, see [http://codespeak.net/lxml/lxmlhtml.html](http://codespeak.net/lxml/lxmlhtml.html).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml.html.clean` 模块定义了一个默认的 `Cleaner` 类，当你调用 `clean_html()` 时会使用它。你可以通过创建自己的实例并调用其
    `clean_html()` 方法来自定义这个类的行为。有关这个类的更多详细信息，请参阅 [http://codespeak.net/lxml/lxmlhtml.html](http://codespeak.net/lxml/lxmlhtml.html)。'
- en: See also
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The `lxml.html` module was introduced in the previous recipe for parsing HTML
    and extracting links. In the next recipe, we'll cover un-escaping HTML entities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中介绍了 `lxml.html` 模块，用于解析 HTML 和提取链接。在下一个配方中，我们将介绍取消转义 HTML 实体。
- en: Converting HTML entities with BeautifulSoup
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BeautifulSoup 转换 HTML 实体
- en: HTML entities are strings such as `&amp;` or `&lt;`. These are encodings of
    normal ASCII characters that have special uses in HTML. For example, `&lt;` is
    the entity for `<`. You can't just have `<` within HTML tags because it is the
    beginning character for an HTML tag, hence the need to escape it and define the
    `&lt;` entity. The entity code for & is `&amp`; which, as we've just seen, is
    the beginning character for an entity code. If you need to process the text within
    an HTML document, then you'll want to convert these entities back to their normal
    characters so you can recognize them and handle them appropriately.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install `BeautifulSoup`, which you should be able to do with
    `sudo pip install BeautifulSoup` or `sudo easy_install BeautifulSoup`. You can
    read more about `BeautifulSoup` at [http://www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BeautifulSoup` is an HTML parser library that also contains an XML parser
    called `BeautifulStoneSoup`. This is what we can use for entity conversion. It''s
    quite simple: create an instance of `BeautifulStoneSoup` given a string containing
    HTML entities and specify the keyword argument `convertEntities=''html''`. Convert
    this instance to a string, and you''ll get the ASCII representation of the HTML
    entities.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's ok to run the string through multiple times, as long as the ASCII characters
    are not by themselves. If your string is just a single ASCII character for an
    HTML entity, that character will be lost.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To make sure the character isn't lost, all that's required is to have another
    character in the string that is not part of an entity code.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To convert the HTML entities, `BeautifulStoneSoup` looks for tokens that look
    like an entity and replaces them with their corresponding value in the `htmlentitydefs.name2codepoint`
    dictionary from the Python standard library. It can do this if the entity token
    is within an HTML tag, or when it's in a normal string.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BeautifulSoup` is an excellent HTML and XML parser in its own right, and can
    be a great alternative to `lxml`. It''s particularly good at handling malformed
    HTML. You can read more about how to use it at [http://www.crummy.com/software/BeautifulSoup/documentation.html](http://www.crummy.com/software/BeautifulSoup/documentation.html).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Extracting URLs with BeautifulSoup
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here's an example of using `BeautifulSoup` to extract URLs, like we did in the
    *Extracting URLs from HTML with lxml* recipe. You first create the `soup` with
    an HTML string, call the `findAll()` method with `'a'` to get all anchor tags,
    and pull out the `'href'` attribute to get the URLs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: See also
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Extracting URLs from HTML with lxml* recipe, we covered how to use `lxml`
    to extract URLs from an HTML string, and we covered *Cleaning and stripping HTML*
    after that recipe.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and converting character encodings
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common occurrence with text processing is finding text that has a non-standard
    character encoding. Ideally, all text would be ASCII or UTF-8, but that's just
    not the reality. In cases when you have non-ASCII or non-UTF-8 text and you don't
    know what the character encoding is, you'll need to detect it and convert the
    text to a standard encoding before further processing it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本处理中，一个常见的情况是找到具有非标准字符编码的文本。理想情况下，所有文本都应该是ASCII或UTF-8，但这只是现实。在您有非ASCII或非UTF-8文本且不知道字符编码的情况下，您需要检测它并将文本转换为标准编码，然后再进一步处理。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You'll need to install the `chardet` module, using `sudo pip install chardet`
    or `sudo easy_install chardet`. You can learn more about `chardet` at [http://chardet.feedparser.org/](http://chardet.feedparser.org/).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装`chardet`模块，使用`sudo pip install chardet`或`sudo easy_install chardet`。您可以在[http://chardet.feedparser.org/](http://chardet.feedparser.org/)了解更多关于`chardet`的信息。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Encoding detection and conversion functions are provided in `encoding.py`.
    These are simple wrapper functions around the `chardet` module. To detect the
    encoding of a string, call `encoding.detect()`. You''ll get back a `dict` containing
    two attributes: `confidence` and `encoding`. `confidence` is a probability of
    how confident `chardet` is that the value for `encoding` is correct.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoding.py`中提供了编码检测和转换函数。这些是围绕`chardet`模块的简单包装函数。要检测字符串的编码，请调用`encoding.detect()`。您将得到一个包含两个属性的`dict`：`confidence`和`encoding`。`confidence`是`chardet`对`encoding`值正确性的置信度概率。'
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here''s some example code using `detect()` to determine character encoding:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用`detect()`来确定字符编码的示例代码：
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To convert a string to a standard `unicode` encoding, call `encoding.convert()`.
    This will decode the string from its original encoding, then re-encode it as UTF-8.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要将字符串转换为标准的`unicode`编码，请调用`encoding.convert()`。这将解码字符串的原始编码，然后将其重新编码为UTF-8。
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `detect()` function is a wrapper around `chardet.detect()` which can handle
    `UnicodeDecodeError` exceptions. In these cases, the string is encoded in UTF-8
    before trying to detect the encoding.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect()`函数是`chardet.detect()`的包装器，可以处理`UnicodeDecodeError`异常。在这些情况下，在尝试检测编码之前，字符串被编码为UTF-8。'
- en: The `convert()` function first calls `detect()` to get the `encoding`, then
    returns a `unicode` string with the `encoding` as the second argument. By passing
    the `encoding` into `unicode()`, the string is decoded from the original encoding,
    allowing it to be re-encoded into a standard encoding.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`convert()`函数首先调用`detect()`以获取`encoding`，然后返回一个带有`encoding`作为第二个参数的`unicode`字符串。通过将`encoding`传递给`unicode()`，字符串从原始编码解码，允许它被重新编码为标准编码。'
- en: There's more...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'The comment at the top of the module, `# -*- coding: utf-8 -*-`, is a hint
    to the Python interpreter, telling it which encoding to use for the strings in
    the code. This is helpful for when you have non-ASCII strings in your source code,
    and is documented in detail at [http://www.python.org/dev/peps/pep-0263/](http://www.python.org/dev/peps/pep-0263/).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '模块顶部的注释`# -*- coding: utf-8 -*-`是给Python解释器的提示，告诉它代码中字符串应使用哪种编码。这对于您源代码中有非ASCII字符串时很有帮助，并在[http://www.python.org/dev/peps/pep-0263/](http://www.python.org/dev/peps/pep-0263/)中详细记录。'
- en: Converting to ASCII
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换为ASCII
- en: If you want pure ASCII text, with non-ASCII characters converted to ASCII equivalents,
    or dropped if there is no equivalent character, then you can use the `unicodedata.normalize()`
    function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要纯ASCII文本，将非ASCII字符转换为ASCII等效字符，或者在没有等效字符的情况下删除，那么您可以使用`unicodedata.normalize()`函数。
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Specifying `'NFKD'` as the first argument ensures the non-ASCII characters are
    replaced with their equivalent ASCII versions, and the final call to `encode()`
    with `'ignore'` as the second argument will remove any extraneous unicode characters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个参数指定为`'NFKD'`确保非ASCII字符被替换为其等效的ASCII版本，并且最终调用`encode()`时使用`'ignore'`作为第二个参数将移除任何多余的Unicode字符。
- en: See also
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Encoding detection and conversion is a recommended first step before doing HTML
    processing with `lxml` or `BeautifulSoup`, covered in the *Extracting URLs from
    HTML with lxml* and *Converting HTML entities with BeautifulSoup* recipes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`lxml`或`BeautifulSoup`进行HTML处理之前，编码检测和转换是推荐的第一步，这包括在*使用lxml从HTML中提取URL*和*使用BeautifulSoup转换HTML实体*的食谱中。
