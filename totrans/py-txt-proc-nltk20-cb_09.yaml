- en: Chapter 9. Parsing Specific Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing dates and times with Dateutil
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time zone lookup and conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging temporal expressions with Timex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting URLs from HTML with lxml
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and stripping HTML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting HTML entities with BeautifulSoup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting and converting character encodings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers parsing specific kinds of data, focusing primarily on dates,
    times, and HTML. Luckily, there are a number of useful libraries for accomplishing
    this, so we don''t have to delve into tricky and overly complicated regular expressions.
    These libraries can be great complements to the NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dateutil`: Provides date/time parsing and time zone conversion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timex`: Can identify time words in text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxml` and `BeautifulSoup`: Can parse, clean, and convert HTML'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chardet`: Detects the character encoding of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The libraries can be useful for pre-processing text before passing it to an
    NLTK object, or post-processing text that has been processed and extracted using
    NLTK. Here's an example that ties many of these tools together.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say you need to parse a blog article about a restaurant. You can use `lxml`
    or `BeautifulSoup` to extract the article text, outbound links, and the date and
    time when the article was written. The date and time can then be parsed to a Python
    `datetime` object with `dateutil`. Once you have the article text, you can use
    `chardet` to ensure it's UTF-8 before cleaning out the HTML and running it through
    NLTK-based part-of-speech tagging, chunk extraction, and/or text classification,
    to create additional metadata about the article. If there's an event happening
    at the restaurant, you may be able to discover that by looking at the time words
    identified by `timex`. The point of this example is that real-world text processing
    often requires more than just NLTK-based natural language processing, and the
    functionality covered in this chapter can help with those additional requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing dates and times with Dateutil
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you need to parse dates and times in Python, there is no better library than
    `dateutil`. The `parser` module can parse `datetime` strings in many more formats
    than can be shown here, while the `tz` module provides everything you need for
    looking up time zones. Combined, these modules make it quite easy to parse strings
    into time zone aware `datetime` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can install `dateutil` using `pip` or `easy_install`, that is `sudo pip
    install dateutil` or `sudo easy_install dateutil`. Complete documentation can
    be found at [http://labix.org/python-dateutil](http://labix.org/python-dateutil).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s dive into a few parsing examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all it takes is importing the `parser` module and calling the
    `parse()` function with a `datetime` string. The parser will do its best to return
    a sensible `datetime` object, but if it cannot parse the string, it will raise
    a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The parser does not use regular expressions. Instead, it looks for recognizable
    tokens and does its best to guess what those tokens refer to. The order of these
    tokens matters, for example, some cultures use a date format that looks like *Month/Day/Year*
    (the default order) while others use a *Day/Month/Year* format. To deal with this,
    the `parse()` function takes an optional keyword argument `dayfirst`, which defaults
    to `False`. If you set it to `True`, it can correctly parse dates in the latter
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Another ordering issue can occur with two-digit years. For example, `'10-9-25'`
    is ambiguous. Since `dateutil` defaults to the *Month-Day-Year* format, `'10-9-25'`
    is parsed to the year 2025\. But if you pass `yearfirst=True` into `parse()`,
    it will be parsed to the year 2010.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `dateutil` parser can also do **fuzzy parsing**, which allows it to ignore
    extraneous characters in a `datetime` string. With the default value of `False`,
    `parse()` will raise a `ValueError` when it encounters unknown tokens. But if
    `fuzzy=True`, then a `datetime` object can usually be returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll use the `tz` module from `dateutil` to do time zone
    lookup and conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Time zone lookup and conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most `datetime` objects returned from the `dateutil` parser are *naive*, meaning
    they don't have an explicit `tzinfo`, which specifies the time zone and UTC offset.
    In the previous recipe, only one of the examples had a `tzinfo`, and that's because
    it's in the standard ISO format for UTC date and time strings. **UTC** is the
    coordinated universal time, and is the same as GMT. **ISO** is the **International
    Standards Organization**, which among other things, specifies standard date and
    time formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Python `datetime` objects can either be *naive* or *aware*. If a `datetime`
    object has a `tzinfo`, then it is aware. Otherwise the `datetime` is naive. To
    make a naive `datetime` object time zone aware, you must give it an explicit `tzinfo`.
    However, the Python `datetime` library only defines an abstract base class for
    `tzinfo`, and leaves it up to the others to actually implement `tzinfo` creation.
    This is where the `tz` module of `dateutil` comes in—it provides everything you
    need to lookup time zones from your OS time zone data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`dateutil` should be installed using `pip` or `easy_install`. You should also
    make sure your operating system has time zone data. On Linux, this is usually
    found in `/usr/share/zoneinfo`, and the Ubuntu package is called `tzdata`. If
    you have a number of files and directories in `/usr/share/zoneinfo`, such as `America/`,
    `Europe/`, and so on, then you should be ready to proceed. The following examples
    show directory paths for Ubuntu Linux.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by getting a UTC `tzinfo` object. This can be done by calling `tz.tzutc()`,
    and you can check that the offset is **0** by calling the `utcoffset()` method
    with a UTC `datetime` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To get `tzinfo` objects for other time zones, you can pass in a time zone file
    path to the `gettz()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can see the UTC offsets are `timedelta` objects, where the first number
    is *days*, and the second number is *seconds*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're storing `datetimes` in a database, it's a good idea to store them
    all in UTC to eliminate any time zone ambiguity. Even if the database can recognize
    time zones, it's still a good practice.
  prefs: []
  type: TYPE_NORMAL
- en: To convert a non-UTC `datetime` object to UTC, it must be made time zone aware.
    If you try to convert a naive `datetime` to UTC, you'll get a `ValueError` exception.
    To make a naive `datetime` time zone aware, you simply call the `replace()` method
    with the correct `tzinfo`. Once a `datetime` object has a `tzinfo`, then UTC conversion
    can be performed by calling the `astimezone()` method with `tz.tzutc()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `tzutc` and `tzfile` objects are both subclasses of `tzinfo`. As such, they
    know the correct UTC offset for time zone conversion (which is 0 for `tzutc`).
    A `tzfile` object knows how to read your operating system's `zoneinfo` files to
    get the necessary offset data. The `replace()` method of a `datetime` object does
    what its name implies—it replaces attributes. Once a `datetime` has a `tzinfo`,
    the `astimezone()` method will be able to convert the time using the UTC offsets,
    and then replace the current `tzinfo` with the new `tzinfo`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that both `replace()` and `astimezone()` return **new** `datetime` objects.
    They do not modify the current object.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can pass a `tzinfos` keyword argument into the `dateutil` parser to detect
    otherwise unrecognized time zones.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the first instance, we get a naive `datetime` since the time zone is not
    recognized. However, when we pass in the `tzinfos` mapping, we get a time zone
    aware `datetime`.
  prefs: []
  type: TYPE_NORMAL
- en: Local time zone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to lookup your local time zone, you can call `tz.tzlocal()`, which
    will use whatever your operating system thinks is the local time zone. In Ubuntu
    Linux, this is usually specified in the `/etc/timezone` file.
  prefs: []
  type: TYPE_NORMAL
- en: Custom offsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can create your own `tzinfo` object with a custom UTC offset using the
    `tzoffset` object. A custom offset of one hour can be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You must provide a name as the first argument, and the offset time in seconds
    as the second argument.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covers parsing `datetime` strings with `dateutil.parser`.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging temporal expressions with Timex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NLTK project has a little known `contrib` repository that contains, among
    other things, a module called `timex.py` that can tag temporal expressions. A
    **temporal expression** is just one or more time words, such as "this week", or
    "next month". These are ambiguous expressions that are relative to some other
    point in time, like when the text was written. The `timex` module provides a way
    to annotate text so these expressions can be extracted for further analysis. More
    on TIMEX can be found at [http://timex2.mitre.org/](http://timex2.mitre.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `timex.py` module is part of the `nltk_contrib` package, which is separate
    from the current version of NLTK. This means you need to install it yourself,
    or use the `timex.py` module that is included with the book's code download. You
    can also download `timex.py` directly from [http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py](http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to install the entire `nltk_contrib` package, you can check out
    the source at [http://nltk.googlecode.com/svn/trunk/](http://nltk.googlecode.com/svn/trunk/)
    and do `sudo python setup.py install` from within the `nltk_contrib` folder. If
    you do this, you'll need to do `from nltk_contrib import timex` instead of just
    `import timex` as done in the following *How to do it...* section.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, you have to download the `timex.py` module into the same folder
    as the rest of the code, so that `import timex` does not cause an `ImportError`.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also need to get the `egenix-mx-base` package installed. This is a C
    extension library for Python, so if you have all the correct Python development
    headers installed, you should be able to do `sudo pip install egenix-mx-base`
    or `sudo easy_install egenix-mx-base`. If you're running Ubuntu Linux, you can
    instead do `sudo apt-get install python-egenix-mxdatetime`. If none of those work,
    you can go to [http://www.egenix.com/products/python/mxBase/](http://www.egenix.com/products/python/mxBase/)
    to download the package and find installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using `timex` is very simple: pass a string into the `timex.tag()` function
    and get back an annotated string. The annotations will be XML `TIMEX` tags surrounding
    each temporal expression.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation of `timex.py` is essentially over 300 lines of conditional
    regular expression matches. When one of the known expressions match, it creates
    a `RelativeDateTime` object (from the `mx.DateTime` module). This `RelativeDateTime`
    is then converted back to a string with surrounding `TIMEX` tags and replaces
    the original matched string in the text.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`timex` is smart enough not to tag expressions that have already been tagged,
    so it''s ok to pass `TIMEX` tagged text into the `tag()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll be extracting URLs from HTML, but the same modules
    and techniques can be used to extract the `TIMEX` tagged expressions for further
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting URLs from HTML with lxml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common task when parsing HTML is extracting links. This is one of the core
    functions of every general web crawler. There are a number of Python libraries
    for parsing HTML, and `lxml` is one of the best. As you'll see, it comes with
    some great helper functions geared specifically towards link extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`lxml` is a Python binding for the C libraries `libxml2` and `libxslt`. This
    makes it a very fast XML and HTML parsing library, while still being *pythonic*.
    However, that also means you need to install the C libraries for it to work. Installation
    instructions are at [http://codespe](http://codespe) [ak.net/lxml/installation.html](http://ak.net/lxml/installation.html).
    However, if you''re running Ubuntu Linux, installation is as easy as `sudo apt-get
    install python-lxml`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`lxml` comes with an `html` module designed specifically for parsing HTML.
    Using the `fromstring()` function, we can parse an HTML string, then get a list
    of all the links. The `iterlinks()` method generates four-tuples of the form `(element,
    attr, link, pos)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`element`: This is the parsed node of the anchor tag from which the `link`
    is extracted. If you''re just interested in the `link`, you can ignore this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attr`: This is the attribute the `link` came from, which is usually `href`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`link`: This is the actual URL extracted from the anchor tag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pos`: This is the numeric index of the anchor tag in the document. The first
    tag has a `pos` of `0`, the second has a `pos` of `1`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following is some code to demonstrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`lxml` parses the HTML into an `ElementTree`. This is a tree structure of parent
    nodes and child nodes, where each node represents an HTML tag, and contains all
    the corresponding attributes of that tag. Once the tree is created, it can be
    iterated on to find elements, such as the `a` or **anchor tag**. The core tree
    handling code is in the `lxml.etree` module, while the `lxml.html` module contains
    only HTML-specific functions for creating and iterating a tree. For complete documentation,
    see the lxml tutorial: [http://codespeak.net/lxml/tutorial.html](http://codespeak.net/lxml/tutorial.html).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll notice in the previous code that the link is **relative**, meaning it's
    not an absolute URL. We can make it **absolute** by calling the `make_links_absolute()`
    method with a base URL before extracting the links.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Extracting links directly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you don't want to do anything other than extract links, you can call the
    `iterlinks()` function with an HTML string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parsing HTML from URLs or files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of parsing an HTML string using the `fromstring()` function, you can
    call the `parse()` function with a URL or file name. For example, `html.parse("http://my/url")`
    or `html.parse("/path/to/file")`. The result will be the same as if you loaded
    the URL or file into a string yourself, then called `fromstring()`.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting links with XPaths
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using the `iterlinks()` method, you can also get links using the
    `xpath()` method, which is a general way to extract whatever you want from HTML
    or XML parse trees.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For more on XPath syntax, see [http://www.w3schools.com/XPath/xpath_syntax.asp](http://www.w3schools.com/XPath/xpath_syntax.asp).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll cover cleaning and stripping HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and stripping HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cleaning up text is one of the unfortunate but entirely necessary aspects of
    text processing. When it comes to parsing HTML, you probably don't want to deal
    with any embedded JavaScript or CSS, and are only interested in the tags and text.
    Or you may want to remove the HTML entirely, and process only the text. This recipe
    covers how to do both of these pre-processing actions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install `lxml`. See the previous recipe or [http://codespeak.net/lxml/installation.html](http://codespeak.net/lxml/installation.html)
    for installation instructions. You'll also need NLTK installed for stripping HTML.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the `clean_html()` function in the `lxml.html.clean` module to remove
    unnecessary HTML tags and embedded JavaScript from an HTML string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The result is much cleaner and easier to deal with. The full module path to
    the `clean_html()` function is used because there's also has a `clean_html()`
    function in the `nltk.util` module, but its purpose is different. The `nltk.util.clean_html()`
    function removes all HTML tags when you just want the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lxml.html.clean_html()` function parses the HTML string into a tree, then
    iterates over and removes all nodes that should be removed. It also cleans nodes
    of unnecessary attributes (such as embedded JavaScript) using regular expression
    matching and substitution.
  prefs: []
  type: TYPE_NORMAL
- en: The `nltk.util.clean_html()` function performs a bunch of regular expression
    substitutions to remove HTML tags. To be safe, it's best to strip the HTML after
    cleaning it to ensure the regular expressions will match.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lxml.html.clean` module defines a default `Cleaner` class that's used when
    you call `clean_html()`. You can customize the behavior of this class by creating
    your own instance and calling its `clean_html()` method. For more details on this
    class, see [http://codespeak.net/lxml/lxmlhtml.html](http://codespeak.net/lxml/lxmlhtml.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lxml.html` module was introduced in the previous recipe for parsing HTML
    and extracting links. In the next recipe, we'll cover un-escaping HTML entities.
  prefs: []
  type: TYPE_NORMAL
- en: Converting HTML entities with BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HTML entities are strings such as `&amp;` or `&lt;`. These are encodings of
    normal ASCII characters that have special uses in HTML. For example, `&lt;` is
    the entity for `<`. You can't just have `<` within HTML tags because it is the
    beginning character for an HTML tag, hence the need to escape it and define the
    `&lt;` entity. The entity code for & is `&amp`; which, as we've just seen, is
    the beginning character for an entity code. If you need to process the text within
    an HTML document, then you'll want to convert these entities back to their normal
    characters so you can recognize them and handle them appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install `BeautifulSoup`, which you should be able to do with
    `sudo pip install BeautifulSoup` or `sudo easy_install BeautifulSoup`. You can
    read more about `BeautifulSoup` at [http://www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BeautifulSoup` is an HTML parser library that also contains an XML parser
    called `BeautifulStoneSoup`. This is what we can use for entity conversion. It''s
    quite simple: create an instance of `BeautifulStoneSoup` given a string containing
    HTML entities and specify the keyword argument `convertEntities=''html''`. Convert
    this instance to a string, and you''ll get the ASCII representation of the HTML
    entities.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's ok to run the string through multiple times, as long as the ASCII characters
    are not by themselves. If your string is just a single ASCII character for an
    HTML entity, that character will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To make sure the character isn't lost, all that's required is to have another
    character in the string that is not part of an entity code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To convert the HTML entities, `BeautifulStoneSoup` looks for tokens that look
    like an entity and replaces them with their corresponding value in the `htmlentitydefs.name2codepoint`
    dictionary from the Python standard library. It can do this if the entity token
    is within an HTML tag, or when it's in a normal string.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BeautifulSoup` is an excellent HTML and XML parser in its own right, and can
    be a great alternative to `lxml`. It''s particularly good at handling malformed
    HTML. You can read more about how to use it at [http://www.crummy.com/software/BeautifulSoup/documentation.html](http://www.crummy.com/software/BeautifulSoup/documentation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting URLs with BeautifulSoup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here's an example of using `BeautifulSoup` to extract URLs, like we did in the
    *Extracting URLs from HTML with lxml* recipe. You first create the `soup` with
    an HTML string, call the `findAll()` method with `'a'` to get all anchor tags,
    and pull out the `'href'` attribute to get the URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Extracting URLs from HTML with lxml* recipe, we covered how to use `lxml`
    to extract URLs from an HTML string, and we covered *Cleaning and stripping HTML*
    after that recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and converting character encodings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common occurrence with text processing is finding text that has a non-standard
    character encoding. Ideally, all text would be ASCII or UTF-8, but that's just
    not the reality. In cases when you have non-ASCII or non-UTF-8 text and you don't
    know what the character encoding is, you'll need to detect it and convert the
    text to a standard encoding before further processing it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install the `chardet` module, using `sudo pip install chardet`
    or `sudo easy_install chardet`. You can learn more about `chardet` at [http://chardet.feedparser.org/](http://chardet.feedparser.org/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Encoding detection and conversion functions are provided in `encoding.py`.
    These are simple wrapper functions around the `chardet` module. To detect the
    encoding of a string, call `encoding.detect()`. You''ll get back a `dict` containing
    two attributes: `confidence` and `encoding`. `confidence` is a probability of
    how confident `chardet` is that the value for `encoding` is correct.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s some example code using `detect()` to determine character encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To convert a string to a standard `unicode` encoding, call `encoding.convert()`.
    This will decode the string from its original encoding, then re-encode it as UTF-8.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `detect()` function is a wrapper around `chardet.detect()` which can handle
    `UnicodeDecodeError` exceptions. In these cases, the string is encoded in UTF-8
    before trying to detect the encoding.
  prefs: []
  type: TYPE_NORMAL
- en: The `convert()` function first calls `detect()` to get the `encoding`, then
    returns a `unicode` string with the `encoding` as the second argument. By passing
    the `encoding` into `unicode()`, the string is decoded from the original encoding,
    allowing it to be re-encoded into a standard encoding.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The comment at the top of the module, `# -*- coding: utf-8 -*-`, is a hint
    to the Python interpreter, telling it which encoding to use for the strings in
    the code. This is helpful for when you have non-ASCII strings in your source code,
    and is documented in detail at [http://www.python.org/dev/peps/pep-0263/](http://www.python.org/dev/peps/pep-0263/).'
  prefs: []
  type: TYPE_NORMAL
- en: Converting to ASCII
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want pure ASCII text, with non-ASCII characters converted to ASCII equivalents,
    or dropped if there is no equivalent character, then you can use the `unicodedata.normalize()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Specifying `'NFKD'` as the first argument ensures the non-ASCII characters are
    replaced with their equivalent ASCII versions, and the final call to `encode()`
    with `'ignore'` as the second argument will remove any extraneous unicode characters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoding detection and conversion is a recommended first step before doing HTML
    processing with `lxml` or `BeautifulSoup`, covered in the *Extracting URLs from
    HTML with lxml* and *Converting HTML entities with BeautifulSoup* recipes.
  prefs: []
  type: TYPE_NORMAL
