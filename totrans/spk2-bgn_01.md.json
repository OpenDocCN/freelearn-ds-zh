["```py\nval textFile = sc.textFile(\"README.md\") \nval wordCounts = textFile.flatMap(line => line.split(\" \")).map(word => \n (word, 1)).reduceByKey((a, b) => a + b) \nwordCounts.collect()\n\n```", "```py\n$ python \nPython 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)  \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin \nType \"help\", \"copyright\", \"credits\" or \"license\" for more information. \n>>> \n\n```", "```py\nexport LC_ALL=en_US.UTF-8 \nexport LANG=en_US.UTF-8\n\n```", "```py\n$ r \nR version 3.2.2 (2015-08-14) -- \"Fire Safety\" \nCopyright (C) 2015 The R Foundation for Statistical Computing \nPlatform: x86_64-apple-darwin13.4.0 (64-bit) \nR is free software and comes with ABSOLUTELY NO WARRANTY. \nYou are welcome to redistribute it under certain conditions. \nType 'license()' or 'licence()' for distribution details. \n  Natural language support but running in an English locale \nR is a collaborative project with many contributors. \nType 'contributors()' for more information and \n'citation()' on how to cite R or R packages in publications. \nType 'demo()' for some demos, 'help()' for on-line help, or \n'help.start()' for an HTML browser interface to help. \nType 'q()' to quit R. \n[Previously saved workspace restored] \n>\n\n```", "```py\n$ mvn -DskipTests -Psparkr clean package\n\n```", "```py\nexport SPARK_HOME=<the Spark installation directory> \nexport PATH=$SPARK_HOME/bin:$PATH\n\n```", "```py\nexport PYSPARK_PYTHON=/usr/bin/python\n\n```", "```py\n# Determine the Python executable to use if PYSPARK_PYTHON or PYSPARK_DRIVER_PYTHON isn't set: \nif hash python2.7 2>/dev/null; then \n  # Attempt to use Python 2.7, if installed: \n  DEFAULT_PYTHON=\"python2.7\" \nelse \n  DEFAULT_PYTHON=\"python\" \nfi\n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/spark-shellUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties \nSetting default log level to \"WARN\". \nTo adjust logging level use sc.setLogLevel(newLevel). \n16/06/28 20:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \n16/06/28 20:53:49 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect. \nSpark context Web UI available at http://192.168.1.6:4040 \nSpark context available as 'sc' (master = local[*], app id = local-1467143629623). \nSpark session available as 'spark'. \nWelcome to \n      ____              __ \n     / __/__  ___ _____/ /__ \n    _\\ \\/ _ \\/ _ `/ __/  '_/ \n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.0.1 \n      /_/ \n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) \nType in expressions to have them evaluated. \nType :help for more information. \nscala> \nscala>exit \n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/pyspark \nPython 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)  \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin \nType \"help\", \"copyright\", \"credits\" or \"license\" for more information. \nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties \nSetting default log level to \"WARN\". \nTo adjust logging level use sc.setLogLevel(newLevel). \n16/06/28 20:58:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \nWelcome to \n      ____              __ \n     / __/__  ___ _____/ /__ \n    _\\ \\/ _ \\/ _ `/ __/  '_/ \n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.1 \n      /_/ \n\nUsing Python version 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015 11:00:19) \nSparkSession available as 'spark'. \n>>>exit() \n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/sparkR \nR version 3.2.2 (2015-08-14) -- \"Fire Safety\" \nCopyright (C) 2015 The R Foundation for Statistical Computing \nPlatform: x86_64-apple-darwin13.4.0 (64-bit) \n\nR is free software and comes with ABSOLUTELY NO WARRANTY. \nYou are welcome to redistribute it under certain conditions. \nType 'license()' or 'licence()' for distribution details. \n\n  Natural language support but running in an English locale \n\nR is a collaborative project with many contributors. \nType 'contributors()' for more information and \n'citation()' on how to cite R or R packages in publications. \n\nType 'demo()' for some demos, 'help()' for on-line help, or \n'help.start()' for an HTML browser interface to help. \nType 'q()' to quit R. \n\n[Previously saved workspace restored] \n\nLaunching java with spark-submit command /Users/RajT/source-code/spark-source/spark-2.0/bin/spark-submit   \"sparkr-shell\" /var/folders/nf/trtmyt9534z03kq8p8zgbnxh0000gn/T//RtmphPJkkF/backend_port59418b49bb6  \nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties \nSetting default log level to \"WARN\". \nTo adjust logging level use sc.setLogLevel(newLevel). \n16/06/28 21:00:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \n\n Welcome to \n    ____              __  \n   / __/__  ___ _____/ /__  \n  _\\ \\/ _ \\/ _ `/ __/  '_/  \n /___/ .__/\\_,_/_/ /_/\\_\\   version  2.0.1 \n    /_/  \n\n Spark context is available as sc, SQL context is available as sqlContext \nDuring startup - Warning messages: \n1: 'SparkR::sparkR.init' is deprecated. \nUse 'sparkR.session' instead. \nSee help(\"Deprecated\")  \n2: 'SparkR::sparkRSQL.init' is deprecated. \nUse 'sparkR.session' instead. \nSee help(\"Deprecated\")  \n>q() \n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/run-example SparkPi \nPi is roughly 3.1484 \n$ ./bin/spark-submit examples/src/main/python/pi.py \nPi is roughly 3.138680 \n$ ./bin/spark-submit examples/src/main/r/dataframe.R \nroot \n |-- name: string (nullable = true) \n |-- age: double (nullable = true) \nroot \n |-- age: long (nullable = true) \n |-- name: string (nullable = true) \n    name \n1 Justin \n\n```", "```py\n$ cd /Users/RajT/temp \n$ ipython notebook \n\n```", "```py\nexport PYSPARK_DRIVER_PYTHON=ipython \nexport PYSPARK_DRIVER_PYTHON_OPTS='notebook' \n\n```", "```py\n$ cd /Users/RajT/temp \n$ pyspark \n\n```", "```py\nSPARK_HOME_DIR <- \"/Users/RajT/source-code/spark-source/spark-2.0\" \nSys.setenv(SPARK_HOME=SPARK_HOME_DIR) \n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"), .libPaths())) \nlibrary(SparkR) \nspark <- sparkR.session(master=\"local[*]\")\n\n```"]