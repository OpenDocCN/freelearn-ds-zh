- en: Chapter 5. Building Models (authored by Renata Nemeth and Gergely Toth)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"All models should be as simple as possible... but no simpler."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Attributed to Albert Einstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"All models are wrong... but some are useful."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – George Box
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After loading and transforming data, in this chapter, we will focus on how to
    build statistical models. Models are representations of reality, and, as the preceding
    citations emphasize, are always simplified representations. Although you can't
    possibly take everything into account, you should be aware about what to include
    and exclude in a good model that provides meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, regression models are discussed on the basis of linear regression
    models and standard modeling. **Generalized Linear Models** (**GLM**) extend these
    to allow the response variables to differ in distribution, which will be covered
    in the [Chapter 6](ch06.html "Chapter 6. Beyond the Linear Trend Line (authored
    by Renata Nemeth and Gergely Toth)"), *Beyond the Linear Trend Line (authored
    by Renata Nemeth and Gergely Toth)*. In all, we will discuss the three most well
    known regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression** for continuous outcomes (birth weight measured in grams)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic regression** for binary outcomes (low birth weight versus normal
    birth weight)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poisson regression** for count data (number of low birth weight infants per
    year or per country)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there are many other regression models, such as *Cox-regression* which
    we will not discuss here, the logic in the building of the models and the interpretation
    are similar. So, after reading this chapter, you will be able to understand those
    without doubt.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will learn the most important things about
    regression models: how to avoid confounding, how to fit, how to interpret, and
    how to choose the best model among the many different options.'
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind multivariate models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you would like to measure the strength of association between a response
    and a predictor, you can choose a simple two-way association measure, such as
    correlation or the odds ratio, depending on the nature of your data. But, if your
    aim is to model a complex mechanism by taking into account other predictors as
    well, you will need regression models.
  prefs: []
  type: TYPE_NORMAL
- en: As Ben Goldacre, the evidence-based columnist for *The Guardian*, tells in his
    brilliant TED talk that the strong association between olive oil consumption and
    young looking skin does not imply that olive oil is beneficial to our skin. When
    modeling a complex association structure, we should also control for other predictors,
    such as smoking status or physical activity, because those who consume more olive
    oil are more likely to live a healthy life in general, so it may not be the olive
    oil itself that prevents skin wrinkles. In short, it seems that the kind of lifestyle
    is likely to confound the relationship between the variables of interest, making
    it appear that there might be causality, when in fact there is none.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A confounder is a third variable that biases (increases or decreases) the association
    we are interested in. The confounder is always associated with both the response
    and the predictor.
  prefs: []
  type: TYPE_NORMAL
- en: If we examine the olive oil and skin wrinkles association again by fixing the
    smoking status, hence building separate models for smokers and non-smokers, the
    association may vanish. Holding the confounders fixed is the main idea behind
    controlling confounding via regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models in general are intended to measure associations between a
    response and a predictor by controlling for others. Potential confounders are
    entered into the model as predictors, and the regression coefficient of the predictor
    (the *partial coefficient*) measures the effect adjusted to the confounders.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with continuous predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with an actual and illuminating example of confounding. Consider
    that we would like to predict the amount of air pollution based on the size of
    the city (measured in population size as thousand of habitants). Air pollution
    is measured by the sulfur dioxide (SO2) concentration in the air, in milligrams
    per cubic meter. We will use the US air pollution data set (Hand and others 1994)
    from the `gamlss.data` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Model interpretation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s draw our very first linear regression model by building a formula. The
    `lm` function from the `stats` package is used to fit linear models, which is
    an important tool for regression modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formula notation is one of the best features of R, which lets you define flexible
    models in a human-friendly way. A typical model has the form of `response ~ terms`,
    where `response` is the continuous response variable, and `terms` provides one
    or a series of numeric variables that specifies a linear predictor for the response.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the variable, `y`, denotes air pollution, while `x3`
    stands for the population size. The coefficient of `x3` says that a one unit (one
    thousand) increase in the population size causes a `0.02` unit (0.02 milligram
    per cubic meter) increase in the sulfur dioxide concentration, and the effect
    is statistically significant with a `p` value of `0.001035`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See more details on the p-value in the *How well does the line fit to the data?*
    section. To keep it simple for now, we will refer to models as statistically significant
    when the *p* value is below `0.05`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intercept in general is the value of the response variable when each predictor
    equals to 0, but in this example, there are no cities without inhabitants, so
    the intercept (17.87) doesn''t have a direct interpretation. The two regression
    coefficients define the regression line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Model interpretation](img/2028OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the intercept (**17.87**) is the value at which the regression
    line crosses the y-axis. The other coefficient (**0.02**) is the slope of the
    regression line: it measures how steep the line is. Here, the function runs uphill
    because the slope is positive (**y** increases as **x3** increases). Similarly,
    if the slope is negative, the function runs downhill.'
  prefs: []
  type: TYPE_NORMAL
- en: You can easily understand the way the estimates were obtained if you realize
    how the line was drawn. This is the line that best fits the data points. Here,
    we refer to the *best fit* as the linear least-squares approach, which is why
    the model is also known as the **ordinary least squares** (**OLS**) regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The least-squares method finds the best fitting line by minimizing the sum
    of the squares of the residuals, where the residuals represent the error, which
    is the difference between the observed value (an original dot in the scatterplot)
    and the fitted or predicted value (a dot on the line with the same *x*-value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Model interpretation](img/2028OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *linear* term in linear regression refers to the fact that we are interested
    in a linear relation, which is more natural, easier to understand, and simpler
    to handle mathematically, as compared to the more complex methods.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the other hand, if we aim to model a more complex mechanism by separating
    the effect of the population size from the effect of the presence of industries,
    we have to control for the variable, `x2`, which describes the number of manufacturers
    employing more than 20 workers. Now, we can either create a new model by `lm(y
    ~ x3 + x2, data = usair)`, or use the `update` function to refit the previous
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, the coefficient of `x3` is `-0.06`! While the crude association between
    air pollution and city size was positive in the previous model, after controlling
    for the number of manufacturers, the association becomes negative. This means
    that a one thousand increase in the population decreases the SO2 concentration
    by 0.06 unit, which is a statistically significant effect.
  prefs: []
  type: TYPE_NORMAL
- en: On first sight, this change of sign from positive to negative may be surprising,
    but it is rather plausible after a closer look; it's definitely not the population
    size, but rather the level of industrialization that affects the air pollution
    directly. In the first model, population size showed a positive effect because
    it implicitly measured industrialization as well. When we hold industrialization
    fixed, the effect of the population size becomes negative, and growing a city
    with a fixed industrialization level spreads the air pollution in a wider range.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can conclude that `x2` is a confounder here, as it biases the association
    between `y` and `x3`. Although it is beyond the scope of our current research
    question, we can interpret the coefficient of `x2` as well. It says that holding
    the city size at a constant level, a one unit increase in the number of manufacturers
    increases the SO2 concentration by 0.08 mgs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the model, we can predict the expected value of the response for any
    combination of predictors. For example, we can predict the expected level of sulfur
    dioxide concentration for a city with 400,000 habitants and 150 manufacturers,
    each of whom employ more than 20 workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You could also calculate the prediction by yourself, multiplying the values
    with the slopes, and then summing them up with the constant—all these numbers
    are simply copied and pasted from the previous model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prediction outside the range of the data is known as extrapolation. The further
    the values are from the data, the riskier your prediction becomes. The problem
    is that you cannot check model assumptions (for example, linearity) outside of
    your sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have two predictors, the regression line is represented by a surface
    in the three dimensional space, which can be easily shown via the `scatterplot3d`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Multiple predictors](img/2028OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As it''s rather hard to interpret this plot, let''s draw the 2-dimensional
    projections of this 3D graph, which might prove to be more informative after all.
    Here, the value of the third, non-presented variable is held at zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Multiple predictors](img/2028OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: According to the changed sign of the slope, it's well worth mentioning that
    the *y-x3* regression line has also changed; from uphill, it became downhill.
  prefs: []
  type: TYPE_NORMAL
- en: Model assumptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression models with standard estimation techniques make a number
    of assumptions about the outcome variable, the predictor variables, and also about
    their relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y* is a continuous variable (not binary, nominal, or ordinal)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The errors (the residuals) are statistically independent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a stochastic linear relationship between *Y* and each *X*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Y* has a normal distribution, holding each *X* fixed'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Y* has the same variance, regardless of the fixed value of the *X*s'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A violation of assumption **2** occurs in trend analysis, if we use time as
    the predictor. Since the consecutive years are not independent, the errors will
    not be independent from each other. For example, if we have a year with high mortality
    from a specific illness, then we can expect the mortality for the next year to
    also be high.
  prefs: []
  type: TYPE_NORMAL
- en: A violation of assumption (**3**) says that the relationship is not exactly
    linear, but there is a deviation from the linear trend line. Assumption **4**
    and **5** require the conditional distribution of *Y* to be normal and having
    the same variance, regardless of the fixed value of *X*s. They are needed for
    inferences of the regression (confidence intervals, *F*- and *t*-tests). Assumption
    **5** is known as the homoscedasticity assumption. If it is violated, heteroscedasticity
    holds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot helps in visualizing these assumptions with a simulated
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Model assumptions](img/2028OS_5_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code bundle, available to be downloaded from the Packt Publishing homepage,
    includes a slightly longer code chunk for the preceding plot with some tweaks
    on the plot margins, legends, and titles. The preceding code block focuses on
    the major parts of the visualization, without wasting too much space in the printed
    book on the style details.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss in more detail, how to assess the model assumptions in [Chapter
    9](ch09.html "Chapter 9. From Big to Small Data"), *From Big to Smaller Data*.
    If some of the assumptions fail, a possible solution is to look for outliers.
    If you have an outlier, do the regression analysis without that observation, and
    determine how the results differ. Ways of outlier detection will be discussed
    in more detail in [Chapter 8](ch08.html "Chapter 8. Polishing Data"), *Polishing
    Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates that dropping an outlier (observation number
    31) may make the assumptions valid. To quickly verify if a model''s assumptions
    are satisfied, use the `gvlma` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that three out of the five assumptions are not satisfied. However,
    if we build the very same model on the same dataset excluding the 31st observation,
    we get much better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This suggests that we must always exclude the 31st observation from the dataset
    when building regression models in the future sections.
  prefs: []
  type: TYPE_NORMAL
- en: However, it's important to note that it is not acceptable to drop an observation
    just because it is an outlier. Before you decide, investigate the particular case.
    If it turns out that the outlier is due to incorrect data, you should drop it.
    Otherwise, run the analysis, both with and without it, and state in your research
    report how the results changed and why you decided on excluding the extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can fit a line for any set of data points; the least squares method will
    find the optimal solution, and the trend line will be interpretable. The regression
    coefficients and the R-squared coefficient are also meaningful, even if the model
    assumptions fail. The assumptions are only needed if you want to interpret the
    p-values, or if you aim to make good predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How well does the line fit in the data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we know that the trend line is the best fitting among the possible
    linear trend lines, we don't know how well this fits the actual data. The significance
    of the regression parameters is obtained by testing the null hypothesis, which
    states that the given parameter equals to zero. The *F-test* in the output pertains
    to the hypothesis that each regression parameter is zero. In a nutshell, it tests
    the significance of the regression in general. A *p-value* below 0.05 can be interpreted
    as "the regression line is significant." Otherwise, there is not much point in
    fitting the regression model at all.
  prefs: []
  type: TYPE_NORMAL
- en: However, even if you have a significant F-value, you cannot say too much about
    the fit of the regression line. We have seen that residuals characterize the error
    of the fit. The R-squared coefficient summarizes them into a single measure. *R-squared*
    is the proportion of the variance in the response variable explained by the regression.
    Mathematically, it is defined as the variance in the predicted *Y* values, divided
    by the variance in the observed *Y* values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some cases, despite the significant F-test, the predictors, according to
    the R-squared, explain only a small proportion (<10 percent) of the total variance.
    You can interpret this by saying that although the predictors have a statistically
    significant effect on the response, the response is formed by a mechanism that
    is much more complex than your model suggests. This phenomenon is common in the
    area of medicine or biology where complex biological processes are modeled, while
    it is less common in the area of econometrics, where macro-level, aggregated variables,
    which usually smooth out small variations in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the population size as the only predictor in our air pollution example,
    the R-squared equals 0.37, so we can say that 37 percent of the variation in SO2
    concentration can be explained by the size of the city:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After adding the number of manufacturers to the model, the R-squared increases
    dramatically and almost doubles its previous value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's important to note here that every time you add an extra predictor to your
    model, the R-squared increases simply because you have more information to predict
    the response, even if the lastly added predictor doesn't have an important effect.
    Consequently, a model with more predictors may appear to have a better fit just
    because it is bigger.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to use the adjusted R-squared, which takes into account the
    number of predictors as well. In the previous example, not only the R-squared
    but also the adjusted R-squared showed a huge advantage in favor of the latter
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The two previous models are nested, which means that the extended model contains
    each predictor of the first one. But unfortunately, the adjusted R-squared cannot
    be used as a base for choosing the best model for non-nested models. If you have
    non-nested models, you can use the **Akaike Information Criterion** (**AIC**)
    measure to select the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'AIC is founded on the information theory. It introduces a penalty term for
    the number of parameters in the model, giving a solution for the problem of bigger
    models tending to show as better fitted. When using this criterion, you should
    select the model with the least AIC. As a rule of thumb, two models are essentially
    indistinguishable if the difference between their AICs is less than 2\. In the
    example that follows, we have two plausible alternative models. Taking the AIC
    into account, `model.4` is better than `model.3`, as its advantage over `model.3`
    is about 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that AIC can tell nothing about the quality of the model in an absolute
    sense; your best model may still fit poorly. It does not provide a test for testing
    model fit either. It is essentially for ranking different models.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have seen only the simple case of both the response and the predictor
    variables being continuous. Now, let''s generalize the model a bit, and enter
    a discrete predictor into the model. Take the `usair` data and add `x5` (precipitation:
    average number of wet days per year) as a predictor with three categories (low,
    middle, and high levels of precipitation), using 30 and 45 as the cut-points.
    The research question is how these precipitation groups are associated with the
    SO2 concentration. The association is not necessary linear, as the following plot
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Discrete predictors](img/2028OS_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The cut-points 30 and 45 were more or less ad hoc. An advanced way to define
    optimal cut-points is to use a regression tree. There are various implementations
    of classification trees in R; a commonly used function is `rpart` from the package
    with the very same name. The regression tree follows an iterative process that
    splits the data into partitions, and then continues splitting each partition into
    smaller groups. In each step, the algorithm selects the best split on the continuous
    precipitation scale, where the best point minimizes the sum of the squared deviations
    from the group-level SO2 mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Discrete predictors](img/2028OS_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The interpretation of the preceding result is rather straightforward; if we
    are looking for two groups that differ highly regarding SO2, the optimal cut-point
    is a precipitation level of 45.34, and if we are looking for three groups, then
    we will have to split the second group by using the cut-point of 30.91, and so
    on. The four box-plots describe the SO2 distribution in the four partitions. So,
    these results confirm our previous assumption, and we have three precipitation
    groups that strongly differ in their level of SO2 concentration.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take a look at [Chapter 10](ch10.html "Chapter 10. Classification and Clustering"),
    *Classification and Clustering*, for more details and examples on decisions trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatterplot also shows that the three groups differ heavily from
    each other. It seems that the SO2 concentration is highest in the middle group,
    and the two other groups are very similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Discrete predictors](img/2028OS_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us refit our linear regression model by adding the three-category
    precipitation to the predictors. Technically, this goes by adding two dummy variables
    (learn more about this type of variable in [Chapter 10](ch10.html "Chapter 10. Classification
    and Clustering"), *Classification and Clustering*) pertaining to the second and
    third group, as shown in the table that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Dummy variables |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Categories | first | second |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| low (0-30) | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| middle (30-45) | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| high (45+) | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'In R, you can run this model using the `glm` (Generalized Linear Models) function,
    because the classic linear regression doesn''t allow non-continuous predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The second group (wet days between 30 and 45) has a higher average by 15.2 units
    of SO2, as compared to the first group. This is controlled by the population size
    and number of manufacturers. The difference is statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, the third group shows only a slight difference when compared
    to the first group (0.04 unit lower), which is not significant. The three group
    mean shows a reversed U-shaped curve. Note that if you used precipitation in its
    original continuous form, implicitly you would assume a linear relation, so you
    wouldn't discover this shape. Another important thing to note is that the U-shaped
    curve here describes the partial association (controlled for `x2` and `x3`), but
    the crude association, presented on the preceding scatterplot, showed a very similar
    picture.
  prefs: []
  type: TYPE_NORMAL
- en: The regression coefficients were interpreted as the difference between the group
    means, and both groups were compared to the omitted category (the first one).
    This is why the omitted category is usually referred to as the reference category.
    This way of entering discrete predictors is called reference-category coding.
    In general, if you have a discrete predictor with *n* categories, you have to
    define (*n-1*) dummies. Of course, if other contrasts are of interest, you can
    easily modify the model by entering dummies referring to other (*n-1*) categories.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you fit linear regression with discrete predictors, the regression slopes
    are the differences in the group means. If you also have other predictors, then
    the group-mean differences will be controlled for these predictors. Remember,
    the key feature of multivariate regression models is that they model partial two-way
    associations, holding the other predictors fixed.
  prefs: []
  type: TYPE_NORMAL
- en: You can go further by entering any other types and any number of predictors.
    If you have an ordinal predictor, it is your decision whether to enter it in its
    original form, assuming a linear relation, or to form dummies and enter each of
    them, allowing any type of relation. If you have no background knowledge on how
    to make this decision, you can try both solutions and compare how the models fit.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the concept of how to build and interpret basic models,
    such as linear regression models. By now, you should be familiar with the motivation
    behind linear regression models; you should know how to control for confounders,
    how to enter discrete predictors, how to fit models in R, and how to interpret
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend this knowledge with generalized models,
    and analyzing the model fit.
  prefs: []
  type: TYPE_NORMAL
