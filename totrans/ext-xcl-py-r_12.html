<html><head></head><body>
		<div><h1 id="_idParaDest-219" class="chapter-number"><a id="_idTextAnchor238"/>12</h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor239"/>Data Analysis and Visualization with R and Python in Excel – A Case Study</h1>
			<p>In this final chapter, we are going<a id="_idIndexMarker1008"/> to perform an analysis—<strong class="bold">visualization</strong> and a simple model—built with data from Excel and place all those outcomes back into it. This can be useful when there is a lot of data, or the calculations themselves are best suited to being done outside of Excel.</p>
			<p>First, we will start with importing our data and then performing some data exploration via visualizations. For this chapter, we are going to use the <code>diamonds</code> dataset from the R package called <code>ggplot2</code>. We will view the data where the price is the outcome and look at it via different facets of the diamond’s characteristics. After the visualizations are done, we will perform some simple modeling to predict the price of a diamond based on its characteristics.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Getting a visualization</li>
				<li>Performing a simple <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model</li>
			</ul>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor240"/>Technical requirements</h1>
			<p>For this chapter, we will be using the following packages/libraries:</p>
			<ul>
				<li><code>ggplot2 3.4.4</code></li>
				<li><code>dplyr 1.1.4</code></li>
				<li><code>healthyR 0.2.1</code></li>
				<li><code>readxl 1.4.3</code></li>
				<li><code>tidyverse 2.0.0</code></li>
				<li><code>janitor 2.2.0</code></li>
				<li><code>writexl 1.5.0</code></li>
				<li><code>healthyR.ai 0.0.13</code></li>
			</ul>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor241"/>Getting visualizations with R</h1>
			<p>In this section, we are going<a id="_idIndexMarker1009"/> to go over getting some visualizations of the data. We will create several visualizations and give short interpretations of the outcomes in them. For this, we will create two histograms in base R and a few different visuals using the <code>ggplot2</code> library.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor242"/>Getting the data</h2>
			<p>The first thing we need to do is load<a id="_idIndexMarker1010"/> the libraries and get the data. I am working in a directory specific to this book so I can source the function directly from the chapter I wrote the <code>read_excel_sheets()?</code> function in; your path might be different. Let’s look at the code up to this point:</p>
			<pre class="source-code">
# Library Load
library(ggplot2)
library(dplyr)
library(healthyR)
library(readxl)
# Source Functions
source(paste0(getwd(),"/Chapter1/excel_sheet_reader.R"))
# Read data
file_path &lt;- paste0(getwd(), "/Chapter12/")
df &lt;- read_excel_sheets(
  filename = paste0(file_path, "diamonds_split.xlsx"),"),
  single_tbl = TRUE
)</pre>			<p>What we have done here is to simply call in a few libraries into our environment, pull in the sheet reading function, and read in our data. We loaded the <code>read_excel_sheets()</code> function into our environment using the <code>source()</code> command. You might be wondering how the data was created for this section, and it is important because it was exported from the <code>ggplot2</code> library. Here is the code if you want to re-create the data so that the preceding code will work, and the following sections<a id="_idIndexMarker1011"/> will also work:</p>
			<pre class="source-code">
# Library Load
library(tidyverse)
library(writexl)
library(janitor)
# Write File to disk
file_path &lt;- paste0(getwd(), "/Chapter12/")
# Split data by cut and clean names of the list
df_list &lt;- split(diamonds, diamonds$cut) |&gt;
  clean_names()
# Write to xlsx
df_list |&gt;
  write_xlsx(paste0(file_path, "diamonds_split.xlsx"))</pre>			<p>Now that we have gone over how to produce and read in the data, let’s start taking a look at some visuals.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor243"/>Visualizing the data</h2>
			<p>We are going to use two different methods<a id="_idIndexMarker1012"/> of creating graphs in this section, firstly, with base R and secondly with <code>ggplot2</code>. With that in mind, let’s get started.</p>
			<h3>Base R visuals</h3>
			<p>The first thing that we are going<a id="_idIndexMarker1013"/> to do is to create some histograms<a id="_idIndexMarker1014"/> of the <code>price</code> column in the <code>diamonds</code> dataset. The price is the outcome variable we will use as the predictor in our model in the next section. First, we need to create a vector of breaks that will get passed to the histograms. There is much literature available on techniques for optimal binning strategies. The basic crux is that this will help provide the appropriate shape to the histogram that best represents the data. That is a separate topic and not one that we will pursue in this book, as it is a topic that can span a book unto itself. There is a function called <code>opt_bin()</code> from the <code>healthyR</code> package that will produce a tibble of break points for a <code>value</code> column that is passed to it. Let’s look at it:</p>
			<pre class="source-code">
breaks &lt;- tibble(x = df$price) |&gt;
  opt_bin(x) |&gt;
  pull(value)
head(breaks)
[1]  326.000 1130.217 1934.435 2738.652 3542.870 4347.087</pre>			<p>The purpose of doing this is to try and capture the proper density of information in the data. The <code>hist()</code> base function does a good job of this already with a standard method. Now, let’s go ahead and create the plots and see the methods side by side. We will use <code>par(mfrow = c(1, 2))</code> so that we can plot them side by side:</p>
			<pre class="source-code">
par(mfrow = c(1, 2))
hist(df$price, main = "Price Histogram - Default binning",
      xlab = "Price", ylab = "Frequency")
hist(df$price, breaks = breaks, main = "Price Histogram - Optimal binning",
      xlab = "Price", ylab = "Frequency")
par(mfrow = c(1, 1))</pre>			<p>Let’s take a look at what it produced:</p>
			<div><div><img src="img/B19142_12_01.jpg" alt="Figure 12.1 – Histogram comparison between default binning and optimal binning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Histogram comparison between default binning and optimal binning</p>
			<p>We can see that the shape of the histogram is slightly different, but again, this strategy might not work for you, or you may have another strategy that you employ with regularity; this was simply a way to illustrate<a id="_idIndexMarker1015"/> that different methods do exist. That is the end of making visuals<a id="_idIndexMarker1016"/> via base R; we will now move on to using <code>ggplot2</code>.</p>
			<h3>Visuals with ggplot2</h3>
			<p>We are now going to make<a id="_idIndexMarker1017"/> the rest of the visuals with <code>ggplot2</code> as I find the syntax<a id="_idIndexMarker1018"/> a bit easier and the graphics one can produce are a bit more sophisticated, aside from the fact that the package is part of the <code>tidyverse</code>, which means it is interoperable with the rest of the packages in it such as <code>dplyr</code>. You may need to install the <code>hexbin</code> package as well. Let’s get started:</p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = carat, y = price, fill = cut)) +
  geom_hex(bins = length(breaks), alpha = 1/5) +
  facet_wrap(~ clarity, scales = "free") +
  theme_minimal() +
  labs(
     x = "Carat",
     y = "Price",
     title = "Diamonds Data",
     fill = "Cut"
  ) +
  hr_scale_color_colorblind()</pre>			<p>Here’s a breakdown<a id="_idIndexMarker1019"/> of the<a id="_idIndexMarker1020"/> code.</p>
			<p>For the data and aesthetics, this is how it goes:</p>
			<ul>
				<li><code>df |&gt; ggplot(...)</code>: This starts the visualization using the data in <code>df</code>.</li>
				<li><code>aes(x = carat, y = price, fill = cut)</code>: This defines aesthetics for the plot:<ul><li><code>x</code>: The x-axis represents the carat weight</li><li><code>y</code>: The y-axis represents the price</li><li><code>fill</code>: The color fill represents the diamond cut</li></ul></li>
			</ul>
			<p>For the hexagon geometry, this is what we have:</p>
			<ul>
				<li><code>geom_hex(bins = length(breaks), alpha = 1/5)</code>: This plots hexagons representing data points.</li>
				<li><code>bins</code>: This controls the number of bins for the hexagonal grid. Here, it uses the same number as defined in <code>breaks</code> (not shown in the provided code).</li>
				<li><code>alpha</code>: This is the opacity of the hexagons, set to 1/5 for better visibility.</li>
			</ul>
			<p>For faceting by clarity, this is what we have:</p>
			<ul>
				<li><code>facet_wrap(~ clarity, scales = "free")</code>: This groups data into subplots based on diamond clarity, with independent color scales for each plot</li>
			</ul>
			<p>These are the themes and labels:</p>
			<ul>
				<li><code>theme_minimal()</code>: This applies a minimal theme for cleaner visuals</li>
				<li><code>labs(..., title = "Diamonds Data")</code>: This adds labels for axes and title.</li>
			</ul>
			<p>This is the code for the colorblind-friendly color scale:</p>
			<ul>
				<li><code>hr_scale_color_colorblind()</code>: This ensures the color palette is optimized for colorblind viewers</li>
			</ul>
			<p>Now, let’s check the output.</p>
			<div><div><img src="img/B19142_12_02.jpg" alt="Figure 12.2 – ggplot2 of diamonds data with hex geometry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – ggplot2 of diamonds data with hex geometry</p>
			<p>Overall, this code visualizes<a id="_idIndexMarker1021"/> the relationship between the carat<a id="_idIndexMarker1022"/> weight, price, and cut of diamonds, considering clarity groups with a colorblind-friendly color scheme.</p>
			<p>The next visual we will see uses a boxplot to check the dispersion of the data:</p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = carat, y = price, fill = cut)) +
  geom_boxplot(alpha = 1/5, outlier.color = "lightgrey") +
  facet_wrap(~ clarity, scales = "free") +
  theme_minimal() +
  labs(
     x = "Carat",
     y = "Price",
     title = "Diamonds Data",
     fille = "Cut"
  ) +
  hr_scale_color_colorblind()</pre>			<p>Again, let’s see<a id="_idIndexMarker1023"/> the output:</p>
			<div><div><img src="img/B19142_12_03.jpg" alt="Figure 12.3 – ggplot2 boxplot of price dispersion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – ggplot2 boxplot of price dispersion</p>
			<p>We can now look at the mean<a id="_idIndexMarker1024"/> price with a question to ponder: does it show<a id="_idIndexMarker1025"/> the information accurately? Let’s refer to the following code:</p>
			<pre class="source-code">
df |&gt;
  summarize(m = mean(price), .by = c(clarity, cut)) |&gt;
  ggplot(aes(x = clarity, y = m, group = cut, color = cut)) +
  geom_point() +
  geom_line() +
  geom_smooth() +
  facet_wrap(~cut, ncol = 2) +
  labs(x= "Clarity",
         y = "Mean Price",
         title = "Mean Price by Clarity and Cut",
         color = "Cut") +
  theme_minimal() +
  hr_scale_color_colorblind()</pre>			<p>The output of this code is as follows:</p>
			<div><div><img src="img/B19142_12_04.jpg" alt="Figure 12.4 – ggplot2 mean price dispersion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – ggplot2 mean price dispersion</p>
			<p>Here is another view of the mean<a id="_idIndexMarker1026"/> price, but this time by looking at the<a id="_idIndexMarker1027"/> mean price per carat:</p>
			<pre class="source-code">
df |&gt;
  summarize(m = mean(price/carat), .by = c(cut, color, clarity)) |&gt;
  ggplot(aes(x = color, y = m, group = clarity, color = clarity)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ cut, ncol = 2, scales = "free") +
  labs(x= "Clarity",
         y = "Mean Price",
         title = "Mean Price per Carat by Clarity, Color and Cut",
         color = "Cut") +
  theme_minimal() +
  hr_scale_color_colorblind()</pre>			<p>Let’s see what story this tells:</p>
			<div><div><img src="img/B19142_12_05.jpg" alt="Figure 12.5 – ggplot mean price per carat"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – ggplot mean price per carat</p>
			<p>These are very good diamonds – does it matter <a id="_idIndexMarker1028"/>what the cut or color<a id="_idIndexMarker1029"/> is as long as the clarity is better than fair? Seems like it does not.</p>
			<p>Lastly, we will look at a histogram of price faceted by cut rather than colored by it and we are going to use the <code>breaks</code> data we created previously. See the following code:</p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = price)) +
  geom_histogram(breaks = breaks, fill = "lightblue",
                        color = "black") +
  theme_minimal() +
  facet_wrap(~ cut, ncol = 2, scales = 'free') +
  labs(x = "Price", y = "Frequency", title = "Price Histogram by Cut")</pre>			<p>Let’s take a last look:</p>
			<div><div><img src="img/B19142_12_06.jpg" alt="Figure 12.6 – Histogram of price faceted by the cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Histogram of price faceted by the cut</p>
			<p>Now that we have created<a id="_idIndexMarker1030"/> all the<a id="_idIndexMarker1031"/> visuals, we can move on to the modeling phase.</p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor244"/>Performing a simple ML model with R</h1>
			<p>In this section, we are going to go over performing<a id="_idIndexMarker1032"/> a simple ML model in R. There are so many different ways to do this in R that it would be impossible for me to list them all, however, CRAN has done this so you and I don’t have to. If you want to see a task view of ML on CRAN, you can follow this link: <a href="https://cran.r-project.org/view=MachineLearning">https://cran.r-project.org/view=MachineLearning</a>.</p>
			<p>For this section, we are going to use the XGBoost algorithm as implemented by the <code>healthyR.ai</code> package. The algorithm is not written differently, the only difference is how data is saved in the output. The <code>healthyR.ai</code> package also contains a preprocessor for the XGBoost algorithm to ensure that the input data matches what the algorithm is expecting before modeling. The two main functions that we will be using are <code>hai_xgboost_data_prepper()</code> and <code>hai_auto_xgboost()</code>.</p>
			<p>We will not cover loading<a id="_idIndexMarker1033"/> the data in again as it was covered previously. Let’s get started!</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor245"/>Data preprocessing</h2>
			<p>Before we get started, we are going<a id="_idIndexMarker1034"/> to preprocess our data so that it meets the needs of the algorithm for modeling. This is made easy by the <code>hai_xgboost_data_prepper()</code> function from the <code>healthyR.ai</code> library. We are going to see what the data looks like before and after the data is processed. Let’s see the following code and then the output:</p>
			<pre class="source-code">
# Lib Load
library(healthyR.ai)
library(dplyr)
glimpse(head(df, 2))
Rows: 2
Columns: 10
$ carat   &lt;dbl&gt; 0.22, 0.86
$ cut      &lt;chr&gt; "Fair", "Fair"
$ color   &lt;chr&gt; "E", "E"
$ clarity &lt;chr&gt; "VS2", "SI2"
$ depth   &lt;dbl&gt; 65.1, 55.1
$ table   &lt;dbl&gt; 61, 69
$ price   &lt;dbl&gt; 337, 2757
$ x         &lt;dbl&gt; 3.87, 6.45
$ y         &lt;dbl&gt; 3.78, 6.33
$ z         &lt;dbl&gt; 2.49, 3.52</pre>			<p>This is the data before processing begins. We see that there are 10 columns, and we can see the datatypes of each of those columns clearly in the output. Now, let’s create a <code>recipe</code> object by passing our data into <code>hai_xgboost_data_prepper()</code> and checking the output from there. This function<a id="_idIndexMarker1035"/> takes two arguments: <code>.data</code> and <code>.recipe_formula</code>:</p>
			<pre class="source-code">
# Pass data through pre-processor
rec_obj &lt;- hai_xgboost_data_prepper(
  .data = df,
  .recipe_formula = price ~ .
)
rec_obj
── Recipe ───────────
── Inputs
Number of variables by role
outcome:   1
predictor: 9
── Operations
• Factor variables from: tidyselect::vars_select_helpers$where(is.character)
• Novel factor level assignment for: recipes::all_nominal_predictors()
• Dummy variables from: recipes::all_nominal_predictors()
• Zero variance filter on: recipes::all_predictors()</pre>			<p>Now, let’s look at the processed<a id="_idIndexMarker1036"/> data. We can see in the following that columns have been added and all the datatypes are now <code>&lt;dbl&gt;</code>, which is what was called for in the preprocessor:</p>
			<pre class="source-code">
# Now see the juiced output
get_juiced_data(rec_obj) |&gt;
  head(2) |&gt;
  glimpse()
Rows: 2
Columns: 24
$ carat            &lt;dbl&gt; 0.22, 0.86
$ depth            &lt;dbl&gt; 65.1, 55.1
$ table            &lt;dbl&gt; 61, 69
$ x                  &lt;dbl&gt; 3.87, 6.45
$ y                  &lt;dbl&gt; 3.78, 6.33
$ z                  &lt;dbl&gt; 2.49, 3.52
$ price            &lt;dbl&gt; 337, 2757
$ cut_Good        &lt;dbl&gt; 0, 0
$ cut_Ideal      &lt;dbl&gt; 0, 0
$ cut_Premium   &lt;dbl&gt; 0, 0
$ cut_Very.Good &lt;dbl&gt; 0, 0
$ color_E         &lt;dbl&gt; 1, 1
$ color_F         &lt;dbl&gt; 0, 0
$ color_G         &lt;dbl&gt; 0, 0
$ color_H         &lt;dbl&gt; 0, 0
$ color_I         &lt;dbl&gt; 0, 0
$ color_J         &lt;dbl&gt; 0, 0
$ clarity_IF     &lt;dbl&gt; 0, 0
$ clarity_SI1   &lt;dbl&gt; 0, 0
$ clarity_SI2   &lt;dbl&gt; 0, 1
$ clarity_VS1   &lt;dbl&gt; 0, 0
$ clarity_VS2   &lt;dbl&gt; 1, 0
$ clarity_VVS1  &lt;dbl&gt; 0, 0
$ clarity_VVS2  &lt;dbl&gt; 0, 0</pre>			<p>Now that we have seen the data<a id="_idIndexMarker1037"/> after processing, let’s use the <code>hai_auto_xgboost()</code> function to perform the modeling. Here is the full function<a id="_idIndexMarker1038"/> call and documentation on it can be at <a href="https://www.spsanderson.com/healthyR.ai/reference/hai_auto_xgboost.html:">https://www.spsanderson.com/healthyR.ai/reference/hai_auto_xgboost.html:</a></p>
			<pre class="source-code">
hai_auto_xgboost(
  .data,
  .rec_obj,
  .splits_obj = NULL,
  .rsamp_obj = NULL,
  .tune = TRUE,
  .grid_size = 10,
  .num_cores = 1,
  .best_metric = "f_meas",
  .model_type = "classification"
)</pre>			<p>We will now create the model and check the output. I am using <code>.num_cores = 10</code>, <code>.best_metric = "rsq"</code>, and <code>.model_type = "regression"</code>, and I do not suggest you run this yourself unless you have plenty of time to spare.</p>
			<p>Now, perform modeling using the <code>hai_auto_xgboost()</code> function:</p>
			<pre class="source-code">
auto_xgb &lt;- hai_auto_xgboost(
  .data = df,
  .rec_obj = rec_obj,
  .best_metric = "rsq",
  .num_cores = 10,
  .model_type = "regression"
)</pre>			<p>This produces a rather large object; on my machine, it is 196.1 MB, with the largest portion coming from <code>$tuned_info</code> sitting at <code>169836312 bytes</code>, which is mainly due to the <code>plotly</code> plot and the Monte Carlo<a id="_idIndexMarker1039"/> cross-validation <code>tibble</code>, due to the size of the incoming data. We can now take a look at some of the objects that are exported:</p>
			<pre class="source-code">
xgb_wflw_fit &lt;- auto_xgb$model_info$fitted_wflw
class(xgb_wflw_fit)
[1] "workflow"
mod_spec &lt;- xgb_wflw_fit[["fit"]][["actions"]][["model"]][["spec"]]
mod_spec
Boosted Tree Model Specification (regression)
Main Arguments:
  trees = 817
  min_n = 17
  tree_depth = 9
  learn_rate = 0.0205081386887847
  loss_reduction = 2.0421383990836e-05
  sample_size = 0.762693894910626
Computational engine: xgboost</pre>			<p>The first thing we did was pull out the fitted workflow object, which can be used to make predictions on data using the generic <code>predict()</code> function. We know it is a <code>workflow</code> object from when we ran <code>class(xgb_wflw_fit)</code>.</p>
			<p>The final thing we do is to actually take a look at the specification of the fitted model itself. This will show us what the parameters were set to during the cross-validation process. It is important to remember that I did not use a seed, which means that you can obtain different results. This was meant to be a primer and not an exhaustive write-up of the inputs and outputs, but rather just a showcase of how an XGBoost model can be fitted to data from an Excel file, given<a id="_idIndexMarker1040"/> one cannot perform such a modeling task with the ease it was done in R.</p>
			<p>Now, we can move on to the Python section, where we will follow a similar workflow for the same dataset.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor246"/>Getting visualizations with Python</h1>
			<p>In this section, we are going<a id="_idIndexMarker1041"/> to go over visualizations of the data in Python, analogous to the preceding R section. We will use <code>plotnine</code> to have visualizations similar to those created in R using <code>ggplot2</code> and provide interpretations of the results.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor247"/>Getting the data</h2>
			<p>Like in the earlier chapters, we will load<a id="_idIndexMarker1042"/> the data using <code>pandas</code>. Just like before, the path to the XLSX file may be different for you from what I have, so adjust the <code>filepath</code> accordingly:</p>
			<pre class="source-code">
import pandas as pd
# Define the file path (may be different for you)
file_path = "./Chapter 12/diamonds.xlsx"
# Load the dataset into a pandas DataFrame
df = pd.read_excel(file_path)
# Display the first few rows of the DataFrame
print(df.head())</pre>			<p>Note that we use the raw <code>diamonds</code> dataset without spitting it first and then recombining it, as it was done<a id="_idIndexMarker1043"/> in the R part of the chapter.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor248"/>Visualizing the data</h2>
			<p>Once we have our data<a id="_idIndexMarker1044"/> loaded, we can use <code>plotnine</code> to create visualizations. In this section, we’ll demonstrate how to visualize various aspects of the <code>diamonds</code> dataset.</p>
			<p>Using the dataset loaded before, we can have a first look at the data:</p>
			<pre class="source-code">
from plotnine import ggplot, aes, geom_bin2d, facet_wrap, theme_minimal, labs, scale_fill_manual
# Define a colorblind-friendly color palette
color_palette = ["#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"]
# Plot using plotnine
(
    ggplot(df, aes(x='carat', y='price', fill='cut')) +
    geom_bin2d(bins=20) +  # Adjust the bins parameter as needed
    facet_wrap('~ clarity', scales='free') +
    theme_minimal() +
    labs(
        x='Carat',
        y='Price',
        title='Diamonds Data',
        fill='Cut'
    ) +
    scale_fill_manual(values=color_palette)
)</pre>			<p>This Python code replicates<a id="_idIndexMarker1045"/> the R code used at the beginning of the chapter using <code>plotnine</code> for data visualization. The <code>ggplot()</code> function initializes the plot, <code>aes() </code>defines the aesthetics, <code>geom_bin2d()</code> adds the geometry, <code>facet_wrap()</code> creates facets, <code>theme_minimal()</code> sets the theme, <code>labs()</code> adds labels, and <code>scale_fill_manual(values=color_palette)</code> ensures the color palette is colorblind friendly using the predefined <code>color_palette</code>.</p>
			<p>The resulting image will look like this:</p>
			<div><div><img src="img/B19142_12_07.jpg" alt="Figure 12.7 – The plotnine scatterplot of the diamonds dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – The plotnine scatterplot of the diamonds dataset</p>
			<p>As you can see, the plot shows the relationship<a id="_idIndexMarker1046"/> between carat weight and price by color-coding the cut of diamonds, using a colorblind-friendly color scheme.</p>
			<p>Let’s have a look at the boxplot of the data (we will not re-import all of the <code>plotnine</code> functions again):</p>
			<pre class="source-code">
from plotnine import geom_boxplot
# Plot using plotnine
(
    ggplot(df, aes(x='carat', y='price', fill='cut')) +
    geom_boxplot(alpha=1/5, outlier_color="lightgrey") +
    facet_wrap('~ clarity', scales='free') +
    theme_minimal() +
    labs(
        x='Carat',
        y='Price',
        title='Diamonds Data',
        fill='Cut'
    ) +
    scale_fill_manual(values=color_palette)
)</pre>			<p>In this code, <code>geom_boxplot()</code> is used to create boxplots. The <code>outlier_color</code> parameter is set to <code>lightgrey</code> to change the color<a id="_idIndexMarker1047"/> of outliers in the boxplot:</p>
			<div><div><img src="img/B19142_12_08.jpg" alt="Figure 12.8 – The boxplot of the diamonds dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – The boxplot of the diamonds dataset</p>
			<p>The core purpose of data visualization<a id="_idIndexMarker1048"/> remains to get insights into the data to better understand it. What if we plot the mean price? Do we see what we need to see?</p>
			<p>We can use the <code>groupby</code> functionality from <code>pandas</code> to aggregate the prices, calculate the mean per group, and create a plot with points, lines, and smoothed lines to visualize the mean price by clarity and cut:</p>
			<pre class="source-code">
from plotnine import geom_point, geom_line, geom_smooth, scale_color_manual
# Plot the mean price
(
    ggplot(df.groupby(['clarity', 'cut']).mean().reset_index(), aes(x='clarity', y='price', group='cut', color='cut')) +
    geom_point() +
    geom_line() +
    geom_smooth() +
    facet_wrap('~ cut', ncol=2) +
    labs(
        x='Clarity',
        y='Mean Price',
        title='Mean Price by Clarity and Cut',
        color='Cut'
    ) +
    theme_minimal() +
    scale_color_manual(values=color_palette)
)</pre>			<p>Let’s have a look at the resulting<a id="_idIndexMarker1049"/> data vizualization:</p>
			<div><div><img src="img/B19142_12_09.jpg" alt="Figure 12.9 – Mean price by clarity and cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Mean price by clarity and cut</p>
			<p>For each cut, a similar curve<a id="_idIndexMarker1050"/> becomes visible: the mean price first rises by clarity and then drops. Both rise and fall are the least relevant for the <strong class="bold">Ideal</strong> clarity while they are the strongest for <strong class="bold">Premium</strong> and <strong class="bold">Very </strong><strong class="bold">Good</strong> clarities.</p>
			<p>Could we gain more insights from plotting the mean price in a different grouping? Let’s take a look at the mean price per carat:</p>
			<pre class="source-code">
# Calculate mean price per carat by clarity, color, and cut
df_mean = df.groupby(['cut', 'color', 'clarity']).apply(lambda x: (x['price'] / x['carat']).mean()).reset_index(name='m')
# Plot using plotnine
(
        ggplot(df_mean, aes(x='color', y='m', group='clarity', color='clarity')) +
        geom_point() +
        geom_line() +
        facet_wrap('~ cut', ncol=2, scales='free') +
        labs(
                x='Clarity',
                y='Mean Price',
                title='Mean Price per Carat by Clarity, 
                Color and Cut',
                color='Cut'
        ) +
        scale_color_manual(values=color_palette)
)</pre>			<p>The resulting image indeed<a id="_idIndexMarker1051"/> shows some interesting things:</p>
			<div><div><img src="img/B19142_12_10.jpg" alt="Figure 12.10 – The mean price per carat by clarity, color, and cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – The mean price per carat by clarity, color, and cut</p>
			<p>For all clarities but <strong class="bold">Fair</strong>, we see that the D color has an extreme price for the IF cut but for the others, the prices remain similar. For <strong class="bold">Fair</strong> clarity, however, the prices show a clear downward trend with the only large price difference being between D and other colors for the I1 cut.</p>
			<p>Finally, before moving<a id="_idIndexMarker1052"/> on to modeling, let’s have a look at the histogram of prices by cut:</p>
			<pre class="source-code">
from plotnine import geom_histogram
# Create a histogram of price by Cut
(
        ggplot(df, aes(x='price')) +
        geom_histogram(fill='lightblue', color='black') +
        theme_minimal() +
        facet_wrap('~ cut', ncol=2, scales='free') +
        labs(x='Price', y='Frequency', title='Price Histogram by Cut')
)</pre>			<p>We use default binning because, unfortunately, the great<a id="_idIndexMarker1053"/> package used for the R version, <code>healthyR</code>, is not available for Python (yet).</p>
			<div><div><img src="img/B19142_12_11.jpg" alt="Figure 12.11 – Price histogram by cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Price histogram by cut</p>
			<p>We can see that the price has a very long tail (that is, extremely high prices are relatively typical even though infrequent) and, surprisingly, we can see a second<a id="_idIndexMarker1054"/> high point for <strong class="bold">Good</strong> and <strong class="bold">Premium</strong> cuts (and to a lesser extent for <strong class="bold">Very Good</strong> cut as well).</p>
			<p>With the data better understood thanks to the visualizations, we can start with the modeling!</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor249"/>Performing a simple ML model with Python</h1>
			<p>In this section, we create<a id="_idIndexMarker1055"/> a simple ML model in Python. Python has grown to be the primary go-to language for ML work (with R as the obvious alternative) and the number of packages implementing ML algorithms is difficult to overestimate. Having said that, <code>sklearn</code> remains the most widely used so we will also choose it for this section. Similarly to the R part of the chapter, we will use the <code>xgboost</code> model because it has a great balance between performance and explainability.</p>
			<p>We will use the data loaded in the previous section.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor250"/>Data preprocessing</h2>
			<p>The first thing to do<a id="_idIndexMarker1056"/> for the modeling phase is to prepare the data. Fortunately, <code>sklearn</code> comes with a preprocessing functionality built-in!</p>
			<p>Let’s review the steps involved in data preprocessing:</p>
			<ul>
				<li><code>sklearn</code> provides methods for imputing missing values or removing rows/columns with missing data.</li>
				<li><code>sklearn</code> offers utilities for scaling features, including standardization (scaling features to have zero mean and unit variance) and normalization (scaling features to a specified range).</li>
				<li><code>sklearn</code> provides methods for one-hot encoding categorical variables or encoding them using ordinal labels.</li>
				<li><code>sklearn</code> offers functions for splitting datasets into training and testing sets with specified proportions.</li>
				<li><code>sklearn</code> supports various feature engineering techniques, such as polynomial<a id="_idIndexMarker1057"/> features generation, interaction terms creation, and dimensionality<a id="_idIndexMarker1058"/> reduction using techniques such as <strong class="bold">principal component </strong><strong class="bold">analysis</strong> (<strong class="bold">PCA</strong>).</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">It is important to do the feature engineering in a way that doesn’t pollute the training data with information from the test data, just like for data cleaning (such as imputation).</p>
			<p>We have covered data cleaning to a great extent in the dedicated chapter, so we will make use of the fact that the <code>diamonds</code> dataset is clean already. We will move on to feature scaling and the encoding of categorical variables instead:</p>
			<pre class="source-code">
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import numpy as np
# Encode categorical variables
encoder = OneHotEncoder()
df_encoded = encoder.fit_transform(df[['cut', 'color', 'clarity']])
# Scale numerical features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['carat', 'depth', 'table', 
    'x', 'y', 'z']])
# Concatenate encoded categorical features with scaled numerical features
df_processed = np.concatenate((df_encoded.toarray(), df_scaled),
    axis=1)
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    df_processed, df["price"], test_size=0.2, random_state=42)</pre>			<p>This code snippet demonstrates<a id="_idIndexMarker1059"/> how to encode categorical variables (cut, color, and clarity) using one-hot encoding and scale numerical features using <code>StandardScaler</code> from <code>sklearn</code>. Then, it concatenates the encoded categorical features with scaled numerical features and splits the dataset into training and testing sets using <code>train_test_split()</code>.</p>
			<p>Let’s compare the data before and after the preprocessing.</p>
			<p>The original dataset looks like this:</p>
			<div><div><img src="img/B19142_12_12.jpg" alt="Figure 12.12 – Raw data as read from Excel"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Raw data as read from Excel</p>
			<p>As you can see, the dataset contains<a id="_idIndexMarker1060"/> a mix of numerical and categorical variables (the latter ones will be encoded using one-hot encoding).</p>
			<p>After the preprocessing, the dataset looks like this:</p>
			<div><div><img src="img/B19142_12_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – Preprocessed training data</p>
			<p>The preprocessed training data shown earlier has fewer rows (the rest makes up the test data) but more columns: while the <code>price</code> column is not present (it’s the variable we want to predict), the categorical variables have been replaced by multiple <code>0</code> and <code>1</code> values – the result of one-hot encoding. For each unique value of each categorical variable, a new column has been introduced that has <code>1</code> if the original dataset had that value and <code>0</code> if it had something else.</p>
			<p>The <code>y_train</code> variable has the value of the <code>price</code> column for each row in the train data.</p>
			<p>With the preprocessed data, we can start the modeling:</p>
			<pre class="source-code">
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
# Instantiate XGBoost regressor
xgb_reg = GradientBoostingRegressor(random_state=42)
# Train the model
xgb_reg.fit(X_train, y_train)
# Predict on the test set
y_pred = xgb_reg.predict(X_test)
# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error:", rmse)</pre>			<p>In this code, we observe the following:</p>
			<ol>
				<li>We import <code>GradientBoostingRegressor</code> from <code>sklearn.ensemble</code>.</li>
				<li>We instantiate<a id="_idIndexMarker1061"/> a gradient boosting regressor (<code>xgb_reg</code>) from <code>scikit-learn</code>’s implementation.</li>
				<li>We train the model using the <code>fit</code> method with the training data (<code>X_train</code> and <code>y_train</code>).</li>
				<li>We make predictions on the test<a id="_idIndexMarker1062"/> set using the <code>predict</code> method and calculate the <code>RMSE</code>) between the predicted values (<code>y_pred</code>) and the actual target values (<code>y_test</code>).</li>
			</ol>
			<p>The RMSE is a widely used metric in regression analysis that measures the average magnitude of the errors between predicted values and observed values. It provides a single numerical value to assess the goodness of fit of a regression model. RMSE is on the same scale (units) as the target variable (<code>price</code>).</p>
			<p>Lower values of RMSE indicate that the model’s predictions are closer to the actual values, implying better performance. In other words, a lower RMSE signifies that the model has a smaller average deviation from the true values, which indicates higher accuracy and a better predictive capability.</p>
			<p>The RMSE is particularly useful because it considers the magnitude of errors and penalizes larger errors more heavily than smaller ones. Therefore, minimizing the RMSE leads to a model that provides more precise and accurate predictions.</p>
			<p>Overall, the RMSE serves as a valuable tool for comparing different regression models and assessing their predictive accuracy in real-world applications.</p>
			<p>The model result has an RMSE of around <code>720</code>, which is significantly lower than the average price (<code>3933</code>) and the standard deviation of the <code>price</code> variable (<code>3989</code>). This is good news, indeed, as it indicates the model fit was quite good.</p>
			<p>Of course, you can consider<a id="_idIndexMarker1063"/> other ML models (random forests, <code>lightgbm</code> or <code>catgbm</code>, or even <strong class="bold">deep learning models</strong>) and other goodness-of-fit metrics (R2, MAE, etc.). This section is intended to be a primer on the end-to-end workflow, so exploring those options is beyond the scope<a id="_idIndexMarker1064"/> of this book.</p>
			<p>Summary</p>
			<p>In this last chapter, we explored techniques for performing data analysis and visualization using R and Python with data sourced from Excel. We began by loading and visualizing the <code>diamonds</code> dataset and the <code>ggplot2</code> and <code>plotnine</code> libraries for data visualization. Through various plots such as boxplots, mean price visualizations, and histograms, we gained insights into the relationships between different variables in the dataset.</p>
			<p>Moving on to ML modeling, we utilized the <code>healthyR</code> and the <code>scikit-learn</code> libraries to preprocess the data, including encoding categorical variables and splitting the dataset into training and testing sets. We then implemented a regression model using the XGBoost algorithm, assessing its performance using the RMSE metric.</p>
			<p>By harnessing the strengths of R, Python, and Excel, users can enhance their analytical capabilities and derive valuable insights from their data.</p>
			<p>Thank you for joining us on this journey through the exciting world of data analysis and visualization with R and Python in Excel. We hope you found the content engaging and the examples insightful. As you continue to explore and implement the knowledge gained from this book, we hope you will discover new possibilities and opportunities in your data-driven endeavors. Happy analyzing and visualizing!</p>
		</div>
	</body></html>