<html><head></head><body>
		<div id="_idContainer148">
			<h1 id="_idParaDest-219" class="chapter-number"><a id="_idTextAnchor238"/>12</h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor239"/>Data Analysis and Visualization with R and Python in Excel – A Case Study</h1>
			<p>In this final chapter, we are going<a id="_idIndexMarker1008"/> to perform an analysis—<strong class="bold">visualization</strong> and a simple model—built with data from Excel and place all those outcomes back into it. This can be useful when there is a lot of data, or the calculations themselves are best suited to being done outside <span class="No-Break">of Excel.</span></p>
			<p>First, we will start with importing our data and then performing some data exploration via visualizations. For this chapter, we are going to use the <strong class="source-inline">diamonds</strong> dataset from the R package called <strong class="source-inline">ggplot2</strong>. We will view the data where the price is the outcome and look at it via different facets of the diamond’s characteristics. After the visualizations are done, we will perform some simple modeling to predict the price of a diamond based on <span class="No-Break">its characteristics.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Getting <span class="No-Break">a visualization</span></li>
				<li>Performing a simple <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) model</span></li>
			</ul>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor240"/>Technical requirements</h1>
			<p>For this chapter, we will be using the <span class="No-Break">following packages/libraries:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">ggplot2 3.4.4</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">dplyr 1.1.4</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">healthyR 0.2.1</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">readxl 1.4.3</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">tidyverse 2.0.0</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">janitor 2.2.0</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">writexl 1.5.0</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">healthyR.ai 0.0.13</strong></span></li>
			</ul>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor241"/>Getting visualizations with R</h1>
			<p>In this section, we are going<a id="_idIndexMarker1009"/> to go over getting some visualizations of the data. We will create several visualizations and give short interpretations of the outcomes in them. For this, we will create two histograms in base R and a few different visuals using the <span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break"> library.</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor242"/>Getting the data</h2>
			<p>The first thing we need to do is load<a id="_idIndexMarker1010"/> the libraries and get the data. I am working in a directory specific to this book so I can source the function directly from the chapter I wrote the <strong class="source-inline">read_excel_sheets()?</strong> function in; your path might be different. Let’s look at the code up to <span class="No-Break">this point:</span></p>
			<pre class="source-code">
# Library Load
library(ggplot2)
library(dplyr)
library(healthyR)
library(readxl)
# Source Functions
source(paste0(getwd(),"/Chapter1/excel_sheet_reader.R"))
# Read data
file_path &lt;- paste0(getwd(), "/Chapter12/")
df &lt;- read_excel_sheets(
  filename = paste0(file_path, "diamonds_split.xlsx"),"),
  single_tbl = TRUE
)</pre>			<p>What we have done here is to simply call in a few libraries into our environment, pull in the sheet reading function, and read in our data. We loaded the <strong class="source-inline">read_excel_sheets()</strong> function into our environment using the <strong class="source-inline">source()</strong> command. You might be wondering how the data was created for this section, and it is important because it was exported from the <strong class="source-inline">ggplot2</strong> library. Here is the code if you want to re-create the data so that the preceding code will work, and the following sections<a id="_idIndexMarker1011"/> will <span class="No-Break">also work:</span></p>
			<pre class="source-code">
# Library Load
library(tidyverse)
library(writexl)
library(janitor)
# Write File to disk
file_path &lt;- paste0(getwd(), "/Chapter12/")
# Split data by cut and clean names of the list
df_list &lt;- split(diamonds, diamonds$cut) |&gt;
  clean_names()
# Write to xlsx
df_list |&gt;
  write_xlsx(paste0(file_path, "diamonds_split.xlsx"))</pre>			<p>Now that we have gone over how to produce and read in the data, let’s start taking a look at <span class="No-Break">some visuals.</span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor243"/>Visualizing the data</h2>
			<p>We are going to use two different methods<a id="_idIndexMarker1012"/> of creating graphs in this section, firstly, with base R and secondly with <strong class="source-inline">ggplot2</strong>. With that in mind, let’s <span class="No-Break">get started.</span></p>
			<h3>Base R visuals</h3>
			<p>The first thing that we are going<a id="_idIndexMarker1013"/> to do is to create some histograms<a id="_idIndexMarker1014"/> of the <strong class="source-inline">price</strong> column in the <strong class="source-inline">diamonds</strong> dataset. The price is the outcome variable we will use as the predictor in our model in the next section. First, we need to create a vector of breaks that will get passed to the histograms. There is much literature available on techniques for optimal binning strategies. The basic crux is that this will help provide the appropriate shape to the histogram that best represents the data. That is a separate topic and not one that we will pursue in this book, as it is a topic that can span a book unto itself. There is a function called <strong class="source-inline">opt_bin()</strong> from the <strong class="source-inline">healthyR</strong> package that will produce a tibble of break points for a <strong class="source-inline">value</strong> column that is passed to it. Let’s look <span class="No-Break">at it:</span></p>
			<pre class="source-code">
breaks &lt;- tibble(x = df$price) |&gt;
  opt_bin(x) |&gt;
  pull(value)
head(breaks)
[1]  326.000 1130.217 1934.435 2738.652 3542.870 4347.087</pre>			<p>The purpose of doing this is to try and capture the proper density of information in the data. The <strong class="source-inline">hist()</strong> base function does a good job of this already with a standard method. Now, let’s go ahead and create the plots and see the methods side by side. We will use <strong class="source-inline">par(mfrow = c(1, 2))</strong> so that we can plot them side <span class="No-Break">by side:</span></p>
			<pre class="source-code">
par(mfrow = c(1, 2))
hist(df$price, main = "Price Histogram - Default binning",
      xlab = "Price", ylab = "Frequency")
hist(df$price, breaks = breaks, main = "Price Histogram - Optimal binning",
      xlab = "Price", ylab = "Frequency")
par(mfrow = c(1, 1))</pre>			<p>Let’s take a look at what <span class="No-Break">it produced:</span></p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B19142_12_01.jpg" alt="Figure 12.1 – Histogram comparison between default binning and optimal binning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Histogram comparison between default binning and optimal binning</p>
			<p>We can see that the shape of the histogram is slightly different, but again, this strategy might not work for you, or you may have another strategy that you employ with regularity; this was simply a way to illustrate<a id="_idIndexMarker1015"/> that different methods do exist. That is the end of making visuals<a id="_idIndexMarker1016"/> via base R; we will now move on to <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break">.</span></p>
			<h3>Visuals with ggplot2</h3>
			<p>We are now going to make<a id="_idIndexMarker1017"/> the rest of the visuals with <strong class="source-inline">ggplot2</strong> as I find the syntax<a id="_idIndexMarker1018"/> a bit easier and the graphics one can produce are a bit more sophisticated, aside from the fact that the package is part of the <strong class="source-inline">tidyverse</strong>, which means it is interoperable with the rest of the packages in it such as <strong class="source-inline">dplyr</strong>. You may need to install the <strong class="source-inline">hexbin</strong> package as well. Let’s <span class="No-Break">get started:</span></p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = carat, y = price, fill = cut)) +
  geom_hex(bins = length(breaks), alpha = 1/5) +
  facet_wrap(~ clarity, scales = "free") +
  theme_minimal() +
  labs(
     x = "Carat",
     y = "Price",
     title = "Diamonds Data",
     fill = "Cut"
  ) +
  hr_scale_color_colorblind()</pre>			<p>Here’s a breakdown<a id="_idIndexMarker1019"/> of <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker1020"/></span><span class="No-Break"> code.</span></p>
			<p>For the data and aesthetics, this is how <span class="No-Break">it goes:</span></p>
			<ul>
				<li><strong class="source-inline">df |&gt; ggplot(...)</strong>: This starts the visualization using the data <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">aes(x = carat, y = price, fill = cut)</strong>: This defines aesthetics for <span class="No-Break">the plot:</span><ul><li><strong class="source-inline">x</strong>: The x-axis represents the <span class="No-Break">carat weight</span></li><li><strong class="source-inline">y</strong>: The y-axis represents <span class="No-Break">the price</span></li><li><strong class="source-inline">fill</strong>: The color fill represents the <span class="No-Break">diamond cut</span></li></ul></li>
			</ul>
			<p>For the hexagon geometry, this is what <span class="No-Break">we have:</span></p>
			<ul>
				<li><strong class="source-inline">geom_hex(bins = length(breaks), alpha = 1/5)</strong>: This plots hexagons representing <span class="No-Break">data points.</span></li>
				<li><strong class="source-inline">bins</strong>: This controls the number of bins for the hexagonal grid. Here, it uses the same number as defined in <strong class="source-inline">breaks</strong> (not shown in the <span class="No-Break">provided code).</span></li>
				<li><strong class="source-inline">alpha</strong>: This is the opacity of the hexagons, set to 1/5 for <span class="No-Break">better visibility.</span></li>
			</ul>
			<p>For faceting by clarity, this is what <span class="No-Break">we have:</span></p>
			<ul>
				<li><strong class="source-inline">facet_wrap(~ clarity, scales = "free")</strong>: This groups data into subplots based on diamond clarity, with independent color scales for <span class="No-Break">each plot</span></li>
			</ul>
			<p>These are the themes <span class="No-Break">and labels:</span></p>
			<ul>
				<li><strong class="source-inline">theme_minimal()</strong>: This applies a minimal theme for <span class="No-Break">cleaner visuals</span></li>
				<li><strong class="source-inline">labs(..., title = "Diamonds Data")</strong>: This adds labels for axes <span class="No-Break">and title.</span></li>
			</ul>
			<p>This is the code for the colorblind-friendly <span class="No-Break">color scale:</span></p>
			<ul>
				<li><strong class="source-inline">hr_scale_color_colorblind()</strong>: This ensures the color palette is optimized for <span class="No-Break">colorblind viewers</span></li>
			</ul>
			<p>Now, let’s check <span class="No-Break">the output.</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B19142_12_02.jpg" alt="Figure 12.2 – ggplot2 of diamonds data with hex geometry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – ggplot2 of diamonds data with hex geometry</p>
			<p>Overall, this code visualizes<a id="_idIndexMarker1021"/> the relationship between the carat<a id="_idIndexMarker1022"/> weight, price, and cut of diamonds, considering clarity groups with a colorblind-friendly <span class="No-Break">color scheme.</span></p>
			<p>The next visual we will see uses a boxplot to check the dispersion of <span class="No-Break">the data:</span></p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = carat, y = price, fill = cut)) +
  geom_boxplot(alpha = 1/5, outlier.color = "lightgrey") +
  facet_wrap(~ clarity, scales = "free") +
  theme_minimal() +
  labs(
     x = "Carat",
     y = "Price",
     title = "Diamonds Data",
     fille = "Cut"
  ) +
  hr_scale_color_colorblind()</pre>			<p>Again, let’s see<a id="_idIndexMarker1023"/> <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B19142_12_03.jpg" alt="Figure 12.3 – ggplot2 boxplot of price dispersion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – ggplot2 boxplot of price dispersion</p>
			<p>We can now look at the mean<a id="_idIndexMarker1024"/> price with a question to ponder: does it show<a id="_idIndexMarker1025"/> the information accurately? Let’s refer to the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
df |&gt;
  summarize(m = mean(price), .by = c(clarity, cut)) |&gt;
  ggplot(aes(x = clarity, y = m, group = cut, color = cut)) +
  geom_point() +
  geom_line() +
  geom_smooth() +
  facet_wrap(~cut, ncol = 2) +
  labs(x= "Clarity",
         y = "Mean Price",
         title = "Mean Price by Clarity and Cut",
         color = "Cut") +
  theme_minimal() +
  hr_scale_color_colorblind()</pre>			<p>The output of this code is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B19142_12_04.jpg" alt="Figure 12.4 – ggplot2 mean price dispersion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – ggplot2 mean price dispersion</p>
			<p>Here is another view of the mean<a id="_idIndexMarker1026"/> price, but this time by looking at the<a id="_idIndexMarker1027"/> mean price <span class="No-Break">per carat:</span></p>
			<pre class="source-code">
df |&gt;
  summarize(m = mean(price/carat), .by = c(cut, color, clarity)) |&gt;
  ggplot(aes(x = color, y = m, group = clarity, color = clarity)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ cut, ncol = 2, scales = "free") +
  labs(x= "Clarity",
         y = "Mean Price",
         title = "Mean Price per Carat by Clarity, Color and Cut",
         color = "Cut") +
  theme_minimal() +
  hr_scale_color_colorblind()</pre>			<p>Let’s see what story <span class="No-Break">this tells:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B19142_12_05.jpg" alt="Figure 12.5 – ggplot mean price per carat"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – ggplot mean price per carat</p>
			<p>These are very good diamonds – does it matter <a id="_idIndexMarker1028"/>what the cut or color<a id="_idIndexMarker1029"/> is as long as the clarity is better than fair? Seems like it <span class="No-Break">does not.</span></p>
			<p>Lastly, we will look at a histogram of price faceted by cut rather than colored by it and we are going to use the <strong class="source-inline">breaks</strong> data we created previously. See the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
df |&gt;
  ggplot(aes(x = price)) +
  geom_histogram(breaks = breaks, fill = "lightblue",
                        color = "black") +
  theme_minimal() +
  facet_wrap(~ cut, ncol = 2, scales = 'free') +
  labs(x = "Price", y = "Frequency", title = "Price Histogram by Cut")</pre>			<p>Let’s take a <span class="No-Break">last look:</span></p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B19142_12_06.jpg" alt="Figure 12.6 – Histogram of price faceted by the cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Histogram of price faceted by the cut</p>
			<p>Now that we have created<a id="_idIndexMarker1030"/> all the<a id="_idIndexMarker1031"/> visuals, we can move on to the <span class="No-Break">modeling phase.</span></p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor244"/>Performing a simple ML model with R</h1>
			<p>In this section, we are going to go over performing<a id="_idIndexMarker1032"/> a simple ML model in R. There are so many different ways to do this in R that it would be impossible for me to list them all, however, CRAN has done this so you and I don’t have to. If you want to see a task view of ML on CRAN, you can follow this <span class="No-Break">link: </span><a href="https://cran.r-project.org/view=MachineLearning"><span class="No-Break">https://cran.r-project.org/view=MachineLearning</span></a><span class="No-Break">.</span></p>
			<p>For this section, we are going to use the XGBoost algorithm as implemented by the <strong class="source-inline">healthyR.ai</strong> package. The algorithm is not written differently, the only difference is how data is saved in the output. The <strong class="source-inline">healthyR.ai</strong> package also contains a preprocessor for the XGBoost algorithm to ensure that the input data matches what the algorithm is expecting before modeling. The two main functions that we will be using are <strong class="source-inline">hai_xgboost_data_prepper()</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">hai_auto_xgboost()</strong></span><span class="No-Break">.</span></p>
			<p>We will not cover loading<a id="_idIndexMarker1033"/> the data in again as it was covered previously. Let’s <span class="No-Break">get started!</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor245"/>Data preprocessing</h2>
			<p>Before we get started, we are going<a id="_idIndexMarker1034"/> to preprocess our data so that it meets the needs of the algorithm for modeling. This is made easy by the <strong class="source-inline">hai_xgboost_data_prepper()</strong> function from the <strong class="source-inline">healthyR.ai</strong> library. We are going to see what the data looks like before and after the data is processed. Let’s see the following code and then <span class="No-Break">the output:</span></p>
			<pre class="source-code">
# Lib Load
library(healthyR.ai)
library(dplyr)
glimpse(head(df, 2))
Rows: 2
Columns: 10
$ carat   &lt;dbl&gt; 0.22, 0.86
$ cut      &lt;chr&gt; "Fair", "Fair"
$ color   &lt;chr&gt; "E", "E"
$ clarity &lt;chr&gt; "VS2", "SI2"
$ depth   &lt;dbl&gt; 65.1, 55.1
$ table   &lt;dbl&gt; 61, 69
$ price   &lt;dbl&gt; 337, 2757
$ x         &lt;dbl&gt; 3.87, 6.45
$ y         &lt;dbl&gt; 3.78, 6.33
$ z         &lt;dbl&gt; 2.49, 3.52</pre>			<p>This is the data before processing begins. We see that there are 10 columns, and we can see the datatypes of each of those columns clearly in the output. Now, let’s create a <strong class="source-inline">recipe</strong> object by passing our data into <strong class="source-inline">hai_xgboost_data_prepper()</strong> and checking the output from there. This function<a id="_idIndexMarker1035"/> takes two arguments: <strong class="source-inline">.data</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">.recipe_formula</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# Pass data through pre-processor
rec_obj &lt;- hai_xgboost_data_prepper(
  .data = df,
  .recipe_formula = price ~ .
)
rec_obj
── Recipe ───────────
── Inputs
Number of variables by role
outcome:   1
predictor: 9
── Operations
• Factor variables from: tidyselect::vars_select_helpers$where(is.character)
• Novel factor level assignment for: recipes::all_nominal_predictors()
• Dummy variables from: recipes::all_nominal_predictors()
• Zero variance filter on: recipes::all_predictors()</pre>			<p>Now, let’s look at the processed<a id="_idIndexMarker1036"/> data. We can see in the following that columns have been added and all the datatypes are now <strong class="source-inline">&lt;dbl&gt;</strong>, which is what was called for in <span class="No-Break">the preprocessor:</span></p>
			<pre class="source-code">
# Now see the juiced output
get_juiced_data(rec_obj) |&gt;
  head(2) |&gt;
  glimpse()
Rows: 2
Columns: 24
$ carat            &lt;dbl&gt; 0.22, 0.86
$ depth            &lt;dbl&gt; 65.1, 55.1
$ table            &lt;dbl&gt; 61, 69
$ x                  &lt;dbl&gt; 3.87, 6.45
$ y                  &lt;dbl&gt; 3.78, 6.33
$ z                  &lt;dbl&gt; 2.49, 3.52
$ price            &lt;dbl&gt; 337, 2757
$ cut_Good        &lt;dbl&gt; 0, 0
$ cut_Ideal      &lt;dbl&gt; 0, 0
$ cut_Premium   &lt;dbl&gt; 0, 0
$ cut_Very.Good &lt;dbl&gt; 0, 0
$ color_E         &lt;dbl&gt; 1, 1
$ color_F         &lt;dbl&gt; 0, 0
$ color_G         &lt;dbl&gt; 0, 0
$ color_H         &lt;dbl&gt; 0, 0
$ color_I         &lt;dbl&gt; 0, 0
$ color_J         &lt;dbl&gt; 0, 0
$ clarity_IF     &lt;dbl&gt; 0, 0
$ clarity_SI1   &lt;dbl&gt; 0, 0
$ clarity_SI2   &lt;dbl&gt; 0, 1
$ clarity_VS1   &lt;dbl&gt; 0, 0
$ clarity_VS2   &lt;dbl&gt; 1, 0
$ clarity_VVS1  &lt;dbl&gt; 0, 0
$ clarity_VVS2  &lt;dbl&gt; 0, 0</pre>			<p>Now that we have seen the data<a id="_idIndexMarker1037"/> after processing, let’s use the <strong class="source-inline">hai_auto_xgboost()</strong> function to perform the modeling. Here is the full function<a id="_idIndexMarker1038"/> call and documentation on it can be <span class="No-Break">at </span><a href="https://www.spsanderson.com/healthyR.ai/reference/hai_auto_xgboost.html:"><span class="No-Break">https://www.spsanderson.com/healthyR.ai/reference/hai_auto_xgboost.html:</span></a></p>
			<pre class="source-code">
hai_auto_xgboost(
  .data,
  .rec_obj,
  .splits_obj = NULL,
  .rsamp_obj = NULL,
  .tune = TRUE,
  .grid_size = 10,
  .num_cores = 1,
  .best_metric = "f_meas",
  .model_type = "classification"
)</pre>			<p>We will now create the model and check the output. I am using <strong class="source-inline">.num_cores = 10</strong>, <strong class="source-inline">.best_metric = "rsq"</strong>, and <strong class="source-inline">.model_type = "regression"</strong>, and I do not suggest you run this yourself unless you have plenty of time <span class="No-Break">to spare.</span></p>
			<p>Now, perform modeling using the <span class="No-Break"><strong class="source-inline">hai_auto_xgboost()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
auto_xgb &lt;- hai_auto_xgboost(
  .data = df,
  .rec_obj = rec_obj,
  .best_metric = "rsq",
  .num_cores = 10,
  .model_type = "regression"
)</pre>			<p>This produces a rather large object; on my machine, it is 196.1 MB, with the largest portion coming from <strong class="source-inline">$tuned_info</strong> sitting at <strong class="source-inline">169836312 bytes</strong>, which is mainly due to the <strong class="source-inline">plotly</strong> plot and the Monte Carlo<a id="_idIndexMarker1039"/> cross-validation <strong class="source-inline">tibble</strong>, due to the size of the incoming data. We can now take a look at some of the objects that <span class="No-Break">are exported:</span></p>
			<pre class="source-code">
xgb_wflw_fit &lt;- auto_xgb$model_info$fitted_wflw
class(xgb_wflw_fit)
[1] "workflow"
mod_spec &lt;- xgb_wflw_fit[["fit"]][["actions"]][["model"]][["spec"]]
mod_spec
Boosted Tree Model Specification (regression)
Main Arguments:
  trees = 817
  min_n = 17
  tree_depth = 9
  learn_rate = 0.0205081386887847
  loss_reduction = 2.0421383990836e-05
  sample_size = 0.762693894910626
Computational engine: xgboost</pre>			<p>The first thing we did was pull out the fitted workflow object, which can be used to make predictions on data using the generic <strong class="source-inline">predict()</strong> function. We know it is a <strong class="source-inline">workflow</strong> object from when we <span class="No-Break">ran </span><span class="No-Break"><strong class="source-inline">class(xgb_wflw_fit)</strong></span><span class="No-Break">.</span></p>
			<p>The final thing we do is to actually take a look at the specification of the fitted model itself. This will show us what the parameters were set to during the cross-validation process. It is important to remember that I did not use a seed, which means that you can obtain different results. This was meant to be a primer and not an exhaustive write-up of the inputs and outputs, but rather just a showcase of how an XGBoost model can be fitted to data from an Excel file, given<a id="_idIndexMarker1040"/> one cannot perform such a modeling task with the ease it was done <span class="No-Break">in R.</span></p>
			<p>Now, we can move on to the Python section, where we will follow a similar workflow for the <span class="No-Break">same dataset.</span></p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor246"/>Getting visualizations with Python</h1>
			<p>In this section, we are going<a id="_idIndexMarker1041"/> to go over visualizations of the data in Python, analogous to the preceding R section. We will use <strong class="source-inline">plotnine</strong> to have visualizations similar to those created in R using <strong class="source-inline">ggplot2</strong> and provide interpretations of <span class="No-Break">the results.</span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor247"/>Getting the data</h2>
			<p>Like in the earlier chapters, we will load<a id="_idIndexMarker1042"/> the data using <strong class="source-inline">pandas</strong>. Just like before, the path to the XLSX file may be different for you from what I have, so adjust the <span class="No-Break"><strong class="source-inline">filepath</strong></span><span class="No-Break"> accordingly:</span></p>
			<pre class="source-code">
import pandas as pd
# Define the file path (may be different for you)
file_path = "./Chapter 12/diamonds.xlsx"
# Load the dataset into a pandas DataFrame
df = pd.read_excel(file_path)
# Display the first few rows of the DataFrame
print(df.head())</pre>			<p>Note that we use the raw <strong class="source-inline">diamonds</strong> dataset without spitting it first and then recombining it, as it was done<a id="_idIndexMarker1043"/> in the R part of <span class="No-Break">the chapter.</span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor248"/>Visualizing the data</h2>
			<p>Once we have our data<a id="_idIndexMarker1044"/> loaded, we can use <strong class="source-inline">plotnine</strong> to create visualizations. In this section, we’ll demonstrate how to visualize various aspects of the <span class="No-Break"><strong class="source-inline">diamonds</strong></span><span class="No-Break"> dataset.</span></p>
			<p>Using the dataset loaded before, we can have a first look at <span class="No-Break">the data:</span></p>
			<pre class="source-code">
from plotnine import ggplot, aes, geom_bin2d, facet_wrap, theme_minimal, labs, scale_fill_manual
# Define a colorblind-friendly color palette
color_palette = ["#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"]
# Plot using plotnine
(
    ggplot(df, aes(x='carat', y='price', fill='cut')) +
    geom_bin2d(bins=20) +  # Adjust the bins parameter as needed
    facet_wrap('~ clarity', scales='free') +
    theme_minimal() +
    labs(
        x='Carat',
        y='Price',
        title='Diamonds Data',
        fill='Cut'
    ) +
    scale_fill_manual(values=color_palette)
)</pre>			<p>This Python code replicates<a id="_idIndexMarker1045"/> the R code used at the beginning of the chapter using <strong class="source-inline">plotnine</strong> for data visualization. The <strong class="source-inline">ggplot()</strong> function initializes the plot, <strong class="source-inline">aes() </strong>defines the aesthetics, <strong class="source-inline">geom_bin2d()</strong> adds the geometry, <strong class="source-inline">facet_wrap()</strong> creates facets, <strong class="source-inline">theme_minimal()</strong> sets the theme, <strong class="source-inline">labs()</strong> adds labels, and <strong class="source-inline">scale_fill_manual(values=color_palette)</strong> ensures the color palette is colorblind friendly using the <span class="No-Break">predefined </span><span class="No-Break"><strong class="source-inline">color_palette</strong></span><span class="No-Break">.</span></p>
			<p>The resulting image will look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B19142_12_07.jpg" alt="Figure 12.7 – The plotnine scatterplot of the diamonds dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – The plotnine scatterplot of the diamonds dataset</p>
			<p>As you can see, the plot shows the relationship<a id="_idIndexMarker1046"/> between carat weight and price by color-coding the cut of diamonds, using a colorblind-friendly <span class="No-Break">color scheme.</span></p>
			<p>Let’s have a look at the boxplot of the data (we will not re-import all of the <strong class="source-inline">plotnine</strong> <span class="No-Break">functions again):</span></p>
			<pre class="source-code">
from plotnine import geom_boxplot
# Plot using plotnine
(
    ggplot(df, aes(x='carat', y='price', fill='cut')) +
    geom_boxplot(alpha=1/5, outlier_color="lightgrey") +
    facet_wrap('~ clarity', scales='free') +
    theme_minimal() +
    labs(
        x='Carat',
        y='Price',
        title='Diamonds Data',
        fill='Cut'
    ) +
    scale_fill_manual(values=color_palette)
)</pre>			<p>In this code, <strong class="source-inline">geom_boxplot()</strong> is used to create boxplots. The <strong class="source-inline">outlier_color</strong> parameter is set to <strong class="source-inline">lightgrey</strong> to change the color<a id="_idIndexMarker1047"/> of outliers in <span class="No-Break">the boxplot:</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B19142_12_08.jpg" alt="Figure 12.8 – The boxplot of the diamonds dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – The boxplot of the diamonds dataset</p>
			<p>The core purpose of data visualization<a id="_idIndexMarker1048"/> remains to get insights into the data to better understand it. What if we plot the mean price? Do we see what we need <span class="No-Break">to see?</span></p>
			<p>We can use the <strong class="source-inline">groupby</strong> functionality from <strong class="source-inline">pandas</strong> to aggregate the prices, calculate the mean per group, and create a plot with points, lines, and smoothed lines to visualize the mean price by clarity <span class="No-Break">and cut:</span></p>
			<pre class="source-code">
from plotnine import geom_point, geom_line, geom_smooth, scale_color_manual
# Plot the mean price
(
    ggplot(df.groupby(['clarity', 'cut']).mean().reset_index(), aes(x='clarity', y='price', group='cut', color='cut')) +
    geom_point() +
    geom_line() +
    geom_smooth() +
    facet_wrap('~ cut', ncol=2) +
    labs(
        x='Clarity',
        y='Mean Price',
        title='Mean Price by Clarity and Cut',
        color='Cut'
    ) +
    theme_minimal() +
    scale_color_manual(values=color_palette)
)</pre>			<p>Let’s have a look at the resulting<a id="_idIndexMarker1049"/> <span class="No-Break">data vizualization:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B19142_12_09.jpg" alt="Figure 12.9 – Mean price by clarity and cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Mean price by clarity and cut</p>
			<p>For each cut, a similar curve<a id="_idIndexMarker1050"/> becomes visible: the mean price first rises by clarity and then drops. Both rise and fall are the least relevant for the <strong class="bold">Ideal</strong> clarity while they are the strongest for <strong class="bold">Premium</strong> and <strong class="bold">Very </strong><span class="No-Break"><strong class="bold">Good</strong></span><span class="No-Break"> clarities.</span></p>
			<p>Could we gain more insights from plotting the mean price in a different grouping? Let’s take a look at the mean price <span class="No-Break">per carat:</span></p>
			<pre class="source-code">
# Calculate mean price per carat by clarity, color, and cut
df_mean = df.groupby(['cut', 'color', 'clarity']).apply(lambda x: (x['price'] / x['carat']).mean()).reset_index(name='m')
# Plot using plotnine
(
        ggplot(df_mean, aes(x='color', y='m', group='clarity', color='clarity')) +
        geom_point() +
        geom_line() +
        facet_wrap('~ cut', ncol=2, scales='free') +
        labs(
                x='Clarity',
                y='Mean Price',
                title='Mean Price per Carat by Clarity, 
                Color and Cut',
                color='Cut'
        ) +
        scale_color_manual(values=color_palette)
)</pre>			<p>The resulting image indeed<a id="_idIndexMarker1051"/> shows some <span class="No-Break">interesting things:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B19142_12_10.jpg" alt="Figure 12.10 – The mean price per carat by clarity, color, and cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – The mean price per carat by clarity, color, and cut</p>
			<p>For all clarities but <strong class="bold">Fair</strong>, we see that the D color has an extreme price for the IF cut but for the others, the prices remain similar. For <strong class="bold">Fair</strong> clarity, however, the prices show a clear downward trend with the only large price difference being between D and other colors for the <span class="No-Break">I1 cut.</span></p>
			<p>Finally, before moving<a id="_idIndexMarker1052"/> on to modeling, let’s have a look at the histogram of prices <span class="No-Break">by cut:</span></p>
			<pre class="source-code">
from plotnine import geom_histogram
# Create a histogram of price by Cut
(
        ggplot(df, aes(x='price')) +
        geom_histogram(fill='lightblue', color='black') +
        theme_minimal() +
        facet_wrap('~ cut', ncol=2, scales='free') +
        labs(x='Price', y='Frequency', title='Price Histogram by Cut')
)</pre>			<p>We use default binning because, unfortunately, the great<a id="_idIndexMarker1053"/> package used for the R version, <strong class="source-inline">healthyR</strong>, is not available for <span class="No-Break">Python (yet).</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B19142_12_11.jpg" alt="Figure 12.11 – Price histogram by cut"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Price histogram by cut</p>
			<p>We can see that the price has a very long tail (that is, extremely high prices are relatively typical even though infrequent) and, surprisingly, we can see a second<a id="_idIndexMarker1054"/> high point for <strong class="bold">Good</strong> and <strong class="bold">Premium</strong> cuts (and to a lesser extent for <strong class="bold">Very Good</strong> cut <span class="No-Break">as well).</span></p>
			<p>With the data better understood thanks to the visualizations, we can start with <span class="No-Break">the modeling!</span></p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor249"/>Performing a simple ML model with Python</h1>
			<p>In this section, we create<a id="_idIndexMarker1055"/> a simple ML model in Python. Python has grown to be the primary go-to language for ML work (with R as the obvious alternative) and the number of packages implementing ML algorithms is difficult to overestimate. Having said that, <strong class="source-inline">sklearn</strong> remains the most widely used so we will also choose it for this section. Similarly to the R part of the chapter, we will use the <strong class="source-inline">xgboost</strong> model because it has a great balance between performance <span class="No-Break">and explainability.</span></p>
			<p>We will use the data loaded in the <span class="No-Break">previous section.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor250"/>Data preprocessing</h2>
			<p>The first thing to do<a id="_idIndexMarker1056"/> for the modeling phase is to prepare the data. Fortunately, <strong class="source-inline">sklearn</strong> comes with a preprocessing <span class="No-Break">functionality built-in!</span></p>
			<p>Let’s review the steps involved in <span class="No-Break">data preprocessing:</span></p>
			<ul>
				<li><strong class="bold">Handling missing values</strong>: Before training a model, it’s essential to address missing values in the dataset. <strong class="source-inline">sklearn</strong> provides methods for imputing missing values or removing rows/columns with <span class="No-Break">missing data.</span></li>
				<li><strong class="bold">Feature scaling</strong>: Many ML algorithms perform better when features are on the same scale. <strong class="source-inline">sklearn</strong> offers utilities for scaling features, including standardization (scaling features to have zero mean and unit variance) and normalization (scaling features to a <span class="No-Break">specified range).</span></li>
				<li><strong class="bold">Encoding categorical variables</strong>: ML algorithms typically require numerical inputs, so categorical variables need to be encoded into numerical representations. <strong class="source-inline">sklearn</strong> provides methods for one-hot encoding categorical variables or encoding them using <span class="No-Break">ordinal labels.</span></li>
				<li><strong class="bold">Splitting data</strong>: Before training a model, it’s essential to split the dataset into training and testing sets to evaluate the model’s performance. <strong class="source-inline">sklearn</strong> offers functions for splitting datasets into training and testing sets with <span class="No-Break">specified proportions.</span></li>
				<li><strong class="bold">Feature engineering</strong>: <strong class="source-inline">sklearn</strong> supports various feature engineering techniques, such as polynomial<a id="_idIndexMarker1057"/> features generation, interaction terms creation, and dimensionality<a id="_idIndexMarker1058"/> reduction using techniques such as <strong class="bold">principal component </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PCA</strong></span><span class="No-Break">).</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">It is important to do the feature engineering in a way that doesn’t pollute the training data with information from the test data, just like for data cleaning (such <span class="No-Break">as imputation).</span></p>
			<p>We have covered data cleaning to a great extent in the dedicated chapter, so we will make use of the fact that the <strong class="source-inline">diamonds</strong> dataset is clean already. We will move on to feature scaling and the encoding of categorical <span class="No-Break">variables instead:</span></p>
			<pre class="source-code">
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import numpy as np
# Encode categorical variables
encoder = OneHotEncoder()
df_encoded = encoder.fit_transform(df[['cut', 'color', 'clarity']])
# Scale numerical features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['carat', 'depth', 'table', 
    'x', 'y', 'z']])
# Concatenate encoded categorical features with scaled numerical features
df_processed = np.concatenate((df_encoded.toarray(), df_scaled),
    axis=1)
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    df_processed, df["price"], test_size=0.2, random_state=42)</pre>			<p>This code snippet demonstrates<a id="_idIndexMarker1059"/> how to encode categorical variables (cut, color, and clarity) using one-hot encoding and scale numerical features using <strong class="source-inline">StandardScaler</strong> from <strong class="source-inline">sklearn</strong>. Then, it concatenates the encoded categorical features with scaled numerical features and splits the dataset into training and testing sets <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">train_test_split()</strong></span><span class="No-Break">.</span></p>
			<p>Let’s compare the data before and after <span class="No-Break">the preprocessing.</span></p>
			<p>The original dataset looks <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B19142_12_12.jpg" alt="Figure 12.12 – Raw data as read from Excel"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Raw data as read from Excel</p>
			<p>As you can see, the dataset contains<a id="_idIndexMarker1060"/> a mix of numerical and categorical variables (the latter ones will be encoded using <span class="No-Break">one-hot encoding).</span></p>
			<p>After the preprocessing, the dataset looks <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B19142_12_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – Preprocessed training data</p>
			<p>The preprocessed training data shown earlier has fewer rows (the rest makes up the test data) but more columns: while the <strong class="source-inline">price</strong> column is not present (it’s the variable we want to predict), the categorical variables have been replaced by multiple <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> values – the result of one-hot encoding. For each unique value of each categorical variable, a new column has been introduced that has <strong class="source-inline">1</strong> if the original dataset had that value and <strong class="source-inline">0</strong> if it had <span class="No-Break">something else.</span></p>
			<p>The <strong class="source-inline">y_train</strong> variable has the value of the <strong class="source-inline">price</strong> column for each row in the <span class="No-Break">train data.</span></p>
			<p>With the preprocessed data, we can start <span class="No-Break">the modeling:</span></p>
			<pre class="source-code">
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
# Instantiate XGBoost regressor
xgb_reg = GradientBoostingRegressor(random_state=42)
# Train the model
xgb_reg.fit(X_train, y_train)
# Predict on the test set
y_pred = xgb_reg.predict(X_test)
# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error:", rmse)</pre>			<p>In this code, we observe <span class="No-Break">the following:</span></p>
			<ol>
				<li>We import <strong class="source-inline">GradientBoostingRegressor</strong> <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">sklearn.ensemble</strong></span><span class="No-Break">.</span></li>
				<li>We instantiate<a id="_idIndexMarker1061"/> a gradient boosting regressor (<strong class="source-inline">xgb_reg</strong>) from <span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break">’s implementation.</span></li>
				<li>We train the model using the <strong class="source-inline">fit</strong> method with the training data (<strong class="source-inline">X_train</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">y_train</strong></span><span class="No-Break">).</span></li>
				<li>We make predictions on the test<a id="_idIndexMarker1062"/> set using the <strong class="source-inline">predict</strong> method and calculate the <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>) (<strong class="source-inline">RMSE</strong>) between the predicted values (<strong class="source-inline">y_pred</strong>) and the actual target <span class="No-Break">values (</span><span class="No-Break"><strong class="source-inline">y_test</strong></span><span class="No-Break">).</span></li>
			</ol>
			<p>The RMSE is a widely used metric in regression analysis that measures the average magnitude of the errors between predicted values and observed values. It provides a single numerical value to assess the goodness of fit of a regression model. RMSE is on the same scale (units) as the target <span class="No-Break">variable (</span><span class="No-Break"><strong class="source-inline">price</strong></span><span class="No-Break">).</span></p>
			<p>Lower values of RMSE indicate that the model’s predictions are closer to the actual values, implying better performance. In other words, a lower RMSE signifies that the model has a smaller average deviation from the true values, which indicates higher accuracy and a better <span class="No-Break">predictive capability.</span></p>
			<p>The RMSE is particularly useful because it considers the magnitude of errors and penalizes larger errors more heavily than smaller ones. Therefore, minimizing the RMSE leads to a model that provides more precise and <span class="No-Break">accurate predictions.</span></p>
			<p>Overall, the RMSE serves as a valuable tool for comparing different regression models and assessing their predictive accuracy in <span class="No-Break">real-world applications.</span></p>
			<p>The model result has an RMSE of around <strong class="source-inline">720</strong>, which is significantly lower than the average price (<strong class="source-inline">3933</strong>) and the standard deviation of the <strong class="source-inline">price</strong> variable (<strong class="source-inline">3989</strong>). This is good news, indeed, as it indicates the model fit was <span class="No-Break">quite good.</span></p>
			<p>Of course, you can consider<a id="_idIndexMarker1063"/> other ML models (random forests, <strong class="source-inline">lightgbm</strong> or <strong class="source-inline">catgbm</strong>, or even <strong class="bold">deep learning models</strong>) and other goodness-of-fit metrics (R<span class="superscript">2</span>, MAE, etc.). This section is intended to be a primer on the end-to-end workflow, so exploring those options is beyond the scope<a id="_idIndexMarker1064"/> of <span class="No-Break">this book.</span></p>
			<p><span class="No-Break">Summary</span></p>
			<p>In this last chapter, we explored techniques for performing data analysis and visualization using R and Python with data sourced from Excel. We began by loading and visualizing the <strong class="source-inline">diamonds</strong> dataset and the <strong class="source-inline">ggplot2</strong> and <strong class="source-inline">plotnine</strong> libraries for data visualization. Through various plots such as boxplots, mean price visualizations, and histograms, we gained insights into the relationships between different variables in <span class="No-Break">the dataset.</span></p>
			<p>Moving on to ML modeling, we utilized the <strong class="source-inline">healthyR</strong> and the <strong class="source-inline">scikit-learn</strong> libraries to preprocess the data, including encoding categorical variables and splitting the dataset into training and testing sets. We then implemented a regression model using the XGBoost algorithm, assessing its performance using the <span class="No-Break">RMSE metric.</span></p>
			<p>By harnessing the strengths of R, Python, and Excel, users can enhance their analytical capabilities and derive valuable insights from <span class="No-Break">their data.</span></p>
			<p>Thank you for joining us on this journey through the exciting world of data analysis and visualization with R and Python in Excel. We hope you found the content engaging and the examples insightful. As you continue to explore and implement the knowledge gained from this book, we hope you will discover new possibilities and opportunities in your data-driven endeavors. Happy analyzing <span class="No-Break">and visualizing!</span></p>
		</div>
	</body></html>