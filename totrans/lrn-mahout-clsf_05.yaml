- en: Chapter 5. Learning the Hidden Markov Model Using Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover one of the most interesting topics of classification
    techniques: the **Hidden Markov Model** (**HMM**). To understand the HMM, we will
    cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic and nondeterministic patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the HMM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Mahout for the HMM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic and nondeterministic patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a deterministic system, each state is solely dependent on the state it was
    previously in. For example, let's take the case of a set of traffic lights. The
    sequence of lights is red → green → amber → red. So, here we know what state will
    follow after the current state. Once the transitions are known, deterministic
    systems are easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'For nondeterministic patterns, consider an example of a person named Bob who
    has his snacks at 4:00 P.M. every day. Let''s say he has any one of the three
    items from the menu: ice cream, juice, or cake. We cannot say for sure what item
    he will have the next day, even if we know what he had today. This is an example
    of a nondeterministic pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: The Markov process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Markov process, the next state is dependent on the previous states.
    If we assume that we have an *n* state system, then the next state is dependent
    on the previous *n* states. This process is called an *n* model order. In the
    Markov process, we make the choice for the next state probabilistically. So, considering
    our previous example, if Bob had juice today, he can have juice, ice cream, or
    cake the next day. In the same way, we can reach any state in the system from
    the previous state. The Markov process is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Markov process](img/4959OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we have *n* states in a process, then we can reach any state with n2 transitions.
    We have a probability of moving to any state, and hence, we will have n2 probabilities
    of doing this. For a Markov process, we will have the following three items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: This refers to the states in the system. In our example, let''s
    say there are three states: state 1, state 2, and state 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition matrix**: This will have the probabilities of moving from one
    state to any other state. An example of the transition matrix is shown in the
    following screenshot:![The Markov process](img/4959OS_05_02.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This matrix shows that if the system was in state 1 yesterday, then the probability
    of it to remain in the same state today will be 0.1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Initial state vector**: This is the vector of the initial state of the system.
    (Any one of the states will have a probability of 1 and the rest will have a probability
    of 0 in this vector.)![The Markov process](img/4959OS_05_03.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Hidden Markov Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Hidden Markov Model** (**HMM**) is a classification technique to predict
    the states of a system by observing the outcomes without having access to the
    actual states themselves. It is a Markov model in which the states are hidden.
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with Bob's snack example we saw earlier. Now assume we have one
    more set of events in place that is directly observable. We know what Bob has
    eaten for lunch and his snacks intake is related to his lunch. So, we have an
    observation state, which is Bob's lunch, and hidden states, which are his snacks
    intake. We want to build an algorithm that can forecast what would be Bob's choice
    of snack based on his lunch.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the Hidden Markov Model](img/4959OS_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to the transition probability matrix in the Hidden Markov Model,
    we have one more matrix that is called an **emission matrix**. This matrix contains
    the probability of the observable state, provided it is assigned a hidden state.
    The emission matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P (observable state | one state)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, a Hidden Markov Model has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State vector**: This contains the probability of the hidden model to be in
    a particular state at the start'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition matrix**: This has the probabilities of a hidden state, given
    the previous hidden state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emission matrix**: Given that the hidden model is in a particular hidden
    state, this has the probabilities of observing a particular observable state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden states**: This refers to the states of the system that can be defined
    by the Hidden Markov Model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observable state**: The states that are visible in the process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the Hidden Markov Model, three types of problems can be solved. The first
    two are related to the pattern recognition problem and the third type of problem
    generates a Hidden Markov Model, given a sequence of observations. Let''s look
    at these three types of problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation**: This is finding out the probability of an observed sequence,
    given an HMM. From the number of different HMMs that describe different systems
    and a sequence of observations, our goal will be to find out which HMM will most
    probably generate the required sequence. We use the forward algorithm to calculate
    the probability of an observation sequence when a particular HMM is given and
    find out the most probable HMM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoding**: This is finding the most probable sequence of hidden states from
    some observations. We use the Viterbi algorithm to determine the most probable
    sequence of hidden states when you have a sequence of observations and an HMM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning**: Learning is generating the HMM from a sequence of observations.
    So, if we have such a sequence, we may wonder which is the most likely model to
    generate this sequence. The forward-backward algorithms are useful in solving
    this problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hidden Markov Model is used in different applications such as speech recognition,
    handwritten letter recognition, genome analysis, parts of speech tagging, customer
    behavior modeling, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout for the Hidden Markov Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Mahout has the implementation of the Hidden Markov Model. It is available
    in the `org.apache.mahout.classifier.sequencelearning.hmm` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall implementation is provided by eight different classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HMMModel`: This is the main class that defines the Hidden Markov Model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HmmTrainer`: This class has algorithms that are used to train the Hidden Markov
    Model. The main algorithms are supervised learning, unsupervised learning, and
    unsupervised Baum-Welch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HmmEvaluator`: This class provides different methods to evaluate an HMM model.
    The following use cases are covered in this class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a sequence of output states from a model (prediction)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the likelihood that a given model will generate the given sequence
    of output states (model likelihood)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the most likely hidden sequence for a given model and a given observed
    sequence (decoding)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HmmAlgorithms`: This class contains implementations of the three major HMM
    algorithms: forward, backward, and Viterbi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HmmUtils`: This is a utility class and provides methods to handle HMM model
    objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomSequenceGenerator`: This is a command-line tool to generate a sequence
    by the given HMM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BaumWelchTrainer`: This is the class to train HMM from the console.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ViterbiEvaluator`: This is also a command-line tool for Viterbi evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's work with Bob's example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the given matrix and the initial probability vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ice cream | Cake | Juice |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.36 | 0.51 | 0.13 |'
  prefs: []
  type: TYPE_TB
- en: 'The following will be the state transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Ice cream | Cake | Juice |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Ice cream** | 0.365 | 0.500 | 0.135 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cake** | 0.250 | 0.125 | 0.625 |'
  prefs: []
  type: TYPE_TB
- en: '| **Juice** | 0.365 | 0.265 | 0.370 |'
  prefs: []
  type: TYPE_TB
- en: 'The following will be the emission matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Spicy food | Normal food | No food |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Ice cream** | 0.1 | 0.2 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cake** | 0.5 | 0.25 | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| **Juice** | 0.80 | 0.10 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: 'Now we will execute a command-line-based example of this problem. We have three
    hidden states of what Bob''s eaten for snacks: ice-cream, cake, or juice. Then,
    we have three observable states of what he is having at lunch: spicy food, normal
    food, or no food at all. Now, the following are the steps to execute from the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory with the name `hmm: mkdir /tmp/hmm`. Go to this directory
    and create the sample input file of the observed states. This will include a sequence
    of Bob''s lunch habit: spicy food (state 0), normal food (state 1), and no food
    (state 2). Execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the BaumWelch algorithm to train the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The parameters used in the preceding command are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`i`: This is the input file location'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`o`: This is the output location for the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nh`: This is the number of hidden states. In our example, it is three (ice
    cream, juice, or cake)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no`: This is the number of observable states. In our example, it is three
    (spicy, normal, or no food)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`e`: This is the epsilon number. This is the convergence threshold value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m`: This is the maximum iteration number'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output on executing the previous command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Mahout for the Hidden Markov Model](img/4959OS_05_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now we have an HMM model that can be used to build a predicted sequence. We
    will run the model to predict the next 15 states of the observable sequence using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The parameters used in the preceding command are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`m`: This is the path for the HMM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`o`: This is the output directory path'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`l`: This is the length of the generated sequence'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To view the prediction for the next 10 observable states, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the previous command is shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Mahout for the Hidden Markov Model](img/4959OS_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: From the output, we can say that the next observable states for Bob's lunch
    will be spicy, spicy, spicy, normal, normal, no food, no food, no food, no food,
    and no food.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will use one more algorithm to predict the hidden state. We will use
    the Viterbi algorithm to predict the hidden states for a given observational state''s
    sequence. We will first create the sequence of the observational state using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use the Viterbi command-line option to generate the output with the
    likelihood of generating this sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The parameters used in the preceding command are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input`: This is the input location of the file'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output`: This is the output location of the Viterbi algorithm''s output'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: This is the HMM model location that we created earlier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`likelihood`: This is the computed likelihood of the observed sequence'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output on executing the previous command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Mahout for the Hidden Markov Model](img/4959OS_05_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Predictions from the Viterbi are saved in the output file and can be seen using
    the `cat` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output shows the predictions for the hidden state:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Mahout for the Hidden Markov Model](img/4959OS_05_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed another classification technique: the Hidden
    Markov Model. You learned about deterministic and nondeterministic patterns. We
    also touched upon the Markov process and Hidden Markov process in general. We
    checked the classes implemented inside Mahout to support the Hidden Markov Model.
    We took up an example to create the HMM model and further used this model to predict
    the observational state''s sequence. We used the Viterbi algorithm implemented
    in Mahout to predict the hidden states in the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in the next chapter, we will cover one more interesting algorithm used
    in classification area: Random forest.'
  prefs: []
  type: TYPE_NORMAL
