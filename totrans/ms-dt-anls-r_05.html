<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Building Models (authored by Renata Nemeth and Gergely Toth)"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Building Models (authored by Renata Nemeth and Gergely Toth)</h1></div></div></div><div class="blockquote"><blockquote class="blockquote"><p>"All models should be as simple as possible... but no simpler."</p><p>                                                                                – Attributed to Albert Einstein</p><p>"All models are wrong... but some are useful."</p><p>                                                                                                           – George Box</p></blockquote></div><p>After loading and transforming data, in this chapter, we will focus on how to build <a class="indexterm" id="id352"/>statistical models. Models are representations of reality, and, as the preceding citations emphasize, are always simplified representations. Although you can't possibly take everything into account, you should be aware about what to include and exclude in a good model that provides meaningful results.</p><p>In this chapter, regression models are discussed on the basis of linear regression models<a class="indexterm" id="id353"/> and standard modeling. <span class="strong"><strong>Generalized Linear Models</strong></span> (<span class="strong"><strong>GLM</strong></span>)<a class="indexterm" id="id354"/> extend these to allow the response variables to differ in distribution, which will be covered in the <a class="link" href="ch06.html" title="Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)">Chapter 6</a>, <span class="emphasis"><em>Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</em></span>. In all, we will discuss the three most well known regression models:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Linear regression</strong></span> for <a class="indexterm" id="id355"/>continuous outcomes (birth weight measured in grams)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Logistic regression</strong></span> for <a class="indexterm" id="id356"/>binary outcomes (low birth weight versus normal birth weight)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Poisson regression</strong></span> <a class="indexterm" id="id357"/>for count data (number of low birth weight infants per year or per country)</li></ul></div><p>Although there are many other regression models, such <a class="indexterm" id="id358"/>as <span class="emphasis"><em>Cox-regression</em></span> which we will not discuss here, the logic in the building of the models and the interpretation are similar. So, after reading this chapter, you will be able to understand those without doubt.</p><p>By the end of this chapter, you will learn the most important things about regression models: how to avoid<a class="indexterm" id="id359"/> confounding, how to fit, how to interpret, and how to choose the best model among the many different options.</p><div class="section" title="The motivation behind multivariate models"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>The motivation behind multivariate models</h1></div></div></div><p>If you would like to<a class="indexterm" id="id360"/> measure the strength of association<a class="indexterm" id="id361"/> between a response and a predictor, you can choose a simple two-way association measure, such as correlation<a class="indexterm" id="id362"/> or the<a class="indexterm" id="id363"/> odds ratio, depending on the nature of your data. But, if your aim is to model a complex mechanism by taking into account other predictors as well, you will need regression models.</p><p>As Ben Goldacre, the evidence-based columnist for <span class="emphasis"><em>The Guardian</em></span>, tells in his brilliant TED talk that the strong association between olive oil consumption and young looking skin does not imply that olive oil is beneficial to our skin. When modeling a complex association structure, we should also <a class="indexterm" id="id364"/>control for other predictors, such as smoking status or physical activity, because those who consume more olive oil are more likely to live a healthy life in general, so it may not be the olive oil itself that prevents skin wrinkles. In short, it seems that the kind of lifestyle is likely to confound the relationship between the variables of interest, making it appear that there might be causality, when in fact there is none.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note41"/>Note</h3><p>A confounder<a class="indexterm" id="id365"/> is a third variable that biases (increases or decreases) the association we are interested in. The confounder is always associated with both the response<a class="indexterm" id="id366"/> and the<a class="indexterm" id="id367"/> predictor.</p></div></div><p>If we examine the olive oil and skin wrinkles association again by fixing the smoking status, hence building separate models for smokers and non-smokers, the association may vanish. Holding the confounders fixed is the main idea behind controlling confounding<a class="indexterm" id="id368"/> via regression models.</p><p>Regression models in general are intended to measure associations between a response and a predictor by controlling for others. Potential confounders are entered into the model as predictors, and the regression coefficient of the predictor (the <span class="emphasis"><em>partial coefficient</em></span>) measures the effect adjusted to the confounders.</p></div></div>
<div class="section" title="Linear regression with continuous predictors"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Linear regression with continuous predictors</h1></div></div></div><p>Let's start with an actual and <a class="indexterm" id="id369"/>illuminating example of confounding. Consider that we would like to predict the amount of air pollution based on the size of the city (measured in population size as thousand of habitants). Air pollution is measured by the sulfur dioxide (SO2) concentration in the air, in milligrams per cubic meter. We will use the US air pollution data set (Hand and others 1994) from<a class="indexterm" id="id370"/> the <code class="literal">gamlss.data</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(gamlss.data)</strong></span>
<span class="strong"><strong>&gt; data(usair)</strong></span>
</pre></div><div class="section" title="Model interpretation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec38"/>Model interpretation</h2></div></div></div><p>Let's draw our very first<a class="indexterm" id="id371"/> linear regression model by <a class="indexterm" id="id372"/>building a formula. The <code class="literal">lm</code> function from the <code class="literal">stats</code> package<a class="indexterm" id="id373"/> is used to fit linear models, which is an important tool for regression modeling:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.0 &lt;- lm(y ~ x3, data = usair)</strong></span>
<span class="strong"><strong>&gt; summary(model.0)</strong></span>

<span class="strong"><strong>Residuals:</strong></span>
<span class="strong"><strong>    Min      1Q  Median      3Q     Max </strong></span>
<span class="strong"><strong>-32.545 -14.456  -4.019  11.019  72.549 </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>             Estimate Std. Error t value Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept) 17.868316   4.713844   3.791 0.000509 ***</strong></span>
<span class="strong"><strong>x3           0.020014   0.005644   3.546 0.001035 ** </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>Residual standard error: 20.67 on 39 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:  0.2438,    Adjusted R-squared:  0.2244 </strong></span>
<span class="strong"><strong>F-statistic: 12.57 on 1 and 39 DF,  p-value: 0.001035</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note42"/>Note</h3><p>Formula notation<a class="indexterm" id="id374"/> is one of the best features of R, which lets you define flexible models in a human-friendly way. A typical model has the form of <code class="literal">response ~ terms</code>, where <code class="literal">response</code> is the continuous response variable, and <code class="literal">terms</code> provides one or a series of numeric variables that specifies a linear predictor for the response.</p></div></div><p>In the preceding <a class="indexterm" id="id375"/>example, the variable, <code class="literal">y</code>, denotes air pollution, while <code class="literal">x3</code> stands for the population size. The coefficient of <code class="literal">x3</code> says that a one unit (one thousand) increase in the population size causes a <code class="literal">0.02</code> <a class="indexterm" id="id376"/>unit (0.02 milligram per cubic meter) increase in the sulfur dioxide concentration, and the effect is statistically significant<a class="indexterm" id="id377"/> with a <code class="literal">p</code> value of <code class="literal">0.001035</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>See more details on the<a class="indexterm" id="id378"/> p-value in the <span class="emphasis"><em>How well does the line fit to the data?</em></span> section. To keep it simple for now, we will refer to models as statistically significant when the <span class="emphasis"><em>p</em></span> value is below <code class="literal">0.05</code>.</p></div></div><p>The intercept in general is the value of the response variable when each predictor equals to 0, but in this example, there are no cities without inhabitants, so the intercept (17.87) doesn't have a direct interpretation. The two regression coefficients <a class="indexterm" id="id379"/>define the <a class="indexterm" id="id380"/>regression line:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plot(y ~ x3, data = usair, cex.lab = 1.5)</strong></span>
<span class="strong"><strong>&gt; abline(model.0, col = "red", lwd = 2.5)</strong></span>
<span class="strong"><strong>&gt; legend('bottomright', legend = 'y ~ x3', lty = 1, col = 'red',</strong></span>
<span class="strong"><strong>+   lwd = 2.5, title = 'Regression line')</strong></span>
</pre></div><div class="mediaobject"><img alt="Model interpretation" src="graphics/2028OS_04_01.jpg"/></div><p>As you can see, the intercept (<span class="strong"><strong>17.87</strong></span>) is the value at which the regression line<a class="indexterm" id="id381"/> crosses <a class="indexterm" id="id382"/>the y-axis. The other coefficient (<span class="strong"><strong>0.02</strong></span>) is the slope of the regression line: it measures how steep the line is. Here, the function runs uphill because the slope is positive (<span class="strong"><strong>y</strong></span> increases as <span class="strong"><strong>x3</strong></span> increases). Similarly, if the slope is negative, the function runs downhill.</p><p>You can easily <a class="indexterm" id="id383"/>understand the way the estimates were obtained if you realize how the line was drawn. This is the line that best fits the data points. Here, we refer to the <span class="emphasis"><em>best fit</em></span> as the linear <a class="indexterm" id="id384"/>least-squares approach, which is why the model is also known as the <a class="indexterm" id="id385"/>
<span class="strong"><strong>ordinary least squares</strong></span> (<span class="strong"><strong>OLS</strong></span>) regression.</p><p>The least-squares method finds the best fitting line by minimizing the sum of the squares of the residuals, where the residuals represent the error, which is the difference between the observed value (an original dot in the scatterplot) and the fitted or predicted value<a class="indexterm" id="id386"/> (a dot on the line with the same <span class="emphasis"><em>x</em></span>-value):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; usair$prediction &lt;- predict(model.0)</strong></span>
<span class="strong"><strong>&gt; usair$residual&lt;- resid(model.0)</strong></span>
<span class="strong"><strong>&gt; plot(y ~ x3, data = usair, cex.lab = 1.5)</strong></span>
<span class="strong"><strong>&gt; abline(model.0, col = 'red', lwd = 2.5)</strong></span>
<span class="strong"><strong>&gt; segments(usair$x3, usair$y, usair$x3, usair$prediction,</strong></span>
<span class="strong"><strong>+   col = 'blue', lty = 2)</strong></span>
<span class="strong"><strong>&gt; legend('bottomright', legend = c('y ~ x3', 'residuals'),</strong></span>
<span class="strong"><strong>+   lty = c(1, 2), col = c('red', 'blue'), lwd = 2.5,</strong></span>
<span class="strong"><strong>+   title = 'Regression line')</strong></span>
</pre></div><div class="mediaobject"><img alt="Model interpretation" src="graphics/2028OS_04_02.jpg"/></div><p>The <span class="emphasis"><em>linear</em></span> term <a class="indexterm" id="id387"/>in linear regression refers to<a class="indexterm" id="id388"/> the fact that we are interested in a linear relation, which is more natural, easier to understand, and simpler to handle mathematically, as compared to the more complex methods.</p></div><div class="section" title="Multiple predictors"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec39"/>Multiple predictors</h2></div></div></div><p>On the other hand, if we<a class="indexterm" id="id389"/> aim to model a more <a class="indexterm" id="id390"/>complex mechanism by separating the effect of the population size from the effect of the presence of industries, we have to control for the variable, <code class="literal">x2</code>, which describes the number of manufacturers employing more than 20 workers. Now, we can either create a new model by <code class="literal">lm(y ~ x3 + x2, data = usair)</code>, or use the <code class="literal">update</code> function to refit the previous model:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.1 &lt;- update(model.0, . ~ . + x2)</strong></span>
<span class="strong"><strong>&gt; summary(model.1)</strong></span>

<span class="strong"><strong>Residuals:</strong></span>
<span class="strong"><strong>    Min      1Q  Median      3Q     Max </strong></span>
<span class="strong"><strong>-22.389 -12.831  -1.277   7.609  49.533 </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>            Estimate Std. Error t value Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept) 26.32508    3.84044   6.855 3.87e-08 ***</strong></span>
<span class="strong"><strong>x3          -0.05661    0.01430  -3.959 0.000319 ***</strong></span>
<span class="strong"><strong>x2           0.08243    0.01470   5.609 1.96e-06 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>Residual standard error: 15.49 on 38 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:  0.5863,    Adjusted R-squared:  0.5645 </strong></span>
<span class="strong"><strong>F-statistic: 26.93 on 2 and 38 DF,  p-value: 5.207e-08</strong></span>
</pre></div><p>Now, the<a class="indexterm" id="id391"/> coefficient<a class="indexterm" id="id392"/> of <code class="literal">x3</code> is <code class="literal">-0.06</code>! While the crude association between air pollution and city size was positive in the previous model, after controlling for the number of manufacturers, the association<a class="indexterm" id="id393"/> becomes negative. This means that a one thousand increase in the population decreases the SO2 concentration by 0.06 unit, which is a statistically significant<a class="indexterm" id="id394"/> effect.</p><p>On first sight, this change of <a class="indexterm" id="id395"/>sign from positive to negative may be surprising, but it is rather plausible after a closer look; it's definitely not the population size, but rather the level of industrialization that affects the air pollution directly. In the first model, population size showed a positive effect because it implicitly measured industrialization as well. When we hold industrialization fixed, the effect of the population size becomes negative, and growing a city with a fixed industrialization level spreads the air pollution in a wider range.</p><p>So, we can conclude that <code class="literal">x2</code> is a confounder here, as it biases the association between <code class="literal">y</code> and <code class="literal">x3</code>. Although it is beyond the scope of our current research question, we can interpret the coefficient of <code class="literal">x2</code> as well. It says that holding the city size at a constant level, a one unit increase in the number of manufacturers increases the SO2 concentration by 0.08 mgs.</p><p>Based on the model, we can predict the expected value of the response for any combination of predictors. For example, we can predict the expected level of sulfur dioxide concentration for a city with 400,000 habitants and 150 manufacturers, each of whom employ more than 20 workers:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; as.numeric(predict(model.1, data.frame(x2 = 150, x3 = 400)))</strong></span>
<span class="strong"><strong>[1] 16.04756</strong></span>
</pre></div><p>You could also calculate the prediction by yourself, multiplying the values with the slopes, and then summing them up with the constant—all these numbers are simply copied and pasted from the previous model summary:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; -0.05661 * 40</strong></span>
<span class="strong"><strong>0 + 0.08243 * 150 + 26.32508 [1] 16.04558</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note44"/>Note</h3><p>Prediction outside the range of the data is known as <a class="indexterm" id="id396"/>extrapolation. The further the values are from the data, the riskier your prediction becomes. The problem is that you cannot check model assumptions (for example, linearity) outside of your sample data.</p></div></div><p>If you have two <a class="indexterm" id="id397"/>predictors, the regression <a class="indexterm" id="id398"/>line is represented by a surface in the three dimensional space, which can be easily shown via the<a class="indexterm" id="id399"/> <code class="literal">scatterplot3d</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(scatterplot3d)</strong></span>
<span class="strong"><strong>&gt; plot3d &lt;- scatterplot3d(usair$x3, usair$x2, usair$y, pch = 19,</strong></span>
<span class="strong"><strong>+   type = 'h', highlight.3d = TRUE, main = '3-D Scatterplot') </strong></span>
<span class="strong"><strong>&gt; plot3d$plane3d(model.1, lty = 'solid', col = 'red')</strong></span>
</pre></div><div class="mediaobject"><img alt="Multiple predictors" src="graphics/2028OS_04_03.jpg"/></div><p>As it's rather hard to interpret this plot, let's draw the 2-dimensional projections of this 3D graph, which might prove to be more informative after all. Here, the value of the third, non-presented variable is held at zero:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; par(mfrow = c(1, 2))</strong></span>
<span class="strong"><strong>&gt; plot(y ~ x3, data = usair, main = '2D projection for x3')</strong></span>
<span class="strong"><strong>&gt; abline(model.1, col = 'red', lwd = 2.5)</strong></span>
<span class="strong"><strong>&gt; plot(y ~ x2, data = usair, main = '2D projection for x2')</strong></span>
<span class="strong"><strong>&gt; abline(lm(y ~ x2 + x3, data = usair), col = 'red', lwd = 2.5)</strong></span>
</pre></div><div class="mediaobject"><img alt="Multiple predictors" src="graphics/2028OS_04_04.jpg"/></div><p>According to the<a class="indexterm" id="id400"/> changed sign of the <a class="indexterm" id="id401"/>slope, it's well worth mentioning that the <span class="emphasis"><em>y-x3</em></span> regression line has also changed; from uphill, it became downhill.</p></div></div>
<div class="section" title="Model assumptions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Model assumptions</h1></div></div></div><p>Linear regression models with <a class="indexterm" id="id402"/>standard estimation techniques make a number of assumptions about the outcome variable, the predictor variables, and also about their relationship:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>Y</em></span> is a continuous variable (not binary, nominal, or ordinal)</li><li class="listitem">The errors (the residuals) are <a class="indexterm" id="id403"/>statistically independent</li><li class="listitem">There is a stochastic<a class="indexterm" id="id404"/> linear relationship <a class="indexterm" id="id405"/>between <span class="emphasis"><em>Y</em></span> and each <span class="emphasis"><em>X</em></span></li><li class="listitem"><span class="emphasis"><em>Y</em></span> has a normal distribution, holding each <span class="emphasis"><em>X</em></span> fixed</li><li class="listitem"><span class="emphasis"><em>Y</em></span> has the same variance, regardless of the fixed value of the <span class="emphasis"><em>X</em></span>s</li></ol></div><p>A violation of assumption <span class="strong"><strong>2</strong></span> occurs in <a class="indexterm" id="id406"/>trend analysis, if we use time as the predictor. Since the consecutive years are not independent, the errors will not be independent from each other. For example, if we have a year with high mortality from a specific illness, then we can expect the mortality for the next year to also be high.</p><p>A violation of assumption (<span class="strong"><strong>3</strong></span>) says that the relationship is not exactly linear, but there is a deviation from the linear trend line. Assumption <span class="strong"><strong>4</strong></span> and <span class="strong"><strong>5</strong></span> require the conditional distribution of <span class="emphasis"><em>Y</em></span> to be normal and having the same <a class="indexterm" id="id407"/>variance, regardless of the fixed value of <span class="emphasis"><em>X</em></span>s. They are needed for inferences of the regression (confidence intervals, <span class="emphasis"><em>F</em></span>- and <span class="emphasis"><em>t</em></span>-tests). Assumption <span class="strong"><strong>5</strong></span> is known as the homoscedasticity<a class="indexterm" id="id408"/> assumption. If it is violated, heteroscedasticity<a class="indexterm" id="id409"/> holds.</p><p>The following plot helps in visualizing these assumptions with a simulated dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(Hmisc)</strong></span>
<span class="strong"><strong>&gt; library(ggplot2)</strong></span>
<span class="strong"><strong>&gt; library(gridExtra)</strong></span>
<span class="strong"><strong>&gt; set.seed(7)</strong></span>
<span class="strong"><strong>&gt; x  &lt;- sort(rnorm(1000, 10, 100))[26:975]</strong></span>
<span class="strong"><strong>&gt; y  &lt;- x * 500 + rnorm(950, 5000, 20000)</strong></span>
<span class="strong"><strong>&gt; df &lt;- data.frame(x = x, y = y, cuts = factor(cut2(x, g = 5)),</strong></span>
<span class="strong"><strong>+                               resid = resid(lm(y ~ x)))</strong></span>
<span class="strong"><strong>&gt; scatterPl &lt;- ggplot(df, aes(x = x, y = y)) +</strong></span>
<span class="strong"><strong>+    geom_point(aes(colour = cuts, fill = cuts), shape = 1,</strong></span>
<span class="strong"><strong>+  show_guide = FALSE) + geom_smooth(method = lm, level = 0.99)</strong></span>
<span class="strong"><strong>&gt; plot_left &lt;- ggplot(df,  aes(x = y, fill = cuts)) +</strong></span>
<span class="strong"><strong>+    geom_density(alpha = .5) + coord_flip() + scale_y_reverse()</strong></span>
<span class="strong"><strong>&gt; plot_right &lt;- ggplot(data = df, aes(x = resid, fill = cuts)) +</strong></span>
<span class="strong"><strong>+    geom_density(alpha = .5) + coord_flip()</strong></span>
<span class="strong"><strong>&gt; grid.arrange(plot_left, scatterPl, plot_right,</strong></span>
<span class="strong"><strong>+    ncol=3, nrow=1, widths=c(1, 3, 1))</strong></span>
</pre></div><div class="mediaobject"><img alt="Model assumptions" src="graphics/2028OS_5_05.jpg"/></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>The code bundle, available to be downloaded from the Packt Publishing homepage, includes a slightly longer code chunk for the preceding plot with some tweaks on the plot margins, legends, and titles. The preceding code block focuses on the major parts of the visualization, without wasting too much space in the printed book on the style details.</p></div></div><p>We will discuss in <a class="indexterm" id="id410"/>more detail, how to assess the model assumptions in <a class="link" href="ch09.html" title="Chapter 9. From Big to Small Data">Chapter 9</a>, <span class="emphasis"><em>From Big to Smaller Data</em></span>. If some of the assumptions fail, a possible solution is to look for outliers. If you have an outlier, do the regression analysis without that observation, and determine how the results differ. Ways of outlier detection will be discussed in more detail in <a class="link" href="ch08.html" title="Chapter 8. Polishing Data">Chapter 8</a>, <span class="emphasis"><em>Polishing Data</em></span>.</p><p>The following example illustrates that dropping an outlier (observation number 31) may make the assumptions valid. To quickly verify if a model's assumptions are satisfied, use the<a class="indexterm" id="id411"/> <code class="literal">gvlma</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(gvlma)</strong></span>
<span class="strong"><strong>&gt; gvlma(model.1)</strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>(Intercept)           x3           x2  </strong></span>
<span class="strong"><strong>   26.32508     -0.05661      0.08243  </strong></span>

<span class="strong"><strong>ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS</strong></span>
<span class="strong"><strong>USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:</strong></span>
<span class="strong"><strong>Level of Significance =  0.05 </strong></span>

<span class="strong"><strong>                     Value  p-value                   Decision</strong></span>
<span class="strong"><strong>Global Stat        14.1392 0.006864 Assumptions NOT satisfied!</strong></span>
<span class="strong"><strong>Skewness            7.8439 0.005099 Assumptions NOT satisfied!</strong></span>
<span class="strong"><strong>Kurtosis            3.9168 0.047805 Assumptions NOT satisfied!</strong></span>
<span class="strong"><strong>Link Function       0.1092 0.741080    Assumptions acceptable.</strong></span>
<span class="strong"><strong>Heteroscedasticity  2.2692 0.131964    Assumptions acceptable.</strong></span>
</pre></div><p>It seems that three out of the<a class="indexterm" id="id412"/> five assumptions are not satisfied. However, if we build the very same model on the same dataset excluding the 31st observation, we get much better results:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.2 &lt;- update(model.1, data = usair[-31, ])</strong></span>
<span class="strong"><strong>&gt; gvlma(model.2)</strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>(Intercept)           x3           x2  </strong></span>
<span class="strong"><strong>   22.45495     -0.04185      0.06847  </strong></span>

<span class="strong"><strong>ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS</strong></span>
<span class="strong"><strong>USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:</strong></span>
<span class="strong"><strong>Level of Significance =  0.05 </strong></span>

<span class="strong"><strong>                    Value p-value                Decision</strong></span>
<span class="strong"><strong>Global Stat        3.7099  0.4467 Assumptions acceptable.</strong></span>
<span class="strong"><strong>Skewness           2.3050  0.1290 Assumptions acceptable.</strong></span>
<span class="strong"><strong>Kurtosis           0.0274  0.8685 Assumptions acceptable.</strong></span>
<span class="strong"><strong>Link Function      0.2561  0.6128 Assumptions acceptable.</strong></span>
<span class="strong"><strong>Heteroscedasticity 1.1214  0.2896 Assumptions acceptable.</strong></span>
</pre></div><p>This suggests that we must always exclude the 31st observation from the dataset when building regression models<a class="indexterm" id="id413"/> in the future sections.</p><p>However, it's important to note that it is not acceptable to drop an observation just because it is an<a class="indexterm" id="id414"/> outlier. Before you decide, investigate the particular case. If it turns out that the outlier is due to incorrect data, you should drop it. Otherwise, run the analysis, both with and without it, and <a class="indexterm" id="id415"/>state in your research report how the results changed and why you decided on excluding<a class="indexterm" id="id416"/> the extreme values.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>You can fit a line for any set of data points; the least squares method will find the optimal solution, and the trend line will be interpretable. The regression coefficients and the R-squared<a class="indexterm" id="id417"/> coefficient are also meaningful, even if the model assumptions fail. The assumptions are only needed if you want to interpret the p-values, or if you aim to make good predictions.</p></div></div></div>
<div class="section" title="How well does the line fit in the data?"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>How well does the line fit in the data?</h1></div></div></div><p>Although we know that the trend <a class="indexterm" id="id418"/>line is the best fitting among the possible linear trend lines, we <a class="indexterm" id="id419"/>don't know how well this fits the actual data. The significance of the regression parameters is obtained by testing the null hypothesis, which states that the given parameter equals to zero. The <a class="indexterm" id="id420"/>
<span class="emphasis"><em>F-test</em></span> in the output pertains to the hypothesis that each regression parameter is zero. In a nutshell, it tests the significance of the regression in general. A <span class="emphasis"><em>p-value</em></span> below 0.05 can be interpreted as "the regression line is significant." Otherwise, there is not much point in fitting the regression model at all.</p><p>However, even if you have a significant F-value, you cannot say too much about the fit of the regression line. We have seen that residuals<a class="indexterm" id="id421"/> characterize the error of the fit. The R-squared coefficient summarizes them into a single measure. <span class="emphasis"><em>R-squared</em></span> is the proportion of the variance in the response variable explained by the regression. Mathematically, it is defined as the variance in the predicted <span class="emphasis"><em>Y</em></span> values, divided by the variance in the observed <span class="emphasis"><em>Y</em></span> values.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note46"/>Note</h3><p>In some cases, despite the significant <a class="indexterm" id="id422"/>F-test, the predictors, according to the R-squared, explain only a small proportion (&lt;10 percent) of the total variance. You can interpret this by saying that although the predictors have a statistically significant<a class="indexterm" id="id423"/> effect on the response, the response is formed by a mechanism that is much more complex than your model suggests. This phenomenon is common in the area of medicine or biology where complex biological processes are modeled, while it is less common in the area of econometrics, where macro-level, aggregated variables, which usually smooth out small variations in the data.</p></div></div><p>If we use the population size as the only predictor in our air pollution example, the R-squared equals 0.37, so we can say that 37 percent of the variation in SO2 concentration can be explained by the size of the city:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.0 &lt;- update(model.0, data = usair[-31, ])</strong></span>
<span class="strong"><strong>&gt; summary(model.0)[c('r.squared', 'adj.r.squared')]</strong></span>
<span class="strong"><strong>$r.squared</strong></span>
<span class="strong"><strong>[1] 0.3728245</strong></span>
<span class="strong"><strong>$adj.r.squared</strong></span>
<span class="strong"><strong>[1] 0.3563199</strong></span>
</pre></div><p>After adding the number of manufacturers to the model, the R-squared increases dramatically and almost doubles its previous value:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(model.2)[c('r.squared', 'adj.r.squared')]</strong></span>
<span class="strong"><strong>$r.squared</strong></span>
<span class="strong"><strong>[1] 0.6433317</strong></span>
<span class="strong"><strong>$adj.r.squared</strong></span>
<span class="strong"><strong>[1] 0.6240523</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>It's important to note here that every time you add an extra predictor to your model, the R-squared increases simply because you have more information to predict the response, even if the lastly added predictor doesn't have an important effect. Consequently, a model with more predictors may appear to have a better fit just because it is bigger.</p></div></div><p>The solution is to use <a class="indexterm" id="id424"/>the <a class="indexterm" id="id425"/>adjusted R-squared, which takes into account the number of<a class="indexterm" id="id426"/> predictors as well. In the previous example, not only the R-squared but also the adjusted R-squared showed a huge advantage in favor of the latter model.</p><p>The two previous models are nested, which means that the extended model contains each predictor of the first one. But unfortunately, the adjusted R-squared cannot be used as a base for choosing the best model for non-nested models. If you have non-nested models, you can use the<a class="indexterm" id="id427"/> <span class="strong"><strong>Akaike Information Criterion</strong></span> (<span class="strong"><strong>AIC</strong></span>) measure to select the best model.</p><p>AIC is founded on the information theory. It introduces a penalty term for the number of parameters in the model, giving a solution for the problem of bigger models tending to show as better fitted. When using this criterion, you should select the model with the least AIC. As a rule of thumb, two models are essentially indistinguishable if the difference between their AICs is less than 2. In the example that follows, we have two plausible alternative models. Taking<a class="indexterm" id="id428"/> the AIC into account, <code class="literal">model.4</code> is better than <code class="literal">model.3</code>, as its <a class="indexterm" id="id429"/>advantage over <code class="literal">model.3</code> is about 10:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(model.3 &lt;- update(model.2, .~. -x2 + x1))$coefficients </strong></span>
<span class="strong"><strong>             Estimate   Std. Error   t value     Pr(&gt;|t|)</strong></span>
<span class="strong"><strong>(Intercept) 77.429836 19.463954376  3.978114 3.109597e-04</strong></span>
<span class="strong"><strong>x3           0.021333  0.004221122  5.053869 1.194154e-05</strong></span>
<span class="strong"><strong>x1          -1.112417  0.338589453 -3.285444 2.233434e-03</strong></span>

<span class="strong"><strong>&gt; summary(model.4 &lt;- update(model.2, .~. -x3 + x1))$coefficients </strong></span>
<span class="strong"><strong>               Estimate   Std. Error   t value     Pr(&gt;|t|)</strong></span>
<span class="strong"><strong>(Intercept) 64.52477966 17.616612780  3.662723 7.761281e-04</strong></span>
<span class="strong"><strong>x2           0.02537169  0.003880055  6.539004 1.174780e-07</strong></span>
<span class="strong"><strong>x1          -0.85678176  0.304807053 -2.810899 7.853266e-03</strong></span>

<span class="strong"><strong>&gt; AIC(model.3, model.4)</strong></span>
<span class="strong"><strong>        df      AIC</strong></span>
<span class="strong"><strong>model.3  4 336.6405</strong></span>
<span class="strong"><strong>model.4  4 326.9136</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note48"/>Note</h3><p>Note that AIC can tell nothing about the quality of the model in an absolute sense; your best model may still fit poorly. It does not provide a test for testing model fit either. It is essentially for ranking different models.</p></div></div></div>
<div class="section" title="Discrete predictors"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Discrete predictors</h1></div></div></div><p>So far, we have seen only the<a class="indexterm" id="id430"/> simple case of both the response and the predictor variables being continuous. Now, let's generalize the model a bit, and enter a discrete predictor into the model. Take the <code class="literal">usair</code> data and add <code class="literal">x5</code> (precipitation: average number of wet days per year) as a predictor with three categories (low, middle, and high levels of precipitation), using 30 and 45 as the cut-points. The research question is how these precipitation groups are associated with the SO2 concentration. The association<a class="indexterm" id="id431"/> is not necessary linear, as the following plot shows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plot(y ~ x5, data = usair, cex.lab = 1.5)</strong></span>
<span class="strong"><strong>&gt; abline(lm(y ~ x5, data = usair), col = 'red', lwd = 2.5, lty = 1)</strong></span>
<span class="strong"><strong>&gt; abline(lm(y ~ x5, data = usair[usair$x5&lt;=45,]),</strong></span>
<span class="strong"><strong>+   col = 'red', lwd = 2.5, lty = 3)</strong></span>
<span class="strong"><strong>&gt; abline(lm(y ~ x5, data = usair[usair$x5 &gt;=30, ]),</strong></span>
<span class="strong"><strong>+   col = 'red', lwd = 2.5, lty = 2)</strong></span>
<span class="strong"><strong>&gt; abline(v = c(30, 45), col = 'blue', lwd = 2.5)</strong></span>
<span class="strong"><strong>&gt; legend('topleft', lty = c(1, 3, 2, 1), lwd = rep(2.5, 4),</strong></span>
<span class="strong"><strong>+   legend = c('y ~ x5', 'y ~ x5 | x5&lt;=45','y ~ x5 | x5&gt;=30',</strong></span>
<span class="strong"><strong>+     'Critical zone'), col = c('red', 'red', 'red', 'blue'))</strong></span>
</pre></div><div class="mediaobject"><img alt="Discrete predictors" src="graphics/2028OS_04_06.jpg"/></div><p>The cut-points 30 and 45<a class="indexterm" id="id432"/> were more or less ad hoc. An advanced way to define optimal cut-points is to use a regression tree. There are various implementations of classification trees in R; a commonly used function is <code class="literal">rpart</code> from the package with the very same name. The regression tree follows an iterative process that splits the data into partitions, and then continues splitting each partition into smaller groups. In each step, the algorithm selects the best split on the continuous precipitation scale, where the best point minimizes the sum of the squared deviations from the group-level SO2 mean:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(partykit)</strong></span>
<span class="strong"><strong>&gt; library(rpart)</strong></span>
<span class="strong"><strong>&gt; plot(as.party(rpart(y ~ x5, data = usair)))</strong></span>
</pre></div><div class="mediaobject"><img alt="Discrete predictors" src="graphics/2028OS_04_07.jpg"/></div><p>The interpretation of the <a class="indexterm" id="id433"/>preceding result is rather straightforward; if we are looking for two groups that differ highly regarding SO2, the optimal cut-point is a precipitation level of 45.34, and if we are looking for three groups, then we will have to split the second group by using the cut-point of 30.91, and so on. The four box-plots describe the SO2 distribution in the four partitions. So, these results confirm our previous assumption, and we have three precipitation groups that strongly differ in their level of SO2 concentration.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>Take a look at <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <span class="emphasis"><em>Classification and Clustering</em></span>, for more details and examples on decisions trees.</p></div></div><p>The following scatterplot also shows that the three groups differ heavily from each other. It seems that the SO2 concentration is highest in the middle group, and the two other groups are very similar:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; usair$x5_3 &lt;- cut2(usair$x5, c(30, 45))</strong></span>
<span class="strong"><strong>&gt; plot(y ~ as.numeric(x5_3), data = usair, cex.lab = 1.5,</strong></span>
<span class="strong"><strong>+   xlab = 'Categorized annual rainfall(x5)', xaxt = 'n')</strong></span>
<span class="strong"><strong>&gt; axis(1, at = 1:3, labels = levels(usair$x5_3))</strong></span>
<span class="strong"><strong>&gt; lines(tapply(usair$y, usair$x5_3, mean), col='red', lwd=2.5, lty=1)</strong></span>
<span class="strong"><strong>&gt; legend('topright', legend = 'Linear prediction', col = 'red')</strong></span>
</pre></div><div class="mediaobject"><img alt="Discrete predictors" src="graphics/2028OS_04_08.jpg"/></div><p>Now, let us refit our linear<a class="indexterm" id="id434"/> regression model by adding the three-category precipitation to the predictors. Technically, this goes by adding two dummy variables (learn more about this type of variable in <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <span class="emphasis"><em>Classification and Clustering</em></span>) pertaining to the second and third group, as shown in the table that follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th colspan="2" style="text-align: center" valign="bottom">
<p>Dummy variables</p>
</th></tr><tr><th style="text-align: left" valign="bottom">
<p>Categories</p>
</th><th style="text-align: left" valign="bottom">
<p>first</p>
</th><th style="text-align: left" valign="bottom">
<p>second</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>low (0-30)</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>middle (30-45)</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>high (45+)</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><p>In R, you can run this model using the <code class="literal">glm</code> (Generalized Linear Models) function, because the classic linear regression doesn't allow non-continuous predictors:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(glmmodel.1 &lt;- glm(y ~ x2 + x3 + x5_3, data = usair[-31, ]))</strong></span>
<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-26.926   -4.780    1.543    5.481   31.280  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                  Estimate Std. Error t value Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept)       14.07025    5.01682   2.805  0.00817 ** </strong></span>
<span class="strong"><strong>x2                 0.05923    0.01210   4.897 2.19e-05 ***</strong></span>
<span class="strong"><strong>x3                -0.03459    0.01172  -2.952  0.00560 ** </strong></span>
<span class="strong"><strong>x5_3[30.00,45.00) 13.08279    5.10367   2.563  0.01482 *  </strong></span>
<span class="strong"><strong>x5_3[45.00,59.80]  0.09406    6.17024   0.015  0.98792    </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for gaussian family taken to be 139.6349)</strong></span>

<span class="strong"><strong>    Null deviance: 17845.9  on 39  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance:  4887.2  on 35  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 317.74</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 2</strong></span>
</pre></div><p>The second <a class="indexterm" id="id435"/>group (wet days between 30 and 45) has a higher average by 15.2 units of SO2, as compared to the first group. This is controlled by the population size and number of manufacturers. The difference is statistically significant.</p><p>On the contrary, the third group shows only a slight difference when compared to the first group (0.04 unit lower), which is not significant. The three group mean shows a reversed U-shaped curve. Note that if you used precipitation in its original continuous form, implicitly you would assume a linear relation, so you wouldn't discover this shape. Another important thing to note is that the U-shaped curve here describes the partial association (controlled for <code class="literal">x2</code> and <code class="literal">x3</code>), but the crude association, presented on the preceding scatterplot, showed a very similar picture.</p><p>The regression coefficients <a class="indexterm" id="id436"/>were interpreted as the difference between the group means, and both groups were compared to the omitted category (the first one). This is why the omitted category is usually referred to as the <a class="indexterm" id="id437"/>reference category. This way of entering discrete predictors is called reference-category coding. In general, if you have a discrete predictor with <span class="emphasis"><em>n</em></span> categories, you have to define (<span class="emphasis"><em>n-1</em></span>) dummies. Of course, if other contrasts are of interest, you can easily modify the model by entering dummies referring to other (<span class="emphasis"><em>n-1</em></span>) categories.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note49"/>Note</h3><p>If you fit linear regression with discrete predictors, the regression slopes are the differences in the group means. If you also have other predictors, then the group-mean differences will be controlled for these predictors. Remember, the key feature of multivariate regression models is that they model partial two-way associations, holding the other predictors fixed.</p></div></div><p>You can go further <a class="indexterm" id="id438"/>by entering any other types and any number of predictors. If you have an ordinal predictor, it is your decision whether to enter it in its original form, assuming a linear relation, or to form dummies and enter each of them, allowing any type of relation. If you have no background knowledge on how to make this decision, you can try both solutions and compare how the models fit.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Summary</h1></div></div></div><p>This chapter introduced the concept of how to build and interpret basic models, such as linear regression models. By now, you should be familiar with the motivation behind linear regression models; you should know how to control for confounders, how to enter discrete predictors, how to fit models in R, and how to interpret the results.</p><p>In the next chapter, we will extend this knowledge with generalized models, and analyzing the model fit.</p></div></body></html>