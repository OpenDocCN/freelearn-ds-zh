- en: Chapter 2. Understanding OpenCL Data Transfer and Partitioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Creating OpenCL buffer objects
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL buffer objects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating OpenCL sub-buffer objects
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL sub-buffer objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding events and event-synchronization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copying data between memory objects
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using work items to partition data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to explore how to invoke the OpenCL's data transfer
    APIs, query memory objects, and data/work partitioning between the GPUs and CPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that not all OpenCL SDKs support the compilation and execution on both
    GPUs and CPUs. AMD's OpenCL implementation supports its own AMD and Intel CPUs
    and GPUs; NVIDIA supports its GPUs and Intel supports its own Intel Core CPUs
    and Intel HD Graphics. Check with the vendor for supported devices.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: In the **Open Computing Language** (**OpenCL**) development, you would inevitably
    need data to be processed, and the standard does not permit you to manipulate
    memory objects directly as you would do when you program in C or C++, because
    the data memory in the host is ultimately transferred to the devices in a heterogeneous
    environment for processing, and previously you would use the programming constructs
    in various libraries or languages to access them which is one of the reasons why
    OpenCL came about; hence to unify these approaches, the standard added abstractions
    to shield the developer from these concerns.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: With respect to data types, there are a few you need to be aware of other than
    the one-dimensional data buffer. OpenCL buffer objects can be used to load and
    store two/three-dimensional data. The next data type in OpenCL is the `image`
    object; these objects are used to store two or three dimensional images (we won't
    cover much of using the `image` objects in this book).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenCL 1.1 new data transfer capabilities includes the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Using sub-buffer objects to distribute regions of a buffer across multiple OpenCL
    devices
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3-component vector data types
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the global work offset which enables kernels to operate on different portions
    of the NDRange—global work offset refers to the data points in the input data
    where work items can start processing
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading, writing, or copying a 1D, 2D or 3D rectangular region of a buffer object
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating OpenCL buffer objects
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood the need to create or wrap our host's
    memory objects into an abstraction that OpenCL can operate on, and in this recipe
    we'll explore how to create a particular type of memory object defined in the
    specification that is commonly used for general purpose computation—buffer object.
    The developer can choose to create a one, two or three dimensional memory object
    that best fits the computational model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Creating buffer objects is simple in OpenCL and is akin to the way in which
    you would use C's memory allocation routines such as `malloc` and `alloca`. But,
    that's where the similarity ends for the reason that OpenCL cannot operate directly
    on memory structures created by those routines. What you can do is to create a
    memory structure that lives on the devices that can be mapped to the memory on
    the host and the data is transferred to the device by issuing memory transfer
    commands to the command queue (which you recall is the conduit to the device).
    What you need to decide is the sort of objects, and how much of these objects
    you would like the device to compute.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we''re going to learn how to create buffer objects based on
    user-defined structures also known as `structs` in the C/C++ language. Before
    that, let''s understand the API:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can create a buffer by specifying which `context` it should attach to (recall
    that contexts can be created with several devices), specify the size of the data,
    and where to reference it with `size` and `host_ptr` respectively, specify how
    memory is to be allocated and whether that memory is to be of type read, read-only,
    read-write, or write only via `flags`; lastly capture the resultant error code
    in `errcode_ret`. Note that `clCreateBuffer` doesn't queue the command to conduct
    the memory transfer from host to device memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a portion of the code from `Ch2/user_buffer/user_buffer.c` where you
    will see how to use the `clCreateBuffer` API to allocate memory for a user-defined
    structure. The problem we are trying to solve in this example is to send a million
    user-defined structures to the device for computation. The computation encapsulated
    by the kernel is a simple one—sum of all elements of each user-structure. The
    astute reader would have noticed we could have demonstrated this data structure
    with a vector data type in OpenCL, `int4`; the reason why we didn''t do it that
    way is a two fold: (a) it''s an example of application domain modeling, (b) because
    in a few paragraphs from current we wanted to illustrate how you could use the
    data type alignment construct, and don''t fret over the data types now because
    we''ll dive into the various data types in the next chapter. Continuing further,
    the user-defined structure is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What you will need to do is to create a buffer on the host application using
    standard C/C++ dynamic/static memory allocation techniques such as `new`, `malloc`,
    and `alloca`. Next, you will need to initialize that data buffer, and finally
    you will have to invoke `clCreateBuffer` and you should make sure it's done prior
    to the call to `clSetKernelArg`; recall that we mentioned that kernels get scheduled
    for execution on the device, well before it executes the kernel code on the device
    it would need data and values to work against, and you can achieve this by an
    invocation to `clSetKernelArg` and you typically do this when the buffer object
    is created.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'The API `clSetKernelArg` looks like the following code and it''ll be important
    for you to understand how it works:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The kernel can take no arguments or at least one and probably more arguments,
    and how you configure them is simple. The following code snippet should complete
    the story:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Therefore, the kernel arguments are configured programmatically with the understanding
    that if the kernel function has `n` arguments then the `arg_index` would range
    from `0` to (`n – 1`).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe from `Ch2/user_buffer/user_buffer.c`,
    with the highlighted commentary:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'On OSX, you would compile the program by running the following command on your
    terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On the Ubuntu Linux 12.04 with Intel OpenCL SDK, the command will be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On the Ubuntu Linux 12.04 with AMD APP SDK v2.8, the command will be as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Regardless of the platform, a binary executable `user_buffer` would be deposited
    locally.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running the application on both platforms, we would get the following result:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works…
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application created a million of the `UserData` objects on the host. Refer
    to the following code snippet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The application then sends it to the device for computation after the program
    and kernel objects have been initialized, and we assign the recently created `UDObj`
    memory object to the kernel as its argument. Refer to the following code snippet:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we issue a kernel execution command to the command-queue, `cQ`, and the
    code will run against the device, the following code snippet demonstrates the
    enqueuing of the kernel:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After that''s done, the data in the device''s memory is read back and we indicated
    that we wish to read the data back until the device has completed its execution
    by passing `CL_TRUE` to indicate blocking read which otherwise could result in
    partial data read back; finally the data is verified, demonstrated by the following
    code snippet:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's explore how we used `clCreateBuffer` further.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, you would want to allocate memory on the device as read-only
    when it comes to providing input to the device and because you want to be sure
    nothing else is writing to the data store. Therefore, the flag `CL_MEM_READ_ONLY`
    is passed, but if your input data was meant to be readable and writable then you
    would need to indicate it using `CL_MEM_READ_WRITE`. Notice that we actually created
    a data store on the host via `ud_in` and, we wanted our OpenCL memory object to
    be the same size as `ud_in` and the `C` statement reflects this; finally we wanted
    OpenCL to know that the new memory object is to copy its values from `ud_in` and
    we provided the flag `CL_MEM_COPY_HOST_PTR` too, and we use the bitwise `OR` operator
    that is represented on the standard US keyboard as a pipe symbol, *|*, to merge
    these two flags.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, you can visualize it to be an 1D-array-of-structs for short or
    an array-of-structures in general.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '| `UserData` | `UserData` | `UserData` | …………………………………………… | `UserData` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Provide the same declaration of the application data type to the OpenCL kernel
    file (`*.cl`) as well as the host application files (`*.c`, `*.h`, `*.cpp`, `*.hpp`);
    else the OpenCL runtime will emit errors to reflect that the struct it is looking
    for does not exist, and the replication is necessary as OpenCL prohibits the `C`
    header file inclusion mechanism.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Let's spend some time to understand the C `struct` we just used in this example.
    The C structure we just used, `UserData`, is an example of an application data
    type. OpenCL makes no requirement about the alignment of OpenCL data types outside
    of buffers and images; hence developers of OpenCL need to make sure the data is
    properly aligned. Fortunately, OpenCL has provided attribute qualifiers so that
    we can annotate our types, functions and variables to suit the algorithm and CPU/GPU
    architecture with the primary motivation being to improve memory bandwidth. The
    alignment needs to be a power of two and at least a perfect multiple of the lowest
    common multiple of all the alignments of all the members of the `struct` or `union`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to Section 6.11.1 Specifiying Attributes of Types in the OpenCL 1.2 specification
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at what is available to developers when it comes to aligning
    data types such as `enum`, `struct`, or `union`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Data alignment is a direct result of how various computer systems restrict the
    allowable addresses for the primitive data types, requiring that the address for
    some type of object must be a multiple of some value *K* (typically 2, 4, or 8),
    and this actually simplifies the design of the hardware between the processor
    and the memory system. For example, if the processor were to always fetch 8 bytes
    from memory with an address that must be a multiple of 8, then the value can be
    read or written in a single memory operation otherwise, the processor needs to
    perform two or more memory accesses.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Alignment is enforced by making sure that every data type is organized and allocated
    in such a way that every object within the type satisfies its alignment restrictions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an example for this illustration. Following is the generic manner
    in which alignment can be defined for application data type such as `UserData`.
    While examining the code, you will notice that without the `aligned` attribute,
    this data structure will be allocated on a 17-byte boundary assuming `int` is
    4-bytes and `char` is 1-byte on a 32-bit / 64-bit system architecture. Once this
    attribute is included, following is the alignment:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The alignment is now determined by the OpenCL compiler to be aligned to 32-bytes
    instead of 17-bytes, that is, summing all the struct member''s sizes, and the
    specification designates the alignment size to be the largest power of 2 and therefore
    it is 25 because, the 24 is 1-byte too many; however if you were to change the
    previous alignment to the following alignment:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then the alignment will be at least 8-bytes as shown in the following code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Equivalently, you can also write in more explicit form as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In general, the golden rule of designing the data to be memory aligned is still
    a necessary practice; a rule of thumb I keep in mind is 16-byte aligned for 128-bit
    access and 32-byte aligned for 256-bit access.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other side of the story, you may find yourself wishing that the alignment
    wasn''t that large, and with OpenCL you can indicate that by using the `packed`
    attribute as in the following code assuming that `LargeUserData` is an imaginary
    large data structure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When you apply this attribute to a `struct` or `union`, you're effectively applying
    the attribute to every member of the data; applying to an `enum` means that the
    OpenCL compiler will select the smallest integral type found on that architecture.
    You can refer to the `Ch2/user_buffer_alignment/user_buffer_align.c` to review
    what's done and how to profile the performance of the application via AMD APP
    SDK in the `readme.txt` file.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL buffer objects
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To retrieve information about a buffer or sub-buffer object, you''ll need to
    use the API `clGetMemObjectInfo` and its signature as in the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To query the memory object, simply pass the object to `memobj` specifying the
    type of information you want in `param_name`, inform OpenCL the size of the returned
    information in `param_value_size` and where to deposit it in `param_value`; the
    last parameter, `param_value_size_ret`, is largely optional but it returns the
    size of the value in `param_value_size`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an excerpt from the code in `Ch2/buffer_query/buffer_query.c` where
    it shows how to extract the information about the memory object, `UDObj` is encapsulated
    into a user-defined function `displayBufferDetails` because, the code can be long
    depending on how many attributes you wish to extract about a memory object and
    you would place the invocation to this function after you''ve created the buffer
    object or if you have been given a handle to the memory object. The following
    code illustrates how it would display the information about a memory object by
    abstracting the OpenCL memory retrieval APIs into the function `displayBufferDetails`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How to do it…
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, as shown in the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'On OSX, you will compile the program by running the following command on your
    terminal:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK, the command will be as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'On Ubuntu Linux 12.04 with AMD APP SDK v2.8, the command will be as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Regardless of the platform, a binary executable `buffer_query` would be deposited
    locally.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the program on an OSX 10.6 and Ubuntu 12.04 with AMD APP SDK v2.7
    would present the following result:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works…
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The host application proceeds to first create the buffer that it will send
    to the device, then the application queries for information about the buffer.
    The full list of attributes that can be queried is as shown in the following table:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '| cl_mem_info | Return type | Info. Returned in param_value |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_TYPE` | `cl_mem_object_type` | It returns `CL_MEM_OBJECT_BUFFER`
    if `memobj` is created with `clCreateBuffer` or `clCreateSubBuffer`. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| `Cl_MEM_FLAGS` | `cl_mem_flags` | It returns the flags argument specified
    when `memobj` is created with `clCreateBuffer`, `clCreateSubBuffer`, `clCreateImage2D`,
    or `clCreateImage3D`. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_SIZE` | `size_t` | It returns the actual size of the data associated
    with `memobj` in bytes. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_HOST_PTR` | `void*` | If `memobj` is created with `clCreateBuffer`
    or `clCreateImage2d`, `clCreateImage3D`, then it returns the `host_ptr` argument
    specified when `memobj` is created.If `memobj` is created with `clCreateSubBuffer`,
    then it returns the `host_ptr` plus `origin` specified when `memobj` was created.See
    `clCreateBuffer` for what `host_ptr` is. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_MAP_COUNT` | `cl_uint` | Map count. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_REFERENCE_COUNT` | `cl_uint` | It returns `memobj`''s reference count.
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_CONTEXT` | `cl_context` | It returns the context specified when the
    memory is created. If `memobj` is created using `clCreateSubBuffer`, the context
    associated with the memory object specified as the `buffer` argument to `clCreateSubBuffer`
    is returned. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_ASSOCIATED_MEMOBJECT` | `cl_mem` | It return memory object from which
    `memobj` is created.In `clCreateSubBuffer`, it returns the `buffer` argument;
    else NULL is returned. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_OFFSET` | `size_t` | Applicable to `memobj` created via `clCreateSubBuffer`.
    It returns offset or 0. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: Creating OpenCL sub-buffer objects
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sub-buffers are incredibly useful data types and as you continue to explore
    OpenCL in this chapter, you'll notice that this data type can be used to partition
    the data and distribute them across your OpenCL devices on your platform.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of this writing, sub-buffer support is not enabled on OpenCL delivered
    in the OSX 10.6, because the official version is OpenCL 1.0\. However, if you
    have OSX 10.7 then you'll be able to run this code without any problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the method signature and examine it:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The argument `buffer` refers to the buffer you created via `clCreateBuffer`,
    the `flags` argument refers to the options you wish this offer to have and if
    it''s zero then the default option is `CL_MEM_READ_WRITE`; this flag can adopt
    any values from the previous table. The argument `bufferType` is of a data structure:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Therefore, you indicate where to start creating the region via the `origin`
    argument and how large it is going to be via the `size` argument.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *How to do it...* section of this recipe there is an excerpt from `Ch2/sub_buffers/sub_buffer.c`
    where we create two sub-buffer objects and each of them holds one-half of the
    data; these two sub-buffers will be sent to each OpenCL device on my setup, and
    they''re computed and results are checked. Conceptually, here''s what the code
    is doing:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/sub_buffers.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe as shown in the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**/* Chop up the data evenly between all devices & create sub-buffers */**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**/* Let OpenCL know that the kernel is suppose to receive an argument */**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As noted earlier, this application doesn''t work on OSX 10.6 and hence to compile
    it using the AMD APP SDK, you will enter the following command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the Intel OpenCL SDK, you will enter the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For NVIDIA on Ubuntu Linux 12.04, you will enter the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Regardless of the platform, a binary executable `sub_buffer` would be deposited
    locally.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'In the setup I have with Ubuntu Linux 12.04 with a NVIDIA GTX460 graphics chip
    with both NVIDIA''s and Intel''s OpenCL toolkit installed, I have the following
    output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the other setup with Ubuntu Linux 12.04 with an ATI 6870x2 graphics chip
    and AMD APP SDK installed, the difference in the output is only that the number
    of platforms is one and data is split between the CPU and GPU:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works…
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application basically discovers all the OpenCL compliant devices and keeps
    tracks of how it discovered. Next, the application uses the prior information
    to divide the data among the devices before enqueuing the data for execution and
    the code snippet demonstrates the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, the data is checked for sanity after reading the data back from the
    device memory to the host memory as the following code snippet shows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: What you've just seen is a data partitioning technique also known as the distributed
    array pattern on a one-dimensional block of data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the distributed array pattern, there had been three general techniques
    that were developed, and they are over one-dimensional and two-dimensional blocks
    of data and finally the block-cyclic pattern.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Depending on whether you've installed one or more OpenCL toolkits from the vendors,
    the OpenCL will report the appropriate platforms and the OpenCL **Installable
    Client Driver** (**ICD**) allows multiple OpenCL implementations to co-exist on
    the same physical machine. Refer to the URL [http://www.khronos.org/registry/cl/extensions/khr/cl_khr_icd.txt](http://www.khronos.org/registry/cl/extensions/khr/cl_khr_icd.txt)
    for more information about ICDs. This explains why your program may display distinct
    numbers for each installed platforms. The ICD actually identifies the vendors
    who provided the OpenCL implementation on the machine you have setup and its main
    function is to expose the platforms to the host code so that the developer may
    choose to run the algorithm in question against. The ICD has two pieces of information—(a)
    entry points to the vendor's OpenCL implementation in the library on the filesystem
    on which it's been installed, (b) the suffix string used to identify the suffix
    for OpenCL extensions provided by that vendor.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL sub-buffer objects
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The retrieval of information about OpenCL sub-buffers is very similar to that
    described in the previous recipe and involves the invocation of `clGetMemObjInfo`.
    Let's take a look at it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OSX Caveat—you will need a OpenCL 1.1, at least the implementation to see this
    build and run; since OSX 10.6 doesn't support that version, you'll have to get
    a OSX 10.7 to get this code to run.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `Ch2/sub_buffer_query/subbuffer_query.c`, you''ll find an excerpt of
    the following code demonstrating how we would pass the sub-buffer memory object
    to our defined function `displayBufferDetails`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During my experimentation, I found that the NVIDIA CUDA 5 OpenCL toolkit was
    stricter in evaluating the attributes in the argument flags that''s passed to
    `clCreateSubBuffer` as compared to AMD''s APP SDK v2.7\. Take note that the bug
    may be fixed by the time you read this book. As a concrete example, the following
    code throws an error using NVIDIA as opposed to AMD when you write:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '`clCreateSubBuffer(buffer,CL_MEM_READ_WRITE|CL_MEM_COPY_HOST_PTR,…)` to reflect
    the fact that `CL_MEM_COPY_HOST_PTR` doesn''t make sense.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, as shown in the following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On the Ubuntu Linux 12.04 with AMD''s APP SDK v2.8, the following command would
    suffice:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For the Intel OpenCL SDK, you would enter the following command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For NVIDIA on Ubuntu Linux 12.04, you would enter the following command :'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Regardless of the platform, a binary executable `subbuffer_query` would be deposited
    locally.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the program, you should get something similar to the following
    output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works…
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application could decipher whether it's an OpenCL sub-buffer object because
    of the two flags introduced in OpenCL 1.2\. They are `CL_MEM_OFFSET` and `CL_MEM_ASSOCIATED_MEMOBJECT`;
    using either one of the flags would reveal whether it's a sub-buffer, but the
    catch is that `CL_MEM_OFFSET` can be zero for a sub-buffer because that indicates
    to OpenCL where to start to extract the data from; a better, recommended option
    is to use `CL_MEM_ASSOCIATED_MEMOBJECT` since the presence implies the argument
    `memobj` is a sub-buffer. See the earlier recipe, *Retrieving information about
    OpenCL buffer objects*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Understanding events and event-synchronization
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous recipes demonstrated how you can create memory objects that encapsulates
    the data that is to be transferred from the host memory to the device memory,
    and discusses how you can partition the input data among the devices via sub-buffers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to develop an understanding of how the developer
    can make use of the event system in OpenCL to control execution of kernel commands
    as well as memory commands. This is beneficial to the developer because it offers
    myriad ways in which you can control execution flow in a heterogeneous environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Events are, generally, passive mechanisms when the developers wish to be notified
    of an occurrence, and having the choice of conducting processing past that occurrence;
    contrasting to the say, polling where it's a more active mechanism as the application
    makes an active enquiry into the current state and decides what to do when a particular
    condition is met.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Events in OpenCL fall into two categories as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Host monitoring events
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command events
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both the event types, the developer needs to create the events explicitly
    and associate them with the objects through waitlists; waitlists are nothing more
    than a container of events that the command must wait upon completion, that is,
    the event's status is `CL_COMPLETE` or `CL_SUCCESS` before progressing. The difference
    between these two event types (as we shall soon see) is in the manner in which
    the next subsequent command in the queue gets executed, host events are updated
    by the developer and when this is done it is indicative by the program source,
    command events in the waitlists on the other hand are updated by the OpenCL runtime.
    Considering that the events held up in the waitlists must be of a certain state
    before the next command executes means that waitlists are actually synchronization
    points since no progress can be made without emptying that list.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by examining the host events. So far, we understood that commands
    needs to be placed onto the command queue so that they can be scheduled for execution,
    and what host monitoring events allow the developer is to monitor the state of
    enqueued command and we can, optionally, attach a callback function to the event
    so that when it returns with a state we desire, the callback function will execute.
    This is made possible via the APIs `clCreateUserEvent`, `clSetUserEventStatus`,
    `clReleaseEvent`, and `clSetEventCallback`. An example in the *How to do it* section
    would illustrate how this can be achieved.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume that a kernel wishes to process two 1D memory objects named `objA` and
    `objB` and write the result to `objC` (for this example, we can ignore the output
    of `objC`). We wish that the copying of input data from `objB` should only take
    place when we have indicated to the host program.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full source is demonstrated in `Ch2/events/{events.c,sample_kernel.cl}`
    and we have to first create the necessary data structures as before; next we will
    create the event object as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this event object, we can next assign a call back function to the event
    and indicate that upon the event''s status changes to `CL_COMPLETE`, the callback
    would execute like the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Then the host program would continue to conduct memory transfers for `objA`
    and `objB`, but it doesn't proceed to process any more OpenCL commands enqueued
    on the command queue till the status of the `event1` is set to `CL_COMPLETE`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Another API we will introduce is the `clWaitForEvents` with it''s signature:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is typically used to stall the host thread until all the commands in the
    event list have completed (the next code snippet demonstrates how).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The next topic we look at are the command events, which are typically used
    when you wish to be notified of certain happenings associated with commands. A
    typical use case would be the following where you have a command-queue and you
    want to be notified of the status of an memory transfer command like `clEnqueueWriteBuffer`
    and take a particular action depending on that status:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can easily extrapolate the scenario where you have a large heterogeneous
    computing environment with large numbers of CPUs and GPUs and obviously you wish
    to maximize your computational power, and the events mechanism in OpenCL allows
    the developer to design how to sequence those computations and coordinate those
    computations. However, as a best practice you probably want to clean up the event
    object associated with the commands, but you need to discover the state of the
    event you''re watching otherwise you might release the event prematurely, and
    here''s how you can do that by polling the API `clGetEventInfo` passing in the
    event you are watching; the following code demonstrates this idea:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: There's more…
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two scenarios that deserve mentioning and they address the situation
    where (a) you like to receive notification for a group of events (assuming that
    they are associated to memory objects) and (b) you like to stall the execution
    of any commands further down the pipeline, that is, command-queue, until this
    group of events you are watching for have completed. The API `clEnqueueMarkerWithWaitList`
    is for the former situation whereas `clEnqueueBarrierWithWaitList` suits the latter.
    You are encouraged to explore them in the OpenCL 1.2 specification.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are still using OpenCL 1.1, you can use `clEnqueueMarker` and `clEnqueueBarrier`
    (which are the older versions of `clEnqueueMarkerWithWaitList` and `clEnqueueBarrierWithWaitList`)
    but be aware that they are both deprecated in OpenCL 1.2.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Copying data between memory objects
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will quickly realize how useful the event mechanism in OpenCL is in controlling
    the various parts of your algorithm, and it can be found in the common kernel
    and memory commands. This recipe will continue from creating memory objects and
    focus on how those memory objects can be transferred from the host memory to the
    device memory and vice versa and we'll be fixated on the data transfer APIs `clEnqueueReadBuffer`
    and `clEnqueueWriteBuffer`, which is for one-dimensional data blocks, and `clEnqueueReadBufferRect`
    and `clEnqueueWriteBufferRect` for two-dimensional data blocks; we'll also look
    at `clEnqueueCopyBuffer` for data transfers between memory objects in the device.
    First, we look at copying data between memory objects.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'There will come times when you have to copy data between distinct memory objects,
    and OpenCL provides us a convenient way to do this via `clEnqueueCopyBuffer`.
    It can only take place between two different memory objects (for example, one
    is a plain buffer and the other is a sub-buffer) or between two similar objects
    (for example, both are sub-buffers or plain buffers) and the area of copy cannot
    overlap. Here''s the method signature:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The list of functions for copying data between memory objects are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBuffer`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImage`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferToImage`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImageToBuffer`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferRect`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To copy a buffer, you need to indicate the source and destination `cl_mem` objects
    via `src_buffer` and `dst_buffer`, indicate where to start the copying by indicating
    the offsets of the `src_buffer` and `dst_buffer` via `src_offset` and `dst_offset`
    respectively together with the size of data to copy via `cb`. If you wish for
    the copying of the data to take place after some operations, you need to indicate
    the number of those operations and a valid array of `cl_event` objects that represent
    each operation via `num_events_in_wait_list` and `event_wait_list` respectively.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take note that you can query the device on the status of the copying, when your
    data array is large, by passing an event object to the `event` argument. Another
    approach is to enqueue a `clEnqueueBarrier` command.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code is an extract from `Ch2/copy_buffer/copy_buffer.c`, and it
    illustrates how to enqueue a `clEnqueueCopyBuffer` command to the command queue,
    and the kernel uses this copy of the data for computation. This process is iterated
    among the detected OpenCL devices on the machine. The following diagram illustrates
    how the original data block (previous diagram) is copied to another `cl_mem` object
    (next diagram) and passed off to the OpenCL devices for computation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/copy_buffers.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, with the highlighted commentary:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'On OSX, you can run the following command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK installed, you can run the following
    command:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'On Ubuntu Linux 12.04 with NVIDIA CUDA 5 installed, you can run the following
    command:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: A binary executable named `copy_buffer` will be deposited on the directory.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how many OpenCL SDKs are installed on your machine, your output
    may vary but on my OSX, the following is the output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works…
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application needed to compute the copied buffer, and you can tell this
    because `clSetKernelArg` was defined that way by this statement:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Next, we can perform a copy operation, which takes place in the device's memory,
    via `clEnqueueCopyBuffer` and finally retrieve the computed values via `clEnqueueReadBuffer`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The created command queue will default to in-order execution, instead of out-of-order
    execution so the device will execute the commands in the order of the queueing.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to talk about the one-dimensional and two-dimensional data
    transfer APIs such as `clEnqueueReadBuffer`, `clEnqueueWriteBuffer`, `clEnqueueWriteBufferRect`,
    and `clEnqueueReadBufferRect` and we are doing this now because you have seen
    that most of our examples, so far, we demonstrated the creation of memory objects
    via `clCreateBuffer` by associating with a memory structure in the host and though
    that might suffice for some situations, you probably want APIs that gives you
    more control when memory objects in the device memory are to be written or read
    from. The control these APIs give you, the developer, is from the fact that they
    are enqueued onto the command-queue with any events the developer might craft;
    and that provides a good permutation of strategies and flexibilities for structuring
    I/O in heterogeneous environments.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that there are similar APIs for reading and writing two or three dimensional
    images to/from host to the device memory. Their names are `clEnqueueReadImage`,
    `clEnqueueWriteImage`, `clEnqueueReadImageRect`, and `clEnqueueWriteImageRect`.
    Refer to the OpenCL 1.2 Specifications for more details.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'These APIs allows us to indicate to the device when we wish the data transfer
    to occur, very much like `clEnqueueCopyBuffer`. Let''s take a look at their method
    signatures:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: These two functions are very similar to one another, and they basically say
    if you wish to read/write to/from a memory buffer , that is, a `cl_mem` object,
    you need to indicate which command queue is it via `command_queue`, what buffer
    it is via `buffer`, whether to be a blocking-read/write via `blocking_read/blocking_write`,
    where to read/write from for what size via `offset` and `cb`, where to read the
    data or write the data to via `ptr`, should this read/write command occur after
    some events via `num_events_in_wait_list` and `event_wait-list`. The last argument
    in the function is `event`, which allows the reading or writing operation to be
    queried which is described in `clEnqueueCopyBuffer`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Blocking reads in `clEnqueuReadBuffer` means that the command does not exit
    until the host pointer has been filled by the device memory buffer; similarly
    blocking-writes in `clEnqueueWriteBuffer` means that the command doesn't exit
    until the entire device memory buffer has been written to by the host pointer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how these calls are used, you can refer to the earlier illustrated code
    in the recipe *Understanding events and event-synchronization* and for your convenience
    the following is the relevant code in `Ch2/events/events.c`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Having the capability to model one-dimensional memory objects is fantastic,
    but OpenCL goes a notch further by facilitating two-dimensional memory object
    memory transfers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of reading a two-dimensional data blocks from the device's
    memory to the output buffer in the host memory; extracted from `Ch2/simple_2d_readwrite/simple_2d_readwrite.c`.
    The code illustrates the usage of the `buffer_origin`, `host_origin`, and `region`
    as in the API. The application will read from the `UDObj cl_mem` object, which
    represents the one-dimensional input data, `hostBuffer`, as a 2 x 2 matrix and
    writes them into the host memory data block represented by `outputPtr`. The application
    reads back the data from the device to host memory and checks for sanity.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个从设备内存读取二维数据块到主机内存输出缓冲区的示例；摘自`Ch2/simple_2d_readwrite/simple_2d_readwrite.c`。代码展示了如何使用`buffer_origin`、`host_origin`和`region`，正如API中所示。应用程序将从表示一维输入数据的`UDObj
    cl_mem`对象中读取，将其作为2 x 2矩阵写入由`outputPtr`表示的主机内存数据块。应用程序将从设备读取数据到主机内存，并检查数据是否合理。
- en: '[PRE57]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In this example, we used the `for` loop and standard array indexing techniques
    in `C` to model how you might iterate through a two-dimensional array and referencing
    the elements so that we progressively copy the input. We won't dwell too much
    into this because, building and running it is very similar to the previous, and
    you should explore the directory to see how the build and program works via the
    Makefile.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了`C`语言中的`for`循环和标准数组索引技术来模拟如何遍历二维数组并引用元素，以便我们逐步复制输入。我们不会过多地深入这个话题，因为构建和运行它与之前非常相似，你应该探索目录以了解如何通过Makefile构建和运行程序。
- en: Using work items to partition data
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用工作项来分区数据
- en: In the previous chapter, we introduced how work can be partitioned in a one-dimensional
    array across several work items (you should flip back now if you cannot remember),
    and also how each work item would obtain an index in which the kernel can use
    to conduct the computation in the kernel code `vector_multiplication`. In this
    recipe, we are going to build on that by exploring two-dimensional data partitioning
    in more detail.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何在多个工作项之间对一维数组进行分区（如果你现在记不起来的话，应该翻回来看），以及每个工作项将如何获得一个索引，内核可以使用这个索引在内核代码`vector_multiplication`中进行计算。在本食谱中，我们将在此基础上，更详细地探索二维数据分区。
- en: By now, you should realize that one of the cornerstones of OpenCL is getting
    the data into the device/s for processing via kernels, and you've seen how data
    can be partitioned among different devices via kernels. In the former, you've
    seen how we used the distributed array pattern to partition the data among the
    devices; this refers to coarse grain data-parallelism. The latter refers to the
    coarse grained task-parallelism that OpenCL provides and it is coarse grained
    because OpenCL is capable of both data-parallelism and task-parallelism.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该意识到OpenCL的一个基石是将数据通过内核传输到设备/进行处理，你已经看到了数据如何通过内核在不同设备之间进行分区。在前者中，你看到了我们如何使用分布式数组模式将数据分区到设备中；这指的是粗粒度数据并行性。后者指的是OpenCL提供的粗粒度任务并行性，它之所以是粗粒度的，是因为OpenCL既能实现数据并行性，也能实现任务并行性。
- en: Most of the code you've seen so far have been using `clEnqueueTask` to execute
    the kernel based on the one-dimensional data blocks and to get your kernel to
    process two or three dimensional data we need to understand `clEnqueueNDRangeKernel`;
    and how data can be laid out conceptually in two or three dimensional space.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你到目前为止看到的代码大部分都使用了`clEnqueueTask`来根据一维数据块执行内核，并且为了让内核处理二维或三维数据，我们需要理解`clEnqueueNDRangeKernel`；以及如何在二维或三维空间中概念性地布局数据。
- en: Tip
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is helpful to visualize the two or three dimensional data layout in the device
    memory to be row-based instead of column-based.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备内存中可视化二维或三维数据布局为基于行而不是基于列是有帮助的。
- en: The `NDRange` in `clEnqueueNDRangeKernel` refers to a data indexing scheme that
    is supposed to span an N-dimensional range of values and hence, the given name.
    Currently, *N* in this N-dimensional index space can be one, two, or three. Next,
    we can split each dimensional into chunks of sizes two, three, four, or more till
    we reached the maximum allowable by the parameter `CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS`.
    Refer to the `Ch1/device_details/device_details.c` on how to obtain the values.
    This would decide how many processing groups we can run in parallel, and in OpenCL
    they are called **work groups**. The work groups would have a number of available
    processing elements that are called **work items** though I like to think of them
    as executable threads.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work through an example using a two-dimensional data size of 12 rows
    by 12 columns, that is, a 12 x 12 matrix. Let''s look at the following diagram
    to understand how the work groups and work items are related to one another:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Using work items to partition data](img/work_partition.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: 'In this example, I''ve decided to partition the two-dimensional space to create
    nine work groups where each work group is a 4 x 4 matrix. Next, to decide how
    many work items there should be in each work group, and you have two choices:
    a) assign one work-item to process each cell in your 4 x 4 matrix, b) assign one
    work item to process n-cells in your 4 x 4 matrix; in the second option it would
    be similar to vector processing where n-values are loaded together for the work
    item to process. Let''s assume that we''ve decided to choose the option a'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll look at the various data types in the [Chapter 3](ch03.html "Chapter 3. Understanding
    OpenCL Data Types"), *Understanding OpenCL Data Types*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, let''s take a detailed look at the API `clEnqueueNDRangeKernel`
    with the following method signature, and understand how to input those values
    with our example:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Let's look at what those variables in `clEnqueueNDRangeKernel` are for; the
    `command_queue` refers to the particular queue like the `kernel`, to execute on.
    Next, you need to indicate how many dimensions your input data has via `work_dim`;
    the next two variables `global_work_size` and `local_work_size` would indicate
    how many work groups there are and how many work items / work threads can execute
    in each work group. Recall that the kernel gets scheduled on the device, but it
    is the work group that gets assign compute units of the device and the work items
    execute on the processing element in the compute unit. Next, if you need the launch
    of the kernel to wait on a couple of events in your algorithm, you can indicate
    them through `num_events_in_wait_list` and `event_wait_list`, and finally if you
    wish to associate an event to this kernel's state you can pass in an event type
    to `event` in this API.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The method signature should not look that intimidating to you by now. Given
    a 12 x 12 matrix partitioned into nine work groups where each work group is a
    4 x 4 matrix and each work item will process one data cell, we will code it like
    the following code snippet:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To ensure you have got your calculations correct, you can use the following
    simple formula:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Number of work-groups = (global_work_size[0]*…*global_work_size[n-1]) / (local_work_size[0]*…*local_work_size[n-1])
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to take a look at how we can enable this task-parallelism
    and data-parallelism to be processed by the CPU and GPU where each device will
    copy a one-dimensional data array from the input buffer and treat it like a two-dimensional
    matrix for parallel computing, and finally output the results to a one-dimensional
    matrix.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `Ch2/work_partition/work_partition.c`, we saw an excerpt where we need to
    copy a million elements from an input buffer to an output buffer using a two-dimensional
    data format. We proceed to partition the data into a 1024 x 1024 matrix where
    each work item processes a single cell and we create work groups of the size 64
    x 2 matrix.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Caveat—during my experimentation, this program crashed when executing on the
    OSX 10.6 Intel Core i5 with OpenCL 1.0 as the work group can only be of size one
    in each dimension. We'll look in the [Chapter 3](ch03.html "Chapter 3. Understanding
    OpenCL Data Types"), *Understanding OpenCL Data Types* on how to make our programs
    more portable.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function, `copy2Dfloat4` is a typical function which is executed
    on the device and we like to express the idea of transferring a vector of elements
    from one point to another and once that's done, the application will conduct a
    data sanity check which will pass or fail the program; Refer to the `Ch2/work_partition/work_partition.cl`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, with the highlighted commentary
    in the following code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'On OSX, you can run the following command:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK installed, you can run the following
    command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'On Ubuntu Linux 12.04 with NVIDIA CUDA 5 installed, you can run the following
    command:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: A binary executable named `work_partition` will be deposited on the directory.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu Linux 12.04 with AMD APP SDK v2.8 and NVIDIA CUDA 5 installed, I
    have the following output. If you ran the program using the Intel® OpenCL SDK,
    then you will not see the output related to the discrete graphics chip. In this
    example, we have demonstrated both coarse-grained and fine-grained data and task
    parallelism:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How it works…
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The host application allocates two buffers that are capable of storing a million
    elements of the data type `cl_float4`, which is a OpenCL `vector` data type. Next
    we proceed to build the program via `clBuildProgramWithSource` (refer to `Ch2/work_partition/work_partition.c`),
    and pick up all the kernels in the kernel file (`*.cl`). Each detected device
    will pick up a one-dimensional input buffer, transform it to a two-dimensional
    matrix, and partition the data among its parallel computing units where each work
    group will compute the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 主应用程序分配了两个缓冲区，可以存储一百万个`cl_float4`类型的数据元素，这是一种OpenCL的`vector`数据类型。接下来，我们通过`clBuildProgramWithSource`（参考`Ch2/work_partition/work_partition.c`）构建程序，并从内核文件（`*.cl`）中提取所有内核。每个检测到的设备都会提取一个一维输入缓冲区，将其转换为二维矩阵，并将数据分配到其并行计算单元中，其中每个工作组将计算以下内容：
- en: Obtain the index for the row via `get_global_id(0)`; which can be thought of
    as the thread's ID in the x-axis
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`get_global_id(0)`获取行的索引；这可以被视为x轴上的线程ID
- en: Obtain the index for the column via `get_global_id(1)`; which can be thought
    of as the thread's ID in the y-axis
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`get_global_id(1)`获取列的索引；这可以被视为y轴上的线程ID
- en: Together with the row and column indexes, perform a memory load of 4 elements
    and store the same via `C(x,y) = A(x,y)`
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与行和列索引一起，执行4个元素的内存加载，并通过`C(x,y) = A(x,y)`将其存储。
- en: The OpenCL runtime would have partition the data among the work groups, together
    with the IDs for the work items as well as work groups; hence there would not
    be a situation where the thread IDs being duplicated and hence waging mayhem on
    the computation (the OpenCL vendor has that responsibility of ensuring it doesn't
    occur). OpenCL knows how to do this because the dimensions of the input data,
    together with the number of work groups and number of executing work items are
    passed via the parameters `work_dim`, `global_work_size`, and `local_work_size`
    in the `clEnqueueNDRangeKernel` API.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCL运行时会将数据分配到工作组中，同时还包括工作项和工作组的ID；因此，不会出现线程ID重复的情况，从而避免对计算造成混乱（OpenCL供应商有责任确保这种情况不会发生）。OpenCL知道如何做到这一点，因为输入数据的维度、工作组的数量以及执行的工作项的数量都是通过`clEnqueueNDRangeKernel`
    API中的`work_dim`、`global_work_size`和`local_work_size`参数传递的。
- en: 'An example should clarify this: Assume that the imaginary input data has two-dimensions
    and the `global_work_size` is 8192 and `local_work_size` is 16*16, then we will
    have 8192/(16*16) = 32 work groups; to be able to reference any element in a two-dimensional
    data block, you will write some code similar to this to generate the global thread
    ID in (this is not the only way to do this, but it is the generally preferred
    method):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例来澄清这一点：假设假设的输入数据具有二维，`global_work_size`为8192，`local_work_size`为16*16，那么我们将有32个工作组；为了能够引用二维数据块中的任何元素，你需要编写一些类似的代码来生成全局线程ID（这不是唯一的方法，但通常是首选方法）：
- en: '[PRE65]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The OpenCL kernel will complete its computation eventually because of an invocation
    to `clWaitForEvents` (we'll talk about this in the next chapter), and then the
    output buffer is stored with data from the device memory via `clEnqueueReadBuffer`
    and the data is sanity checked.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调用了`clWaitForEvents`（我们将在下一章中讨论这一点），OpenCL内核最终将完成其计算，然后通过`clEnqueueReadBuffer`将设备内存中的数据存储到输出缓冲区，并对数据进行合理性检查。
