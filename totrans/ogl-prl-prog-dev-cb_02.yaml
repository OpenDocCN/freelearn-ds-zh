- en: Chapter 2. Understanding OpenCL Data Transfer and Partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating OpenCL buffer objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL buffer objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating OpenCL sub-buffer objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL sub-buffer objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding events and event-synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copying data between memory objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using work items to partition data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to explore how to invoke the OpenCL's data transfer
    APIs, query memory objects, and data/work partitioning between the GPUs and CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that not all OpenCL SDKs support the compilation and execution on both
    GPUs and CPUs. AMD's OpenCL implementation supports its own AMD and Intel CPUs
    and GPUs; NVIDIA supports its GPUs and Intel supports its own Intel Core CPUs
    and Intel HD Graphics. Check with the vendor for supported devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Open Computing Language** (**OpenCL**) development, you would inevitably
    need data to be processed, and the standard does not permit you to manipulate
    memory objects directly as you would do when you program in C or C++, because
    the data memory in the host is ultimately transferred to the devices in a heterogeneous
    environment for processing, and previously you would use the programming constructs
    in various libraries or languages to access them which is one of the reasons why
    OpenCL came about; hence to unify these approaches, the standard added abstractions
    to shield the developer from these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: With respect to data types, there are a few you need to be aware of other than
    the one-dimensional data buffer. OpenCL buffer objects can be used to load and
    store two/three-dimensional data. The next data type in OpenCL is the `image`
    object; these objects are used to store two or three dimensional images (we won't
    cover much of using the `image` objects in this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenCL 1.1 new data transfer capabilities includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using sub-buffer objects to distribute regions of a buffer across multiple OpenCL
    devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3-component vector data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the global work offset which enables kernels to operate on different portions
    of the NDRange—global work offset refers to the data points in the input data
    where work items can start processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading, writing, or copying a 1D, 2D or 3D rectangular region of a buffer object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating OpenCL buffer objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood the need to create or wrap our host's
    memory objects into an abstraction that OpenCL can operate on, and in this recipe
    we'll explore how to create a particular type of memory object defined in the
    specification that is commonly used for general purpose computation—buffer object.
    The developer can choose to create a one, two or three dimensional memory object
    that best fits the computational model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating buffer objects is simple in OpenCL and is akin to the way in which
    you would use C's memory allocation routines such as `malloc` and `alloca`. But,
    that's where the similarity ends for the reason that OpenCL cannot operate directly
    on memory structures created by those routines. What you can do is to create a
    memory structure that lives on the devices that can be mapped to the memory on
    the host and the data is transferred to the device by issuing memory transfer
    commands to the command queue (which you recall is the conduit to the device).
    What you need to decide is the sort of objects, and how much of these objects
    you would like the device to compute.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we''re going to learn how to create buffer objects based on
    user-defined structures also known as `structs` in the C/C++ language. Before
    that, let''s understand the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can create a buffer by specifying which `context` it should attach to (recall
    that contexts can be created with several devices), specify the size of the data,
    and where to reference it with `size` and `host_ptr` respectively, specify how
    memory is to be allocated and whether that memory is to be of type read, read-only,
    read-write, or write only via `flags`; lastly capture the resultant error code
    in `errcode_ret`. Note that `clCreateBuffer` doesn't queue the command to conduct
    the memory transfer from host to device memory.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a portion of the code from `Ch2/user_buffer/user_buffer.c` where you
    will see how to use the `clCreateBuffer` API to allocate memory for a user-defined
    structure. The problem we are trying to solve in this example is to send a million
    user-defined structures to the device for computation. The computation encapsulated
    by the kernel is a simple one—sum of all elements of each user-structure. The
    astute reader would have noticed we could have demonstrated this data structure
    with a vector data type in OpenCL, `int4`; the reason why we didn''t do it that
    way is a two fold: (a) it''s an example of application domain modeling, (b) because
    in a few paragraphs from current we wanted to illustrate how you could use the
    data type alignment construct, and don''t fret over the data types now because
    we''ll dive into the various data types in the next chapter. Continuing further,
    the user-defined structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What you will need to do is to create a buffer on the host application using
    standard C/C++ dynamic/static memory allocation techniques such as `new`, `malloc`,
    and `alloca`. Next, you will need to initialize that data buffer, and finally
    you will have to invoke `clCreateBuffer` and you should make sure it's done prior
    to the call to `clSetKernelArg`; recall that we mentioned that kernels get scheduled
    for execution on the device, well before it executes the kernel code on the device
    it would need data and values to work against, and you can achieve this by an
    invocation to `clSetKernelArg` and you typically do this when the buffer object
    is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The API `clSetKernelArg` looks like the following code and it''ll be important
    for you to understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel can take no arguments or at least one and probably more arguments,
    and how you configure them is simple. The following code snippet should complete
    the story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the kernel arguments are configured programmatically with the understanding
    that if the kernel function has `n` arguments then the `arg_index` would range
    from `0` to (`n – 1`).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe from `Ch2/user_buffer/user_buffer.c`,
    with the highlighted commentary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'On OSX, you would compile the program by running the following command on your
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On the Ubuntu Linux 12.04 with Intel OpenCL SDK, the command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On the Ubuntu Linux 12.04 with AMD APP SDK v2.8, the command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the platform, a binary executable `user_buffer` would be deposited
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running the application on both platforms, we would get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application created a million of the `UserData` objects on the host. Refer
    to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The application then sends it to the device for computation after the program
    and kernel objects have been initialized, and we assign the recently created `UDObj`
    memory object to the kernel as its argument. Refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we issue a kernel execution command to the command-queue, `cQ`, and the
    code will run against the device, the following code snippet demonstrates the
    enqueuing of the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After that''s done, the data in the device''s memory is read back and we indicated
    that we wish to read the data back until the device has completed its execution
    by passing `CL_TRUE` to indicate blocking read which otherwise could result in
    partial data read back; finally the data is verified, demonstrated by the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's explore how we used `clCreateBuffer` further.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, you would want to allocate memory on the device as read-only
    when it comes to providing input to the device and because you want to be sure
    nothing else is writing to the data store. Therefore, the flag `CL_MEM_READ_ONLY`
    is passed, but if your input data was meant to be readable and writable then you
    would need to indicate it using `CL_MEM_READ_WRITE`. Notice that we actually created
    a data store on the host via `ud_in` and, we wanted our OpenCL memory object to
    be the same size as `ud_in` and the `C` statement reflects this; finally we wanted
    OpenCL to know that the new memory object is to copy its values from `ud_in` and
    we provided the flag `CL_MEM_COPY_HOST_PTR` too, and we use the bitwise `OR` operator
    that is represented on the standard US keyboard as a pipe symbol, *|*, to merge
    these two flags.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, you can visualize it to be an 1D-array-of-structs for short or
    an array-of-structures in general.
  prefs: []
  type: TYPE_NORMAL
- en: '| `UserData` | `UserData` | `UserData` | …………………………………………… | `UserData` |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Provide the same declaration of the application data type to the OpenCL kernel
    file (`*.cl`) as well as the host application files (`*.c`, `*.h`, `*.cpp`, `*.hpp`);
    else the OpenCL runtime will emit errors to reflect that the struct it is looking
    for does not exist, and the replication is necessary as OpenCL prohibits the `C`
    header file inclusion mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Let's spend some time to understand the C `struct` we just used in this example.
    The C structure we just used, `UserData`, is an example of an application data
    type. OpenCL makes no requirement about the alignment of OpenCL data types outside
    of buffers and images; hence developers of OpenCL need to make sure the data is
    properly aligned. Fortunately, OpenCL has provided attribute qualifiers so that
    we can annotate our types, functions and variables to suit the algorithm and CPU/GPU
    architecture with the primary motivation being to improve memory bandwidth. The
    alignment needs to be a power of two and at least a perfect multiple of the lowest
    common multiple of all the alignments of all the members of the `struct` or `union`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to Section 6.11.1 Specifiying Attributes of Types in the OpenCL 1.2 specification
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at what is available to developers when it comes to aligning
    data types such as `enum`, `struct`, or `union`.
  prefs: []
  type: TYPE_NORMAL
- en: Data alignment is a direct result of how various computer systems restrict the
    allowable addresses for the primitive data types, requiring that the address for
    some type of object must be a multiple of some value *K* (typically 2, 4, or 8),
    and this actually simplifies the design of the hardware between the processor
    and the memory system. For example, if the processor were to always fetch 8 bytes
    from memory with an address that must be a multiple of 8, then the value can be
    read or written in a single memory operation otherwise, the processor needs to
    perform two or more memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment is enforced by making sure that every data type is organized and allocated
    in such a way that every object within the type satisfies its alignment restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an example for this illustration. Following is the generic manner
    in which alignment can be defined for application data type such as `UserData`.
    While examining the code, you will notice that without the `aligned` attribute,
    this data structure will be allocated on a 17-byte boundary assuming `int` is
    4-bytes and `char` is 1-byte on a 32-bit / 64-bit system architecture. Once this
    attribute is included, following is the alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The alignment is now determined by the OpenCL compiler to be aligned to 32-bytes
    instead of 17-bytes, that is, summing all the struct member''s sizes, and the
    specification designates the alignment size to be the largest power of 2 and therefore
    it is 25 because, the 24 is 1-byte too many; however if you were to change the
    previous alignment to the following alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the alignment will be at least 8-bytes as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Equivalently, you can also write in more explicit form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In general, the golden rule of designing the data to be memory aligned is still
    a necessary practice; a rule of thumb I keep in mind is 16-byte aligned for 128-bit
    access and 32-byte aligned for 256-bit access.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other side of the story, you may find yourself wishing that the alignment
    wasn''t that large, and with OpenCL you can indicate that by using the `packed`
    attribute as in the following code assuming that `LargeUserData` is an imaginary
    large data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When you apply this attribute to a `struct` or `union`, you're effectively applying
    the attribute to every member of the data; applying to an `enum` means that the
    OpenCL compiler will select the smallest integral type found on that architecture.
    You can refer to the `Ch2/user_buffer_alignment/user_buffer_align.c` to review
    what's done and how to profile the performance of the application via AMD APP
    SDK in the `readme.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL buffer objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To retrieve information about a buffer or sub-buffer object, you''ll need to
    use the API `clGetMemObjectInfo` and its signature as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To query the memory object, simply pass the object to `memobj` specifying the
    type of information you want in `param_name`, inform OpenCL the size of the returned
    information in `param_value_size` and where to deposit it in `param_value`; the
    last parameter, `param_value_size_ret`, is largely optional but it returns the
    size of the value in `param_value_size`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an excerpt from the code in `Ch2/buffer_query/buffer_query.c` where
    it shows how to extract the information about the memory object, `UDObj` is encapsulated
    into a user-defined function `displayBufferDetails` because, the code can be long
    depending on how many attributes you wish to extract about a memory object and
    you would place the invocation to this function after you''ve created the buffer
    object or if you have been given a handle to the memory object. The following
    code illustrates how it would display the information about a memory object by
    abstracting the OpenCL memory retrieval APIs into the function `displayBufferDetails`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'On OSX, you will compile the program by running the following command on your
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK, the command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with AMD APP SDK v2.8, the command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the platform, a binary executable `buffer_query` would be deposited
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the program on an OSX 10.6 and Ubuntu 12.04 with AMD APP SDK v2.7
    would present the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The host application proceeds to first create the buffer that it will send
    to the device, then the application queries for information about the buffer.
    The full list of attributes that can be queried is as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| cl_mem_info | Return type | Info. Returned in param_value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_TYPE` | `cl_mem_object_type` | It returns `CL_MEM_OBJECT_BUFFER`
    if `memobj` is created with `clCreateBuffer` or `clCreateSubBuffer`. |'
  prefs: []
  type: TYPE_TB
- en: '| `Cl_MEM_FLAGS` | `cl_mem_flags` | It returns the flags argument specified
    when `memobj` is created with `clCreateBuffer`, `clCreateSubBuffer`, `clCreateImage2D`,
    or `clCreateImage3D`. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_SIZE` | `size_t` | It returns the actual size of the data associated
    with `memobj` in bytes. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_HOST_PTR` | `void*` | If `memobj` is created with `clCreateBuffer`
    or `clCreateImage2d`, `clCreateImage3D`, then it returns the `host_ptr` argument
    specified when `memobj` is created.If `memobj` is created with `clCreateSubBuffer`,
    then it returns the `host_ptr` plus `origin` specified when `memobj` was created.See
    `clCreateBuffer` for what `host_ptr` is. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_MAP_COUNT` | `cl_uint` | Map count. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_REFERENCE_COUNT` | `cl_uint` | It returns `memobj`''s reference count.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_CONTEXT` | `cl_context` | It returns the context specified when the
    memory is created. If `memobj` is created using `clCreateSubBuffer`, the context
    associated with the memory object specified as the `buffer` argument to `clCreateSubBuffer`
    is returned. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_ASSOCIATED_MEMOBJECT` | `cl_mem` | It return memory object from which
    `memobj` is created.In `clCreateSubBuffer`, it returns the `buffer` argument;
    else NULL is returned. |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_MEM_OFFSET` | `size_t` | Applicable to `memobj` created via `clCreateSubBuffer`.
    It returns offset or 0. |'
  prefs: []
  type: TYPE_TB
- en: Creating OpenCL sub-buffer objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sub-buffers are incredibly useful data types and as you continue to explore
    OpenCL in this chapter, you'll notice that this data type can be used to partition
    the data and distribute them across your OpenCL devices on your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of this writing, sub-buffer support is not enabled on OpenCL delivered
    in the OSX 10.6, because the official version is OpenCL 1.0\. However, if you
    have OSX 10.7 then you'll be able to run this code without any problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the method signature and examine it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The argument `buffer` refers to the buffer you created via `clCreateBuffer`,
    the `flags` argument refers to the options you wish this offer to have and if
    it''s zero then the default option is `CL_MEM_READ_WRITE`; this flag can adopt
    any values from the previous table. The argument `bufferType` is of a data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, you indicate where to start creating the region via the `origin`
    argument and how large it is going to be via the `size` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *How to do it...* section of this recipe there is an excerpt from `Ch2/sub_buffers/sub_buffer.c`
    where we create two sub-buffer objects and each of them holds one-half of the
    data; these two sub-buffers will be sent to each OpenCL device on my setup, and
    they''re computed and results are checked. Conceptually, here''s what the code
    is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/sub_buffers.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**/* Chop up the data evenly between all devices & create sub-buffers */**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**/* Let OpenCL know that the kernel is suppose to receive an argument */**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As noted earlier, this application doesn''t work on OSX 10.6 and hence to compile
    it using the AMD APP SDK, you will enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Intel OpenCL SDK, you will enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For NVIDIA on Ubuntu Linux 12.04, you will enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the platform, a binary executable `sub_buffer` would be deposited
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the setup I have with Ubuntu Linux 12.04 with a NVIDIA GTX460 graphics chip
    with both NVIDIA''s and Intel''s OpenCL toolkit installed, I have the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the other setup with Ubuntu Linux 12.04 with an ATI 6870x2 graphics chip
    and AMD APP SDK installed, the difference in the output is only that the number
    of platforms is one and data is split between the CPU and GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application basically discovers all the OpenCL compliant devices and keeps
    tracks of how it discovered. Next, the application uses the prior information
    to divide the data among the devices before enqueuing the data for execution and
    the code snippet demonstrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the data is checked for sanity after reading the data back from the
    device memory to the host memory as the following code snippet shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: What you've just seen is a data partitioning technique also known as the distributed
    array pattern on a one-dimensional block of data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the distributed array pattern, there had been three general techniques
    that were developed, and they are over one-dimensional and two-dimensional blocks
    of data and finally the block-cyclic pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on whether you've installed one or more OpenCL toolkits from the vendors,
    the OpenCL will report the appropriate platforms and the OpenCL **Installable
    Client Driver** (**ICD**) allows multiple OpenCL implementations to co-exist on
    the same physical machine. Refer to the URL [http://www.khronos.org/registry/cl/extensions/khr/cl_khr_icd.txt](http://www.khronos.org/registry/cl/extensions/khr/cl_khr_icd.txt)
    for more information about ICDs. This explains why your program may display distinct
    numbers for each installed platforms. The ICD actually identifies the vendors
    who provided the OpenCL implementation on the machine you have setup and its main
    function is to expose the platforms to the host code so that the developer may
    choose to run the algorithm in question against. The ICD has two pieces of information—(a)
    entry points to the vendor's OpenCL implementation in the library on the filesystem
    on which it's been installed, (b) the suffix string used to identify the suffix
    for OpenCL extensions provided by that vendor.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving information about OpenCL sub-buffer objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The retrieval of information about OpenCL sub-buffers is very similar to that
    described in the previous recipe and involves the invocation of `clGetMemObjInfo`.
    Let's take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OSX Caveat—you will need a OpenCL 1.1, at least the implementation to see this
    build and run; since OSX 10.6 doesn't support that version, you'll have to get
    a OSX 10.7 to get this code to run.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `Ch2/sub_buffer_query/subbuffer_query.c`, you''ll find an excerpt of
    the following code demonstrating how we would pass the sub-buffer memory object
    to our defined function `displayBufferDetails`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During my experimentation, I found that the NVIDIA CUDA 5 OpenCL toolkit was
    stricter in evaluating the attributes in the argument flags that''s passed to
    `clCreateSubBuffer` as compared to AMD''s APP SDK v2.7\. Take note that the bug
    may be fixed by the time you read this book. As a concrete example, the following
    code throws an error using NVIDIA as opposed to AMD when you write:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clCreateSubBuffer(buffer,CL_MEM_READ_WRITE|CL_MEM_COPY_HOST_PTR,…)` to reflect
    the fact that `CL_MEM_COPY_HOST_PTR` doesn''t make sense.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'On the Ubuntu Linux 12.04 with AMD''s APP SDK v2.8, the following command would
    suffice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Intel OpenCL SDK, you would enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For NVIDIA on Ubuntu Linux 12.04, you would enter the following command :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the platform, a binary executable `subbuffer_query` would be deposited
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the program, you should get something similar to the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application could decipher whether it's an OpenCL sub-buffer object because
    of the two flags introduced in OpenCL 1.2\. They are `CL_MEM_OFFSET` and `CL_MEM_ASSOCIATED_MEMOBJECT`;
    using either one of the flags would reveal whether it's a sub-buffer, but the
    catch is that `CL_MEM_OFFSET` can be zero for a sub-buffer because that indicates
    to OpenCL where to start to extract the data from; a better, recommended option
    is to use `CL_MEM_ASSOCIATED_MEMOBJECT` since the presence implies the argument
    `memobj` is a sub-buffer. See the earlier recipe, *Retrieving information about
    OpenCL buffer objects*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding events and event-synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous recipes demonstrated how you can create memory objects that encapsulates
    the data that is to be transferred from the host memory to the device memory,
    and discusses how you can partition the input data among the devices via sub-buffers.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to develop an understanding of how the developer
    can make use of the event system in OpenCL to control execution of kernel commands
    as well as memory commands. This is beneficial to the developer because it offers
    myriad ways in which you can control execution flow in a heterogeneous environment.
  prefs: []
  type: TYPE_NORMAL
- en: Events are, generally, passive mechanisms when the developers wish to be notified
    of an occurrence, and having the choice of conducting processing past that occurrence;
    contrasting to the say, polling where it's a more active mechanism as the application
    makes an active enquiry into the current state and decides what to do when a particular
    condition is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'Events in OpenCL fall into two categories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Host monitoring events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both the event types, the developer needs to create the events explicitly
    and associate them with the objects through waitlists; waitlists are nothing more
    than a container of events that the command must wait upon completion, that is,
    the event's status is `CL_COMPLETE` or `CL_SUCCESS` before progressing. The difference
    between these two event types (as we shall soon see) is in the manner in which
    the next subsequent command in the queue gets executed, host events are updated
    by the developer and when this is done it is indicative by the program source,
    command events in the waitlists on the other hand are updated by the OpenCL runtime.
    Considering that the events held up in the waitlists must be of a certain state
    before the next command executes means that waitlists are actually synchronization
    points since no progress can be made without emptying that list.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by examining the host events. So far, we understood that commands
    needs to be placed onto the command queue so that they can be scheduled for execution,
    and what host monitoring events allow the developer is to monitor the state of
    enqueued command and we can, optionally, attach a callback function to the event
    so that when it returns with a state we desire, the callback function will execute.
    This is made possible via the APIs `clCreateUserEvent`, `clSetUserEventStatus`,
    `clReleaseEvent`, and `clSetEventCallback`. An example in the *How to do it* section
    would illustrate how this can be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume that a kernel wishes to process two 1D memory objects named `objA` and
    `objB` and write the result to `objC` (for this example, we can ignore the output
    of `objC`). We wish that the copying of input data from `objB` should only take
    place when we have indicated to the host program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full source is demonstrated in `Ch2/events/{events.c,sample_kernel.cl}`
    and we have to first create the necessary data structures as before; next we will
    create the event object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this event object, we can next assign a call back function to the event
    and indicate that upon the event''s status changes to `CL_COMPLETE`, the callback
    would execute like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Then the host program would continue to conduct memory transfers for `objA`
    and `objB`, but it doesn't proceed to process any more OpenCL commands enqueued
    on the command queue till the status of the `event1` is set to `CL_COMPLETE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Another API we will introduce is the `clWaitForEvents` with it''s signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This is typically used to stall the host thread until all the commands in the
    event list have completed (the next code snippet demonstrates how).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next topic we look at are the command events, which are typically used
    when you wish to be notified of certain happenings associated with commands. A
    typical use case would be the following where you have a command-queue and you
    want to be notified of the status of an memory transfer command like `clEnqueueWriteBuffer`
    and take a particular action depending on that status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'You can easily extrapolate the scenario where you have a large heterogeneous
    computing environment with large numbers of CPUs and GPUs and obviously you wish
    to maximize your computational power, and the events mechanism in OpenCL allows
    the developer to design how to sequence those computations and coordinate those
    computations. However, as a best practice you probably want to clean up the event
    object associated with the commands, but you need to discover the state of the
    event you''re watching otherwise you might release the event prematurely, and
    here''s how you can do that by polling the API `clGetEventInfo` passing in the
    event you are watching; the following code demonstrates this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two scenarios that deserve mentioning and they address the situation
    where (a) you like to receive notification for a group of events (assuming that
    they are associated to memory objects) and (b) you like to stall the execution
    of any commands further down the pipeline, that is, command-queue, until this
    group of events you are watching for have completed. The API `clEnqueueMarkerWithWaitList`
    is for the former situation whereas `clEnqueueBarrierWithWaitList` suits the latter.
    You are encouraged to explore them in the OpenCL 1.2 specification.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are still using OpenCL 1.1, you can use `clEnqueueMarker` and `clEnqueueBarrier`
    (which are the older versions of `clEnqueueMarkerWithWaitList` and `clEnqueueBarrierWithWaitList`)
    but be aware that they are both deprecated in OpenCL 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: Copying data between memory objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will quickly realize how useful the event mechanism in OpenCL is in controlling
    the various parts of your algorithm, and it can be found in the common kernel
    and memory commands. This recipe will continue from creating memory objects and
    focus on how those memory objects can be transferred from the host memory to the
    device memory and vice versa and we'll be fixated on the data transfer APIs `clEnqueueReadBuffer`
    and `clEnqueueWriteBuffer`, which is for one-dimensional data blocks, and `clEnqueueReadBufferRect`
    and `clEnqueueWriteBufferRect` for two-dimensional data blocks; we'll also look
    at `clEnqueueCopyBuffer` for data transfers between memory objects in the device.
    First, we look at copying data between memory objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'There will come times when you have to copy data between distinct memory objects,
    and OpenCL provides us a convenient way to do this via `clEnqueueCopyBuffer`.
    It can only take place between two different memory objects (for example, one
    is a plain buffer and the other is a sub-buffer) or between two similar objects
    (for example, both are sub-buffers or plain buffers) and the area of copy cannot
    overlap. Here''s the method signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of functions for copying data between memory objects are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBuffer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImage`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferToImage`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImageToBuffer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferRect`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To copy a buffer, you need to indicate the source and destination `cl_mem` objects
    via `src_buffer` and `dst_buffer`, indicate where to start the copying by indicating
    the offsets of the `src_buffer` and `dst_buffer` via `src_offset` and `dst_offset`
    respectively together with the size of data to copy via `cb`. If you wish for
    the copying of the data to take place after some operations, you need to indicate
    the number of those operations and a valid array of `cl_event` objects that represent
    each operation via `num_events_in_wait_list` and `event_wait_list` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take note that you can query the device on the status of the copying, when your
    data array is large, by passing an event object to the `event` argument. Another
    approach is to enqueue a `clEnqueueBarrier` command.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code is an extract from `Ch2/copy_buffer/copy_buffer.c`, and it
    illustrates how to enqueue a `clEnqueueCopyBuffer` command to the command queue,
    and the kernel uses this copy of the data for computation. This process is iterated
    among the detected OpenCL devices on the machine. The following diagram illustrates
    how the original data block (previous diagram) is copied to another `cl_mem` object
    (next diagram) and passed off to the OpenCL devices for computation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/copy_buffers.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, with the highlighted commentary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'On OSX, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK installed, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with NVIDIA CUDA 5 installed, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: A binary executable named `copy_buffer` will be deposited on the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how many OpenCL SDKs are installed on your machine, your output
    may vary but on my OSX, the following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The application needed to compute the copied buffer, and you can tell this
    because `clSetKernelArg` was defined that way by this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can perform a copy operation, which takes place in the device's memory,
    via `clEnqueueCopyBuffer` and finally retrieve the computed values via `clEnqueueReadBuffer`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The created command queue will default to in-order execution, instead of out-of-order
    execution so the device will execute the commands in the order of the queueing.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to talk about the one-dimensional and two-dimensional data
    transfer APIs such as `clEnqueueReadBuffer`, `clEnqueueWriteBuffer`, `clEnqueueWriteBufferRect`,
    and `clEnqueueReadBufferRect` and we are doing this now because you have seen
    that most of our examples, so far, we demonstrated the creation of memory objects
    via `clCreateBuffer` by associating with a memory structure in the host and though
    that might suffice for some situations, you probably want APIs that gives you
    more control when memory objects in the device memory are to be written or read
    from. The control these APIs give you, the developer, is from the fact that they
    are enqueued onto the command-queue with any events the developer might craft;
    and that provides a good permutation of strategies and flexibilities for structuring
    I/O in heterogeneous environments.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that there are similar APIs for reading and writing two or three dimensional
    images to/from host to the device memory. Their names are `clEnqueueReadImage`,
    `clEnqueueWriteImage`, `clEnqueueReadImageRect`, and `clEnqueueWriteImageRect`.
    Refer to the OpenCL 1.2 Specifications for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'These APIs allows us to indicate to the device when we wish the data transfer
    to occur, very much like `clEnqueueCopyBuffer`. Let''s take a look at their method
    signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: These two functions are very similar to one another, and they basically say
    if you wish to read/write to/from a memory buffer , that is, a `cl_mem` object,
    you need to indicate which command queue is it via `command_queue`, what buffer
    it is via `buffer`, whether to be a blocking-read/write via `blocking_read/blocking_write`,
    where to read/write from for what size via `offset` and `cb`, where to read the
    data or write the data to via `ptr`, should this read/write command occur after
    some events via `num_events_in_wait_list` and `event_wait-list`. The last argument
    in the function is `event`, which allows the reading or writing operation to be
    queried which is described in `clEnqueueCopyBuffer`.
  prefs: []
  type: TYPE_NORMAL
- en: Blocking reads in `clEnqueuReadBuffer` means that the command does not exit
    until the host pointer has been filled by the device memory buffer; similarly
    blocking-writes in `clEnqueueWriteBuffer` means that the command doesn't exit
    until the entire device memory buffer has been written to by the host pointer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how these calls are used, you can refer to the earlier illustrated code
    in the recipe *Understanding events and event-synchronization* and for your convenience
    the following is the relevant code in `Ch2/events/events.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Having the capability to model one-dimensional memory objects is fantastic,
    but OpenCL goes a notch further by facilitating two-dimensional memory object
    memory transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of reading a two-dimensional data blocks from the device's
    memory to the output buffer in the host memory; extracted from `Ch2/simple_2d_readwrite/simple_2d_readwrite.c`.
    The code illustrates the usage of the `buffer_origin`, `host_origin`, and `region`
    as in the API. The application will read from the `UDObj cl_mem` object, which
    represents the one-dimensional input data, `hostBuffer`, as a 2 x 2 matrix and
    writes them into the host memory data block represented by `outputPtr`. The application
    reads back the data from the device to host memory and checks for sanity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used the `for` loop and standard array indexing techniques
    in `C` to model how you might iterate through a two-dimensional array and referencing
    the elements so that we progressively copy the input. We won't dwell too much
    into this because, building and running it is very similar to the previous, and
    you should explore the directory to see how the build and program works via the
    Makefile.
  prefs: []
  type: TYPE_NORMAL
- en: Using work items to partition data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced how work can be partitioned in a one-dimensional
    array across several work items (you should flip back now if you cannot remember),
    and also how each work item would obtain an index in which the kernel can use
    to conduct the computation in the kernel code `vector_multiplication`. In this
    recipe, we are going to build on that by exploring two-dimensional data partitioning
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should realize that one of the cornerstones of OpenCL is getting
    the data into the device/s for processing via kernels, and you've seen how data
    can be partitioned among different devices via kernels. In the former, you've
    seen how we used the distributed array pattern to partition the data among the
    devices; this refers to coarse grain data-parallelism. The latter refers to the
    coarse grained task-parallelism that OpenCL provides and it is coarse grained
    because OpenCL is capable of both data-parallelism and task-parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code you've seen so far have been using `clEnqueueTask` to execute
    the kernel based on the one-dimensional data blocks and to get your kernel to
    process two or three dimensional data we need to understand `clEnqueueNDRangeKernel`;
    and how data can be laid out conceptually in two or three dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is helpful to visualize the two or three dimensional data layout in the device
    memory to be row-based instead of column-based.
  prefs: []
  type: TYPE_NORMAL
- en: The `NDRange` in `clEnqueueNDRangeKernel` refers to a data indexing scheme that
    is supposed to span an N-dimensional range of values and hence, the given name.
    Currently, *N* in this N-dimensional index space can be one, two, or three. Next,
    we can split each dimensional into chunks of sizes two, three, four, or more till
    we reached the maximum allowable by the parameter `CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS`.
    Refer to the `Ch1/device_details/device_details.c` on how to obtain the values.
    This would decide how many processing groups we can run in parallel, and in OpenCL
    they are called **work groups**. The work groups would have a number of available
    processing elements that are called **work items** though I like to think of them
    as executable threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work through an example using a two-dimensional data size of 12 rows
    by 12 columns, that is, a 12 x 12 matrix. Let''s look at the following diagram
    to understand how the work groups and work items are related to one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using work items to partition data](img/work_partition.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, I''ve decided to partition the two-dimensional space to create
    nine work groups where each work group is a 4 x 4 matrix. Next, to decide how
    many work items there should be in each work group, and you have two choices:
    a) assign one work-item to process each cell in your 4 x 4 matrix, b) assign one
    work item to process n-cells in your 4 x 4 matrix; in the second option it would
    be similar to vector processing where n-values are loaded together for the work
    item to process. Let''s assume that we''ve decided to choose the option a'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll look at the various data types in the [Chapter 3](ch03.html "Chapter 3. Understanding
    OpenCL Data Types"), *Understanding OpenCL Data Types*.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, let''s take a detailed look at the API `clEnqueueNDRangeKernel`
    with the following method signature, and understand how to input those values
    with our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at what those variables in `clEnqueueNDRangeKernel` are for; the
    `command_queue` refers to the particular queue like the `kernel`, to execute on.
    Next, you need to indicate how many dimensions your input data has via `work_dim`;
    the next two variables `global_work_size` and `local_work_size` would indicate
    how many work groups there are and how many work items / work threads can execute
    in each work group. Recall that the kernel gets scheduled on the device, but it
    is the work group that gets assign compute units of the device and the work items
    execute on the processing element in the compute unit. Next, if you need the launch
    of the kernel to wait on a couple of events in your algorithm, you can indicate
    them through `num_events_in_wait_list` and `event_wait_list`, and finally if you
    wish to associate an event to this kernel's state you can pass in an event type
    to `event` in this API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method signature should not look that intimidating to you by now. Given
    a 12 x 12 matrix partitioned into nine work groups where each work group is a
    4 x 4 matrix and each work item will process one data cell, we will code it like
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure you have got your calculations correct, you can use the following
    simple formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Number of work-groups = (global_work_size[0]*…*global_work_size[n-1]) / (local_work_size[0]*…*local_work_size[n-1])
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to take a look at how we can enable this task-parallelism
    and data-parallelism to be processed by the CPU and GPU where each device will
    copy a one-dimensional data array from the input buffer and treat it like a two-dimensional
    matrix for parallel computing, and finally output the results to a one-dimensional
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `Ch2/work_partition/work_partition.c`, we saw an excerpt where we need to
    copy a million elements from an input buffer to an output buffer using a two-dimensional
    data format. We proceed to partition the data into a 1024 x 1024 matrix where
    each work item processes a single cell and we create work groups of the size 64
    x 2 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Caveat—during my experimentation, this program crashed when executing on the
    OSX 10.6 Intel Core i5 with OpenCL 1.0 as the work group can only be of size one
    in each dimension. We'll look in the [Chapter 3](ch03.html "Chapter 3. Understanding
    OpenCL Data Types"), *Understanding OpenCL Data Types* on how to make our programs
    more portable.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function, `copy2Dfloat4` is a typical function which is executed
    on the device and we like to express the idea of transferring a vector of elements
    from one point to another and once that's done, the application will conduct a
    data sanity check which will pass or fail the program; Refer to the `Ch2/work_partition/work_partition.cl`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main part of this recipe, with the highlighted commentary
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'On OSX, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with Intel OpenCL SDK installed, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu Linux 12.04 with NVIDIA CUDA 5 installed, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: A binary executable named `work_partition` will be deposited on the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu Linux 12.04 with AMD APP SDK v2.8 and NVIDIA CUDA 5 installed, I
    have the following output. If you ran the program using the Intel® OpenCL SDK,
    then you will not see the output related to the discrete graphics chip. In this
    example, we have demonstrated both coarse-grained and fine-grained data and task
    parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The host application allocates two buffers that are capable of storing a million
    elements of the data type `cl_float4`, which is a OpenCL `vector` data type. Next
    we proceed to build the program via `clBuildProgramWithSource` (refer to `Ch2/work_partition/work_partition.c`),
    and pick up all the kernels in the kernel file (`*.cl`). Each detected device
    will pick up a one-dimensional input buffer, transform it to a two-dimensional
    matrix, and partition the data among its parallel computing units where each work
    group will compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the index for the row via `get_global_id(0)`; which can be thought of
    as the thread's ID in the x-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain the index for the column via `get_global_id(1)`; which can be thought
    of as the thread's ID in the y-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together with the row and column indexes, perform a memory load of 4 elements
    and store the same via `C(x,y) = A(x,y)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCL runtime would have partition the data among the work groups, together
    with the IDs for the work items as well as work groups; hence there would not
    be a situation where the thread IDs being duplicated and hence waging mayhem on
    the computation (the OpenCL vendor has that responsibility of ensuring it doesn't
    occur). OpenCL knows how to do this because the dimensions of the input data,
    together with the number of work groups and number of executing work items are
    passed via the parameters `work_dim`, `global_work_size`, and `local_work_size`
    in the `clEnqueueNDRangeKernel` API.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example should clarify this: Assume that the imaginary input data has two-dimensions
    and the `global_work_size` is 8192 and `local_work_size` is 16*16, then we will
    have 8192/(16*16) = 32 work groups; to be able to reference any element in a two-dimensional
    data block, you will write some code similar to this to generate the global thread
    ID in (this is not the only way to do this, but it is the generally preferred
    method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The OpenCL kernel will complete its computation eventually because of an invocation
    to `clWaitForEvents` (we'll talk about this in the next chapter), and then the
    output buffer is stored with data from the device memory via `clEnqueueReadBuffer`
    and the data is sanity checked.
  prefs: []
  type: TYPE_NORMAL
