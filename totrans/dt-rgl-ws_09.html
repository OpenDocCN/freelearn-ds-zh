<html><head></head><body><div><div><div></div>
		</div>
		<div><h1 id="_idParaDest-279"><a id="_idTextAnchor287"/>9. Applications in Business Use Cases and Conclusion of the Course</h1>
		</div>
		<div><p class="callout-heading">Overview</p>
			<p class="callout">This chapter will allow you to utilize the skills you have learned throughout the course of the previous chapters. You will be able to easily handle data wrangling tasks for business use cases. Throughout the chapter, you will be testing the data wrangling skills you've acquired so far by applying them on interesting business problems. These tests, will help you shore up your data wrangling skills, thus giving you the confidence to use them to tackle interesting business problems in the real world.</p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor288"/>Introduction</h1>
			<p>In the previous chapter, we learned about databases. It is time to combine our knowledge of data wrangling and Python with a realistic scenario. Usually, data from one source is often inadequate to perform analysis. Generally, a data wrangler has to distinguish between relevant and non-relevant data and combine data from different sources.</p>
			<p>The primary job of a data wrangling expert is to pull data from multiple sources, format and clean it (impute the data if it is missing), and finally combine it in a coherent manner to prepare a dataset for further analysis by data scientists or machine learning engineers.</p>
			<p>In this chapter, we will try to mimic a typical task flow by downloading and using two different datasets from reputed web portals. Each dataset contains partial data pertaining to the key question that is being asked. Let's examine this more closely.</p>
			<h1 id="_idParaDest-281"><a id="_idTextAnchor289"/>Applying Your Knowledge to a Data Wrangling Task</h1>
			<p>Suppose you are asked the following question:</p>
			<p><em class="italic">In India, did the enrollment in primary/secondary/tertiary education increase with the improvement of per capita GDP in the past 15 years? To provide an accurate and analyzed result, machine learning and data visualization techniques will be used by an expert data scientist</em>. The actual modeling and analysis will be done by a senior data scientist, who will use machine learning and data visualization for analysis. As a data wrangling expert, <em class="italic">your job will be to acquire and provide a clean dataset that contains educational enrollment and GDP data side by side</em>.</p>
			<p>Suppose you have a link for a dataset from the United Nations and you can download the dataset of education (for all the nations around the world). But this dataset has some missing values and, moreover, it does not have any <strong class="bold">Gross Domestic Product</strong> (<strong class="bold">GDP</strong>) information. Someone has also given you another separate CSV file (downloaded from the World Bank site) that contains GDP data but in a messy format. </p>
			<p>In the following activity, we will examine how to handle these two separate sources and clean the data to prepare a simple final dataset with the required data and save it to the local drive as a SQL database file:</p>
			<div><div><img src="img/B15780_09_01.jpg" alt="Figure 9.1: Pictorial representation of merging education and economic data&#13;&#10;" width="1665" height="597"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1: Pictorial representation of merging education and economic data</p>
			<p>You are encouraged to follow along with the code and results in the notebook and try to understand and internalize the nature of the data wrangling flow. You are also encouraged to try extracting various data from these files and answer your own questions about a nation's socio-economic factors and their inter-relationships.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Coming up with interesting questions about social, economic, technological, and geo-political topics and then answering them using freely available data and a little bit of programming knowledge is one of the most fun ways to learn about any data science topic. You will get a taste of that process in this chapter.</p>
			<p>Let's take a look at the following table, which shows information from a dataset of education from the UN data:</p>
			<div><div><img src="img/B15780_09_02.jpg" alt="Figure 9.2: UN data " width="1665" height="871"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2: UN data </p>
			<p>From the preceding table, we can observe that we are missing some data. Let's say we decide to impute these data points by performing simple linear interpolation between the available data points. We can take a calculator and compute those values and manually create a dataset. But being a data wrangler, we will, of course, take advantage of Python programming, and use <code>pandas</code> imputation methods for this task.</p>
			<p>But to do that, we need to create a DataFrame with missing values in it; that is, we need to append another DataFrame with missing values to the current DataFrame.</p>
			<h2 id="_idParaDest-282">Ac<a id="_idTextAnchor290"/>tivity 9.01: Data Wrangling Task – Fixing UN Data</h2>
			<p>The goal of this activity is to perform data analysis on the UN data to find out whether the enrollment in primary, secondary, or tertiary education has increased with the improvement of per capita GDP in the past 15 years. For this task, we will need to clean or wrangle the two datasets, that is, the education enrollment and GDP data.</p>
			<p>The UN data is available at <a href="https://packt.live/30ZIS4N">https://packt.live/30ZIS4N</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you download the CSV file and open it using Excel, then you will see that the <code>Footnotes</code> column sometimes contains useful notes. We may not want to drop it in the beginning. If we are interested in a particular country's data (like we are in this task), then it may well turn out that <code>Footnotes</code> will be <code>NaN</code>, that is, blank. In that case, we can drop it at the end. But for some countries or regions, it may contain information.</p>
			<p>These steps will guide you through this activity:</p>
			<ol>
				<li>Download the dataset from the UN data from GitHub from the following link: <a href="https://packt.live/2AMoeu6">https://packt.live/2AMoeu6</a>.<p>The UN data contains missing values. Clean the data to prepare a simple final dataset with the required data and save it to your local drive as a SQL database file.</p></li>
				<li>Use the <code>pd.read_csv</code> method of <code>pandas</code> to create a DataFrame.</li>
				<li>Since the first row does not contain useful information, skip it using the <code>skiprows</code> parameter.</li>
				<li>Drop the column region/country/area and source.</li>
				<li>Assign the following names as columns of the DataFrame: Region/County/Area, Year, Data, Value, and Footnotes.</li>
				<li>Check how many unique values are present in the <code>Footnotes</code> column.</li>
				<li>Check the type of the <code>value</code> column.</li>
				<li>Create a function to convert the value column into floating-point numbers.</li>
				<li>Use the <code>apply</code> method to apply this function to a value.</li>
				<li>Print the unique values in the data column.</li>
			</ol>
			<p>The final output should be as follows:</p>
			<div><div><img src="img/B15780_09_03.jpg" alt="Figure 9.3: Bar plot for the enrollment in primary education in the USA&#13;&#10;" width="673" height="307"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3: Bar plot for the enrollment in primary education in the USA</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor330">this link</a>.</p>
			<p>Wit<a id="_idTextAnchor291"/>h this, we've reached the end of this activity. Here, we have looked into how to examine a particular real-life dataset to see what kind of data is missing. We also used the interpolate method from our DataFrame to fill in certain missing values.</p>
			<h2 id="_idParaDest-283">Act<a id="_idTextAnchor292"/>ivity 9.02: Data Wrangling Task – Cleaning GDP Data</h2>
			<p>The GDP data is available at <a href="https://data.worldbank.org/">https://data.worldbank.org/</a> and is available on GitHub at <a href="https://packt.live/2AMoeu6">https://packt.live/2AMoeu6</a>.</p>
			<p>In this activity, we will clean the GDP data. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Create three DataFrames from the original DataFrame using filtering. Create the <code>df_primary</code>, <code>df_secondary</code>, and <code>df_tertiary</code> DataFrames for students enrolled in primary education, secondary education, and tertiary education in thousands, respectively.</li>
				<li>Plot bar charts of the enrollment of primary students in a low-income country such as India and a higher-income country such as the USA.</li>
				<li>Since there is missing data, use <code>pandas</code> imputation methods to impute these data points by simple linear interpolation between data points. To do that, create a DataFrame with missing values inserted and append a new DataFrame with missing values to the current DataFrame.</li>
				<li>(For India) Append the rows corresponding to the missing years: <code>2004 – 2009</code>, <code>2011 – 2013</code>.</li>
				<li>Create a dictionary of values with <code>np.nan</code>. Note that there are <code>9</code> missing data points, so we need to create a list with identical values repeated <code>9</code> times.</li>
				<li>Create a DataFrame of missing values (from the preceding dictionary) that we can append.</li>
				<li>Append the DataFrames together.</li>
				<li>Sort by year and reset the indices using <code>reset_index</code>. Use <code>inplace=True</code> to execute the changes on the DataFrame itself.</li>
				<li>Use the interpolate method for linear interpolation. It fills all the <code>NaN</code> values with linearly interpolated values. See the following link for more details about this method: <a href="http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html">http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html</a>.</li>
				<li>Repeat the same steps for USA (or other countries).</li>
				<li>If there are values that are unfilled, use the <code>limit</code> and <code>limit_direction</code> parameters with the interpolate method to fill them in.</li>
				<li>Plot the final graph using the new data.</li>
				<li>Read the GDP data using the <code>pandas</code> <code>read_csv</code> method. It will generally throw an error.</li>
				<li>To avoid errors, try using the <code>error_bad_lines = False</code> option.</li>
				<li>Since there is no delimiter in the file, add the <code>\t</code> delimiter.</li>
				<li>Use the <code>skiprows</code> function to remove rows that are not useful.</li>
				<li>Examine the dataset. Filter the dataset with information that states that it is similar to the previous education dataset.</li>
				<li>Reset the index for this new dataset.</li>
				<li>Drop the rows that aren't useful and re-index the dataset.</li>
				<li>Rename the columns properly. This is necessary for merging the two datasets.</li>
				<li>We will concentrate only on the data from <code>2003</code> to <code>2016</code>. Eliminate the remaining data.</li>
				<li>Create a new DataFrame called <code>df_gdp</code> with rows <code>43</code> to <code>56</code>.</li>
			</ol>
			<p>The final output should be as follows:</p>
			<div><div><img src="img/B15780_09_04.jpg" alt="Figure 9.4: DataFrame focusing on year&#13;&#10;" width="1156" height="643"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4: DataFrame focusing on year</p>
			<p>Now that we've seen how to clean and format the datasets, in the following activity, we'll learn how to merge these two datasets.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor332">this link</a>.</p>
			<h2 id="_idParaDest-284">Acti<a id="_idTextAnchor293"/>vity 9.03: Data Wrangling Task – Merging UN Data and GDP Data</h2>
			<p>The aim of this activity is to merge the two datasets: UN data and GDP data.</p>
			<p>The steps to merge these two databases is as follows:</p>
			<ol>
				<li value="1">Reset the indexes for merging.</li>
				<li>Merge the two DataFrames, <code>primary_enrollment_india</code> and <code>df_gdp</code>, on the <code>Year</code> column.</li>
				<li>Drop the data, footnotes, and region/county/area.</li>
				<li>Rearrange the columns for proper viewing and presentation.</li>
			</ol>
			<p>The output should be as follows:</p>
			<div><div><img src="img/B15780_09_05.jpg" alt="Figure 9.5: Final output&#13;&#10;" width="648" height="344"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5: Final output</p>
			<p>In this activity, we saw how to merge two DataFrames to create a unified view and how to examine that view a little bit. In the next activity, we will learn how to store some of that data in a database.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor334">this link</a>.</p>
			<h2 id="_idParaDest-285">Activ<a id="_idTextAnchor294"/>ity 9.04: Data Wrangling Task – Connecting the New Data to the Database</h2>
			<p>The steps to connect the data to the database is as follows:</p>
			<ol>
				<li value="1">Import the <code>sqlite3</code> module of Python and use the <code>connect</code> function to connect to the database. The main database engine is embedded. But for a different database such as <code>Postgresql</code> or <code>MySQL</code>, we will need to connect to them using those credentials. We designate <code>Year</code> as the <code>PRIMARY KEY</code> of this table.</li>
				<li>Then, run a loop with the dataset rows one by one to insert them into the table.</li>
			</ol>
			<p>The output: If we look at the current folder, we should see a file called <code>Education_GDP.db</code>, and if we examine that using a database viewer program, we will see the data transferred being there.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor336">this link</a>.</p>
			<p>If we<a id="_idTextAnchor295"/> look at the current folder, we should see a file called <code>Education_GDP.db</code>, and if we can examine that using a database viewer program, we will see that the data has been transferred there.</p>
			<p>In these activities, we have examined a complete data wrangling flow, including reading data from the web and a local drive and filtering, cleaning, quick visualization, imputation, indexing, merging, and writing back to a database table. We also wrote custom functions to transform some of the data and saw how to handle situations where we may get errors upon reading the file.</p>
			<h1 id="_idParaDest-286"><a id="_idTextAnchor296"/>An Extension to Data Wrangling</h1>
			<p>This is the concluding chapter of this book; we want to give you a broad overview of some of the exciting technologies and frameworks that you may need to learn about beyond data wrangling to work as a full-stack data scientist. Data wrangling is an essential part of the whole data science and analytics pipeline, but it is not the whole enterprise. You have learned invaluable skills and techniques in this book, but it is always good to broaden your horizons and look beyond to see what other tools that are out there that can give you an edge in this competitive and ever-changing world.</p>
			<h2 id="_idParaDest-287"><a id="_idTextAnchor297"/>Additional Skills Required to Become a Data Scientist</h2>
			<p>To practice as a fully qualified data scientist/analyst, you should have some basic skills in your repertoire, irrespective of the particular programming language you choose to focus on. These skills and know-how are language-agnostic and can be utilized with any framework that you have to embrace, depending on your organization and business needs. We will describe them in brief here:</p>
			<ul>
				<li><strong class="bold">Git and version control</strong>: Git and version control is what RDBMS is to data storage and query. It simply means that there is a huge gap between the pre and post Git era of version controlling your code. As you may have noticed, all the notebooks for this book are hosted on GitHub, and this was done to take advantage of the powerful Git <strong class="bold">Version Control System</strong> (<strong class="bold">VCS</strong>). It gives you, out of the box, version control, history, branching facilities for different code, merging different code branches, and advanced operations such as cherry picking, diff, and so on. It is a very essential tool to master as you can be almost sure that you will face it at one point of time in your journey. </li>
				<li><strong class="bold">Linux command line</strong>: People coming from a Windows background (or even MacOS, if you have not done any development before) are not very familiar, usually, with the command line. The superior UI of those OSes hides the low-level details of interaction with the OS using a command line. However, as a data professional, it is important that you know the command line well. There are so many operations that you can do by simply using the command line that it is astonishing.</li>
				<li><strong class="bold">SQL and basic relational database concepts</strong>: We dedicated an entire chapter to SQL and RDBMS, Chapter 8, SQL and RDBMS. However, as we have already mentioned there, it was really not enough. This is a vast subject and needs years of study to be mastered. Try to read more about it (including getting theory and practical experience) from books and online sources. Don't forget that, despite all the other sources of data being used nowadays, we still have hundreds of millions of bytes of structured data stored in legacy database systems. You can be sure to come across one, sooner or later.</li>
				<li><strong class="bold">Docker and containerization</strong>: Since its first release in 2013, Docker has changed the way we distribute and deploy software in server-based applications. It gives you a clean and lightweight abstraction over the underlying OS and lets you iterate fast on development without the headache of creating and maintaining a proper environment. It is very useful in both the development and production phases. With virtually no competitor present, they are becoming the default in the industry very fast. We strongly advise that you explore it in great detail.</li>
			</ul>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor298"/>Basic Familiarity with Big Data and Cloud Technologies</h2>
			<p>Big data and cloud platforms are the latest trends. We will introduce them here briefly and we encourage you to go ahead and learn about them as much as you can. If you are planning to grow as a data professional, then you can be sure that without these necessary skills, it will be hard for you to transition to the next level:</p>
			<ul>
				<li><strong class="bold">Fundamental characteristics of big data</strong>: Big data is simply data that is very big in size. The term size is a bit ambiguous here. It can mean one static chunk of data (such as the detail census data of a big country such as India or the US) or data that is dynamically generated as time passes, and each time it is huge. To give an example for the second category, we can think of how much data is generated by Facebook per day. It's about 500+ TB per day. You can easily imagine that we will need specialized tools to deal with that amount of data. There are three different categories of big data, that is, structured, unstructured, and semi-structured. The main features that define big data are volume, variety, velocity, and variability.</li>
				<li><strong class="bold">Hadoop ecosystem</strong>: Apache Hadoop (and the related ecosystem) is a software framework that aims to use the MapReduce programming model to simplify the storage and processing of big data. It has since become one of the backbones of big data processing in the industry. The modules in Hadoop are designed to keep in mind that hardware failures are common occurrences, and they should be automatically handled by the framework. The four base modules of Hadoop are common, HDFS, YARN, and MapReduce. The Hadoop ecosystem consists of Apache Pig, Apache Hive, Apache Impala, Apache Zookeeper, Apache HBase, and more. They are very important bricks in many high-demand and cutting-edge data pipelines. We encourage you to study them in more depth. They are essential in any industry that aims to leverage data.</li>
				<li><strong class="bold">Apache Spark</strong>: Apache Spark is a general-purpose cluster computing framework that was initially developed at the University of California, Barkley, and released in 2014. It gives you an interface to program an entire cluster of computers with built-in data parallelism and fault tolerance. It contains Spark Core, Spark SQL, Spark Streaming, MLlib (for machine learning), and GraphX. It is now one of the main frameworks that's used in the industry to process a huge amount of data in real time based on streaming data. We encourage you to read about it and master it if you want to go toward real-time data engineering.</li>
				<li><strong class="bold">Amazon Web Services (AWS)</strong>: Amazon Web Services (often abbreviated to AWS) are a bunch of managed services offered by Amazon ranging from Infrastructure-as-a-Service, Database-as-a-Service, Machine-Learning-as-a-Service, cache, load balancer, NoSQL database, to message queues and several other types. They are very useful for all sorts of applications. It can be a simple web app or a multi-cluster data pipeline. Many famous companies run their entire infrastructure on AWS (such as Netflix). They give us on-demand provisioning, easy scaling, a managed environment, a slick UI to control everything, and also a very powerful command-line client. They also expose a rich set of APIs, and we can find an AWS API client in virtually any programming language. The Python one is called Boto3. If you are planning to become a data professional, then it can be said with near certainty that you will end up using many of their services at one point or another.</li>
			</ul>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor299"/>What Goes with Data Wrangling?</h2>
			<p>We learned in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Data Wrangling with Python</em>, that the process of data wrangling lies in-between data gathering and advanced analytics, including visualization and machine learning. However, the boundaries that exist between these processes may not always be strict and rigid. It depends largely on organizational culture and team composition.</p>
			<p>Therefore, we need to not only be aware of data wrangling but also the other components of the data science platform to wrangle data effectively. Even if you are performing pure data wrangling tasks, having a good grasp over how data is sourced and utilized will give you an edge for coming up with unique and efficient solutions to complex data wrangling problems and enhance the value of those solutions for the machine learning scientist or the business domain expert:</p>
			<div><div><img src="img/B15780_01_01.jpg" alt="Figure 9.6: Process of data wrangling&#13;&#10;" width="721" height="406"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6: Process of data wrangling</p>
			<p>Now, we have, in fact, already laid out a solid groundwork in this book for the data platform part, assuming that it is an integral part of data wrangling workflow. For example, we have covered web scraping, working with RESTful APIs, and database access and manipulation using Python libraries in detail.</p>
			<p>We also touched on basic visualization techniques and plotting functions in Python using <code>matplotlib</code>. However, there are other advanced statistical plotting libraries, such as <code>seaborn</code>, that you can master for more sophisticated visualization for data science tasks.</p>
			<p>Business logic and domain expertise is a varied topic and it can only be learned on the job; however, it will come eventually with experience. If you have an academic background and/or work experience in domains such as finance, medicine and healthcare, or engineering, that knowledge will come in handy in your data science career.</p>
			<p>The fruit of the hard work of data wrangling is realized fully in the domain of machine learning. It is the science and engineering of making machines learn patterns and insights from data for predictive analytics and intelligent, automated decision-making with a deluge of data that cannot be analyzed efficiently by humans. Machine learning has become one of the most sought-after skills in the modern technology landscape. It has truly become one of the most exciting and promising intellectual fields, with applications ranging from e-commerce to healthcare and virtually everything in-between. Data wrangling is intrinsically linked with machine learning as it prepares the data so that it's suitable for intelligent algorithms to process. Even if you start your career in data wrangling, it could be a natural progression to move to machine learning.</p>
			<p>Packt has published numerous books and books on this topic that you should explore. In the next section, we will touch upon some approaches to adopt and Python libraries to check out that will give you a boost in your learning.</p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor300"/>Tips and Tricks for Mastering Machine Learning</h2>
			<p>Machine learning is difficult to start with. We have listed some structured MOOCs and incredible free resources that are available so that you can begin your journey:</p>
			<ul>
				<li>Understand the definition of and differentiation between the buzzwords –  artificial intelligence, machine learning, deep learning, and data science. Cultivate the habit of reading great posts or listening to the expert talks on these topics and understand their true reach and applicability to some business problem.</li>
				<li>There are some great MOOCs that will help you understand all these and also learn advanced skills in machine learning and AI. Some of them are as follows:<p>a) Machine Learning – Andrew Ng, Stanford University</p><p>b) Machine Learning for Undergraduates – Nando de Freitas, University of British Columbia</p><p>c) Machine Learning – Tom Mitchell, CMU</p><p>d) Deep Learning – Nando de Freitas, University of Oxford</p></li>
				<li>Stay updated with the recent trends by watching videos, reading books such as <em class="italic">The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</em>, as well as articles, and following influential blogs such as KDnuggets, Brandon Rohrer's blog, Open AI's blog about their research, Toward Data Science publication on Medium, and so on.</li>
				<li>As you learn new algorithms or concepts, pause and analyze how you can apply these machine learning concepts or algorithms in your daily work. This is the best method for learning and expanding your knowledge base.</li>
				<li>If you choose Python as your preferred language for machine learning tasks, then you have a great machine learning library in <code>scikit-learn</code>. It is the most widely used general machine learning package in the Python ecosystem. <code>scikit-learn</code> has a wide variety of supervised and unsupervised learning algorithms, which are exposed via a stable consistent interface. Moreover, it is specifically designed to interface seamlessly with other popular data wrangling and numerical libraries, such as NumPy and pandas.</li>
				<li>Another hot skill in today's job market is deep learning. Packt has many books on deep learning. For Python libraries, you can learn and practice with <strong class="bold">TensorFlow</strong>, <strong class="bold">Keras</strong>, or <strong class="bold">PyTorch</strong> for deep learning.</li>
			</ul>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor301"/>Summary</h1>
			<p>Data is everywhere and it is all around us. In these nine chapters, we have learned how data from different types and sources can be cleaned, corrected, and combined. Hopefully, this chapter must have tested your skills enough to shore up the concepts you've learned so far. If you want, you can revisit some of the prior chapters to practice your data wrangling skills a bit more. Using the power of Python and the knowledge of data wrangling and applying the tricks and tips that you have studied in this book, you are ready to be a data wrangler.</p>
		</div>
		<div><div></div>
		</div>
	</div></body></html>