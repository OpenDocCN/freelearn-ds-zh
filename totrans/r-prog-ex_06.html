<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Understanding Reviews with Text Analysis</h1>
                
            
            
                
<p class="calibre2">It is well known that a very large percentage of relevant information originates in an unstructured form, an important player being text data. Text analysis, <strong class="calibre1">Natural Language Processing</strong> (<strong class="calibre1">NLP</strong>), <strong class="calibre1">Information Retrieval</strong> (<strong class="calibre1">IR</strong>), and <strong class="calibre1">Statistical Learning</strong> (<strong class="calibre1">SL</strong>) are some areas focused on developing techniques and processes to deal with this data. These techniques and processes discover and present knowledge, facts, business rules, relationships, among others, that is otherwise locked in textual form, impenetrable to automated processing.</p>
<p class="calibre2">Given the explosion of textual data we see nowadays, an important skill for analysts such as statisticians and data scientists is to be able to work efficiently with this data and find the insights they are looking for. In this chapter, we will try to predict whether a customer is going to make repeated purchases given the reviews being sent to The Cake Factory.</p>
<p class="calibre2">Since text analysis is a very broad research area, we need to narrow the techniques we will look at in this chapter to the most important ones. We will take a Pareto approach by focusing on 20% of techniques that will be used 80% of the time when doing text analysis. Some of the important topics covered in this chapter are as follows:</p>
<ul class="calibre11">
<li class="calibre12">Document feature matrices as a basic data structure</li>
<li class="calibre12">Random forests for predictive modeling with text data</li>
<li class="calibre12">Term frequency-inverse document frequencies for measuring importance</li>
<li class="calibre12">N-gram modeling to bring back order into the analysis</li>
<li class="calibre12">Singular vector decomposition for dimensionality reduction</li>
<li class="calibre12">Cosine similarity to find similar feature vectors</li>
<li class="calibre12">Sentiment analysis as an added vector feature</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">This chapter's required packages</h1>
                
            
            
                
<p class="calibre2">Setting up the packages for this chapter may be a bit cumbersome because some of the packages depend on operating system libraries which can vary from computer to computer. Please check  <a href="part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730" class="calibre4">Appendix</a><em class="calibre19">, Required Packages</em> for specific instructions on how to install them for your operating system.</p>
<table class="calibre51">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="mce-root3"><strong class="calibre1">Package</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Reason</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">lsa</kbd></td>
<td class="calibre8">Cosine similarity computation</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">rilba</kbd></td>
<td class="calibre8">Efficient SVD decomposition</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">caret</kbd></td>
<td class="calibre8">Machine learning framework</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">twitteR</kbd></td>
<td class="calibre8">Interface to Twitter's API</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">quanteda</kbd></td>
<td class="calibre8">Text data processing</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">sentimentr</kbd></td>
<td class="calibre8">Text data sentiment analysis</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">randomForest</kbd></td>
<td class="calibre8">
<p class="calibre2">Random forest models</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">We will use the <kbd class="calibre9">rilba</kbd> package (which depends on C code) to compute a part of the <strong class="calibre1">Singular Value Decomposition</strong> (<strong class="calibre1">SVD</strong>) efficiently using the <em class="calibre19">Augmented Implicitly Restarted Lanczos Bidiagonalization Methods, by Baglama and Reichel, 2005</em>, <a href="http://www.math.uri.edu/~jbaglama/papers/paper14.pdf" class="calibre4">http://www.math.uri.edu/~jbaglama/papers/paper14.pdf</a>).</p>
<p class="calibre2">We will use the <kbd class="calibre9">parallel</kbd> package to perform parallel processing since some text analysis can potentially require a lot of computations. The <kbd class="calibre9">parallel</kbd> package is the most general parallelization package in R for now, but it has been reported to not work correctly in some systems. Other options are <kbd class="calibre9">doParallel</kbd>, <kbd class="calibre9">doMC</kbd> and <kbd class="calibre9">doSNOW</kbd>. If you run into trouble when using one <kbd class="calibre9">parallel</kbd>, try switching to one of the other packages. The code to make them work is very similar.</p>
<p class="calibre2">Regarding text data, there are a few packages you can use in R. The most common ones are the <kbd class="calibre9">tm</kbd> package and the <kbd class="calibre9">quanteda</kbd> package. Both are excellent, and differ mostly in style. All the functionality we will see in this chapter can be used with either one of them, but we chose to work with the <kbd class="calibre9">quanteda</kbd> package. It is built with the <kbd class="calibre9">stringi</kbd> package for processing text, the <kbd class="calibre9">data.table</kbd> package for large documents, and the <kbd class="calibre9">Matrix</kbd> package to handle sparse objects. Therefore you can expect it to be very fast and handle Unicode and UTF-8 very well.</p>
<p class="calibre2">If you don't know what Unicode and UTF-8 are, I suggest you read up on them. Very roughly you can think of Unicode as being a standard of IDs for characters, while UTF-8 being a translation of this IDs into bytes computers can understand. During this chapter we won't worry about encodings (all the data is in UTF-8), but it's something that often comes up when working with text data, and is important to handle correctly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">What is text analysis and how does it work?</h1>
                
            
            
                
<p class="calibre2">Text analysis is the process of deriving information from text. Information is typically derived through techniques such as IR, NLP, and SL, and it involves structuring text, deriving patterns with the structured data, and finally evaluating and interpreting the output. The basic models used for text analysis are the bag-of-words models, the vector space model, and the semantic parsing model.</p>
<p class="calibre2">The bag-of-words model is a simplified text representation in which a text (a review in our case) is represented as the set of its terms (words), disregarding grammar and word order but keeping multiplicity (hence the term bag). After transforming the text into a bag-of-words and structuring into a corpus (a structured collection of the text data), we can calculate various measures to characterize the text into a vector space. The bag-of-words model is commonly used in SL methods, and we will use it with random forests in this chapter. In practice, it is used as a tool of feature generation. The following image explains the bag-of-words model:</p>
<div><img src="img/00047.jpeg" class="calibre52"/></div>
<p>Bag-of-words vs semantic parsing</p>
<p class="calibre2">The vector space model uses the bag-of-words that you extracted from the documents to create a feature vector for each text, where each feature is a term and the feature's value is a term weight. The term weight may be a binary value (1 indicating that the term occurred in the document and 0 indicating that it did not), a <strong class="calibre1">term frequency</strong> (<strong class="calibre1">TF</strong>) value (indicating how many times the term occurred in the document), or a <strong class="calibre1">term frequency-inverse document frequency</strong> (<strong class="calibre1">TF-IDF</strong>) value (indicating how important a term is to a text given its corpus). More complex weighting mechanisms exist which are focused on specific problems, but these are the most common ones, and are the ones we will focus on.</p>
<p class="calibre2">Given what we mentioned earlier, a text turns out to be a feature vector, and each feature vector corresponds to a point in a vector space. The model for this vector space is such that there is an axis for every term in the vocabulary, and so the vector space is <em class="calibre19">n</em>-dimensional, where <em class="calibre19">n</em> is the size of the vocabulary in all the data being analyzed (this can be huge). Sometimes, it helps to think about these concepts geometrically. The bag-of-words model and vector space model refer to different aspects of characterizing a body of text, and they complement each other. The following image explains the vector space model:</p>
<div><img src="img/00048.jpeg" class="calibre53"/></div>
<p>Bag-of-words to vector space</p>
<p class="calibre2">An important weakness of the bag-of-words model is the fact that it ignores the terms' semantic context. More complex models exist that attempt to correct these deficiencies. Semantic parsing is one of them, and it's the process of mapping a natural-language sentence into a formal representation of its meaning. It mainly uses combinations of inductive logic programming and statistical learning. These types of techniques become more useful when dealing with complex texts. Even though we won't touch further on them on this book, they are a powerful tool and are a very interesting area of research.</p>
<p class="calibre2">As an example, if you try to think of the representation of the following quote using the bag-of-words model and the semantic parsing model, you may intuitively think that the first one can give nonsense results while the second one can provide at least some understanding, and you would be correct.</p>
<p>"The fish trap exists because of the fish. Once you've gotten the fish you can forget the trap. The rabbit snare exists because of the rabbit. Once you've gotten the rabbit, you can forget the snare. Words exist because of meaning. Once you've gotten the meaning, you can forget the words. Where can I find a man who has forgotten words so I can talk with him?"</p>
<p>– The Writings of Chuang Tzu, 4th century B.C. (Original text in Chinese)</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Preparing, training, and testing data</h1>
                
            
            
                
<p class="calibre2">As always, we will start by setting up our data. In this case, the data is the messages received by our fantasy company, The Cake Factory. These are in the <kbd class="calibre9">client_messages.RDS</kbd> file that we created in <a href="part0091.html#2MP360-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 4</a>, <em class="calibre19">Simulating Sales Data and Working with Databases</em>. The data contains 300 observations for 8 variables: <kbd class="calibre9">SALE_ID</kbd>, <kbd class="calibre9">DATE</kbd>, <kbd class="calibre9">STARS</kbd>, <kbd class="calibre9">SUMMARY</kbd>, <kbd class="calibre9">MESSAGE</kbd>, <kbd class="calibre9">LAT</kbd>, <kbd class="calibre9">LNG</kbd>, and <kbd class="calibre9">MULT_PURCHASES</kbd>. During this chapter, we will work with the <kbd class="calibre9">MESSAGE</kbd> and <kbd class="calibre9">MULT_PURCHASES</kbd> variables.</p>
<p class="calibre2">We will set up our seed to have reproducible results. Keep in mind that this should be before every function call that involves some randomization. We will show it just once here to save space and avoid repeating ourselves, but keep that in mind when you are trying to generate reproducible results:</p>
<pre class="mce-root">set.seed(12345)</pre>
<p class="calibre2">Next, we need to make sure that we don't have any missing data in the relevant variables. To do so, we use the <kbd class="calibre9">complete.cases()</kbd> function together with the negation (<kbd class="calibre9">!</kbd>) and the <kbd class="calibre9">sum()</kbd> function to get the total number of <kbd class="calibre9">NA</kbd> values' in each variable. As you can see, we don't have any missing data:</p>
<pre class="mce-root">sum(!complete.cases(client_messages$MESSAGE))
<strong class="calibre1">#&gt; 0</strong>

sum(!complete.cases(client_messages$MULT_PURCHASES))
<strong class="calibre1">#&gt; 0</strong></pre>
<p class="calibre2">If you have missing data, instead of using some imputation mechanism which is normally done in some data analysis scenarios, you want to remove those observations from this data, since it's easier to get this wrong due to the non-continuous characteristics of textual data.</p>
<p class="calibre2">As you will often find when working on interesting real-world problems in predictive analysis, it's not uncommon to work with disproportionate data. In this case, as can be seen with the shown code, we have around 63% of multiple purchases. This is not very disproportionate, but we still have to play on the safe side by keeping the training and testing data with similar proportions:</p>
<pre class="mce-root">prop.table(table(client_messages$MULT_PURCHASES))
<strong class="calibre1">#&gt;     FALSE      TRUE
#&gt; 0.3621262 0.6378738</strong></pre>
<p class="calibre2">For data with the disproportionality problem, maintaining the same proportions in the testing and training sets is important to get accurate results. Therefore, we need to make sure our sampling method maintains these proportions. To do so, we will use the <kbd class="calibre9">createDataPartition()</kbd> function from the <kbd class="calibre9">caret</kbd> package to extract the indexes for each of the training and testing sets. It will create balanced data splits, and, in this case, it will use 70% of the data for training with a single partition:</p>
<pre class="mce-root">indexes &lt;- createDataPartition(<br class="title-page-name"/>    client_messages$MULT_PURCHASES,<br class="title-page-name"/>    list = FALSE,<br class="title-page-name"/>    times = 1,<br class="title-page-name"/>    p = 0.7<br class="title-page-name"/>)<br class="title-page-name"/>train &lt;- client_messages[ indexes, ]<br class="title-page-name"/>test &lt;- client_messages[-indexes, ]</pre>
<p class="calibre2">To make sure that our proportions are being maintained, we can check each of them individually just as we did before with the full data:</p>
<pre class="mce-root">prop.table(table(train$MULT_PURCHASES))
<strong class="calibre1">#&gt;     FALSE      TRUE
#&gt; 0.3632075 0.6367925</strong>

prop.table(table(test$MULT_PURCHASES))
<strong class="calibre1">#&gt;     FALSE      TRUE
#&gt; 0.3595506 0.6404494</strong></pre>
<p class="calibre2">Now that we have our training and testing sets ready, we can start cleaning and setting up our text data as we will do in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Building the corpus with tokenization and data cleaning</h1>
                
            
            
                
<p class="calibre2">The first thing we need to create when working with text data is to extract the tokens that will be used to create our corpus. Simply, these tokens are all the terms found in every text in our data, put together, and removed the ordering or grammatical context. To create them, we use the <kbd class="calibre9">tokens()</kbd> function and the related functions from the <kbd class="calibre9">quanteda</kbd> package. As you can imagine, our data will not only contain words, but also punctuation marks, numbers, symbols, and other characters like hyphens. Depending on the context of the problem you're working with, you may find it quite useful to remove all of them as we do here. However, keep in mind that in some contexts some of these special characters can be meaningful (for example, the hashtag symbol (#) can be relevant when analyzing Twitter data):</p>
<pre class="mce-root">tokens &lt;- tokens(
    train$MESSAGE,
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE,
    remove_hyphens = TRUE
)</pre>
<p class="calibre2">As you can imagine, there will be a huge number of tokens in the data because there must be one for each unique word in our data. These raw tokens will probably be useless (contain a low signal/noise ratio) if we don't apply some filtering. We'll start by ignoring capitalization. In our context, <em class="calibre19">something</em> and <em class="calibre19">something</em> should be equivalent. Therefore, we use the <kbd class="calibre9">tokens_tolower()</kbd> function to make all tokens lowercase:</p>
<pre class="mce-root">tokens &lt;- tokens_tolower(tokens)</pre>
<p class="calibre2">Also, keep in mind that common words like <em class="calibre19">the</em>, <em class="calibre19">a</em>, and <em class="calibre19">to</em> are almost always the terms with highest frequency in the text and are not particularly important to derive insights. We should thus remove them, as we do with the <kbd class="calibre9">tokens_select()</kbd> function:</p>
<pre class="mce-root">tokens &lt;- tokens_select(tokens, stopwords(), selection = "remove")</pre>
<p class="calibre2">Word stems allow us to reduce the number of tokens that overall share the same meaning. For example, the words <em class="calibre19">probability</em>, <em class="calibre19">probably</em>, and <em class="calibre19">probable</em> probably have the same meaning and their differences are mostly syntactic. Therefore, we could represent all of them with a single token like <em class="calibre19">probab</em> and reduce our feature space considerably. Note that all of these filters have assumptions behind them about the problem we're dealing with, and you should make sure those assumptions are valid. In our case, they are as follows:</p>
<pre class="mce-root">tokens &lt;- tokens_wordstem(tokens, language = "english")</pre>
<p class="calibre2">Instead of having to repeat this by hand every time, we want to create tokens; we can wrap all of these filters in a single function and make our lives a little bit easier down the road. The careful reader will note the <kbd class="calibre9">token_ngrams()</kbd> function and the corresponding <kbd class="calibre9">n_grams = 1</kbd> default parameter. We dedicate a section to this later, but for now, just know that <kbd class="calibre9">n_grams = 1</kbd> means that we want single words in our tokens:</p>
<pre class="mce-root">build_tokens &lt;- function(data, n_grams = 1) {<br class="title-page-name"/>    tokens &lt;- tokens(<br class="title-page-name"/>        data,<br class="title-page-name"/>        remove_punct = TRUE,<br class="title-page-name"/>        remove_numbers = TRUE,<br class="title-page-name"/>        remove_symbols = TRUE,<br class="title-page-name"/>       remove_hyphens = TRUE<br class="title-page-name"/>    )<br class="title-page-name"/>    tokens &lt;- tokens_tolower(tokens)<br class="title-page-name"/>    tokens &lt;- tokens_select(tokens, stopwords(), selection = "remove")<br class="title-page-name"/>    tokens &lt;- tokens_wordstem(tokens, language = "english")<br class="title-page-name"/>    tokens &lt;- tokens_ngrams(tokens, n = 1:n_grams)<br class="title-page-name"/>    return(tokens)<br class="title-page-name"/>}</pre>
<p class="calibre2">Now, even though we have shown the code that is used for this chapter's example, we will use a smaller example (one sentence) so that you can get a visual idea of what's going on in each step. You should definitely get in the habit of doing this yourself when exploring a problem to make sure everything is working as you expect it. We put the code for all the steps here and, after reading the preceding paragraphs, you should be able to identify the differences in each step:</p>
<pre class="mce-root">sentence &lt;- "If it looks like a duck, swims like a duck,
             and quacks like a duck, then it probably is a duck."

tokens &lt;- tokens(sentence)
tokens
<strong class="calibre1">#&gt; tokens from 1 document.
#&gt; text1 :
#&gt;  [1] "If"       "it"       "looks"    "like"     "a"        "duck"
#&gt;  [7] ","        "swims"    "like"     "a"        "duck"     ","
#&gt; [13] "and"      "quacks"   "like"     "a"        "duck"     ","
#&gt; [19] "then"     "it"       "probably" "is"       "a"        "duck"
#&gt; [25] "."</strong>

tokens &lt;- tokens(sentence, remove_punct = TRUE)
tokens
<strong class="calibre1">#&gt; tokens from 1 document.
#&gt; text1 :
#&gt;  [1] "If"       "it"       "looks"    "like"     "a"        "duck"
#&gt;  [7] "swims"    "like"     "a"        "duck"     "and"      "quacks"
#&gt; [13] "like"     "a"        "duck"     "then"     "it"       "probably"
#&gt; [19] "is"       "a"        "duck"</strong>

tokens &lt;- tokens_tolower(tokens)
tokens
<strong class="calibre1">#&gt; tokens from 1 document.
#&gt; text1 :
#&gt;  [1] "if"       "it"       "looks"    "like"     "a"        "duck"
#&gt;  [7] "swims"    "like"     "a"        "duck"     "and"      "quacks"
#&gt; [13] "like"     "a"        "duck"     "then"     "it"       "probably"
#&gt; [19] "is"       "a"        "duck"</strong>

tokens &lt;- tokens_select(tokens, stopwords(), selection = "remove")
tokens
<strong class="calibre1">#&gt; tokens from 1 document.
#&gt; text1 :
#&gt;  [1] "looks"    "like"     "duck"     "swims"    "like"     "duck"
#&gt;  [7] "quacks"   "like"     "duck"     "probably" "duck"</strong>

tokens &lt;- tokens_wordstem(tokens, language = "english")
tokens
<strong class="calibre1">#&gt; tokens from 1 document.
#&gt; text1 :
#&gt;  [1] "look"    "like"    "duck"    "swim"    "like"    "duck"    "quack"
#&gt;  [8] "like"    "duck"    "probabl" "duck"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Document feature matrices</h1>
                
            
            
                
<p class="calibre2">Once we have our tokens ready, we need to create our corpus. At the most basic level, a <strong class="calibre1">corpus</strong> is a collection of texts that includes document-level variables specific to each text. The most basic <em class="calibre19">corpus</em> uses the bag-of-words and vector space models to create a matrix in which each row represents a text in our collection (a client message in our case), and each column represents a term. Each of the values in the matrix would be a 1 or a 0, indicating whether or not a specific term is included in a specific text. This is a very basic representation that we will not use. We will use a <strong class="calibre1">document-feature matrix</strong> (<strong class="calibre1">DFM</strong>), which has the same structure but, instead of using an indicator variable (1s and 0s), it will contain the number of times a term occurred within a text, using the multiplicity characteristic from the bag-of-words model. To create it, we use the <kbd class="calibre9">dfm()</kbd> function from the <kbd class="calibre9">quanteda</kbd> package:</p>
<pre class="mce-root">train.dfm &lt;- dfm(tokens)</pre>
<p class="calibre2">To get a visual example, here's the DFM for the example shown in the previous section. We can see a couple of things here. First, it's a special object with some metadata, number of documents (one sentence in our case), number of features (which is the number of tokens), dimensions, and the actual values for each text in our data. This is our corpus. If we had more than one sentence, we would see more than one row in it:</p>
<pre class="mce-root">dfm(tokens)
<strong class="calibre1">#&gt; Document-feature matrix of: 1 document, 6 features (0% sparse).
#&gt; 1 x 6 sparse Matrix of class "dfmSparse"
#&gt;        features
#&gt;  docs  look like duck swim quack probabl
#&gt; text1  1    3    4    1    1     1</strong></pre>
<p class="calibre2">Now, forget about the one-sentence example, and let's go back to our client messages example. In that case the <kbd class="calibre9">tokens</kbd> object will be much larger. Normally, we tokenize and create our DFM in the same way. Therefore, we create the function that makes it a little easier for us:</p>
<pre class="mce-root">build_dfm &lt;- function(data, n_grams = 1) {<br class="title-page-name"/>    tokens &lt;- build_tokens(data, n_grams)<br class="title-page-name"/>    return(dfm(tokens))<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can create our DFM easily with the following:</p>
<pre class="mce-root">train.dfm &lt;- build_dfm(train$MESSAGE)</pre>
<p class="calibre2">To get an idea of our DFM's characteristics, you can simply print it. As you can see our training DFM has 212 documents (client messages) and 2,007 features (tokens). Clearly, most documents will not contain most of the features. Therefore we have a sparse structure, meaning that 98.4% of the entries in the DFM are actually zero. The educated reader will identify this as being the curse of dimensionality problem common to machine learning, and specially damaging in text analysis. As we will see later, this can be a computational bottleneck that we need to deal with:</p>
<pre class="mce-root">train.dfm<br class="title-page-name"/><strong class="calibre1">#&gt; Document-feature matrix of: 212 documents, 2,007 features (98.4% sparse)</strong></pre>
<p class="calibre2">Often, tokenization requires some additional pre-processing. As you know by now, the tokens we find in our tokenization process end up being column (feature) names in our DFM. If these names contain symbols inside or start with numbers (for example, <em class="calibre19">something&amp;odd</em> or <em class="calibre19">45pieces</em>), then some of our analysis algorithms will complain by throwing errors. We want to prevent that when we transform our DFM into a data frame. We can do so with the convenient <kbd class="calibre9">make.names()</kbd> function. We will also add the <kbd class="calibre9">MULT_PURCHASES</kbd> value (our dependent variable) to our newly created data frame at this point:</p>
<pre class="mce-root">dfm.df &lt;- cbind(MULT_PURCHASES = train$MULT_PURCHASES, data.frame(dfm))
names(dfm.df) &lt;- make.names(names(dfm.df))</pre>
<p class="calibre2">Again, to avoid having to repeat this boilerplate code, we can create our own function that packs this functionality, and easily create our data frame for analysis:</p>
<pre class="mce-root">build_dfm_df &lt;- function(data, dfm) {<br class="title-page-name"/>    df &lt;- cbind(MULT_PURCHASES = data$MULT_PURCHASES, data.frame(dfm))<br class="title-page-name"/>    names(df) &lt;- make.names(names(df))<br class="title-page-name"/>    return(df)<br class="title-page-name"/>}<br class="title-page-name"/>train.dfm.df &lt;- build_dfm_df(train, train.dfm)</pre>
<p class="calibre2">At this point, our training data is ready for analysis in the form of a data frame that can be used by our predictive models. To finish the section, if you want to know which are the most frequent terms in the data, you can use the <kbd class="calibre9">topfeatures()</kbd> function. In this case, most of the features can be intuitively guessed in their context. The only one that may require some explanation is the <kbd class="calibre9">br</kbd> feature. It comes from the fact that our data is coming from HTML pages that contain <kbd class="calibre9">&lt;br&gt;</kbd> strings that signal for a new line in the text (a break, hence the <kbd class="calibre9">br</kbd>). We could remove this feature if we wanted to, but we will leave for now:</p>
<pre class="mce-root">topfeatures(train.dfm)
<strong class="calibre1">#&gt;     br    like    tast  flavor     one    just   coffe    good     tri product
#&gt;    220     107     101      87      82      75      72      71      70      67</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Training models with cross validation</h1>
                
            
            
                
<p class="calibre2">In this section, we will efficiently train our first predictive model for this example and build the corresponding confusion matrix. Most of the functionality comes from the excellent <kbd class="calibre9">caret</kbd> package. You can find more information on the vast features within this package that we will not explore in this book in its documentation (<a href="http://topepo.github.io/caret/index.html" class="calibre4">http://topepo.github.io/caret/index.html</a>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Training our first predictive model</h1>
                
            
            
                
<p class="calibre2">Following best practices, we will use <strong class="calibre1">Cross Validation</strong> (<strong class="calibre1">CV</strong>) as the basis of our modeling process. Using CV we can create estimates of how well our model will do with unseen data. CV is powerful, but the downside is that it requires more processing and therefore more time. If you can take the computational complexity, you should definitely take advantage of it in your projects.</p>
<p class="calibre2">Going into the mathematics behind CV is outside of the scope of this book. If interested, you can find out more information on Wikipedia (<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" class="calibre4">https://en.wikipedia.org/wiki/Cross-validation_(statistics)</a>). The basic idea is that the training data will be split into various parts, and each of these parts will be taken out of the rest of the training data one at a time, keeping all remaining parts together. The parts that are kept together will be used to train the model, while the part that was taken out will be used for testing, and this will be repeated by rotating the parts such that every part is taken out once. This allows you to test the training procedure more thoroughly, before doing the final testing with the testing data.</p>
<p class="calibre2">We use the <kbd class="calibre9">trainControl()</kbd> function to set our repeated CV mechanism with five splits and two repeats. This object will be passed to our predictive models, created with the <kbd class="calibre9">caret</kbd> package, to automatically apply this control mechanism within them:</p>
<pre class="mce-root">cv.control &lt;- trainControl(method = "repeatedcv", number = 5, repeats = 2)</pre>
<p class="calibre2">Our predictive models pick for this example are R<em class="calibre19">andom Forests</em> (RF). We will very briefly explain what RF are, but the interested reader is encouraged to look into James, Witten, Hastie, and Tibshirani's excellent "<em class="calibre19">Statistical Learning</em>" (Springer, 2013). RF are a non-linear model used to generate predictions. A <em class="calibre19">tree</em> is a structure that provides a clear path from inputs to specific outputs through a branching model. In predictive modeling they are used to find limited input-space areas that perform well when providing predictions. RF create many such trees and use a mechanism to aggregate the predictions provided by this trees into a single prediction. They are a very powerful and popular Machine Learning model.</p>
<p class="calibre2">Let's have a look at the random forests example:</p>
<div><img src="img/00049.jpeg" class="calibre54"/></div>
<p>Random forests aggregate trees</p>
<p class="calibre2">To train our model, we use the <kbd class="calibre9">train()</kbd> function passing a formula that signals R to use <kbd class="calibre9">MULT_PURCHASES</kbd> as the dependent variable and everything else (<kbd class="calibre9">~ .</kbd>) as the independent variables, which are the token frequencies. It also specifies the data, the method (<kbd class="calibre9">"rf"</kbd> stands for random forests), the control mechanism we just created, and the number of tuning scenarios to use:</p>
<pre class="mce-root">model.1 &lt;- train(
    MULT_PURCHASES ~ .,
    data = train.dfm.df,
    method = "rf",
    trControl = cv.control,
    tuneLength = 5
)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Improving speed with parallelization</h1>
                
            
            
                
<p class="calibre2">If you actually executed the previous code in your computer before reading this, you may have found that it took a long time to finish (8.41 minutes in our case). As we mentioned earlier, text analysis suffers from very high dimensional structures which take a long time to process. Furthermore, using CV runs will take a long time to run. To cut down on the total execution time, use the <kbd class="calibre9">doParallel</kbd> package to allow for multi-core computers to do the training in parallel and substantially cut down on time. </p>
<p class="calibre2">We proceed to create the <kbd class="calibre9">train_model()</kbd> function, which takes the data and the control mechanism as parameters. It then makes a cluster object with the <kbd class="calibre9">makeCluster()</kbd> function with a number of available cores (processors) equal to the number of cores in the computer, detected with the <kbd class="calibre9">detectCores()</kbd> function. Note that if you're planning on using your computer to do other tasks while you train your models, you should leave one or two cores free to avoid choking your system (you can then use <kbd class="calibre9">makeCluster(detectCores() - 2)</kbd> to accomplish this). After that, we start our time measuring mechanism, train our model, print the total time, stop the cluster, and return the resulting model.</p>
<pre class="mce-root">train_model &lt;- function(data, cv.control) {<br class="title-page-name"/>    cluster &lt;- makeCluster(detectCores())<br class="title-page-name"/>    registerDoParallel(cluster)<br class="title-page-name"/>    start.time &lt;- Sys.time()<br class="title-page-name"/>    model &lt;- train(<br class="title-page-name"/>        MULT_PURCHASES ~ .,<br class="title-page-name"/>        data = data,<br class="title-page-name"/>        method = "rf",<br class="title-page-name"/>        trControl = cv.control,<br class="title-page-name"/>        tuneLength = 5<br class="title-page-name"/>    )<br class="title-page-name"/>    print(Sys.time() - start.time)<br class="title-page-name"/>    stopCluster(cluster)<br class="title-page-name"/>    return(model)<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can retrain the same model much faster. The time reduction will depend on your computer's available resources. In the case of an 8-core system with 32 GB of memory available, the total time was 3.34 minutes instead of the previous 8.41 minutes, which implies that with parallelization, it only took 39% of the original time. Not bad right? We will study more the mechanics of parallelization and its advantages and disadvantages more in <a href="part0229.html#6QCGQ0-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 9</a>, <em class="calibre19">Implementing An Efficient Simple Moving Average</em>. Let's have look at how the model is trained:</p>
<pre class="mce-root">model.1 &lt;- train_model(train.dfm.df, cv.control)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Computing predictive accuracy and confusion matrices</h1>
                
            
            
                
<p class="calibre2">Now that we have our trained model, we can see its results and ask it to compute some predictive accuracy metrics. We start by simply printing the object we get back from the <kbd class="calibre9">train()</kbd> function. As can be seen, we have some useful metadata, but what we are concerned with right now is the predictive accuracy, shown in the <kbd class="calibre9">Accuracy</kbd> column. From the five values we told the function to use as testing scenarios, the best model was reached when we used 356 out of the 2,007 available features (tokens). In that case, our predictive accuracy was 65.36%.</p>
<p class="calibre2">If we take into account the fact that the proportions in our data were around 63% of cases with multiple purchases, we have made an improvement. This can be seen by the fact that if we just guessed the class with the most observations (<kbd class="calibre9">MULT_PURCHASES</kbd> being true) for all the observations, we would only have a 63% accuracy, but using our model we were able to improve toward 65%. This is a 3% improvement. We will try to increase this improvement as we go through this chapter.</p>
<p class="calibre2">Keep in mind that this is a randomized process, and the results will be different every time you train these models. That's why we want a repeated CV as well as various testing scenarios to make sure that our results are robust:</p>
<pre class="mce-root">model.1
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt;  212 samples
#&gt; 2007 predictors
#&gt;    2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 170, 169, 170, 169, 170, 169, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;      2  0.6368771  0.00000000
#&gt;     11  0.6439092  0.03436849
#&gt;     63  0.6462901  0.07827322
#&gt;    356  0.6536545  0.16160573
#&gt;   2006  0.6512735  0.16892126
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 356.</strong></pre>
<p class="calibre2">To create a confusion matrix, we can use the <kbd class="calibre9">confusionMatrix()</kbd> function and send it the model's predictions first and the real values second. This will not only create the confusion matrix for us, but also compute some useful metrics such as sensitivity and specificity. We won't go deep into what these metrics mean or how to interpret them since that's outside the scope of this book, but we highly encourage the reader to study them using the resources cited in this chapter:</p>
<pre class="mce-root">confusionMatrix(model.1$finalModel$predicted, train$MULT_PURCHASES)
<strong class="calibre1">#&gt; Confusion Matrix and Statistics
#&gt;
#&gt;           Reference
#&gt; Prediction FALSE TRUE
#&gt;      FALSE    18   19
#&gt;      TRUE     59  116
#&gt;
#&gt;                Accuracy : 0.6321
#&gt;                  95% CI : (0.5633, 0.6971)
#&gt;     No Information Rate : 0.6368
#&gt;     P-Value [Acc &gt; NIR] : 0.5872
#&gt;
#&gt;                   Kappa : 0.1047
#&gt;  Mcnemar's Test P-Value : 1.006e-05
#&gt;
#&gt;             Sensitivity : 0.23377
#&gt;             Specificity : 0.85926
#&gt;          Pos Pred Value : 0.48649
#&gt;          Neg Pred Value : 0.66286
#&gt;              Prevalence : 0.36321
#&gt;          Detection Rate : 0.08491
#&gt;    Detection Prevalence : 0.17453
#&gt;       Balanced Accuracy : 0.54651
#&gt;
#&gt;        'Positive' Class : FALSE</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Improving our results with TF-IDF</h1>
                
            
            
                
<p class="calibre2">In general in text analysis, a high raw count for a term inside a text does not necessarily mean that the term is more important for the text. One of the most important ways to normalize the term frequencies is to weigh a term by how often it appears not only in a text, but also in the entire corpus.</p>
<p class="calibre2">The more a word appears inside a given text and doesn't appear too much across the whole corpus, it means that it's probably important for that specific text. However, if the term appears a lot inside a text, but also appears a lot in other texts in the corpus, it's probably not important for the specific text, but for the entire corpus, and this dilutes it's predictive power.</p>
<p class="calibre2">In IR, TF-IDF is one of the most popular term-weighting schemes and it's the mathematical implementation of the idea expressed in the preceding paragraph. The TF-IDF value increases proportionally to the number of times a word appears in a given text, but is diluted by the frequency of the word in the entire corpus (not only the given text), which helps to adjust for the fact that some words appear more frequently in general. It is a powerful technique for enhancing the information contained within a DFM.</p>
<p class="calibre2">The TF normalizes all documents in the corpus to be length independent. The <strong class="calibre1">inverse document frequency</strong> (<strong class="calibre1">IDF</strong>) accounts for the frequency of term appearance in all documents in the corpus. The multiplication of TF by IDF takes both of these concepts into account by multiplying them. The mathematical definitions are as follows:</p>
<p class="cdpaligncenter1"><em class="calibre19">n(DFM):= number  of texts in DFM</em></p>
<p class="cdpaligncenter1"><em class="calibre19">Freq(term, text):= count of term in text</em></p>
<p class="cdpaligncenter1"><em class="calibre19">Freq(term, DFM):= count of term in DFM</em></p>
<div><img class="fm-editor-equation2" src="img/00050.jpeg"/></div>
<div><img class="fm-editor-equation3" src="img/00051.jpeg"/></div>
<div><img class="fm-editor-equation4" src="img/00052.jpeg"/></div>
<p class="calibre2">Now we are going to show how to program this TF-IDF statistic with R functions. Remember that we're working with a <kbd class="calibre9">dfm</kbd>, so we can use vectorized operations to make our functions efficient and easy to program. The first three functions <kbd class="calibre9">term_frequency()</kbd>, <kbd class="calibre9">inverse_document_frequency()</kbd>, and <kbd class="calibre9">tf_idf()</kbd>, should be easy to understand.</p>
<p class="calibre2">The <kbd class="calibre9">build_tf_idf()</kbd> function makes use of these functions to actually build the TF-IDF-weighted DFM. The idea being that we need to apply the functions we created to the rows or columns as necessary using the <kbd class="calibre9">apply()</kbd> function. We need to transpose the structure we get to get the texts in the rows and the features in the columns which is why we use the <kbd class="calibre9">t()</kbd> function midway through. Finally, we need to realize that sometimes we get NA's for certain combinations of data (try to figure out these cases yourself to make sure you understand) and we need to substitute them with zeros:</p>
<pre class="mce-root">build_tf_idf &lt;- function(dfm, idf = NULL) {<br class="title-page-name"/>    tf &lt;- apply(as.matrix(dfm), 1, term_frequency)<br class="title-page-name"/>    if (is.null(idf)) {<br class="title-page-name"/>        idf &lt;- apply(as.matrix(dfm), 2, inverse_document_frequency)<br class="title-page-name"/>    }<br class="title-page-name"/>    tfidf &lt;- t(apply(tf, 2, tf_idf, idf = idf))<br class="title-page-name"/>    incomplete_cases &lt;- which(!complete.cases(tfidf))<br class="title-page-name"/>    tfidf[incomplete_cases, ] &lt;- rep(0.0, ncol(tfidf))<br class="title-page-name"/>    return(tfidf)<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can easily build our TF-IDF-weighted DFM using our <kbd class="calibre9">build_tf_df()</kbd> function and create the corresponding data frame as we have done earlier:</p>
<pre class="mce-root">train.tfidf &lt;- build_tf_idf(train.dfm)<br class="title-page-name"/>train.tfidf.df &lt;- build_dfm_df(train, train.tfidf)</pre>
<p class="calibre2">Now the values inside our TF-IDF-weighted DFM will not only be integer values, which corresponded to frequency counts, but will be floating point values that correspond to the TF-IDF weights instead. We can train our next model using the same <kbd class="calibre9">train_model()</kbd> we used earlier:</p>
<pre class="mce-root">model.2 &lt;- train_model(train.tfidf.df, cv.control)</pre>
<p class="calibre2">This time, our training process took 2.97 minutes. To see the results, simply print the model object. Remember that our previous predictive accuracy was of 65.36%. Now that we have used the TF-IDF-weighted DFM, it increased to 66.48%. This is not a drastic improvement, and it's due to the specific data we are working with. When working with other data or domains, you can expect this increase to be much larger:</p>
<pre class="mce-root">model.2
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt;  212 samples
#&gt; 2007 predictors
#&gt;    2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 170, 170, 170, 169, 169, 169, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;      2  0.6368771  0.00000000
#&gt;     11  0.6368771  0.00000000
#&gt;     63  0.6392580  0.01588785
#&gt;    356  0.6603544  0.13818300
#&gt;   2006  0.6648948  0.18269878
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 2006.</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Adding flexibility with N-grams</h1>
                
            
            
                
<p class="calibre2">The bag-of-words model takes into account isolated terms called <strong class="calibre1">unigrams</strong>. This looses the order of the words, which can be important in some cases. A generalization of the technique is called n-grams, where we use single words as well as word pairs or word triplets, in the case of bigrams and trigrams, respectively. The n-gram refers to the general case where you keep up to <kbd class="calibre9">n</kbd> words together in the data. Naturally this representation exhibits unfavorable combinatorial complexity characteristics and makes the data grow exponentially. When dealing with a large corpus this can take significant computing power.</p>
<p class="calibre2">With the <kbd class="calibre9">sentence</kbd> object we created before to exemplify how the tokenization process works (it contains the sentence: <kbd class="calibre9">If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck</kbd>.) and the <kbd class="calibre9">build_dfm()</kbd> function we created with the <kbd class="calibre9">n_grams</kbd> argument, you can compare the resulting DFM with <kbd class="calibre9">n_grams = 2</kbd> to the one with <kbd class="calibre9">n_grams = 1</kbd>. After analyzing the features in this DFM, you should have a clear idea on how the tokenization process filters some data out and how the bigrams are created. As you can see, n-grams can potentially bring back some of the lost word ordering, which sometimes can be very useful:</p>
<pre class="mce-root">build_dfm(sentence, n_grams = 2)
<strong class="calibre1">#&gt; Document-feature matrix of: 1 document, 14 features (0% sparse).
#&gt; 1 x 14 sparse Matrix of class "dfmSparse"
#&gt;          features
#&gt;  docs    look like duck swim quack probabl look_like like_duck duck_swim
#&gt; text1    1    3    4    1    1     1       1         3         1
#&gt;          features
#&gt;  docs    swim_like duck_quack quack_like duck_probabl probabl_duck
#&gt; text1    1         1          1          1            1</strong></pre>
<p class="calibre2">To retrain our full model, we will recreate our TF-IDF-weighted DFM with bigrams this time and its corresponding data frame. Using the function we created earlier, it can easily be done with the following code:</p>
<pre class="mce-root">train.bigrams.dfm &lt;- build_dfm(train$MESSAGE, n_grams = 2)<br class="title-page-name"/>train.bigrams.tfidf &lt;- build_tf_idf(train.bigrams.dfm)<br class="title-page-name"/>train.bigrams.tfidf.df &lt;- build_dfm_df(train, train.bigrams.tfidf)</pre>
<p class="calibre2">Now we will retrain the model and analyze its results. This can potentially take a lot of time depending on the computer you're using for training it. In our 8-core computer with 32 GB of memory, it took 21.62 minutes when executed in parallel. This is due to the large increase in the number of predictors. As you can see, we now have 9,366 predictors instead of the 2,007 predictors we had before. This huge 4x increase is due to the bigrams.</p>
<p class="calibre2">In this particular case, it seems that the added complexity from the bigrams doesn't increase our predictive accuracy. As a matter of fact, it decreases it. This can be for a couple of reasons, one of which is the increased sparsity, which implies a lower signal/noise ratio. In the next section, we will try to increase this ratio while keeping the bigrams:</p>
<pre class="mce-root">model.3 &lt;- train_model(train.bigrams.tfidf.df, cv.control)<br class="title-page-name"/>model.3
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt;  212 samples
#&gt; 9366 predictors
#&gt;    2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 170, 170, 169, 170, 169, 170, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;      2  0.6368771   0.000000000
#&gt;     16  0.6368771   0.000000000
#&gt;    136  0.6344961  -0.004672897
#&gt;   1132  0.6133998  -0.007950251
#&gt;   9365  0.6109081   0.051144597
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 2.</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reducing dimensionality with SVD</h1>
                
            
            
                
<p class="calibre2">As we have seen in the previous section, the dimensionality course in our data was amplified due to the n-gram technique. We would like to be able to use n-grams to bring back word ordering into our DFM, but we would like to reduce the feature space at the same time. To accomplish this, we can use a number of different dimensionality reduction techniques. In this case, we will show how to use the SVD.</p>
<p class="calibre2">The SVD helps us compress the data by using it's singular vectors instead of the original features. The math behind the technique is out of the scope of the book, but we encourage you to look at Meyer's, <em class="calibre19">Matrix Analysis &amp; Applied Linear Algebra, 2000</em>. Basically, you can think of the singular vectors as the important directions in the data, so instead of using our <em class="calibre19">normal</em> axis, we can use these singular vectors in a transformed space where we have the largest signal/noise ratio possible. Computing the full SVD can potentially take a very large amount of time and we don't really need all the singular vectors. Therefore, we will use the <kbd class="calibre9">irlba</kbd> package to make use of the <strong class="calibre1">Implicitly Restarted Lanczos Bidiagonalization Algorithm</strong> (<strong class="calibre1">IRLBA</strong>) for a fast partial SVD, which is much faster.</p>
<p class="calibre2">When using the partial SVD as our DFM, we are actually working in a different vector space, where each feature is no longer a token, but a combination of tokens. These new features are not easy to comprehend and you shouldn't try to. Treat it like a <em class="calibre19">black-box</em> model, knowing that you're operating in a higher signal/noise ratio space than you started in while drastically reducing it's dimensions. In our case, we will make the new space 1/4 of the original space. To do so, we will create a wrapper function to measure the time it takes to actually compute the partial SVD. The actual computation will be done with the <kbd class="calibre9">irlba()</kbd> function, sending the TF-IDF-weighted bigrams DFM and the number of singular vectors we want (1/4 of the possible ones) as the <kbd class="calibre9">nv</kbd> parameter:</p>
<pre class="mce-root">build_svd &lt;- function(dfm) {<br class="title-page-name"/>    dfm &lt;- t(dfm)<br class="title-page-name"/>    start.time &lt;- Sys.time()<br class="title-page-name"/>    svd &lt;- irlba(dfm, nv = min(nrow(dfm), ncol(dfm)) / 4)<br class="title-page-name"/>    print(Sys.time() - start.time)<br class="title-page-name"/>    return(svd)<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can easily create the partial SVD and the corresponding data frame. We also proceed to retrain our model. Note that even though it is conceptual, the <kbd class="calibre9">train.bigrams.svd</kbd> is our new DFM, in practice, within R, it's an object that contains our DFM as well as other data. Our DFM is in the <kbd class="calibre9">v</kbd> object within the <kbd class="calibre9">train.bigrams.svd</kbd> object, which is what we send to the <kbd class="calibre9">buildf_dfm_df()</kbd> function. Another important object within <kbd class="calibre9">train.bigrams.svd</kbd> is <kbd class="calibre9">d</kbd>,  which contains the singular values from the decomposition.</p>
<p class="calibre2">As you can see, our feature space was drastically reduced to only 53 features (which is approximately 1/4 of the 212 samples available). However, our predictive accuracy was not higher than our previous results either. This means that probably bigrams are not adding too much information for this particular problem:</p>
<pre class="mce-root">train.bigrams.svd &lt;- build_svd(train.bigrams.tfidf)<br class="title-page-name"/>train.bigrams.svd.df &lt;- build_dfm_df(train, train.bigrams.svd$v)<br class="title-page-name"/>model.4 &lt;- train_model(train.bigrams.svd.df, cv.control)<br class="title-page-name"/>model.4
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt; 212 samples
#&gt;  53 predictors
#&gt;   2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 169, 170, 170, 170, 169, 170, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;    2    0.6344408  0.05602509
#&gt;   14    0.6225360  0.06239153
#&gt;   27    0.6272979  0.09265294
#&gt;   40    0.6485604  0.13698858
#&gt;   53    0.6366002  0.12574827
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 40.</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Extending our analysis with cosine similarity</h1>
                
            
            
                
<p class="calibre2">Now we proceed to another technique familiar in linear algebra which operates on a vector space. The technique is known as <strong class="calibre1">cosine similarity</strong> (<strong class="calibre1">CS</strong>), and its purpose is to find vectors that are similar (or different) from each other. The idea is to measure the direction similarity (not magnitude) among client messages, and try to use it to predict similar outcomes when it comes to multiple purchases. The cosine similarity will be between 0 and 1 when the vectors are orthogonal and perpendicular, respectively. However, this similarity should not be interpreted as percentage because the movement rate for the cosine function is not linear. This means that a movement from 0.2 to 0.3 does not represent a similar movement magnitude from 0.8 to 0.9.</p>
<p class="calibre2">Given two vectors (rows in our DFM), the cosine similarity among them is computed by taking the dot product between them and dividing it by the product of the Euclidian norms. To review what these concepts mean, take a look at Meyer's, <em class="calibre19">Matrix Analysis &amp; Applied Linear Algebra, 2000</em>.</p>
<div><img class="alignnone1" src="img/00053.jpeg"/></div>
<p class="calibre2">We create the <kbd class="calibre9">cosine_similarties()</kbd> function that will make use of the <kbd class="calibre9">cosine()</kbd> function from the <kbd class="calibre9">lsa</kbd> package. We send it a data frame and remove the first column, which corresponds to the dependent variable <kbd class="calibre9">MULT_PURCHASES</kbd>, and we use the transpose to make sure that we're working with the correct orientation:</p>
<pre class="mce-root">cosine_similarities &lt;- function(df) {<br class="title-page-name"/>    return(cosine(t(as.matrix(df[, -c(1)]))))<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we create the <kbd class="calibre9">mean_cosine_similarities()</kbd> function, which will take the cosine similarity among those texts that correspond to clients that have performed multiple purchases and will take the means of these similarities. We need to take the mean because we are computing many similarities among many vectors, and we want to aggregate them for each one of them. We could use other aggregation mechanisms, but the mean is fine for now:</p>
<pre class="mce-root">mean_cosine_similarities &lt;- function(df) {<br class="title-page-name"/>    similarities &lt;- cosine_similarities(df)<br class="title-page-name"/>    indexes &lt;- which(df$MULT_PURCHASES == TRUE)<br class="title-page-name"/>    df$MULT_PURCHASES_SIMILARITY &lt;- rep(0.0, nrow(df))<br class="title-page-name"/>    for (i in 1:nrow(df)) {<br class="title-page-name"/>        df$MULT_PURCHASES_SIMILARITY[i] &lt;- mean(similarities[i, indexes])<br class="title-page-name"/>    }<br class="title-page-name"/>    return(df)<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can use this function to generate a new DFM's data frame that will be used to train a new model, which will take into account the cosine similarity among texts. As we saw earlier, it seems that using bigrams is not helping too much for this particular data. In the next section, we will try a different, very interesting, technique, sentiment analysis. Let's look at the following code:</p>
<pre class="mce-root">train.bigrams.svd.sim.df &lt;- mean_cosine_similarities(train.bigrams.svd.df)<br class="title-page-name"/>model.5 &lt;- train_model(train.bigrams.svd.sim.df, cv.control)<br class="title-page-name"/>model.5
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt; 212 samples
#&gt;  54 predictors
#&gt;   2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 169, 170, 170, 170, 169, 170, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;    2    0.6460687  0.08590598
#&gt;   15    0.6227021  0.05793928
#&gt;   28    0.6437431  0.12111778
#&gt;   41    0.6296788  0.09535957
#&gt;   54    0.6227021  0.07662715
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 2.</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Digging deeper with sentiment analysis</h1>
                
            
            
                
<p class="calibre2">We have now seen that vector space operations did not work too well regarding the predictive accuracy of our model. In this section, we will attempt a technique which is very different and is closer to the semantic parsing model we mentioned at the beginning of this chapter. We will try sentiment analysis.</p>
<p class="calibre2">We will not only take into account the words in a text, but we will also take into account shifters (that is, negators, amplifiers, de-amplifiers, and adversative conjunctions). A negator flips the sign of a polarized word (for example, I do not like it). An amplifier increases the impact of a polarized word (for example, I really like it.). A de-amplifier reduces the impact of a polarized word (for example, I hardly like it). An adversative conjunction overrules the previous clause containing a polarized word (for example, I like it but it's not worth it). This can be very powerful with some types of data.</p>
<p class="calibre2">Our sentiment analysis will produce a number, which will indicate the sentiment measured from the text. These numbers are unbounded and can be either positive or negative, corresponding to positive or negative sentiments. The larger the number, the stronger the inferred sentiment. To implement the technique, we will use the <kbd class="calibre9">sentimentr</kbd> package, which includes a clever algorithm to compute these sentiments. For the enthusiast, the details of the equation used are in its documentation (<a href="https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf" class="calibre4">https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf</a>).</p>
<p class="calibre2">To apply this technique, we send messages to the <kbd class="calibre9">sentiment_by()</kbd> function. This will give us back an object that contains, among other things, the <kbd class="calibre9">word_count</kbd> value and <kbd class="calibre9">ave_sentiment</kbd>, which is the average sentiment measured in all the sentences within a given text (<kbd class="calibre9">sentinmentr</kbd> internally splits each text into its components (sentences) and measures sentiment for each of them). We then add this objects into our DFM and proceed to train our model.</p>
<p class="calibre2">As you can see, this time we get a large increase in the predictive accuracy of the model up to 71.73%. This means that the sentiment feature is highly predictive compared to the other features we engineered in previous sections. Even though we could continue mixing models and exploring to see if we can get even higher predictive accuracy, we will stop at this point since you probably understand how to do these on your own at this point:</p>
<pre class="mce-root">train.sentiment &lt;- sentiment_by(train$MESSAGE)<br class="title-page-name"/>train.sentiments.df &lt;- cbind(<br class="title-page-name"/>    train.tfidf.df,<br class="title-page-name"/>    WORD_COUNT = train.sentiment$word_count,<br class="title-page-name"/>    SENTIMENT = train.sentiment$ave_sentiment<br class="title-page-name"/>)<br class="title-page-name"/>model.6 &lt;- train_model(train.sentiments.df, cv.control)<br class="title-page-name"/>model.6
<strong class="calibre1">#&gt; Random Forest
#&gt;
#&gt;  212 samples
#&gt; 2009 predictors
#&gt;    2 classes: 'FALSE', 'TRUE'
#&gt;
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 2 times)
#&gt; Summary of sample sizes: 170, 170, 169, 170, 169, 170, ...
#&gt; Resampling results across tuning parameters:
#&gt;
#&gt;   mtry  Accuracy   Kappa
#&gt;      2  0.6368771  0.00000000
#&gt;     11  0.6440753  0.04219596
#&gt;     63  0.6863787  0.22495962
#&gt;    356  0.6935770  0.28332726
#&gt;   2008  0.7173198  0.31705425
#&gt;
#&gt; Accuracy was used to select the optimal model using  the largest value.
#&gt; The final value used for the model was mtry = 2008.</strong></pre>
<p class="calibre2">Sentiment analysis, even though it looks very easy since it did not require a lot of code thanks to the <kbd class="calibre9">sentimentr</kbd> package, is actually a very hard area with active research behind it. It's very important for companies to understand how their customers feel about them, and do so accurately. It is and will continue to be a very interesting area of research.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing our predictive model with unseen data</h1>
                
            
            
                
<p class="calibre2">Now that we have our final model, we need to validate its results by testing it with unseen data. This will give us the confidence that our model is well trained and will probably produce similar results where new data is handed to use.</p>
<p class="calibre2">A careful reader should have noticed that we used the TF-IDF data frame when creating our sentiment analysis data, and not any of the ones we create later with combinations of bigrams, SVDs, and cosine similarities, which operate in a different semantic space due to the fact they are transformations of the original DFM. Therefore, before we can actually use our trained model to make predictions on the test data, we need to transform it into an equivalent space as our training data. Otherwise, we would be comparing apples and oranges, which would give us nonsense results.</p>
<p class="calibre2">To make sure that we're working in the same semantic space, we will apply the TF-IDF weights to our test data. However, if you think about it, there probably are a lot of terms in our test data that were not present in our training data. Therefore, our DFMs will have different dimensions for our training and testing sets. This is a problem, and we need to make sure that they are the same. To accomplish this, we build the DFM for the testing data and apply a filter to it which only keeps the terms that are present in our training DFM:</p>
<pre class="mce-root">test.dfm &lt;- build_dfm(test)<br class="title-page-name"/>test.dfm &lt;- dfm_select(test.dfm, pattern = train.dfm, selection = "keep")</pre>
<p class="calibre2">Furthermore, if you think about it, the corpus-weighing part of the TF-IDF, that is the IDF, will also be different than these two datasets due to the change in the term space for the corpus. Therefore, we need to make sure that we ignore those terms that are new (that is, were not seen in our training data) and use the IDF from our training procedure to make sure that our tests are valid. To accomplish this, we will first compute only the IDF part of the TF-IDF for our training data and use that when computing the TF-IDF for our testing data:</p>
<pre class="mce-root">train.idf &lt;- apply(as.matrix(train.dfm), 2, inverse_document_frequency)<br class="title-page-name"/>test.tfidf &lt;- build_tf_idf(test.dfm, idf = train.idf)<br class="title-page-name"/>test.tfidf.df &lt;- build_dfm_df(test, test.tfidf)</pre>
<p class="calibre2">Now that we have the testing data projected into the vector space of the training data, we can compute the sentiment analysis for the new testing DFM and compute our predictions:</p>
<pre class="mce-root">test.sentiment &lt;- sentiment_by(test$MESSAGE)<br class="title-page-name"/>test.sentiments.df &lt;- cbind(<br class="title-page-name"/>    test.tfidf.df,<br class="title-page-name"/>    WORD_COUNT = test.sentiment$word_count,<br class="title-page-name"/>    SENTIMENT = test.sentiment$ave_sentiment<br class="title-page-name"/>)</pre>
<p class="calibre2">Note that we are not training a new model in this case, we are just using the last model we created and use that to provide predictions for the testing DFM:</p>
<pre class="mce-root">predictions &lt;- predict(model.6, test.sentiments.df)</pre>
<p class="calibre2">To know how well we predicted, we can just print a model as we did before because we would just be looking at the results from the training process that does not include predictions for the testing data. What we need to do is create a confusion matrix and compute the predictive accuracy metrics as we did before with the <kbd class="calibre9">confusionMatrix()</kbd> function.</p>
<p class="calibre2">As you can see, our results seem to be valid since we got a predictive accuracy of 71.91% with previously unseen data, which is very close to the predictive accuracy of the training data, and is 12% more than just guessing actual multiple purchases proportion. For text data and the problem we're dealing with, these results are pretty good.</p>
<p class="calibre2">If you know how to interpret the other metrics, make sure that you compare them to ones we had for our first model to realize how our results evolved during the chapter.</p>
<p class="calibre2">If you're not familiar with them, we suggest you take a look at James, Witten, Hastie, and Tibshirani's, <em class="calibre19">Statistical Learning, 2013</em>:</p>
<pre class="mce-root">confusionMatrix(predictions, test$MULT_PURCHASES)
<strong class="calibre1">#&gt; Confusion Matrix and Statistics
#&gt;
#&gt;           Reference
#&gt; Prediction FALSE TRUE
#&gt;      FALSE    11    4
#&gt;      TRUE     21   53
#&gt;
#&gt;                Accuracy : 0.7191
#&gt;                  95% CI : (0.6138, 0.8093)
#&gt;     No Information Rate : 0.6404
#&gt;     P-Value [Acc &gt; NIR] : 0.073666
#&gt;
#&gt;                   Kappa : 0.3096
#&gt;  Mcnemar's Test P-Value : 0.001374
#&gt;
#&gt;             Sensitivity : 0.3438

#&gt;             Specificity : 0.9298
#&gt;          Pos Pred Value : 0.7333
#&gt;          Neg Pred Value : 0.7162
#&gt;              Prevalence : 0.3596
#&gt;          Detection Rate : 0.1236
#&gt;    Detection Prevalence : 0.1685
#&gt;       Balanced Accuracy : 0.6368
#&gt;
#&gt;        'Positive' Class : FALSE</strong></pre>
<p class="calibre2">If you're going to test with a DFM that was created using an SVD, you need to make the corresponding transformation to make sure you're working in the correct semantic space before producing any predictions. If you used a procedure like the one shown in this chapter, you need to left-multiply the testing DFM (with similar transformations as your training DFM) with the vector of singular values adjusted by sigma, while transposing the structures accordingly. The exact transformation will depend on the data structures you're using and processes you applied to them, but always remember to make sure that both your training and testing data operate in the same semantic space:</p>
<pre class="mce-root">sigma.inverse &lt;- 1 / train.bigrams.svd$d<br class="title-page-name"/>u.transpose &lt;- t(train.bigrams.svd$u)<br class="title-page-name"/>test.bigrams.svd &lt;- t(sigma.inverse * u.transpose %*% t(test.bigrams.tfidf))<br class="title-page-name"/>test.bigrams.svd.df &lt;- build_dfm_df(test, test.bigrams.svd)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Retrieving text data from Twitter</h1>
                
            
            
                
<p class="calibre2">Before we finish this chapter, we will very briefly touch on a completely different, yet very sought-after topic, that is, getting data from Twitter. In case you want to apply predictive models, you will need to link the Twitter data to a variable you want to predict, which normally comes from other data. However, something you can easily do is measure the sentiment around a topic using the techniques we showed in a previous section.</p>
<p class="calibre2">The <kbd class="calibre9">twitteR</kbd> package actually makes it very easy for us to retrieve Twitter data. To do so, we will create a <strong class="calibre1">Twitter App</strong> within Twitter, which will give us access to the data feed. To accomplish this, we need to generate four strings within your Twitter account that will be the keys to using the API. These keys are used to validate your permissions and monitor your usage in general. Specifically, you need four strings, the <kbd class="calibre9">consumer_key</kbd> value, the <kbd class="calibre9">consumer_secret</kbd>, the <kbd class="calibre9">access_token</kbd>, and the <kbd class="calibre9">access_secret</kbd>. To retrieve them, go to the Twitter Apps website (<a href="https://apps.twitter.com/" class="calibre4">https://apps.twitter.com/</a>), click on Create New App, and input the information required. The name for your Twitter App must be unique across all of the Twitter Apps. Don't worry about picking a complex name, you'll never use that string again. Also make sure that you read the Twitter Developer Agreement and that you agree with it.</p>
<p class="calibre2">Once inside the dashboard for your app, go to the Keys and Access Tokens tab, and generate a key and access token with their corresponding secret keys. Make sure that you copy those strings exactly as they will grant you access to the data feed. Substitute them instead of the ones shown here (which no longer work since they were deleted after writing this book), and execute the <kbd class="calibre9">setup_twitter_oauth()</kbd> function. If everything went as expected, you should now have connected your R session to the data feed:</p>
<pre class="mce-root">consumer_key &lt;- "b9SGfRpz4b1rnHFtN2HtiQ9xl"<br class="title-page-name"/>consumer_secret &lt;- "YMifSUmCJ4dlgB8RVxKRNcTLQw7Y4IBwDwBRkdz2Va1vcQjOP0"<br class="title-page-name"/>access_token &lt;- "171370802-RTl4RBpMDaSFdVf5q9xrSWQKxtae4Wi3y76Ka4Lz"<br class="title-page-name"/>access_secret &lt;- "dHfbMtmpeA2QdOH5cYPXO5b4hF8Nj6LjxELfOMSwHoUB8"<br class="title-page-name"/>setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)</pre>
<p class="calibre2">To retrieve data, we will create a wrapper function that will save us writing boilerplate again and again every time we want new data. The <kbd class="calibre9">get_twitter_data()</kbd> function takes a keyword we're searching for within Twitter and the number of messages we want to retrieve. It then goes on to get the data from Twitter using the <kbd class="calibre9">searchTwitter()</kbd> function (in English), transform the results into a data frame with the <kbd class="calibre9">twListToDF()</kbd> function, and send that back to the user:</p>
<pre class="mce-root">get_twitter_data &lt;- function(keyword, n) {<br class="title-page-name"/>    return(twListToDF(searchTwitter(keyword, n, lang = "en")))<br class="title-page-name"/>}</pre>
<p class="calibre2">Now we can easily search for messages that contain the word "<em class="calibre19">cake</em>" inside by executing the following code. As you can see, we don't only get the messages, but we also get a lot of metadata, like whether or not the tweet has been favorite; if so, how many times, whether it was a reply, when was it created, and the coordinates of where the tweet was sent if they are available, among other things:</p>
<pre class="mce-root">cake_data &lt;- get_twitter_data("cake", 250)

names(cake_data)
<strong class="calibre1">#&gt;  [1] "text"          "favorited"     "favoriteCount" "replyToSN"
#&gt;  [5] "created"       "truncated"     "replyToSID"    "id"
#&gt;  [9] "replyToUID"    "statusSource"  "screenName"    "retweetCount"
#&gt; [13] "isRetweet"     "retweeted"     "longitude"     "latitude"</strong></pre>
<p class="calibre2">As an exercise, get data from Twitter with the mechanism shown precedingly and use it to create a world map that shows where the tweets are coming from and colors the pin locations using the sentiment inferred from the tweet. Having a piece of code like that can be fun to play with and joke around with your friends as well as for making real business decisions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="calibre2">In this chapter, we showed how to perform predictive analysis using text data. To do so, we showed how to tokenize text to extract relevant words, how to build and work with <strong class="calibre1">document-feature matrices</strong> (<strong class="calibre1">DFMs</strong>), how to apply transformations to DFMs to explore different predictive models using term frequency-inverse document frequency weights, n-grams, partial singular value decompositions, and cosine similarities, and how to use these data structures within random forests to produce predictions. You learned why these techniques may be important for some problems and how to combine them. We also showed how to include sentiment analysis inferred from text to increase the predictive power of our models. Finally, we showed how to retrieve live data from Twitter that can be used to analyze what people are saying in the social network shortly after they have said it.</p>
<p class="calibre2">We encourage you to combine the knowledge from this chapter with that from previous chapters to try to gain deeper insights. For example, what happens if we use trigrams of quadgrams? What happens if we include other features in the data (for example, coordinates or client IDs)? And, what happens if we use other predictive models instead of random forests, such as support vector machines?</p>
<p class="calibre2">In the next chapter, we will look at how to use what we have done during the last three chapters to produce reports that can be automatically generated when we receive new data from The Cake Factory. We will cover how to produce PDFs automatically, how to create presentations that can be automatically updated, as well as other interesting reporting techniques.</p>
<p class="calibre2"/>


            

            
        
    </body></html>