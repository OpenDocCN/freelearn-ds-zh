<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer265" class="Content">
			<h1 id="_idParaDest-267"><em class="italics"><a id="_idTextAnchor320"/>Chapter 9</em></h1>
		</div>
		<div id="_idContainer266" class="Content">
			<h1 id="_idParaDest-268"><a id="_idTextAnchor321"/>Application of Data Wrangling in Real Life</h1>
		</div>
		<div id="_idContainer267" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Perform data wrangling on multiple full-fledged datasets from renowned sources</li>
				<li class="bullets">Create a unified dataset that can be passed on to a data science team for machine learning and predictive analytics</li>
				<li class="bullets">Relate data wrangling to version control, containerization, cloud services for data analytics, and big data technologies such as Apache Spark and Hadoop</li>
			</ul>
			<p>In this chapter, you will apply your gathered knowledge on real-life datasets and investigate various aspects of it.</p>
		</div>
		<div id="_idContainer270" class="Content">
			<h2 id="_idParaDest-269"><a id="_idTextAnchor322"/>Introduction</h2>
			<p>We learned about databases in the previous chapter, so now it is time to combine the knowledge of data wrangling and Python with a real-world scenario. In the real world, data from one source is often inadequate to perform analysis. Generally, a data wrangler has to distinguish between relevant and non-relevant data and combine data from different sources.</p>
			<p>The primary job of a data wrangling expert is to pull data from multiple sources, format and clean it (impute the data if it is missing), and finally combine it in a coherent manner to prepare a dataset for further analysis by data scientists or machine learning engineers.</p>
			<p>In this topic, we will try to mimic such a typical task flow by downloading and using two different datasets from reputed web portals. Each of the datasets contains partial data pertaining to the key question that is being asked. Let's examine it more closely.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor323"/>Applying Your Knowledge to a Real-life Data Wrangling Task</h2>
			<p>Suppose you are asked this question: <strong class="bold">In India, did the enrollment in primary/secondary/tertiary education increase with the improvement of per capita GDP in the past 15 years? </strong>The actual modeling and analysis will be done by some senior data scientist, who will use machine learning and data visualization for analysis. As a data wrangling expert, <strong class="bold">your job will be to acquire and provide a clean dataset that contains educational enrollment and GDP data side by side</strong>.</p>
			<p>Suppose you have a link for a dataset from the United Nations and you can download the dataset of education (for all the nations around the world). But this dataset has some missing values and moreover it does not have any GDP information. Someone has also given you another separate CSV file (downloaded from the World Bank site) which contains GDP data but in a messy format.</p>
			<p>In this activity, we will examine how to handle these two separate sources and clean the data to prepare a simple final dataset with the required data and save it to the local drive as a SQL database file:</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="Images/C11065_09_01.jpg" alt="Figure 9.1: Pictorial representation of the merging of education and economic data" width="1767" height="680"/>
				</div>
			</div>
			<h6>Figure 9.1: Pictorial representation of the merging of education and economic data</h6>
			<p>You are encouraged to follow along with the code and results in the notebook and try to understand and internalize the nature of the data wrangling flow. You are also encouraged to try extracting various data from these files and answer your own questions about a nations' socio-economic factors and their inter-relationships.</p>
			<h4>Note</h4>
			<p class="callout">Coming up with interesting questions about social, economic, technological, and geo-political topics and then answering them using freely available data and a little bit of programming knowledge is one of most fun ways to learn about any data science topic. You will get a flare of that process in this chapter.</p>
			<p><strong class="bold">Data Imputation</strong></p>
			<p>Clearly, we are missing some data. Let's say we decide to impute these data points by simple linear interpolation between the available data points. We can take out a pen and paper or a calculator and compute those values and manually create a dataset. But being a data wrangler, we will of course take advantage of Python programming, and use pandas imputation methods for this task.</p>
			<p>But to do that, we first need to create a DataFrame with missing values in it, that is, we need to append another DataFrame with missing values to the current DataFrame.</p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor324"/>Activity 12: Data Wrangling Task – Fixing UN Data</h2>
			<p>S<a id="_idTextAnchor325"/>uppose the agenda of the data analysis is to find out whether the enrolment in primary, secondary, or tertiary education has increased with the improvement of per capita GDP in the past 15 years. For this task, we will first need to clean or wrangle the two datasets, that is, the Education Enrolment and GDP data.</p>
			<p>The UN data is available on <a href="https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/SYB61_T07_Education.csv">https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/SYB61_T07_Education.csv</a>.</p>
			<h4>Note</h4>
			<p class="callout">If you download the CSV file and open it using Excel, then you will see that the <strong class="inline">Footnotes</strong> column sometimes contains useful notes. We may not want to drop it in the beginning. If we are interested in a particular country's data (like we are in this task), then it may well turn out that <strong class="inline">Footnotes</strong> will be <strong class="inline">NaN</strong>, that is, blank. In that case, we can drop it at the end. But for some countries or regions, it may contain information.</p>
			<p>These steps will guide you to find the solution:</p>
			<ol>
				<li>Download the dataset from the UN data from GitHub from the following link: <a href="https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity13/India_World_Bank_Info.csv">https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity13/India_World_Bank_Info.csv</a>.<p>The UN data has missing values. Clean the data to prepare a simple final dataset with the required data and save it to the local drive as a SQL database file.</p></li>
				<li>Use the <strong class="inline">pd.read_csv</strong> method of pandas to create a DataFrame.</li>
				<li>Since the first row does not contain useful information, skip it using the <strong class="inline">skiprows</strong> parameter.</li>
				<li>Drop the column region/country/area and source.</li>
				<li>Assign the following names as columns of DataFrame: Region/County/Area, Year, Data, Value, and Footnotes.</li>
				<li>Check how many unique values are present in the <strong class="inline">Footnotes</strong> column.</li>
				<li>Check the type of value column.</li>
				<li>Create a function to convert the value column into a floating-point.</li>
				<li>Use the <strong class="inline">apply</strong> method to apply this function to a value.</li>
				<li>Print the unique values in the data column.<h4>Note:</h4><p class="callout">The solution for this activity can be found on page 338.</p></li>
			</ol>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor326"/>Activity 13: Data Wrangling Task – Cleaning GDP Data</h2>
			<p>The GDP data is available on <a href="https://data.worldbank.org/">https://data.worldbank.org/</a> and it is available on GitHub at <a href="https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/India_World_Bank_Info.csv">https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/India_World_Bank_Info.csv</a>.</p>
			<p>In this activity, we will clean the GDP data.</p>
			<ol>
				<li value="1">Create three DataFrames from the original DataFrame using filtering. Create the <strong class="inline">df_primary, df_secondary,</strong> and<strong class="inline"> df_tertiary DataFrames </strong>for students enrolled in primary education, secondary education, and tertiary education in thousands, respectively.</li>
				<li>Plot bar charts of the enrollment of primary students in a low-income country like India and a higher-income country like the USA.</li>
				<li>Since there is missing data, use pandas imputation methods to impute these data points by simple linear interpolation between data points. To do that, create a DataFrame with missing values inserted and append a new DataFrame with missing values to the current DataFrame.</li>
				<li>(For India) Append the rows corresponding to the missing years - <strong class="bold">2004 - 2009</strong>, <strong class="bold">2011 – 2013</strong>.</li>
				<li>Create a dictionary of values with <strong class="inline">np.nan</strong>. Note that there are 9 missing data points, so we need to create a list with identical values repeated 9 times.</li>
				<li>Create a DataFrame of missing values (from the preceding dictionary) that we can append.</li>
				<li>Append the DataFrames together.</li>
				<li>Sort by Year and reset the indices using <strong class="inline">reset_index</strong>. Use <strong class="inline">inplace=True</strong> to execute the changes on the DataFrame itself.</li>
				<li>Use the interpolate method for linear interpolation. It fills all the NaNs by linearly interpolated values. See the following link for more details about this method: <a href="http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html">http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html</a>.</li>
				<li>Repeat the same steps for USA (or other countries).</li>
				<li>If there are values that are unfilled, use the <strong class="inline">limit</strong> and <strong class="inline">limit_direction</strong> parameters with the interpolate method to fill them in.</li>
				<li>Plot the final graph using the new data.</li>
				<li>Read the GDP data using the pandas <strong class="inline">read_csv</strong> method. It will generally throw an error.</li>
				<li>To avoid errors, try the <strong class="inline">error_bad_lines</strong> <strong class="inline">=</strong> <strong class="inline">False</strong> option.</li>
				<li>Since there is no delimiter in the file, add the <strong class="inline">\t</strong> delimiter.</li>
				<li>Use the <strong class="inline">skiprows</strong> function to remove rows that are not useful.</li>
				<li>Examine the dataset. Filter the dataset with information that states that it is similar to the previous education dataset.</li>
				<li>Reset the index for this new dataset.</li>
				<li>Drop the not useful rows and re-index the dataset.</li>
				<li>Rename the columns properly. This is necessary for merging the two datasets.</li>
				<li>We will concentrate only on the data from 2003 to 2016. Eliminate the remaining data.</li>
				<li>Create a new DataFrame called <strong class="inline">df_gdp</strong> with rows 43 to 56.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 338.</p></li>
			</ol>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor327"/>Activity 14: Data Wrangling Task – Merging UN Data and GDP Data</h2>
			<p>The steps to merge the databases is as follows:</p>
			<ol>
				<li value="1">Reset the indexes for merging.</li>
				<li>Merge the two DataFrames, <strong class="inline">primary_enrollment_india</strong> and <strong class="inline">df_gdp</strong>, on the Year column.</li>
				<li>Drop the data, footnotes, and region/county/area.</li>
				<li>Rearrange the columns for proper viewing and presentation.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 345.</p></li>
			</ol>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor328"/>Activity 15: Data Wrangling Task – Connecting the New Data to the Database</h2>
			<p>The steps to connect the data to the database is as follows:</p>
			<ol>
				<li value="1">Import the <strong class="inline">sqlite3</strong> module of Python and use the <strong class="inline">connect</strong> function to connect to the database. The main database engine is embedded. But for a different database like <strong class="inline">Postgresql</strong> or <strong class="inline">MySQL</strong>, we will need to connect to them using those credentials. We designate <strong class="inline">Year</strong> as the <strong class="inline">PRIMARY</strong> <strong class="inline">KEY</strong> of this table.</li>
				<li>Then, run a loop with the dataset rows one by one to insert them into the table.</li>
				<li>If we look at the current folder, we should see a file called <strong class="inline">Education_GDP.db</strong>, and if we examine that using a database viewer program, we can see the data transferred there.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 347.</p></li>
			</ol>
			<p>In this notebook, we examined a complete data wrangling flow, including reading data from the web and local drive, filtering, cleaning, quick visualization, imputation, indexing, merging, and writing back to a database table. We also wrote custom functions to transform some of the data and saw how to handle situations where we may get errors when reading the file.</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor329"/>An Extension to Data Wrangling</h2>
			<p>This is the concluding chapter of our book, where we want to give you a broad overview of some of the exciting technologies and frameworks that you may need to learn beyond data wrangling to work as a full-stack data scientist. Data wrangling is an essential part of the whole data science and analytics pipeline, but it is not the whole enterprise. You have learned invaluable skills and techniques in this book, but it is always good to broaden your horizons and look beyond to see what other tools that are out there can give you an edge in this competitive and ever-changing world.</p>
			<h3 id="_idParaDest-276"><a id="_idTextAnchor330"/>Additional Skills Required to Become a Data Scientist</h3>
			<p>To practice as a fully qualified data scientist/analyst, you should have some basic skills in your repertoire, irrespective of the particular programming language you choose to focus on. These skills and know-hows are language agnostic and can be utilized with any framework that you have to embrace, depending on your organization and business needs. We describe them in brief here:</p>
			<ul>
				<li><strong class="bold">Git and version control</strong>: Git to version control is what RDBMS is to data storage and query. It simply means that there is a huge gap between the pre and post Git era of version controlling your code. As you may have noticed, a<a id="_idTextAnchor331"/>ll the notebooks for this book/book are hosted on GitHub, and this was done to take advantage of the powerful Git VCS. It gives you, out of the box, version control, history, branching facilities for different code, merging different code branches, and advanced operations like cherry picking, diff, and so on. It is an very essential tool to master as you can be almost sure that you will face it at one point of time in your journey. Packt has a very good book on it. You can check that out for more information.</li>
				<li><strong class="bold">Linux command line</strong>: People coming from a Windows background (or even Mac, if you have not done any development before) are not very familiar, usually, with the command line. The superior UI of those OSes hides the low level details of interaction with the OS using a command line. However, as a data professional, it is important that you know the command line well. There are so many operations that you can do by simply using the command line that it is astonishing.</li>
				<li><strong class="bold">SQL and basic relational database concepts</strong>: We dedicated an entire chapter to SQL and RDBMS. However, as we already mentioned there, it was really not enough. This is a vast subject and needs years of study to master it. Try to read more about it (Including Theory and Practical) from books and online sources. Do not forget that, despite all the other sources of data being used nowadays, we still have hundreds of millions of bytes of structured data stored in legacy database systems. You can be sure to come across one, sooner or later.</li>
				<li><strong class="bold">Docker and containerization</strong>: Since its first release in 2013, Docker has changed the way we distribute and deploy software in server-based applications. It gives you a clean and lightweight abstraction over the underlying OS and lets you iterate fast on development without the headache of creating and maintaining a proper environment. It is very useful in both the development and production phases. Without virtually no competitor present, they are becoming the default in the industry very fast. We strongly advise you to explore it in great detail.</li>
			</ul>
			<h3 id="_idParaDest-277"><a id="_idTextAnchor332"/>Basic Familiarity with Big Data and Cloud Technologies</h3>
			<p>Big data and cloud platforms are the latest trend. We will introduce them here with one or two short sentences and we encourage you to go ahead and learn about them as much as you can. If you are planning to grow as a data professional, then you can be sure that without these necessary skills it will be hard for you to transition to the next level:</p>
			<ul>
				<li><strong class="bold">Fundamental characteristics of big data</strong>: Big data is simply data that is very big in size. The term size is a bit ambiguous here. It can mean one static chunk of data (like the detail census data of a big country like India or the US) or data that is dynamically generated as time passes, and each time it is huge. To give an example for the second category, we can think of how much data is generated by Facebook per day. It's about 500+ Terabytes per day. You can easily imagine that we will need specialized tools to deal with that amount of data. There are three different categories of big data, that is, Structured, Unstructured, and Semi-Structured. The main features that define big data are Volume, Variety, Velocity, and Variability.</li>
				<li><strong class="bold">Hadoop ecosystem</strong>: Apache Hadoop (and the related ecosystem) is a software framework that aims to use the Map-Reduce programming model to simplify the storage and processing of big data. It has since become one of the backbones of big data processing in the industry. The modules in Hadoop are designed keeping in mind that hardware failures are common occurrences, and they should be automatically handled by the framework. The four base modules of Hadoop are common, HDFS, YARN, and MapReduce. The Hadoop ecosystem consists of Apache Pig, Apache Hive, Apache Impala, Apache Zookeeper, Apache HBase, and more. They are very important bricks in many high demand and cutting-edge data pipelines. We encourage you to study more about them. They are essential in any industry that aims to leverage data.</li>
				<li><strong class="bold">Apache Spark</strong>: Apache Spark is a general purpose Cluster Computing framework that was initially developed at the University of California, Barkley, and released in 2014. It gives you an interface to program an entire cluster of computers with built-in data parallelism and fault tolerance. It contains Spark Core, Spark SQL, Spark Streaming, MLib (for machine learning), and GraphX. It is now one of the main frameworks that's used in the industry to process a huge amount of data in real time based on streaming data. We encourage you to read and master it if you want to go toward real time data engineering.</li>
				<li><strong class="bold">Amazon Web service (AWS)</strong>: Amazon Web Services (often abbreviated as AWS) are a bunch of managed services offered by Amazon ranging from infrastructure-as-a-Service, Database-as-a-Service, MachineLearning-as-a-Service, Cache, Load Balancer, NoSQL database, to Message Queues and several other types. They are very useful for all sorts of applications. It can be a simple web app or a multi-cluster data pipeline. Many famous companies run their entire infrastructure on AWS (such as Netflix). They give us on-demand provision, easy scaling, a managed environment, a slick UI to control everything, and also a very powerful command-line client. They also expose a rich set of APIs and we can find an AWS API client in virtually any programming language. The Python one is called Boto3. If you are planning to become a data professional, then it can be said with near certainty that you will end up using many of their services at one point or another.</li>
			</ul>
			<h3 id="_idParaDest-278"><a id="_idTextAnchor333"/>What Goes with Data Wrangling?</h3>
			<p>We learned in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Data Wrangling with Python</em>, that the process of data wrangling lies in-between data gathering and advanced analytics, including visualization and machine learning. However, the boundaries that exist in-between these processes may not always be strict and rigid. It depends largely on the organizational culture and team composition.</p>
			<p>Therefore, we need to not only be aware of the data wrangling but also the other components of the data science platform to wrangle data effectively. Even if you are performing pure data wrangling tasks, having a good grasp over how data is sourced and utilized will give you an edge for coming up with unique and efficient solutions to complex data wrangling problems and enhance the value of those solutions to the machine learning scientist or the business domain expert:</p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="Images/Figure_1.1.jpg" alt="Figure 9.2: Process of data wrangling" width="1280" height="720"/>
				</div>
			</div>
			<h6>Figure 9.2: Process of data wrangling</h6>
			<p>Now, we have, in fact, already laid out a solid groundwork in this book for the data platform part, assuming that it is an integral part of data wrangling workflow. For example, we have covered web scraping, working with RESTful APIs, and database access and manipulation using Python libraries in detail.</p>
			<p>We have also touched on basic visualization techniques and plotting functions in Python using matplotlib. However, there are other advanced statistical plotting libraries such as <strong class="bold">Seaborn</strong> that you can master for more sophisticated visualization for data science tasks.</p>
			<p>Business logic and domain expertise is the most varied topic and it can only be learned on the job, however it will come eventually with experience. If you have an academic background and/or work experience in any domain such as finance, medicine and healthcare, and engineering, that knowledge will come in handy in your data science career.</p>
			<p>The fruit of the hard work of data wrangling is realized fully in the domain of machine learning. It is the science and engineering of making machines learn patterns and insights from data for predictive analytics and intelligent, automated decision-making with a deluge of data, which cannot be analyzed efficiently by humans. Machine learning has become one of the most sought-after skills in the modern technology landscape. It has truly become one of the most exciting and promising intellectual fields, with applications ranging from e-commerce to healthcare and virtually everything in-between. Data wrangling is intrinsically linked with machine learning as it prepares the data so that it's suitable for intelligent algorithms to process. Even if you start your career in data wrangling, it could be a natural progression to move to machine learning.</p>
			<p>Packt has published numerous books and books on this topic that you should explore. In the next section, we will touch upon some approaches to adopt and Python libraries to check out for giving you a boost in your learning.</p>
			<h3 id="_idParaDest-279"><a id="_idTextAnchor334"/>Tips and Tricks for Mastering Machine Learning</h3>
			<p>Machine learning is difficult to start with. We have listed some structured MOOCs and incredible free resources that are available so that you can begin your journey:</p>
			<ul>
				<li>Understand the definition of and differentiation between the buzzwords — artificial intelligence, machine learning, deep learning, and data science. Cultivate the habit of reading great posts or listening to the expert talks, on these topics, and understand their true reach and applicability in some business problem.</li>
				<li>Stay updated with the recent trends by watching videos, reading books like <em class="italics">The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</em>, and articles and following influential blogs like KDnuggets, Brandon Rohrer's blog, Open AI's blog about their research, Towards Data Science publication on Medium, and so on.</li>
				<li>As you learn new algorithms or concepts, pause and analyze how you can apply these machine learning concepts or algorithm in your daily work. This is the best method for learning and expanding your knowledge base.</li>
				<li>If you choose Python as your preferred language for machine learning tasks, you have a great ML library in <strong class="bold">scikit-learn</strong>. It is the most widely used general machine learning package in the Python ecosystem. scikit-learn has a wide variety of supervised and unsupervised learning algorithms, which are exposed via a stable consistent interface. Moreover, it is specifically designed to interface seamlessly with other popular data wrangling and numerical libraries such as NumPy and pandas.</li>
				<li>Another hot skill in today's job market is deep learning. Packt has many books and books on this topic and there are excellent MOOC books from Bookra where you can study deep learning. For Python libraries, you can learn and practice with <strong class="bold">TensorFlow</strong>, <strong class="bold">Keras</strong>, or <strong class="bold">PyTorch</strong> for deep learning.</li>
			</ul>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor335"/>Summary</h2>
			<p>Data is everywhere and it is all around us. In these nine chapters, we have learned about how data from different types and sources can be cleaned, corrected, and combined. Using the power of Python and the knowledge of data wrangling and applying the tricks and tips that you have studied in this book, you are ready to be a data wrangler.</p>
		</div>
	</div>



  </body></html>