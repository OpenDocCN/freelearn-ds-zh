<html><head></head><body>
		<div id="_idContainer189">
			<h1 id="_idParaDest-116"><em class="italic"><a id="_idTextAnchor118"/>Chapter 11</em>: Building a Production Data Pipeline</h1>
			<p>In this chapter, you will build a production data pipeline using the features and techniques that you have learned in this section of the book. The data pipeline will be broken into processor groups that perform a single task. Those groups will be version controlled and they will use the NiFi variable registry so that they can be deployed in a production environment.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Creating a test and production environment</li>
				<li>Building a production data pipeline</li>
				<li>Deploying a data pipeline in production</li>
			</ul>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor119"/>Creating a test and production environment</h1>
			<p>In<a id="_idIndexMarker580"/> this chapter, we <a id="_idIndexMarker581"/>will return to using PostgreSQL for both the extraction and loading of data. The data pipeline will require a test and production environment, each of which will have a staging and a warehouse table. To create the databases and tables, you will use <strong class="bold">PgAdmin4</strong>. </p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor120"/>Creating the databases</h2>
			<p>To use<a id="_idIndexMarker582"/> PgAdmin4, perform the<a id="_idIndexMarker583"/> following steps:</p>
			<ol>
				<li>Browse to <strong class="source-inline">http://localhostw/pgadmin4/l</strong>, enter your username and password, and then click the <strong class="bold">Login</strong> button. Once logged in, expand the server icon in the left panel. </li>
				<li>To create the databases, right-click on the databases icon and select <strong class="bold">Create</strong> | <strong class="bold">Database</strong>. Name the database <strong class="source-inline">test</strong>.</li>
				<li>Next, you <a id="_idIndexMarker584"/>will need to add the tables. To create the staging table, right-click on <strong class="bold">Tables</strong> | <strong class="bold">Create</strong> | <strong class="bold">Table</strong>. On the <strong class="bold">General</strong> tab, name the<a id="_idIndexMarker585"/> table <strong class="source-inline">staging</strong>. Then, select the <strong class="bold">Columns</strong> tab. Using the plus sign, create the fields shown in the following screenshot:<div id="_idContainer181" class="IMG---Figure"><img src="image/Figure_11.1_B15739.jpg" alt="Figure 11.1 – The columns used in the staging table&#13;&#10;"/></div><p class="figure-caption">Figure 11.1 – The columns used in the staging table</p></li>
				<li>Save the<a id="_idIndexMarker586"/> table when you are done. You will need to create this<a id="_idIndexMarker587"/> table once more for the test database and twice more for the production database. To save some time, you can use <strong class="bold">CREATE Script</strong> to do this for you. Right-click on the staging table, and then select <strong class="bold">Scripts</strong> | <strong class="bold">CREATE Script</strong>, as shown in the following screenshot:<div id="_idContainer182" class="IMG---Figure"><img src="image/Figure_11.2_B15739.jpg" alt="Figure 11.2 – Generating the CREATE script&#13;&#10;"/></div><p class="figure-caption">Figure 11.2 – Generating the CREATE script</p></li>
				<li>A window will open in the main screen with the SQL required to generate the table. By changing the name from <strong class="source-inline">staging</strong> to <strong class="source-inline">warehouse</strong>, you can make the warehouse table in test, which will be identical to staging. Once you have made the change, click the play button in the toolbar. </li>
				<li>Lastly, right-click on <strong class="bold">Databases</strong> and create a new database named <strong class="source-inline">production</strong>. Use the script to create both the tables.</li>
			</ol>
			<p>Now that you have the tables created for the test and production environments, you will need a data lake.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor121"/>Populating a data lake</h2>
			<p>A <strong class="bold">data lake</strong> is usually a <a id="_idIndexMarker588"/>place on disk where files are stored. Usually, you will find data lakes using Hadoop for the <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>) and the other <a id="_idIndexMarker589"/>tools built on top of the Hadoop ecosystem. In this chapter, we will just drop files in a folder to simulate how reading from the data lake would work. </p>
			<p>To create the data lake, you can use Python and the Faker library. Before you write the code, create a folder to act as the data lake. I have created a folder named <strong class="source-inline">datalake</strong> in my home directory.</p>
			<p>To populate the data lake, you will need to write JSON files with information about an individual. This is similar to the JSON and CSV code you wrote in the first section of this book. The steps are as follows:</p>
			<ol>
				<li value="1">Import the libraries, set the data lake directory, and set <strong class="source-inline">userid</strong> to <strong class="source-inline">1</strong>. The <strong class="source-inline">userid</strong> variable is going to be a primary key, so we need it to be distinct – incrementing will do that for us:<p class="source-code">from faker import Faker</p><p class="source-code">import json</p><p class="source-code">import os</p><p class="source-code">os.chdir("/home/paulcrickard/datalake")</p><p class="source-code">fake=Faker()</p><p class="source-code">userid=1</p></li>
				<li>Next, create a loop that generates a data object containing the user ID, name, age, street, city, state, and zip of a fake individual. The <strong class="source-inline">fname</strong> variable holds the first and last name of a person without a space in the middle. If you had a space, Linux would wrap the file in quotes:<p class="source-code">for i in range(1000):</p><p class="source-code">    name=fake.name()</p><p class="source-code">    fname=name.replace(" ","-")+'.json'</p><p class="source-code">    data={</p><p class="source-code">        "userid":userid,</p><p class="source-code">        "name":name,</p><p class="source-code">        "age":fake.random_int(min=18, max=101, step=1),</p><p class="source-code">        "street":fake.street_address(),</p><p class="source-code">        "city":fake.city(),</p><p class="source-code">        "state":fake.state(),</p><p class="source-code">        "zip":fake.zipcode()</p><p class="source-code">    }</p></li>
				<li>Lastly, dump the <a id="_idIndexMarker590"/>JSON object and then write it to a file named after the person. Close the file and let the loop continue:<p class="source-code">    datajson=json.dumps(data)</p><p class="source-code">    output=open(fname,'w')</p><p class="source-code">    userid+=1</p><p class="source-code">    output.write(datajson)</p><p class="source-code">    output.close()</p></li>
			</ol>
			<p>Run the preceding code and you will have 1,000 JSON files in your data lake. Now you can start building the data pipeline.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor122"/>Building a production data pipeline</h1>
			<p>The data pipeline<a id="_idIndexMarker591"/> you build will do the following:</p>
			<ul>
				<li>Read files from the data lake.</li>
				<li>Insert the files into staging.</li>
				<li>Validate the staging data.</li>
				<li>Move staging to the warehouse.</li>
			</ul>
			<p>The final <a id="_idIndexMarker592"/>data pipeline will look like the following screenshot:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/Figure_11.3_B15739.jpg" alt="Figure 11.3 – The final version of the data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – The final version of the data pipeline</p>
			<p>We will build the data pipeline processor group by processor group. The first processor group will read the data lake.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor123"/>Reading the data lake</h2>
			<p>In the first section of<a id="_idIndexMarker593"/> this book, you read files from NiFi and will do the same here. This processor group will consist of three processors – <strong class="source-inline">GetFile</strong>, <strong class="source-inline">EvaluateJsonPath</strong>, and <strong class="source-inline">UpdateCounter</strong> – and an output port. Drag the processors and port to the canvas. In the following sections, you will configure them.</p>
			<h3>GetFile</h3>
			<p>The <strong class="source-inline">GetFile</strong> processor<a id="_idIndexMarker594"/> reads files from a folder, in this case, our data lake. If you were reading a data lake in Hadoop, you would switch out this processor for the <strong class="source-inline">GetHDFS</strong> processor. To configure the processor, specify the input directory; in my case, it is <strong class="source-inline">/home/paulcrickard/datalake</strong>. Make sure <strong class="bold">Keep Source File</strong> is set to <strong class="bold">True</strong>. If you wanted to move the processed files and drop them somewhere else, you could do this as well. Lastly, I have set <strong class="bold">File Filter</strong> to a regex pattern to match the JSON file extension – <strong class="source-inline">^.*\.([jJ][sS][oO][nN]??)$</strong>. If you leave the default, it will work, but if there are other files in the folder, NiFi will try to grab them and will fail.</p>
			<h3>EvaluateJsonPath</h3>
			<p>The <strong class="source-inline">EvaluateJsonPath</strong> processor<a id="_idIndexMarker595"/> will extract the fields from the JSON and put them into flowfile attributes. To do so, set the <strong class="bold">Destination</strong> property to <strong class="bold">flowfile-attribute</strong>. Leave the rest of the properties as the default. Using the plus sign, create a property for each field in the JSON. The configuration is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/Figure_11.4_B15739.jpg" alt="Figure 11.4 – The configuration for the EvaluateJsonPath processor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The configuration for the EvaluateJsonPath processor</p>
			<p>This would be<a id="_idIndexMarker596"/> enough to complete the task of reading from the data lake, but we will add one more processor for monitoring.</p>
			<h3>UpdateCounter</h3>
			<p>This processor<a id="_idIndexMarker597"/> allows you to create an increment counter. As flowfiles pass through, we can hold a count of how many are being processed. This processor does not manipulate or change any of our data, but will allow us to monitor the progress of the processor group. We will be able to see the number of FlowFiles that have moved through the processor. This is a more accurate way than using the GUI display, but it only shows the number of records in the last 5 minutes. To configure the processor, leave the <strong class="bold">Delta</strong> property set to <strong class="source-inline">1</strong> and set the <strong class="bold">Counter Name</strong> property to <strong class="source-inline">datalakerecordsprocessed</strong>.</p>
			<p>To finish this section of the data pipeline, drag an output port to the canvas and name it <strong class="source-inline">OutputDataLake</strong>. Exit the processor group and right-click, select <strong class="bold">Version</strong>, and start version control. I set <strong class="bold">Flow Name</strong> to <strong class="source-inline">ReadDataLake</strong>, wrote a short description and version comments, and then performed a save.</p>
			<p class="callout-heading">NiFi-Registry</p>
			<p class="callout">I have created a new bucket named <strong class="source-inline">DataLake</strong>. To create buckets, you can browse to the registry at <strong class="source-inline">http://localhost:18080/nifi-registry/</strong>. Click the wrench in the right corner and then click the <strong class="bold">NEW BUCKET</strong> button. Name and save the bucket.</p>
			<p>The first processor<a id="_idIndexMarker598"/> group is complete. You can use this processor group any time you need to read from the data lake. The processor group will hand you every file with the fields extracted. If the data lake changed, you would only need to fix this one processor group to update all of your data pipelines. </p>
			<p>Before continuing down the data pipeline, the next section will take a small diversion to show how you can attach other processor groups.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor124"/>Scanning the data lake</h2>
			<p>The goal of the data pipeline is to <a id="_idIndexMarker599"/>read the data lake and put the data in the data warehouse. But let's assume there is another department at our company that needs to monitor the data lake for certain people – maybe VIP customers. Instead of building a new data pipeline, you can just add their task to the <strong class="source-inline">ReadDataLake</strong> processor group.</p>
			<p>The <strong class="source-inline">ScanLake</strong> processor group has an input port that is connected to the output of the <strong class="source-inline">ReadDataLake</strong> processor. It uses the <strong class="source-inline">ScanContent</strong> processor attached to the <strong class="source-inline">EvaluateJsonPath</strong> processor, which is terminated at the <strong class="source-inline">PutSlack</strong> processor, as well as sending the data through to an output port. The flow is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/Figure_11.5_B15739.jpg" alt="Figure 11.5 – The ScanLake processor group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – The ScanLake processor group</p>
			<p>The<a id="_idIndexMarker600"/> previous chapter used the <strong class="source-inline">PutSlack</strong> processor and you are already familiar with the <strong class="source-inline">EvaluateJsonPath</strong> processor. <strong class="source-inline">ScanContent</strong>, however, is a new processor. The <strong class="source-inline">ScanContent</strong> processor allows you to look at fields in the flowfile content and compare them to a dictionary file – a file with content on each line that you are looking for. I have put a single name in a file at <strong class="source-inline">/home/paulcrickard/data.txt</strong>. I configured the processor by setting the path as the value of the <strong class="bold">Dictionary File</strong> property. Now, when a file comes through that contains that name, I will get a message on Slack.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor125"/>Inserting the data into staging</h2>
			<p>The<a id="_idIndexMarker601"/> data we read was from the data lake and will not be removed, so we do not <a id="_idIndexMarker602"/>need to take any intermediary steps, such as writing data to a file, as we would have done had the data been from a transactional database. But what we will do is place the data in a staging table to make sure that everything works as we expect before putting it in the data warehouse. To insert the data into staging only requires one processor, <strong class="source-inline">PutSQL</strong>.</p>
			<h3>PutSQL</h3>
			<p>The <strong class="source-inline">PutSQL</strong> processor<a id="_idIndexMarker603"/> will allow you to execute an <strong class="source-inline">INSERT</strong> or <strong class="source-inline">UPDATE</strong> operation on a database table. The processor allows you to specify the query in the contents of a flowfile, or you can hardcode the query to use as a property in the processor. For this example, I have hardcoded the query in the <strong class="bold">SQL Statement</strong> property, which is shown as follows:</p>
			<p class="source-code">INSERT INTO ${table} VALUES ('${userid}', '${name}',${age},'${street}','${city}','${state}','${zip}');</p>
			<p>The preceding query takes the attributes from the flowfile and passes them into the query, so while it is hardcoded, it will change based on the flowfiles it receives. You may have noticed that you have not used <strong class="source-inline">${table}</strong> in any of the <strong class="source-inline">EvaluateJsonPath</strong> processors. I have declared a variable using the NiFi registry and added it to the processor group scope. The value of the table will be <strong class="source-inline">staging</strong> for this test environment, but will change later when we deploy the data pipeline to production.</p>
			<p>You will also need to add a <strong class="bold">Java Database Connection</strong> (<strong class="bold">JDBC</strong>) pool, which you have done in earlier chapters of this book. You can specify the batch size, the number of records to retrieve, and whether you want the processor to roll back on failure. Setting <strong class="bold">Rollback on Failure</strong> to <strong class="bold">True</strong> is how you can create atomicity in your transactions. If a single flowfile in a batch fails, the processor will stop and nothing else can continue.</p>
			<p>I have connected the processor to another <strong class="source-inline">UpdateCounter</strong> processor. This processor creates and updates <strong class="source-inline">InsertedStaging</strong>. The counter should match <strong class="source-inline">datalakerecordsprocessor</strong> when everything has finished. The <strong class="source-inline">UpdateCounter</strong> processor connects to an output port named <strong class="source-inline">OutputStaging</strong>.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor126"/>Querying the staging database</h2>
			<p>The next processor <a id="_idIndexMarker604"/>group is for querying the staging database. Now that the data has been loaded, we can query the database to make sure all the records have actually made it in. You could perform other validation steps or queries to see whether the results match what you would expect – if you have data analysts, they would be a good source of information for defining these queries. In the following sections, you will query the staging database and route the results based on whether it meets your criteria.</p>
			<h3>ExecuteSQLRecord</h3>
			<p>In the <a id="_idIndexMarker605"/>previous processor group, you used the <strong class="source-inline">PutSQL</strong> processor to insert data into the database, but in this processor group, you want to perform a <strong class="source-inline">select</strong> query. The <strong class="source-inline">select</strong> query is shown as follows:</p>
			<p class="source-code">select count(*) from ${table}</p>
			<p>The preceding query is set as the value of the optional SQL <strong class="source-inline">select</strong> query property. The <strong class="source-inline">${table}</strong> is a NiFi variable registry variable assigned to the processor group and has a value of <strong class="source-inline">staging</strong>. You will need to define a JDBC connection and a record writer in the processor properties. The record writer is a JSON record set writer. The return value of the processor will be a JSON object with one field – <strong class="source-inline">count</strong>. This processor is sent to an <strong class="source-inline">EvaluateJsonPath</strong> processor to extract the count as <strong class="source-inline">recordcount</strong>. That processor is then sent to the next processor.</p>
			<h3>RouteOnAttribute</h3>
			<p>The <strong class="source-inline">RouteOnAttribute</strong> processor allows you <a id="_idIndexMarker606"/>to use expressions or values to define where a flowfile goes. To configure the processor, I have set the <strong class="bold">Routing strategy</strong> to <strong class="bold">Route to Property name</strong>. I have also created a property named <strong class="source-inline">allrecords</strong> and set the value to a NiFi expression, shown as follows:</p>
			<p class="source-code">${recordcount:ge( 1000 )}</p>
			<p>The preceding expression evaluates the <strong class="source-inline">recordcount</strong> attribute to see whether it is greater than or equal to 1,000. If it is, it will route on this relationship. I have attached the output to an output port named <strong class="source-inline">OutputQueryStaging</strong>.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor127"/>Validating the staging data</h2>
			<p>The previous <a id="_idIndexMarker607"/>processor group did some validation and you could stop there. However, Great Expectations is an excellent library for handling validation for you. You learned about Great Expectations in <a href="B15739_07_ePub_AM.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a><em class="italic">, Features of a Production Pipeline</em>, but I will cover it quickly again here. </p>
			<p>To use Great Expectations, you need to create a project folder. I have done that in the following code snippet and initialized Great Expectations:</p>
			<p class="source-code">mkdir staging</p>
			<p class="source-code">great_expectations init</p>
			<p>You will be prompted to create your validation suite. Choose <strong class="bold">Relational database (SQL)</strong>, then <strong class="bold">Postgres</strong>, and provide the required information. The prompts will look like the following screenshot:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/Figure_11.6_B15739.jpg" alt="Figure 11.6 – Configuring Great Expectations to work with PostgreSQL&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Configuring Great Expectations to work with PostgreSQL</p>
			<p>When it is finished, Great <a id="_idIndexMarker608"/>Expectations will attempt to connect to the database. If successful, it will provide the URL for your documents. Since the table is empty, it will not create a very detailed validation suite. You can edit the suite using the following command:</p>
			<p class="source-code">great_expectations suite edit staging.validation</p>
			<p>This will launch a Jupyter notebook with the code for the suite. I have deleted one line that sets the number of rows between 0 and 0, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/Figure_11.7_B15739.jpg" alt="Figure 11.7 – Editing the Great Expectations suite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Editing the Great Expectations suite</p>
			<p>After deleting the<a id="_idIndexMarker609"/> highlighted line, run all the cells in the notebook. Now you can refresh your documents and you will see that the expectation on the row number is no longer part of the suite, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/Figure_11.8_B15739.jpg" alt="Figure 11.8 – Great Expectations documents for the suite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Great Expectations documents for the suite</p>
			<p>Now that the suite<a id="_idIndexMarker610"/> is complete, you need to generate a file that you can run to launch the validation. Use the following command to create a tap using the <strong class="source-inline">staging.validation</strong> suite and output the <strong class="source-inline">sv.py</strong> file:</p>
			<p class="source-code">great_expectations tap new staging.validation sv.py</p>
			<p>Now you can run this file to validate the test database staging table. </p>
			<p>The first <a id="_idIndexMarker611"/>processor receives flowfiles from an input port that is connected to the output port of the <strong class="source-inline">QueryStaging</strong> processor group. It connects to an <strong class="source-inline">ExecuteStreamCommand</strong> processor.</p>
			<h3>ExecuteStreamCommand</h3>
			<p>The <strong class="source-inline">ExecuteStreamCommand</strong> will execute<a id="_idIndexMarker612"/> a command and listen for output, streaming the results. Since the <strong class="source-inline">sv.py</strong> file only prints a single line and exits, there is no stream, but if your command had multiple outputs, the processor would grab them all as they were output. </p>
			<p>To configure the processor, set the <strong class="bold">Command Arguments</strong> property to <strong class="source-inline">sv.py</strong>, the <strong class="bold">Command Path</strong> to <strong class="bold">Python3</strong>, and the working directory to the location of the <strong class="source-inline">sv.py</strong> file.</p>
			<p>The processor connects to an <strong class="source-inline">EvaluateJsonPath</strong> processor that extracts <strong class="source-inline">$.result</strong> and sends it to a <strong class="source-inline">RouteOnAttribute</strong> processor. I have configured a single property and accorded it the value <strong class="source-inline">pass</strong>:</p>
			<p class="source-code">${result:startsWith('pass')}</p>
			<p>The preceding expression checks the result attribute to see whether it matches <strong class="source-inline">pass</strong>. If so, the processor sends the flowfile to an output port.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor128"/>Insert Warehouse</h2>
			<p>You have <a id="_idIndexMarker613"/>made it to the last processor group – <strong class="bold">Insert Warehouse</strong>. The data has been staged and validated successfully and is ready to move to the warehouse. This processor group uses an <strong class="source-inline">ExecuteSQLRecord</strong> and a <strong class="source-inline">PutSQL</strong> processor.</p>
			<h3>ExecuteSQLRecord</h3>
			<p><strong class="source-inline">ExecuteSQLProcessor</strong> performs<a id="_idIndexMarker614"/> a select operation on the staging table. It has a variable table defined in the NiFi variable registry pointing to staging. The query is a <strong class="source-inline">select *</strong> query, as shown:</p>
			<p class="source-code">select * from ${table}</p>
			<p>This query is the value of the SQL <strong class="source-inline">select</strong> query property. You will need to set up a <strong class="source-inline">Database Pooling Connection</strong> service and a <strong class="source-inline">Record Writer</strong> service. <strong class="source-inline">Record Writer</strong> will be a <strong class="source-inline">JsonRecordSetWriter</strong> and you will need to make sure that you set <strong class="bold">Output Grouping</strong> to <strong class="bold">One Line per Object</strong>. This processor sends the output to the <strong class="source-inline">SplitText</strong> processor, which connects to the <strong class="source-inline">EvalueJsonPath</strong> processor, which is a direct copy of the <a id="_idIndexMarker615"/>one from the <strong class="source-inline">ReadDataLake</strong> processor group that connects to the final <strong class="source-inline">PutSQL</strong> processor.</p>
			<h3>PutSQL</h3>
			<p>The <strong class="source-inline">PutSQL</strong> processor<a id="_idIndexMarker616"/> puts all of the data from the <strong class="source-inline">staging</strong> table into the final data <strong class="source-inline">warehouse</strong> table. You can configure the batch size and the rollback on failure properties. I have set the SQL Statement property to the same as when it was inserted into <strong class="source-inline">staging</strong>, except the variable for the table has been changed to <strong class="source-inline">warehouse</strong> and we set it to <strong class="source-inline">warehouse</strong> in the NiFi variable registry. The query is as follows:</p>
			<p class="source-code">INSERT INTO ${warehouse} VALUES ('${userid}', '${name}',${age},'${street}','${city}','${state}','${zip}');</p>
			<p>I have terminated the processor for all relationships as this is the end of the data pipeline. If you start all the processor groups, you will have data in your <strong class="source-inline">staging</strong> and <strong class="source-inline">warehouse</strong> tables. You can check your counters to see whether the records processed are the same as the number of records inserted. If everything worked correctly, you can now deploy your data pipeline to production.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor129"/>Deploying a data pipeline in production</h1>
			<p>In the <a id="_idIndexMarker617"/>previous chapter, you learned how to deploy data to <a id="_idIndexMarker618"/>production, so I will not go into any great depth here, but merely provide a review. To put the new data pipeline into production, perform the following steps:</p>
			<ol>
				<li value="1">Browse to your production NiFi instance. I have another instance of NiFi running on port <strong class="source-inline">8080</strong> on localhost.</li>
				<li>Drag and drop processor groups to the canvas and select <strong class="bold">Import</strong>. Choose the latest version of the processor groups you just built.</li>
				<li>Modify the variables on the processor groups to point to the database production. The table names can stay the same.</li>
			</ol>
			<p>You can then run the data pipeline and you will see that the data is populated in the production database <strong class="source-inline">staging</strong> and <strong class="source-inline">warehouse</strong> tables.</p>
			<p>The data pipeline you just built read files from a data lake, put them into a database table, ran a query to validate<a id="_idIndexMarker619"/> the table, and then inserted them into the <a id="_idIndexMarker620"/>warehouse. You could have built this data pipeline with a handful of processors and been done, but when you build for production, you will need to provide error checking and monitoring. Spending the time up front to build your data pipelines properly will save you a lot of time when something changes or breaks in production. You will be well positioned to debug and modify your data pipeline.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor130"/>Summary</h1>
			<p>In this chapter, you learned how to build and deploy a production data pipeline. You learned how to create <strong class="source-inline">TEST</strong> and <strong class="source-inline">PRODUCTION</strong> environments and built the data pipeline in <strong class="source-inline">TEST</strong>. You used the filesystem as a sample data lake and learned how you would read files from the lake and monitor them as they were processed. Instead of loading data into the data warehouse, this chapter taught you how to use a staging database to hold the data so that it could be validated before being loaded into the data warehouse. Using Great Expectations, you were able to build a validation processor group that would scan the staging database to determine whether the data was ready to be loaded into the data warehouse. Lastly, you learned how to deploy the data pipeline into <strong class="source-inline">PRODUCTION</strong>. With these skills, you can now fully build, test, and deploy production batch data pipelines.</p>
			<p>In the next chapter, you will learn how to build Apache Kafka clusters. Using Kafka, you will begin to learn how to process data streams. This data is usually near real time, as opposed to the batch processing you have been currently working with. You will install and configure the cluster to run on a single machine, or multiple devices if you have them.</p>
		</div>
	</body></html>