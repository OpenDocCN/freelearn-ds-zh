<html><head></head><body>
		<div><h1 id="_idParaDest-116"><em class="italic"><a id="_idTextAnchor118"/>Chapter 11</em>: Building a Production Data Pipeline</h1>
			<p>In this chapter, you will build a production data pipeline using the features and techniques that you have learned in this section of the book. The data pipeline will be broken into processor groups that perform a single task. Those groups will be version controlled and they will use the NiFi variable registry so that they can be deployed in a production environment.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Creating a test and production environment</li>
				<li>Building a production data pipeline</li>
				<li>Deploying a data pipeline in production</li>
			</ul>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor119"/>Creating a test and production environment</h1>
			<p>In<a id="_idIndexMarker580"/> this chapter, we <a id="_idIndexMarker581"/>will return to using PostgreSQL for both the extraction and loading of data. The data pipeline will require a test and production environment, each of which will have a staging and a warehouse table. To create the databases and tables, you will use <strong class="bold">PgAdmin4</strong>. </p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor120"/>Creating the databases</h2>
			<p>To use<a id="_idIndexMarker582"/> PgAdmin4, perform the<a id="_idIndexMarker583"/> following steps:</p>
			<ol>
				<li>Browse to <code>http://localhostw/pgadmin4/l</code>, enter your username and password, and then click the <strong class="bold">Login</strong> button. Once logged in, expand the server icon in the left panel. </li>
				<li>To create the databases, right-click on the databases icon and select <code>test</code>.</li>
				<li>Next, you <a id="_idIndexMarker584"/>will need to add the tables. To create the staging table, right-click on <code>staging</code>. Then, select the <strong class="bold">Columns</strong> tab. Using the plus sign, create the fields shown in the following screenshot:<div><img src="img/Figure_11.1_B15739.jpg" alt="Figure 11.1 – The columns used in the staging table&#13;&#10;"/></div><p class="figure-caption">Figure 11.1 – The columns used in the staging table</p></li>
				<li>Save the<a id="_idIndexMarker586"/> table when you are done. You will need to create this<a id="_idIndexMarker587"/> table once more for the test database and twice more for the production database. To save some time, you can use <strong class="bold">CREATE Script</strong> to do this for you. Right-click on the staging table, and then select <strong class="bold">Scripts</strong> | <strong class="bold">CREATE Script</strong>, as shown in the following screenshot:<div><img src="img/Figure_11.2_B15739.jpg" alt="Figure 11.2 – Generating the CREATE script&#13;&#10;"/></div><p class="figure-caption">Figure 11.2 – Generating the CREATE script</p></li>
				<li>A window will open in the main screen with the SQL required to generate the table. By changing the name from <code>staging</code> to <code>warehouse</code>, you can make the warehouse table in test, which will be identical to staging. Once you have made the change, click the play button in the toolbar. </li>
				<li>Lastly, right-click on <code>production</code>. Use the script to create both the tables.</li>
			</ol>
			<p>Now that you have the tables created for the test and production environments, you will need a data lake.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor121"/>Populating a data lake</h2>
			<p>A <strong class="bold">data lake</strong> is usually a <a id="_idIndexMarker588"/>place on disk where files are stored. Usually, you will find data lakes using Hadoop for the <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>) and the other <a id="_idIndexMarker589"/>tools built on top of the Hadoop ecosystem. In this chapter, we will just drop files in a folder to simulate how reading from the data lake would work. </p>
			<p>To create the data lake, you can use Python and the Faker library. Before you write the code, create a folder to act as the data lake. I have created a folder named <code>datalake</code> in my home directory.</p>
			<p>To populate the data lake, you will need to write JSON files with information about an individual. This is similar to the JSON and CSV code you wrote in the first section of this book. The steps are as follows:</p>
			<ol>
				<li value="1">Import the libraries, set the data lake directory, and set <code>userid</code> to <code>1</code>. The <code>userid</code> variable is going to be a primary key, so we need it to be distinct – incrementing will do that for us:<pre>from faker import Faker
import json
import os
os.chdir("/home/paulcrickard/datalake")
fake=Faker()
userid=1</pre></li>
				<li>Next, create a loop that generates a data object containing the user ID, name, age, street, city, state, and zip of a fake individual. The <code>fname</code> variable holds the first and last name of a person without a space in the middle. If you had a space, Linux would wrap the file in quotes:<pre>for i in range(1000):
    name=fake.name()
    fname=name.replace(" ","-")+'.json'
    data={
        "userid":userid,
        "name":name,
        "age":fake.random_int(min=18, max=101, step=1),
        "street":fake.street_address(),
        "city":fake.city(),
        "state":fake.state(),
        "zip":fake.zipcode()
    }</pre></li>
				<li>Lastly, dump the <a id="_idIndexMarker590"/>JSON object and then write it to a file named after the person. Close the file and let the loop continue:<pre>    datajson=json.dumps(data)
    output=open(fname,'w')
    userid+=1
    output.write(datajson)
    output.close()</pre></li>
			</ol>
			<p>Run the preceding code and you will have 1,000 JSON files in your data lake. Now you can start building the data pipeline.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor122"/>Building a production data pipeline</h1>
			<p>The data pipeline<a id="_idIndexMarker591"/> you build will do the following:</p>
			<ul>
				<li>Read files from the data lake.</li>
				<li>Insert the files into staging.</li>
				<li>Validate the staging data.</li>
				<li>Move staging to the warehouse.</li>
			</ul>
			<p>The final <a id="_idIndexMarker592"/>data pipeline will look like the following screenshot:</p>
			<div><div><img src="img/Figure_11.3_B15739.jpg" alt="Figure 11.3 – The final version of the data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – The final version of the data pipeline</p>
			<p>We will build the data pipeline processor group by processor group. The first processor group will read the data lake.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor123"/>Reading the data lake</h2>
			<p>In the first section of<a id="_idIndexMarker593"/> this book, you read files from NiFi and will do the same here. This processor group will consist of three processors – <code>GetFile</code>, <code>EvaluateJsonPath</code>, and <code>UpdateCounter</code> – and an output port. Drag the processors and port to the canvas. In the following sections, you will configure them.</p>
			<h3>GetFile</h3>
			<p>The <code>GetFile</code> processor<a id="_idIndexMarker594"/> reads files from a folder, in this case, our data lake. If you were reading a data lake in Hadoop, you would switch out this processor for the <code>GetHDFS</code> processor. To configure the processor, specify the input directory; in my case, it is <code>/home/paulcrickard/datalake</code>. Make sure <code>^.*\.([jJ][sS][oO][nN]??)$</code>. If you leave the default, it will work, but if there are other files in the folder, NiFi will try to grab them and will fail.</p>
			<h3>EvaluateJsonPath</h3>
			<p>The <code>EvaluateJsonPath</code> processor<a id="_idIndexMarker595"/> will extract the fields from the JSON and put them into flowfile attributes. To do so, set the <strong class="bold">Destination</strong> property to <strong class="bold">flowfile-attribute</strong>. Leave the rest of the properties as the default. Using the plus sign, create a property for each field in the JSON. The configuration is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_11.4_B15739.jpg" alt="Figure 11.4 – The configuration for the EvaluateJsonPath processor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The configuration for the EvaluateJsonPath processor</p>
			<p>This would be<a id="_idIndexMarker596"/> enough to complete the task of reading from the data lake, but we will add one more processor for monitoring.</p>
			<h3>UpdateCounter</h3>
			<p>This processor<a id="_idIndexMarker597"/> allows you to create an increment counter. As flowfiles pass through, we can hold a count of how many are being processed. This processor does not manipulate or change any of our data, but will allow us to monitor the progress of the processor group. We will be able to see the number of FlowFiles that have moved through the processor. This is a more accurate way than using the GUI display, but it only shows the number of records in the last 5 minutes. To configure the processor, leave the <code>1</code> and set the <code>datalakerecordsprocessed</code>.</p>
			<p>To finish this section of the data pipeline, drag an output port to the canvas and name it <code>OutputDataLake</code>. Exit the processor group and right-click, select <code>ReadDataLake</code>, wrote a short description and version comments, and then performed a save.</p>
			<p class="callout-heading">NiFi-Registry</p>
			<p class="callout">I have created a new bucket named <code>DataLake</code>. To create buckets, you can browse to the registry at <code>http://localhost:18080/nifi-registry/</code>. Click the wrench in the right corner and then click the <strong class="bold">NEW BUCKET</strong> button. Name and save the bucket.</p>
			<p>The first processor<a id="_idIndexMarker598"/> group is complete. You can use this processor group any time you need to read from the data lake. The processor group will hand you every file with the fields extracted. If the data lake changed, you would only need to fix this one processor group to update all of your data pipelines. </p>
			<p>Before continuing down the data pipeline, the next section will take a small diversion to show how you can attach other processor groups.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor124"/>Scanning the data lake</h2>
			<p>The goal of the data pipeline is to <a id="_idIndexMarker599"/>read the data lake and put the data in the data warehouse. But let's assume there is another department at our company that needs to monitor the data lake for certain people – maybe VIP customers. Instead of building a new data pipeline, you can just add their task to the <code>ReadDataLake</code> processor group.</p>
			<p>The <code>ScanLake</code> processor group has an input port that is connected to the output of the <code>ReadDataLake</code> processor. It uses the <code>ScanContent</code> processor attached to the <code>EvaluateJsonPath</code> processor, which is terminated at the <code>PutSlack</code> processor, as well as sending the data through to an output port. The flow is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_11.5_B15739.jpg" alt="Figure 11.5 – The ScanLake processor group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – The ScanLake processor group</p>
			<p>The<a id="_idIndexMarker600"/> previous chapter used the <code>PutSlack</code> processor and you are already familiar with the <code>EvaluateJsonPath</code> processor. <code>ScanContent</code>, however, is a new processor. The <code>ScanContent</code> processor allows you to look at fields in the flowfile content and compare them to a dictionary file – a file with content on each line that you are looking for. I have put a single name in a file at <code>/home/paulcrickard/data.txt</code>. I configured the processor by setting the path as the value of the <strong class="bold">Dictionary File</strong> property. Now, when a file comes through that contains that name, I will get a message on Slack.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor125"/>Inserting the data into staging</h2>
			<p>The<a id="_idIndexMarker601"/> data we read was from the data lake and will not be removed, so we do not <a id="_idIndexMarker602"/>need to take any intermediary steps, such as writing data to a file, as we would have done had the data been from a transactional database. But what we will do is place the data in a staging table to make sure that everything works as we expect before putting it in the data warehouse. To insert the data into staging only requires one processor, <code>PutSQL</code>.</p>
			<h3>PutSQL</h3>
			<p>The <code>PutSQL</code> processor<a id="_idIndexMarker603"/> will allow you to execute an <code>INSERT</code> or <code>UPDATE</code> operation on a database table. The processor allows you to specify the query in the contents of a flowfile, or you can hardcode the query to use as a property in the processor. For this example, I have hardcoded the query in the <strong class="bold">SQL Statement</strong> property, which is shown as follows:</p>
			<pre>INSERT INTO ${table} VALUES ('${userid}', '${name}',${age},'${street}','${city}','${state}','${zip}');</pre>
			<p>The preceding query takes the attributes from the flowfile and passes them into the query, so while it is hardcoded, it will change based on the flowfiles it receives. You may have noticed that you have not used <code>${table}</code> in any of the <code>EvaluateJsonPath</code> processors. I have declared a variable using the NiFi registry and added it to the processor group scope. The value of the table will be <code>staging</code> for this test environment, but will change later when we deploy the data pipeline to production.</p>
			<p>You will also need to add a <strong class="bold">Java Database Connection</strong> (<strong class="bold">JDBC</strong>) pool, which you have done in earlier chapters of this book. You can specify the batch size, the number of records to retrieve, and whether you want the processor to roll back on failure. Setting <strong class="bold">Rollback on Failure</strong> to <strong class="bold">True</strong> is how you can create atomicity in your transactions. If a single flowfile in a batch fails, the processor will stop and nothing else can continue.</p>
			<p>I have connected the processor to another <code>UpdateCounter</code> processor. This processor creates and updates <code>InsertedStaging</code>. The counter should match <code>datalakerecordsprocessor</code> when everything has finished. The <code>UpdateCounter</code> processor connects to an output port named <code>OutputStaging</code>.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor126"/>Querying the staging database</h2>
			<p>The next processor <a id="_idIndexMarker604"/>group is for querying the staging database. Now that the data has been loaded, we can query the database to make sure all the records have actually made it in. You could perform other validation steps or queries to see whether the results match what you would expect – if you have data analysts, they would be a good source of information for defining these queries. In the following sections, you will query the staging database and route the results based on whether it meets your criteria.</p>
			<h3>ExecuteSQLRecord</h3>
			<p>In the <a id="_idIndexMarker605"/>previous processor group, you used the <code>PutSQL</code> processor to insert data into the database, but in this processor group, you want to perform a <code>select</code> query. The <code>select</code> query is shown as follows:</p>
			<pre>select count(*) from ${table}</pre>
			<p>The preceding query is set as the value of the optional SQL <code>select</code> query property. The <code>${table}</code> is a NiFi variable registry variable assigned to the processor group and has a value of <code>staging</code>. You will need to define a JDBC connection and a record writer in the processor properties. The record writer is a JSON record set writer. The return value of the processor will be a JSON object with one field – <code>count</code>. This processor is sent to an <code>EvaluateJsonPath</code> processor to extract the count as <code>recordcount</code>. That processor is then sent to the next processor.</p>
			<h3>RouteOnAttribute</h3>
			<p>The <code>RouteOnAttribute</code> processor allows you <a id="_idIndexMarker606"/>to use expressions or values to define where a flowfile goes. To configure the processor, I have set the <code>allrecords</code> and set the value to a NiFi expression, shown as follows:</p>
			<pre>${recordcount:ge( 1000 )}</pre>
			<p>The preceding expression evaluates the <code>recordcount</code> attribute to see whether it is greater than or equal to 1,000. If it is, it will route on this relationship. I have attached the output to an output port named <code>OutputQueryStaging</code>.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor127"/>Validating the staging data</h2>
			<p>The previous <a id="_idIndexMarker607"/>processor group did some validation and you could stop there. However, Great Expectations is an excellent library for handling validation for you. You learned about Great Expectations in <a href="B15739_07_ePub_AM.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a><em class="italic">, Features of a Production Pipeline</em>, but I will cover it quickly again here. </p>
			<p>To use Great Expectations, you need to create a project folder. I have done that in the following code snippet and initialized Great Expectations:</p>
			<pre>mkdir staging
great_expectations init</pre>
			<p>You will be prompted to create your validation suite. Choose <strong class="bold">Relational database (SQL)</strong>, then <strong class="bold">Postgres</strong>, and provide the required information. The prompts will look like the following screenshot:</p>
			<div><div><img src="img/Figure_11.6_B15739.jpg" alt="Figure 11.6 – Configuring Great Expectations to work with PostgreSQL&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Configuring Great Expectations to work with PostgreSQL</p>
			<p>When it is finished, Great <a id="_idIndexMarker608"/>Expectations will attempt to connect to the database. If successful, it will provide the URL for your documents. Since the table is empty, it will not create a very detailed validation suite. You can edit the suite using the following command:</p>
			<pre>great_expectations suite edit staging.validation</pre>
			<p>This will launch a Jupyter notebook with the code for the suite. I have deleted one line that sets the number of rows between 0 and 0, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_11.7_B15739.jpg" alt="Figure 11.7 – Editing the Great Expectations suite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Editing the Great Expectations suite</p>
			<p>After deleting the<a id="_idIndexMarker609"/> highlighted line, run all the cells in the notebook. Now you can refresh your documents and you will see that the expectation on the row number is no longer part of the suite, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_11.8_B15739.jpg" alt="Figure 11.8 – Great Expectations documents for the suite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Great Expectations documents for the suite</p>
			<p>Now that the suite<a id="_idIndexMarker610"/> is complete, you need to generate a file that you can run to launch the validation. Use the following command to create a tap using the <code>staging.validation</code> suite and output the <code>sv.py</code> file:</p>
			<pre>great_expectations tap new staging.validation sv.py</pre>
			<p>Now you can run this file to validate the test database staging table. </p>
			<p>The first <a id="_idIndexMarker611"/>processor receives flowfiles from an input port that is connected to the output port of the <code>QueryStaging</code> processor group. It connects to an <code>ExecuteStreamCommand</code> processor.</p>
			<h3>ExecuteStreamCommand</h3>
			<p>The <code>ExecuteStreamCommand</code> will execute<a id="_idIndexMarker612"/> a command and listen for output, streaming the results. Since the <code>sv.py</code> file only prints a single line and exits, there is no stream, but if your command had multiple outputs, the processor would grab them all as they were output. </p>
			<p>To configure the processor, set the <code>sv.py</code>, the <code>sv.py</code> file.</p>
			<p>The processor connects to an <code>EvaluateJsonPath</code> processor that extracts <code>$.result</code> and sends it to a <code>RouteOnAttribute</code> processor. I have configured a single property and accorded it the value <code>pass</code>:</p>
			<pre>${result:startsWith('pass')}</pre>
			<p>The preceding expression checks the result attribute to see whether it matches <code>pass</code>. If so, the processor sends the flowfile to an output port.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor128"/>Insert Warehouse</h2>
			<p>You have <a id="_idIndexMarker613"/>made it to the last processor group – <code>ExecuteSQLRecord</code> and a <code>PutSQL</code> processor.</p>
			<h3>ExecuteSQLRecord</h3>
			<p><code>ExecuteSQLProcessor</code> performs<a id="_idIndexMarker614"/> a select operation on the staging table. It has a variable table defined in the NiFi variable registry pointing to staging. The query is a <code>select *</code> query, as shown:</p>
			<pre>select * from ${table}</pre>
			<p>This query is the value of the SQL <code>select</code> query property. You will need to set up a <code>Database Pooling Connection</code> service and a <code>Record Writer</code> service. <code>Record Writer</code> will be a <code>JsonRecordSetWriter</code> and you will need to make sure that you set <code>SplitText</code> processor, which connects to the <code>EvalueJsonPath</code> processor, which is a direct copy of the <a id="_idIndexMarker615"/>one from the <code>ReadDataLake</code> processor group that connects to the final <code>PutSQL</code> processor.</p>
			<h3>PutSQL</h3>
			<p>The <code>PutSQL</code> processor<a id="_idIndexMarker616"/> puts all of the data from the <code>staging</code> table into the final data <code>warehouse</code> table. You can configure the batch size and the rollback on failure properties. I have set the SQL Statement property to the same as when it was inserted into <code>staging</code>, except the variable for the table has been changed to <code>warehouse</code> and we set it to <code>warehouse</code> in the NiFi variable registry. The query is as follows:</p>
			<pre>INSERT INTO ${warehouse} VALUES ('${userid}', '${name}',${age},'${street}','${city}','${state}','${zip}');</pre>
			<p>I have terminated the processor for all relationships as this is the end of the data pipeline. If you start all the processor groups, you will have data in your <code>staging</code> and <code>warehouse</code> tables. You can check your counters to see whether the records processed are the same as the number of records inserted. If everything worked correctly, you can now deploy your data pipeline to production.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor129"/>Deploying a data pipeline in production</h1>
			<p>In the <a id="_idIndexMarker617"/>previous chapter, you learned how to deploy data to <a id="_idIndexMarker618"/>production, so I will not go into any great depth here, but merely provide a review. To put the new data pipeline into production, perform the following steps:</p>
			<ol>
				<li value="1">Browse to your production NiFi instance. I have another instance of NiFi running on port <code>8080</code> on localhost.</li>
				<li>Drag and drop processor groups to the canvas and select <strong class="bold">Import</strong>. Choose the latest version of the processor groups you just built.</li>
				<li>Modify the variables on the processor groups to point to the database production. The table names can stay the same.</li>
			</ol>
			<p>You can then run the data pipeline and you will see that the data is populated in the production database <code>staging</code> and <code>warehouse</code> tables.</p>
			<p>The data pipeline you just built read files from a data lake, put them into a database table, ran a query to validate<a id="_idIndexMarker619"/> the table, and then inserted them into the <a id="_idIndexMarker620"/>warehouse. You could have built this data pipeline with a handful of processors and been done, but when you build for production, you will need to provide error checking and monitoring. Spending the time up front to build your data pipelines properly will save you a lot of time when something changes or breaks in production. You will be well positioned to debug and modify your data pipeline.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor130"/>Summary</h1>
			<p>In this chapter, you learned how to build and deploy a production data pipeline. You learned how to create <code>TEST</code> and <code>PRODUCTION</code> environments and built the data pipeline in <code>TEST</code>. You used the filesystem as a sample data lake and learned how you would read files from the lake and monitor them as they were processed. Instead of loading data into the data warehouse, this chapter taught you how to use a staging database to hold the data so that it could be validated before being loaded into the data warehouse. Using Great Expectations, you were able to build a validation processor group that would scan the staging database to determine whether the data was ready to be loaded into the data warehouse. Lastly, you learned how to deploy the data pipeline into <code>PRODUCTION</code>. With these skills, you can now fully build, test, and deploy production batch data pipelines.</p>
			<p>In the next chapter, you will learn how to build Apache Kafka clusters. Using Kafka, you will begin to learn how to process data streams. This data is usually near real time, as opposed to the batch processing you have been currently working with. You will install and configure the cluster to run on a single machine, or multiple devices if you have them.</p>
		</div>
	</body></html>