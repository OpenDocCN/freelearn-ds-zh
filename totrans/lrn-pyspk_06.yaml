- en: Chapter 6. Introducing the ML Package
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：介绍ML包
- en: In the previous chapter, we worked with the MLlib package in Spark that operated
    strictly on RDDs. In this chapter, we move to the ML part of Spark that operates
    strictly on DataFrames. Also, according to the Spark documentation, the primary
    machine learning API for Spark is now the DataFrame-based set of models contained
    in the `spark.ml` package.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了Spark的MLlib包，该包严格在RDD上操作。在本章中，我们将转向Spark的ML部分，该部分严格在DataFrame上操作。此外，根据Spark文档，Spark的主要机器学习API现在是包含在`spark.ml`包中的基于DataFrame的模型集。
- en: So, let's get to it!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we will reuse a portion of the dataset we played within the
    previous chapter. The data can be downloaded from [http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重用上一章中我们使用的数据集的一部分。数据可以从[http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz)下载。
- en: 'In this chapter, you will learn how to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Prepare transformers, estimators, and pipelines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备转换器、估计器和管道
- en: Predict the chances of infant survival using models available in the ML package
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ML包中的模型预测婴儿生存的机会
- en: Evaluate the performance of the model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型的性能
- en: Perform parameter hyper-tuning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行参数超调
- en: Use other machine-learning models available in the package
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用包中可用的其他机器学习模型
- en: Overview of the package
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包的概述
- en: 'At the top level, the package exposes three main abstract classes: a `Transformer`,
    an `Estimator`, and a `Pipeline`. We will shortly explain each with some short
    examples. We will provide more concrete examples of some of the models in the
    last section of this chapter.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高级别，该包公开了三个主要抽象类：一个`Transformer`，一个`Estimator`和一个`Pipeline`。我们将很快通过一些简短的示例来解释每个类。我们将在本章的最后部分提供一些模型的具体示例。
- en: Transformer
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器
- en: The `Transformer` class, like the name suggests, *transforms* your data by (normally)
    appending a new column to your DataFrame.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`类，正如其名称所暗示的，通过（通常）向DataFrame中添加新列来*转换*你的数据。'
- en: 'At the high level, when deriving from the `Transformer` abstract class, each
    and every new `Transformer` needs to implement a `.transform(...)` method. The
    method, as a first and normally the only obligatory parameter, requires passing
    a DataFrame to be transformed. This, of course, varies *method-by-method* in the
    ML package: other *popular* parameters are `inputCol` and `outputCol`; these,
    however, frequently default to some predefined values, such as, for example, `''features''`
    for the `inputCol` parameter.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高级别，当从`Transformer`抽象类派生时，每个新的`Transformer`都需要实现一个`.transform(...)`方法。这个方法，作为一个首要且通常是唯一必需的参数，需要传递一个要转换的DataFrame。当然，在ML包中，这会*方法各异*：其他*常用*参数是`inputCol`和`outputCol`；然而，这些参数通常默认为一些预定义的值，例如，对于`inputCol`参数，默认值可能是`'features'`。
- en: 'There are many `Transformers` offered in the `spark.ml.feature` and we will
    briefly describe them here (before we use some of them later in this chapter):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.ml.feature`提供了许多`Transformers`，我们将在下面简要描述它们（在我们本章后面使用它们之前）：'
- en: '`Binarizer`: Given a threshold, the method takes a continuous variable and
    transforms it into a binary one.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Binarizer`：给定一个阈值，该方法将连续变量转换为二进制变量。'
- en: '`Bucketizer`: Similar to the `Binarizer`, this method takes a list of thresholds
    (the `splits` parameter) and transforms a continuous variable into a multinomial
    one.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Bucketizer`：类似于`Binarizer`，该方法接受一系列阈值（`splits`参数）并将连续变量转换为多项式变量。'
- en: '`ChiSqSelector`: For the categorical target variables (think classification
    models), this feature allows you to select a predefined number of features (parameterized
    by the `numTopFeatures` parameter) that explain the variance in the target the
    best. The selection is done, as the name of the method suggests, using a Chi-Square
    test. It is one of the two-step methods: first, you need to `.fit(...)` your data
    (so the method can calculate the Chi-square tests). Calling the `.fit(...)` method
    (you pass your DataFrame as a parameter) returns a `ChiSqSelectorModel` object
    that you can then use to transform your DataFrame using the `.transform(...)`
    method.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChiSqSelector`: 对于分类目标变量（例如分类模型），此功能允许你选择一个预定义数量的特征（由`numTopFeatures`参数参数化），这些特征最好地解释了目标中的方差。选择是通过名称暗示的方法完成的，即使用卡方检验。这是两步方法之一：首先，你需要`.fit(...)`你的数据（以便方法可以计算卡方检验）。调用`.fit(...)`方法（你传递DataFrame作为参数）返回一个`ChiSqSelectorModel`对象，然后你可以使用该对象通过`.transform(...)`方法转换你的DataFrame。'
- en: Note
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More information on Chi-squares can be found here: [http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html](http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html).'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更多关于卡方检验的信息可以在这里找到：[http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html](http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html)。
- en: '`CountVectorizer`: This is useful for a tokenized text (such as `[[''Learning'',
    ''PySpark'', ''with'', ''us''],[''us'', ''us'', ''us'']]`). It is one of two-step
    methods: first, you need to `.fit(...)`, that is, learn the patterns from your
    dataset, before you can `.transform(...)` with the `CountVectorizerModel` returned
    by the `.fit(...)` method. The output from this transformer, for the tokenized
    text presented previously, would look similar to this: `[(4, [0, 1, 2, 3], [1.0,
    1.0, 1.0, 1.0]),(4, [3], [3.0])]`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CountVectorizer`: 这对于标记化文本（例如`[[''Learning'', ''PySpark'', ''with'', ''us''],[''us'',
    ''us'', ''us'']]`）非常有用。它是两步方法之一：首先，你需要使用`.fit(...)`（即从你的数据集中学习模式），然后你才能使用由`.fit(...)`方法返回的`CountVectorizerModel`进行`.transform(...)`。此转换器的输出，对于前面提供的标记化文本，将类似于以下内容：`[(4,
    [0, 1, 2, 3], [1.0, 1.0, 1.0, 1.0]),(4, [3], [3.0])]`。'
- en: '`DCT`: The Discrete Cosine Transform takes a vector of real values and returns
    a vector of the same length, but with the sum of cosine functions oscillating
    at different frequencies. Such transformations are useful to extract some underlying
    frequencies in your data or in data compression.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DCT`: 离散余弦变换（Discrete Cosine Transform）将一个实数值向量转换为一个长度相同的向量，但其中的余弦函数以不同的频率振荡。这种变换对于从你的数据或数据压缩中提取一些基本频率非常有用。'
- en: '`ElementwiseProduct`: A method that returns a vector with elements that are
    products of the vector passed to the method, and a vector passed as the `scalingVec`
    parameter. For example, if you had a `[10.0, 3.0, 15.0]` vector and your `scalingVec`
    was `[0.99, 3.30, 0.66]`, then the vector you would get would look as follows:
    `[9.9, 9.9, 9.9]`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ElementwiseProduct`: 这是一个返回元素为传递给方法的向量与作为`scalingVec`参数传递的向量乘积的向量的方法。例如，如果你有一个`[10.0,
    3.0, 15.0]`向量，并且你的`scalingVec`是`[0.99, 3.30, 0.66]`，那么你将得到的向量将如下所示：`[9.9, 9.9,
    9.9]`。'
- en: '`HashingTF`: A hashing trick transformer that takes a list of tokenized text
    and returns a vector (of predefined length) with counts. From PySpark''s documentation:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HashingTF`: 这是一个哈希技巧转换器，它接受一个标记化文本列表并返回一个向量（具有预定义的长度）和计数。从PySpark的文档中：'
- en: '"Since a simple modulo is used to transform the hash function to a column index,
    it is advisable to use a power of two as the numFeatures parameter; otherwise
    the features will not be mapped evenly to the columns."'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"由于使用了简单的模运算将哈希函数转换为列索引，因此建议将numFeatures参数设置为2的幂；否则，特征将不会均匀地映射到列上。"'
- en: '`IDF`: This method computes an **Inverse Document Frequency** for a list of
    documents. Note that the documents need to already be represented as a vector
    (for example, using either the `HashingTF` or `CountVectorizer`).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IDF`: 此方法计算文档列表的**逆文档频率**。请注意，文档需要已经表示为向量（例如，使用`HashingTF`或`CountVectorizer`）。'
- en: '`IndexToString`: A complement to the `StringIndexer` method. It uses the encoding
    from the `StringIndexerModel` object to reverse the string index to original values.
    As an aside, please note that this sometimes does not work and you need to specify
    the values from the `StringIndexer`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IndexToString`: 这是`StringIndexer`方法的补充。它使用`StringIndexerModel`对象的编码将字符串索引反转回原始值。顺便提一下，请注意，这有时可能不起作用，你需要从`StringIndexer`指定值。'
- en: '`MaxAbsScaler`: Rescales the data to be within the `[-1.0, 1.0]` range (thus,
    it does not shift the center of the data).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxAbsScaler`: 将数据重新缩放到`[-1.0, 1.0]`范围内（因此，它不会移动数据的中心）。'
- en: '`MinMaxScaler`: This is similar to the `MaxAbsScaler` with the difference that
    it scales the data to be in the `[0.0, 1.0]` range.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler`: 与`MaxAbsScaler`类似，但不同之处在于它将数据缩放到`[0.0, 1.0]`范围内。'
- en: '`NGram`: This method takes a list of tokenized text and returns *n-grams*:
    pairs, triples, or *n-mores* of subsequent words. For example, if you had a `[''good'',
    ''morning'', ''Robin'', ''Williams'']` vector you would get the following output:
    `[''good morning'', ''morning Robin'', ''Robin Williams'']`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NGram`: 此方法接受一个标记化文本的列表，并返回*n-gram*：后续单词的成对、三元组或*n-mores*。例如，如果您有一个`[''good'',
    ''morning'', ''Robin'', ''Williams'']`向量，您将得到以下输出：`[''good morning'', ''morning
    Robin'', ''Robin Williams'']`。'
- en: '`Normalizer`: This method scales the data to be of unit norm using the p-norm
    value (by default, it is L2).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalizer`: 此方法使用p-norm值（默认为L2）将数据缩放到单位范数。'
- en: '`OneHotEncoder`: This method encodes a categorical column to a column of binary
    vectors.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneHotEncoder`: 此方法将分类列编码为二进制向量列。'
- en: '`PCA`: Performs the data reduction using principal component analysis.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PCA`: 使用主成分分析进行数据降维。'
- en: '`PolynomialExpansion`: Performs a polynomial expansion of a vector. For example,
    if you had a vector symbolically written as `[x, y, z]`, the method would produce
    the following expansion: `[x, x*x, y, x*y, y*y, z, x*z, y*z, z*z]`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PolynomialExpansion`: 对向量执行多项式展开。例如，如果您有一个符号写为`[x, y, z]`的向量，该方法将生成以下展开：`[x,
    x*x, y, x*y, y*y, z, x*z, y*z, z*z]`。'
- en: '`QuantileDiscretizer`: Similar to the `Bucketizer` method, but instead of passing
    the splits parameter, you pass the `numBuckets` one. The method then decides,
    by calculating approximate quantiles over your data, what the splits should be.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QuantileDiscretizer`: 与`Bucketizer`方法类似，但您不是传递`splits`参数，而是传递`numBuckets`。然后，方法通过计算数据上的近似分位数来决定应该是什么分割。'
- en: '`RegexTokenizer`: This is a string tokenizer using regular expressions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegexTokenizer`: 这是一个使用正则表达式的字符串分词器。'
- en: '`RFormula`: For those of you who are avid R users, you can pass a formula such
    as `vec ~ alpha * 3 + beta` (assuming your `DataFrame` has the `alpha` and `beta`
    columns) and it will produce the `vec` column given the expression.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RFormula`: 对于那些热衷于使用R的用户，您可以通过传递一个公式，例如`vec ~ alpha * 3 + beta`（假设您的`DataFrame`有`alpha`和`beta`列），它将根据表达式生成`vec`列。'
- en: '`SQLTransformer`: Similar to the previous, but instead of R-like formulas,
    you can use SQL syntax.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SQLTransformer`: 与之前类似，但您可以使用SQL语法而不是R-like公式。'
- en: Tip
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'The `FROM` statement should be selecting from `__THIS__`, indicating you are
    accessing the DataFrame. For example: `SELECT alpha * 3 + beta AS vec FROM __THIS__`.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`FROM`语句应选择`__THIS__`，表示您正在访问DataFrame。例如：`SELECT alpha * 3 + beta AS vec FROM
    __THIS__`。'
- en: '`StandardScaler`: Standardizes the column to have a 0 mean and standard deviation
    equal to 1.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler`: 将列标准化，使其具有0均值和标准差等于1。'
- en: '`StopWordsRemover`: Removes stop words (such as `''the''` or `''a''`) from
    a tokenized text.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordsRemover`: 从标记化文本中移除停用词（如`''the''`或`''a''`）。'
- en: '`StringIndexer`: Given a list of all the words in a column, this will produce
    a vector of indices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StringIndexer`: 给定一个列中所有单词的列表，这将生成一个索引向量。'
- en: '`Tokenizer`: This is the default tokenizer that converts the string to lower
    case and then splits on space(s).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tokenizer`: 这是一个默认的分词器，它将字符串转换为小写，然后根据空格分割。'
- en: '`VectorAssembler`: This is a highly useful transformer that collates multiple
    numeric (vectors included) columns into a single column with a vector representation.
    For example, if you had three columns in your DataFrame:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorAssembler`: 这是一个非常有用的转换器，它将多个数值（包括向量）列合并成一个具有向量表示的单列。例如，如果您在DataFrame中有三个列：'
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of calling:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调用以下内容的输出：
- en: '[PRE1]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It would look as follows:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它看起来如下：
- en: '[PRE2]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`VectorIndexer`: This is a method for indexing categorical columns into a vector
    of indices. It works in a *column-by-column* fashion, selecting distinct values
    from the column, sorting and returning an index of the value from the map instead
    of the original value.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorIndexer`: 这是一个将分类列索引到索引向量的方法。它以*列-by-列*的方式工作，从列中选择不同的值，排序并返回映射中的值的索引，而不是原始值。'
- en: '`VectorSlicer`: Works on a feature vector, either dense or sparse: given a
    list of indices, it extracts the values from the feature vector.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorSlicer`: 在特征向量上工作，无论是密集的还是稀疏的：给定一个索引列表，它从特征向量中提取值。'
- en: '`Word2Vec`: This method takes a sentence (string) as an input and transforms
    it into a map of `{string, vector}` format, a representation that is useful in
    natural language processing.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec`: 这种方法将一个句子（字符串）作为输入，并将其转换成 `{string, vector}` 格式的映射，这种表示在自然语言处理中非常有用。'
- en: Note
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that there are many methods in the ML package that have an E letter next
    to it; this means the method is currently in beta (or Experimental) and it sometimes
    might fail or produce erroneous results. Beware.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，ML 包中有很多方法旁边都有一个 E 字母；这意味着该方法目前处于测试版（或实验性）状态，有时可能会失败或产生错误的结果。请小心。
- en: Estimators
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计器
- en: Estimators can be thought of as statistical models that need to be estimated
    to make predictions or classify your observations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器可以被视为需要估计以进行预测或对观测值进行分类的统计模型。
- en: If deriving from the abstract `Estimator` class, the new model has to implement
    the `.fit(...)` method that fits the model given the data found in a DataFrame
    and some default or user-specified parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从抽象的 `Estimator` 类派生，新的模型必须实现 `.fit(...)` 方法，该方法根据 DataFrame 中的数据和一些默认或用户指定的参数来拟合模型。
- en: There are a lot of estimators available in PySpark and we will now shortly describe
    the models available in Spark 2.0.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 中有很多估计器可用，我们现在将简要描述 Spark 2.0 中可用的模型。
- en: Classification
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: 'The ML package provides a data scientist with seven classification models to
    choose from. These range from the simplest ones (such as logistic regression)
    to more sophisticated ones. We will provide short descriptions of each of them
    in the following section:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习包为数据科学家提供了七种分类模型可供选择。这些模型从最简单的（例如逻辑回归）到更复杂的模型不等。我们将在下一节中简要介绍每种模型：
- en: '`LogisticRegression`: The benchmark model for classification. The logistic
    regression uses a logit function to calculate the probability of an observation
    belonging to a particular class. At the time of writing, the PySpark ML supports
    only binary classification problems.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression`: 分类领域的基准模型。逻辑回归使用 logit 函数来计算观测值属于特定类别的概率。在撰写本文时，PySpark
    ML 仅支持二元分类问题。'
- en: '`DecisionTreeClassifier`: A classifier that builds a decision tree to predict
    a class for an observation. Specifying the `maxDepth` parameter limits the depth
    the tree grows, the `minInstancePerNode` determines the minimum number of observations
    in the tree node required to further split, the `maxBins` parameter specifies
    the maximum number of bins the continuous variables will be split into, and the
    `impurity` specifies the metric to measure and calculate the information gain
    from the split.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier`: 一个构建决策树以预测观测值类别的分类器。指定 `maxDepth` 参数限制树的生长深度，`minInstancePerNode`
    确定树节点中所需的最小观测值数量以进一步分割，`maxBins` 参数指定连续变量将被分割成的最大箱数，而 `impurity` 指定用于衡量和计算分割信息增益的指标。'
- en: '`GBTClassifier`: A **Gradient Boosted Trees** model for classification. The
    model belongs to the family of ensemble models: models that combine multiple weak
    predictive models to form a strong one. At the moment, the `GBTClassifier` model
    supports binary labels, and continuous and categorical features.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GBTClassifier`: 一个用于分类的 **梯度提升树** 模型。该模型属于集成模型家族：这些模型将多个弱预测模型组合成一个强模型。目前，`GBTClassifier`
    模型支持二元标签以及连续和分类特征。'
- en: '`RandomForestClassifier`: This model produces multiple decision trees (hence
    the name—forest) and uses the `mode` output of those decision trees to classify
    observations. The `RandomForestClassifier` supports both binary and multinomial
    labels.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`: 该模型生成多个决策树（因此得名——森林），并使用这些决策树的 `mode` 输出来对观测值进行分类。`RandomForestClassifier`
    支持二元和多项式标签。'
- en: '`NaiveBayes`: Based on the Bayes'' theorem, this model uses conditional probability
    theory to classify observations. The `NaiveBayes` model in PySpark ML supports
    both binary and multinomial labels.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NaiveBayes`: 基于贝叶斯定理，该模型使用条件概率理论对观测值进行分类。PySpark ML 中的 `NaiveBayes` 模型支持二元和多项式标签。'
- en: '`MultilayerPerceptronClassifier`: A classifier that mimics the nature of a
    human brain. Deeply rooted in the Artificial Neural Networks theory, the model
    is a black-box, that is, it is not easy to interpret the internal parameters of
    the model. The model consists, at a minimum, of three, fully connected `layers`
    (a parameter that needs to be specified when creating the model object) of artificial
    neurons: the input layer (that needs to be equal to the number of features in
    your dataset), a number of hidden layers (at least one), and an output layer with
    the number of neurons equal to the number of categories in your label. All the
    neurons in the input and hidden layers have a sigmoid activation function, whereas
    the activation function of the neurons in the output layer is softmax.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`多层感知器分类器`：一种模仿人类大脑性质的分类器。深深植根于人工神经网络理论，该模型是一个黑盒，即模型的内部参数不易解释。该模型至少由三个全连接的`层`（在创建模型对象时需要指定的参数）组成的人工神经元：输入层（需要等于数据集中的特征数量），若干隐藏层（至少一个），以及一个输出层，其神经元数量等于标签中的类别数量。输入层和隐藏层中的所有神经元都有sigmoid激活函数，而输出层神经元的激活函数是softmax。'
- en: '`OneVsRest`: A reduction of a multiclass classification to a binary one. For
    example, in the case of a multinomial label, the model can train multiple binary
    logistic regression models. For example, if `label == 2`, the model will build
    a logistic regression where it will convert the `label == 2` to `1` (all remaining
    label values would be set to `0`) and then train a binary model. All the models
    are then scored and the model with the highest probability wins.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`一对抗`：将多类分类减少为二类分类。例如，在多项式标签的情况下，模型可以训练多个二元逻辑回归模型。例如，如果`label == 2`，则模型将构建一个逻辑回归，其中将`label
    == 2`转换为`1`（所有剩余的标签值将设置为`0`），然后训练一个二元模型。然后对所有模型进行评分，概率最高的模型获胜。'
- en: Regression
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: 'There are seven models available for regression tasks in the PySpark ML package.
    As with classification, these range from some basic ones (such as the obligatory
    linear regression) to more complex ones:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark ML包中提供了七个回归任务模型。与分类类似，这些模型从一些基本的（如强制性的线性回归）到更复杂的模型：
- en: '`AFTSurvivalRegression`: Fits an Accelerated Failure Time regression model.
    It is a parametric model that assumes that a marginal effect of one of the features
    accelerates or decelerates a life expectancy (or process failure). It is highly
    applicable for the processes with well-defined stages.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`加速失效时间回归`：拟合加速失效时间回归模型。这是一个参数模型，假设某个特征的一个边缘效应会加速或减慢预期寿命（或过程失效）。它非常适合具有明确阶段的流程。'
- en: '`DecisionTreeRegressor`: Similar to the model for classification with an obvious
    distinction that the label is continuous instead of binary (or multinomial).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`决策树回归器`：与分类模型类似，但有一个明显的区别，即标签是连续的而不是二元的（或多项式的）。'
- en: '`GBTRegressor`: As with the `DecisionTreeRegressor`, the difference is the
    data type of the label.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`梯度提升回归器`：与`决策树回归器`类似，区别在于标签的数据类型。'
- en: '`GeneralizedLinearRegression`: A family of linear models with differing kernel
    functions (link functions). In contrast to the linear regression that assumes
    normality of error terms, the GLM allows the label to have different error term
    distributions: the `GeneralizedLinearRegression` model from the PySpark ML package
    supports `gaussian`, `binomial`, `gamma`, and `poisson` families of error distributions
    with a host of different link functions.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`广义线性回归`：具有不同核函数（链接函数）的线性模型族。与假设误差项正态性的线性回归相比，GLM允许标签具有不同的误差项分布：PySpark ML包中的`广义线性回归`模型支持`高斯`、`二项式`、`伽马`和`泊松`误差分布族，以及众多不同的链接函数。'
- en: '`IsotonicRegression`: A type of regression that fits a free-form, non-decreasing
    line to your data. It is useful to fit the datasets with ordered and increasing
    observations.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`等调回归`：一种回归类型，将自由形式的非递减线拟合到您的数据。对于具有有序和递增观测值的数据集来说很有用。'
- en: '`LinearRegression`: The most simple of regression models, it assumes a linear
    relationship between features and a continuous label, and normality of error terms.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`线性回归`：回归模型中最简单的一种，它假设特征与连续标签之间存在线性关系，以及误差项的正态性。'
- en: '`RandomForestRegressor`: Similar to either `DecisionTreeRegressor` or `GBTRegressor`,
    the `RandomForestRegressor` fits a continuous label instead of a discrete one.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`随机森林回归器`：类似于`决策树回归器`或`梯度提升回归器`，`随机森林回归器`拟合的是连续标签而不是离散标签。'
- en: Clustering
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Clustering is a family of unsupervised models that are used to find underlying
    patterns in your data. The PySpark ML package provides the four most popular models
    at the moment:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一组无监督模型，用于在数据中找到潜在的规律。PySpark ML包目前提供了四个最流行的模型：
- en: '`BisectingKMeans`: A combination of the k-means clustering method and hierarchical
    clustering. The algorithm begins with all observations in a single cluster and
    iteratively splits the data into `k` clusters.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BisectingKMeans`: k均值聚类方法和层次聚类的组合。算法开始时将所有观测值放在一个簇中，然后迭代地将数据分割成`k`个簇。'
- en: Note
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Check out this website for more information on pseudo-algorithms: [http://minethedata.blogspot.com/2012/08/bisecting-k-means.html](http://minethedata.blogspot.com/2012/08/bisecting-k-means.html).'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查阅此网站以获取有关伪算法的更多信息：[http://minethedata.blogspot.com/2012/08/bisecting-k-means.html](http://minethedata.blogspot.com/2012/08/bisecting-k-means.html)。
- en: '`KMeans`: This is the famous k-mean algorithm that separates data into `k`
    clusters, iteratively searching for centroids that minimize the sum of square
    distances between each observation and the centroid of the cluster it belongs
    to.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMeans`: 这是著名的k均值算法，它将数据分离成`k`个簇，迭代地寻找使每个观测值与其所属簇的质心之间的平方距离之和最小的质心。'
- en: '`GaussianMixture`: This method uses `k` Gaussian distributions with unknown
    parameters to dissect the dataset. Using the Expectation-Maximization algorithm,
    the parameters for the Gaussians are found by maximizing the log-likelihood function.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GaussianMixture`: 此方法使用具有未知参数的`k`个高斯分布来剖析数据集。通过最大化对数似然函数，使用期望最大化算法找到高斯参数。'
- en: Tip
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Beware that for datasets with many features this model might perform poorly
    due to the curse of dimensionality and numerical issues with Gaussian distributions.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，对于具有许多特征的集合，由于维度诅咒和高斯分布的数值问题，此模型可能表现不佳。
- en: '`LDA`: This model is used for topic modeling in natural language processing
    applications.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA`: 该模型用于自然语言处理应用中的主题建模。'
- en: There is also one recommendation model available in PySpark ML, but we will
    refrain from describing it here.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark ML中还有一个推荐模型可用，但我们将在此处不对其进行描述。
- en: Pipeline
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pipeline
- en: A `Pipeline` in PySpark ML is a concept of an *end-to-end* transformation-estimation
    process (with distinct stages) that ingests some raw data (in a DataFrame form),
    performs the necessary data carpentry (transformations), and finally estimates
    a statistical model (estimator).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark ML中的`Pipeline`是一个端到端转换-估计过程（具有不同的阶段）的概念，它接收一些原始数据（以DataFrame形式），执行必要的数据整理（转换），并最终估计一个统计模型（估计器）。
- en: Tip
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A `Pipeline` can be purely transformative, that is, consisting of `Transformer`s
    only.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline`可以是纯粹的转换型，即仅由`Transformer`组成。'
- en: A `Pipeline` can be thought of as a chain of multiple discrete stages. When
    a `.fit(...)` method is executed on a `Pipeline` object, all the stages are executed
    in the order they were specified in the `stages` parameter; the `stages` parameter
    is a list of `Transformer` and `Estimator` objects. The `.fit(...)` method of
    the `Pipeline` object executes the `.transform(...)` method for the `Transformer`s
    and the `.fit(...)` method for the `Estimators`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将`Pipeline`视为多个离散阶段的链。当在`Pipeline`对象上执行`.fit(...)`方法时，所有阶段都按照在`stages`参数中指定的顺序执行；`stages`参数是一个`Transformer`和`Estimator`对象的列表。`Pipeline`对象的`.fit(...)`方法执行`Transformer`的`.transform(...)`方法和`Estimator`的`.fit(...)`方法。
- en: 'Normally, the output of a preceding stage becomes the input for the following
    stage: when deriving from either the `Transformer` or `Estimator` abstract classes,
    one needs to implement the `.getOutputCol()` method that returns the value of
    the `outputCol` parameter specified when creating an object.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，前一个阶段的输出成为下一个阶段的输入：当从`Transformer`或`Estimator`抽象类派生时，需要实现`.getOutputCol()`方法，该方法返回创建对象时指定的`outputCol`参数的值。
- en: Predicting the chances of infant survival with ML
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ML预测婴儿生存的机会
- en: In this section, we will use the portion of the dataset from the previous chapter
    to present the ideas of PySpark ML.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用上一章的数据集的一部分来展示PySpark ML的思想。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you have not yet downloaded the data while reading the previous chapter,
    it can be accessed here: [http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在阅读上一章时还没有下载数据，可以在此处访问：[http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz)。
- en: In this section, we will, once again, attempt to predict the chances of the
    survival of an infant.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次尝试预测婴儿生存的机会。
- en: Loading the data
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'First, we load the data with the help of the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用以下代码加载数据：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We specify the schema of the DataFrame; our severely limited dataset now only
    has 17 columns.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定DataFrame的模式；我们的数据集现在只有17列。
- en: Creating transformers
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建转换器
- en: Before we can use the dataset to estimate a model, we need to do some transformations.
    Since statistical models can only operate on numeric data, we will have to encode
    the `BIRTH_PLACE` variable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用数据集估计模型之前，我们需要进行一些转换。由于统计模型只能操作数值数据，我们将不得不对`BIRTH_PLACE`变量进行编码。
- en: 'Before we do any of this, since we will use a number of different feature transformations
    later in this chapter, let''s import them all:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行任何操作之前，由于我们将在本章后面使用许多不同的特征转换，让我们导入它们：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To encode the `BIRTH_PLACE` column, we will use the `OneHotEncoder` method.
    However, the method cannot accept `StringType` columns; it can only deal with
    numeric types so first we will cast the column to an `IntegerType`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对`BIRTH_PLACE`列进行编码，我们将使用`OneHotEncoder`方法。然而，该方法不能接受`StringType`列；它只能处理数值类型，因此我们首先将列转换为`IntegerType`：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Having done this, we can now create our first `Transformer`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在可以创建我们的第一个`Transformer`：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s now create a single column with all the features collated together.
    We will use the `VectorAssembler` method:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个包含所有特征的单一列。我们将使用`VectorAssembler`方法：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `inputCols` parameter passed to the `VectorAssembler` object is a list of
    all the columns to be combined together to form the `outputCol`—the `'features'`.
    Note that we use the output of the encoder object (by calling the `.getOutputCol()`
    method), so we do not have to remember to change this parameter's value should
    we change the name of the output column in the encoder object at any point.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`VectorAssembler`对象的`inputCols`参数是一个列表，其中包含所有要组合在一起形成`outputCol`（即`'features'`）的列。请注意，我们使用编码器对象的输出（通过调用`.getOutputCol()`方法），因此我们不必记住在编码器对象中更改输出列名称时更改此参数的值。
- en: It's now time to create our first estimator.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候创建我们的第一个估计器了。
- en: Creating an estimator
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建估计器
- en: 'In this example, we will (once again) use the logistic regression model. However,
    later in the chapter, we will showcase some more complex models from the `.classification`
    set of PySpark ML models, so we load the whole section:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将（再次）使用逻辑回归模型。然而，在本章的后面，我们将展示一些来自PySpark ML模型`.classification`集合的更复杂模型，因此我们加载整个部分：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once loaded, let''s create the model by using the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载，让我们使用以下代码创建模型：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We would not have to specify the `labelCol` parameter if our target column had
    the name `'label'`. Also, if the output of our `featuresCreator` was not called
    `'features',` we would have to specify the `featuresCol` by (most conveniently)
    calling the `getOutputCol()` method on the `featuresCreator` object.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的目标列名为`'label'`，我们就不必指定`labelCol`参数。此外，如果我们的`featuresCreator`的输出不是名为`'features'`，我们就必须通过（最方便的）在`featuresCreator`对象上调用`getOutputCol()`方法来指定`featuresCol`。
- en: Creating a pipeline
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建pipeline
- en: 'All that is left now is to create a `Pipeline` and fit the model. First, let''s
    load the `Pipeline` from the ML package:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的只是创建一个`Pipeline`并拟合模型。首先，让我们从ML包中加载`Pipeline`：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Creating a `Pipeline` is really easy. Here''s how our pipeline should look
    like conceptually:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`Pipeline`非常简单。以下是我们的pipeline在概念上的样子：
- en: '![Creating a pipeline](img/B05793_06_01.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![创建pipeline](img/B05793_06_01.jpg)'
- en: 'Converting this structure into a `Pipeline` is a *walk in the park*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个结构转换为`Pipeline`是一件轻而易举的事情：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That's it! Our `pipeline` is now created so we can (finally!) estimate the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们的`pipeline`现在已经创建好了，我们可以（终于！）估计模型了。
- en: Fitting the model
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型拟合
- en: 'Before you fit the model, we need to split our dataset into training and testing
    datasets. Conveniently, the DataFrame API has the `.randomSplit(...)` method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型之前，我们需要将我们的数据集分成训练集和测试集。方便的是，DataFrame API有`.randomSplit(...)`方法：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first parameter is a list of dataset proportions that should end up in,
    respectively, `births_train` and `births_test` subsets. The `seed` parameter provides
    a seed to the randomizer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是一个列表，其中包含应该分别进入`births_train`和`births_test`子集的数据集比例。`seed`参数为随机化器提供一个种子。
- en: Note
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can also split the dataset into more than two subsets as long as the elements
    of the list sum up to 1, and you unpack the output into as many subsets.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 只要列表的元素之和为1，你就可以将数据集分成超过两个子集，并将输出解包成尽可能多的子集。
- en: 'For example, we could split the births dataset into three subsets like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将出生数据集分成三个子集，如下所示：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code would put a random 70% of the births dataset into the `train`
    object, 20% would go to the `test`, and the `val` DataFrame would hold the remaining
    10%.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码会将出生数据集的70%随机放入`train`对象中，20%会进入`test`，而`val` DataFrame将保留剩余的10%。
- en: 'Now it is about time to finally run our pipeline and estimate our model:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候最终运行我们的流水线和估计我们的模型了：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `.fit(...)` method of the pipeline object takes our training dataset as
    an input. Under the hood, the `births_train` dataset is passed first to the `encoder`
    object. The DataFrame that is created at the `encoder` stage then gets passed
    to the `featuresCreator` that creates the `'features'` column. Finally, the output
    from this stage is passed to the `logistic` object that estimates the final model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线对象的`.fit(...)`方法将我们的训练数据集作为输入。在内部，`births_train`数据集首先传递给`encoder`对象。在`encoder`阶段创建的DataFrame随后传递给`featuresCreator`，它创建`'features'`列。最后，这一阶段的输出传递给`logistic`对象，它估计最终模型。
- en: 'The `.fit(...)` method returns the `PipelineModel` object (the `model` object
    in the preceding snippet) that can then be used for prediction; we attain this
    by calling the `.transform(...)` method and passing the testing dataset created
    earlier. Here''s what the `test_model` looks like in the following command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`.fit(...)`方法返回`PipelineModel`对象（前面代码片段中的`model`对象），然后可以用于预测；我们通过调用`.transform(...)`方法并传递之前创建的测试数据集来实现这一点。以下命令中的`test_model`看起来是这样的：'
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It generates the following output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以下输出：
- en: '![Fitting the model](img/B05793_06_02.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![拟合模型](img/B05793_06_02.jpg)'
- en: 'As you can see, we get all the columns from the `Transfomers` and `Estimators`.
    The logistic regression model outputs several columns: the `rawPrediction` is
    the value of the linear combination of features and the β coefficients, the `probability`
    is the calculated probability for each of the classes, and finally, the `prediction`
    is our final class assignment.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们得到了来自`Transfomers`和`Estimators`的所有列。逻辑回归模型输出几个列：`rawPrediction`是特征和β系数的线性组合的值，`probability`是每个类计算的概率，最后是`prediction`，即我们的最终类别分配。
- en: Evaluating the performance of the model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型的性能
- en: 'Obviously, we would like to now test how well our model did. PySpark exposes
    a number of evaluation methods for classification and regression in the `.evaluation`
    section of the package:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们现在想测试我们的模型表现如何。PySpark在包的`.evaluation`部分暴露了多个用于分类和回归的评估方法：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will use the `BinaryClassficationEvaluator` to test how well our model performed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`BinaryClassficationEvaluator`来测试我们的模型表现如何：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `rawPredictionCol` can either be the `rawPrediction` column produced by
    the estimator or the `probability`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`rawPredictionCol`可以是估计器生成的`rawPrediction`列或`probability`。'
- en: 'Let''s see how well our model performed:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的模型表现如何：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code produces the following result:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下结果：
- en: '![Evaluating the performance of the model](img/B05793_06_03.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型的性能](img/B05793_06_03.jpg)'
- en: The area under the ROC of 74% and area under PR of 71% shows a well-defined
    model, but nothing out of extraordinary; if we had other features, we could drive
    this up, but this is not the purpose of this chapter (nor the book, for that matter).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线下74%的面积和PR曲线下71%的面积显示了一个定义良好的模型，但并没有什么异常之处；如果我们有其他特征，我们可以进一步提高这个值，但这不是本章（甚至整本书）的目的。
- en: Saving the model
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型
- en: 'PySpark allows you to save the `Pipeline` definition for later use. It not
    only saves the pipeline structure, but also all the definitions of all the `Transformers`
    and `Estimators`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark允许你保存`Pipeline`定义以供以后使用。它不仅保存了流水线结构，还保存了所有`Transformers`和`Estimators`的定义：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So, you can load it up later and use it straight away to `.fit(...)` and predict:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以稍后加载它并直接使用它来`.fit(...)`和预测：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produces the same result (as expected):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了相同的结果（正如预期）：
- en: '![Saving the model](img/B05793_06_04.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![保存模型](img/B05793_06_04.jpg)'
- en: If you, however, want to save the estimated model, you can also do that; instead
    of saving the `Pipeline`, you need to save the `PipelineModel`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想要保存估计的模型，你也可以这样做；你不需要保存`Pipeline`，而是需要保存`PipelineModel`。
- en: Tip
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Note, that not only the `PipelineModel` can be saved: virtually all the models
    that are returned by calling the `.fit(...)` method on an `Estimator` or `Transformer`
    can be saved and loaded back to be reused.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，不仅`PipelineModel`可以被保存：几乎所有通过在`Estimator`或`Transformer`上调用`.fit(...)`方法返回的模型都可以被保存并重新加载以供重用。
- en: 'To save your model, see the following the example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存你的模型，请参考以下示例：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding script uses the `.load(...)` method, a class method of the `PipelineModel`
    class, to reload the estimated model. You can compare the result of `test_reloadedModel.take(1)`
    with the output of `test_model.take(1)` we presented earlier.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的脚本使用了`PipelineModel`类的类方法`.load(...)`来重新加载估计的模型。你可以将`test_reloadedModel.take(1)`的结果与之前展示的`test_model.take(1)`的输出进行比较。
- en: Parameter hyper-tuning
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数超调
- en: Rarely, our first model would be the best we can do. By simply looking at our
    metrics and accepting the model because it passed our pre-conceived performance
    thresholds is hardly a scientific method for finding the best model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 很少情况下，我们的第一个模型就是我们能做的最好的。仅仅通过查看我们的指标并接受模型因为它通过了我们预想的性能阈值，这几乎不是寻找最佳模型的科学方法。
- en: 'A concept of parameter hyper-tuning is to find the best parameters of the model:
    for example, the maximum number of iterations needed to properly estimate the
    logistic regression model or maximum depth of a decision tree.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 参数超调的概念是找到模型的最佳参数：例如，正确估计逻辑回归模型所需的最大迭代次数或决策树的最大深度。
- en: 'In this section, we will explore two concepts that allow us to find the best
    parameters for our models: grid search and train-validation splitting.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两个概念，这些概念可以帮助我们找到模型的最佳参数：网格搜索和训练-验证拆分。
- en: Grid search
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Grid search is an exhaustive algorithm that loops through the list of defined
    parameter values, estimates separate models, and chooses the best one given some
    evaluation metric.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一个穷举算法，它遍历定义的参数值列表，估计单独的模型，并根据某些评估指标选择最佳模型。
- en: 'A note of caution should be stated here: if you define too many parameters
    you want to optimize over, or too many values of these parameters, it might take
    a lot of time to select the best model as the number of models to estimate would
    grow very quickly as the number of parameters and parameter values grow.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出的是：如果你定义了太多的参数想要优化，或者这些参数的值太多，选择最佳模型可能需要很长时间，因为随着参数和参数值的增加，估计的模型数量会迅速增长。
- en: 'For example, if you want to fine-tune two parameters with two parameter values,
    you would have to fit four models. Adding one more parameter with two values would
    require estimating eight models, whereas adding one more additional value to our
    two parameters (bringing it to three values for each) would require estimating
    nine models. As you can see, this can quickly get out of hand if you are not careful.
    See the following chart to inspect this visually:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想要微调两个参数，并且每个参数有两个值，那么你需要拟合四个模型。增加一个额外的参数并赋予两个值，将需要估计八个模型，而将我们的两个参数增加一个额外的值（使每个参数变为三个值），将需要估计九个模型。正如你所见，如果不小心，这会迅速变得难以控制。请查看以下图表以直观地检查这一点：
- en: '![Grid search](img/B05793_06_05.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![网格搜索](img/B05793_06_05.jpg)'
- en: 'After this cautionary tale, let''s get to fine-tuning our parameters space.
    First, we load the `.tuning` part of the package:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个警示故事之后，让我们开始微调我们的参数空间。首先，我们加载包的`.tuning`部分：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let''s specify our model and the list of parameters we want to loop through:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们指定我们的模型和想要遍历的参数列表：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, we specify the model we want to optimize the parameters of. Next, we
    decide which parameters we will be optimizing, and what values for those parameters
    to test. We use the `ParamGridBuilder()` object from the `.tuning` subpackage,
    and keep adding the parameters to the grid with the `.addGrid(...)` method: the
    first parameter is the parameter object of the model we want to optimize (in our
    case, these are `logistic.maxIter` and `logistic.regParam`), and the second parameter
    is a list of values we want to loop through. Calling the `.build()` method on
    the `.ParamGridBuilder` builds the grid.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定我们想要优化参数的模型。接下来，我们决定我们将优化哪些参数，以及测试这些参数的哪些值。我们使用`.tuning`子包中的`ParamGridBuilder()`对象，并使用`.addGrid(...)`方法向网格中添加参数：第一个参数是我们想要优化的模型的参数对象（在我们的例子中，这些是`logistic.maxIter`和`logistic.regParam`），第二个参数是我们想要遍历的值的列表。在`.ParamGridBuilder`上调用`.build()`方法将构建网格。
- en: 'Next, we need some way of comparing the models:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种比较模型的方法：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So, once again, we''ll use the `BinaryClassificationEvaluator`. It is time
    now to create the logic that will do the validation work for us:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们再次使用 `BinaryClassificationEvaluator`。现在是时候创建一个逻辑，为我们执行验证工作：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `CrossValidator` needs the `estimator`, the `estimatorParamMaps`, and the
    `evaluator` to do its job. The model loops through the grid of values, estimates
    the models, and compares their performance using the `evaluator`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrossValidator` 需要估计器、估计器参数映射和评估器来完成其工作。模型遍历值网格，估计模型，并使用评估器比较它们的性能。'
- en: 'We cannot use the data straight away (as the `births_train` and `births_test`
    still have the `BIRTHS_PLACE` column not encoded) so we create a purely transforming
    `Pipeline`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接使用数据（因为 `births_train` 和 `births_test` 仍然有未编码的 `BIRTHS_PLACE` 列），因此我们创建了一个纯转换的
    `Pipeline`：
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Having done this, we are ready to find the optimal combination of parameters
    for our model:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们就可以找到我们模型的最佳参数组合：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `cvModel` will return the best model estimated. We can now use it to see
    if it performed better than our previous model:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`cvModel` 将返回最佳估计模型。现在我们可以使用它来查看它是否比我们之前的模型表现更好：'
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding code will produce the following result:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码将产生以下结果：
- en: '![Grid search](img/B05793_06_06.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![网格搜索](img/B05793_06_06.jpg)'
- en: 'As you can see, we got a slightly better result. What parameters does the best
    model have? The answer is a little bit convoluted, but here''s how you can extract
    it:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们得到了一个稍微好一点的结果。最佳模型有哪些参数？答案是有点复杂，但以下是您可以提取它的方法：
- en: '[PRE29]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code produces the following output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码产生以下输出：
- en: '![Grid search](img/B05793_06_07.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![网格搜索](img/B05793_06_07.jpg)'
- en: Train-validation splitting
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练-验证分割
- en: 'The `TrainValidationSplit` model, to select the best model, performs a random
    split of the input dataset (the training dataset) into two subsets: smaller training
    and validation subsets. The split is only performed once.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainValidationSplit` 模型，为了选择最佳模型，将输入数据集（训练数据集）随机分割成两个子集：较小的训练集和验证集。分割只进行一次。'
- en: 'In this example, we will also use the `ChiSqSelector` to select only the top
    five features, thus limiting the complexity of our model:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们还将使用 `ChiSqSelector` 来选择前五个特征，从而限制我们模型的复杂性：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `numTopFeatures` specifies the number of features to return. We will put
    the selector after the `featuresCreator`, so we call the `.getOutputCol()` on
    the `featuresCreator`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`numTopFeatures` 指定了要返回的特征数量。我们将选择器放在 `featuresCreator` 之后，因此我们在 `featuresCreator`
    上调用 `.getOutputCol()`。'
- en: 'We covered creating the `LogisticRegression` and `Pipeline` earlier, so we
    will not explain how these are created again here:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经介绍了如何创建 `LogisticRegression` 和 `Pipeline`，因此我们在这里不再解释它们的创建方法：
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `TrainValidationSplit` object gets created in the same fashion as the `CrossValidator`
    model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainValidationSplit` 对象的创建方式与 `CrossValidator` 模型相同：'
- en: '[PRE32]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As before, we fit our data to the model, and calculate the results:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将数据拟合到模型中，并计算结果：
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code prints out the following output:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码输出了以下内容：
- en: '![Train-validation splitting](img/B05793_06_08.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![训练-验证分割](img/B05793_06_08.jpg)'
- en: Well, the model with less features certainly performed worse than the full model,
    but the difference was not that great. Ultimately, it is a performance trade-off
    between a more complex model and the less sophisticated one.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，具有较少特征的模型肯定比完整模型表现差，但差距并不大。最终，这是一个在更复杂的模型和不太复杂的模型之间的性能权衡。
- en: Other features of PySpark ML in action
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark ML 的其他功能实战
- en: At the beginning of this chapter, we described most of the features of the PySpark
    ML library. In this section, we will provide examples of how to use some of the
    `Transformers` and `Estimators`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们描述了 PySpark ML 库的大部分功能。在本节中，我们将提供一些使用 `Transformers` 和 `Estimators`
    的示例。
- en: Feature extraction
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: We have used quite a few models from this submodule of PySpark. In this section,
    we'll show you how to use the most useful ones (in our opinion).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了 PySpark 子模块中相当多的模型。在本节中，我们将向您展示如何使用我们认为最有用的模型。
- en: NLP - related feature extractors
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLP 相关的特征提取器
- en: As described earlier, the `NGram` model takes a list of tokenized text and produces
    pairs (or n-grams) of words.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`NGram` 模型接受一个分词文本的列表，并生成单词对（或 n-gram）。
- en: 'In this example, we will take an excerpt from PySpark''s documentation and
    present how to clean up the text before passing it to the `NGram` model. Here''s
    how our dataset looks like (abbreviated for brevity):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将从PySpark的文档中摘取一段内容，展示在传递给`NGram`模型之前如何清理文本。以下是我们的数据集的样子（为了简洁而省略）：
- en: Tip
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For the full view of how the following snippet looks like, please download
    the code from our GitHub repository: [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看以下代码片段的完整视图，请从我们的GitHub仓库下载代码：[https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark)。
- en: 'We copied these four paragraphs from the description of the DataFrame usage
    in `Pipelines`: [http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe](http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`Pipelines`中DataFrame使用的描述中复制了这四个段落：[http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe](http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe)。
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Each row in our single-column DataFrame is just a bunch of text. First, we
    need to tokenize this text. To do so we will use the `RegexTokenizer` instead
    of just the `Tokenizer` as we can specify the pattern(s) we want the text to be
    broken at:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们单列DataFrame中的每一行只是一堆文本。首先，我们需要对这段文本进行分词。为此，我们将使用`RegexTokenizer`而不是仅仅使用`Tokenizer`，因为我们可以指定我们想要文本在何处被分割的模式：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The pattern here splits the text on any number of spaces, but also removes
    commas, full stops, backslashes, and quotation marks. A single row from the output
    of the `tokenizer` looks similar to this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的模式在任意数量的空格处分割文本，同时也移除了逗号、句号、反斜杠和引号。`tokenizer`输出中的一个单行看起来类似于这样：
- en: '![NLP - related feature extractors](img/B05793_06_09.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![NLP - 相关特征提取器](img/B05793_06_09.jpg)'
- en: As you can see, the `RegexTokenizer` not only splits the sentences in to words,
    but also normalizes the text so each word is in small-caps.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`RegexTokenizer`不仅将句子分割成单词，而且还对文本进行了规范化，使得每个单词都是小写。
- en: 'However, there is still plenty of junk in our text: words such as `be`, `a`,
    or `to` normally provide us with nothing useful when analyzing a text. Thus, we
    will remove these so called `stopwords` using nothing else other than the `StopWordsRemover(...)`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的文本中仍然有很多垃圾信息：例如`be`、`a`或`to`等单词在分析文本时通常不会提供任何有用的信息。因此，我们将使用`StopWordsRemover(...)`移除这些所谓的`停用词`：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of the method looks as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的输出如下所示：
- en: '![NLP - related feature extractors](img/B05793_06_10.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![NLP - 相关特征提取器](img/B05793_06_10.jpg)'
- en: 'Now we only have the useful words. So, let''s build our `NGram` model and the
    `Pipeline`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只有有用的单词。所以，让我们构建我们的`NGram`模型和`Pipeline`：
- en: '[PRE37]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that we have the `pipeline`, we follow in a very similar fashion as before:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`pipeline`，我们将按照之前非常相似的方式继续：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code produces the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![NLP - related feature extractors](img/B05793_06_11.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![NLP - 相关特征提取器](img/B05793_06_11.jpg)'
- en: That's it. We have got our n-grams and we can now use them in further NLP processing.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经得到了我们的n-gram，现在我们可以将它们用于进一步的NLP处理。
- en: Discretizing continuous variables
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离散化连续变量
- en: Ever so often, we deal with a continuous feature that is highly non-linear and
    really hard to fit in our model with only one coefficient.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常处理一个高度非线性且很难只用一个系数来拟合模型中的连续特征。
- en: In such a situation, it might be hard to explain the relationship between such
    a feature and the target with just one coefficient. Sometimes, it is useful to
    band the values into discrete buckets.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可能很难只用一个系数来解释这种特征与目标之间的关系。有时，将值分组到离散的桶中是有用的。
- en: 'First, let''s create some fake data with the help of the following code:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下代码创建一些假数据：
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can create a DataFrame by using the following code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下代码创建一个DataFrame：
- en: '[PRE40]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Discretizing continuous variables](img/B05793_06_12.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![离散化连续变量](img/B05793_06_12.jpg)'
- en: 'Next, we will use the `QuantileDiscretizer` model to split our continuous variable
    into five buckets (the `numBuckets` parameter):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`QuantileDiscretizer`模型将我们的连续变量分割成五个桶（`numBuckets`参数）：
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s see what we have got:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE42]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Our function now looks as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能现在看起来如下：
- en: '![Discretizing continuous variables](img/B05793_06_13.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![离散化连续变量](img/B05793_06_13.jpg)'
- en: We can now treat this variable as categorical and use the `OneHotEncoder` to
    encode it for future use.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这个变量视为分类变量，并使用`OneHotEncoder`对其进行编码以供将来使用。
- en: Standardizing continuous variables
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化连续变量
- en: Standardizing continuous variables helps not only in better understanding the
    relationships between the features (as interpreting the coefficients becomes easier),
    but it also aids computational efficiency and protects from running into some
    numerical traps. Here's how you do it with PySpark ML.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化连续变量不仅有助于更好地理解特征之间的关系（因为解释系数变得更容易），而且还有助于计算效率，并防止遇到一些数值陷阱。这是使用PySpark ML如何做到这一点。
- en: 'First, we need to create a vector representation of our continuous variable
    (as it is only a single float):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建我们连续变量的向量表示（因为它只是一个单一的浮点数）：
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we build our `normalizer` and the `pipeline`. By setting the `withMean`
    and `withStd` to `True`, the method will remove the mean and scale the variance
    to be of unit length:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建我们的`normalizer`和`pipeline`。通过将`withMean`和`withStd`设置为`True`，该方法将移除均值并将方差缩放到单位长度：
- en: '[PRE44]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here''s what the transformed data would look like:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这是转换后的数据看起来会是什么样子：
- en: '![Standardizing continuous variables](img/B05793_06_14.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![标准化连续变量](img/B05793_06_14.jpg)'
- en: As you can see, the data now oscillates around 0 with the unit variance (the
    green line).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据现在围绕0振荡，具有单位方差（绿色线）。
- en: Classification
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: So far we have only used the `LogisticRegression` model from PySpark ML. In
    this section, we will use the `RandomForestClassfier` to, once again, model the
    chances of survival for an infant.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了PySpark ML中的`LogisticRegression`模型。在本节中，我们将使用`RandomForestClassfier`再次对婴儿生存的机会进行建模。
- en: 'Before we can do that, though, we need to cast the `label` feature to `DoubleType`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够做到这一点之前，我们需要将`label`特征转换为`DoubleType`：
- en: '[PRE45]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now that we have the label converted to double, we are ready to build our model.
    We progress in a similar fashion as before with the distinction that we will reuse
    the `encoder` and `featureCreator` from earlier in the chapter. The `numTrees`
    parameter specifies how many decision trees should be in our random forest, and
    the `maxDepth` parameter limits the depth of the trees:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将标签转换为双精度，我们准备好构建我们的模型。我们以类似的方式前进，区别在于我们将重用本章早些时候的`encoder`和`featureCreator`。`numTrees`参数指定我们的随机森林中应该有多少决策树，而`maxDepth`参数限制了树的深度：
- en: '[PRE46]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s now see how the `RandomForestClassifier` model performs compared to
    the `LogisticRegression`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看`RandomForestClassifier`模型与`LogisticRegression`相比的表现：
- en: '[PRE47]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We get the following results:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![Classification](img/B05793_06_15.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![分类](img/B05793_06_15.jpg)'
- en: 'Well, as you can see, the results are better than the logistic regression model
    by roughly 3 percentage points. Let''s test how well would a model with one tree
    do:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，正如你所见，结果比逻辑回归模型好大约3个百分点。让我们测试一下只有一个树的模型表现如何：
- en: '[PRE48]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code gives us the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给出了以下结果：
- en: '![Classification](img/B05793_06_16.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![分类](img/B05793_06_16.jpg)'
- en: Not bad at all! It actually performed better than the random forest model in
    terms of the precision-recall relationship and only slightly worse in terms of
    the area under the ROC. We just might have found a winner!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表现相当不错！实际上，它在精确度-召回率关系方面表现得比随机森林模型更好，只是在ROC曲线下的面积上略差一些。我们可能已经找到了一个赢家！
- en: Clustering
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Clustering is another big part of machine learning: quite often, in the real
    world, we do not have the luxury of having the target feature, so we need to revert
    to an unsupervised learning paradigm, where we try to uncover patterns in the
    data.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是机器学习的一个重要部分：在现实世界中，我们往往没有目标特征的奢侈，因此我们需要回到无监督学习范式，试图在数据中揭示模式。
- en: Finding clusters in the births dataset
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在出生数据集中寻找聚类
- en: 'In this example, we will use the `k-means` model to find similarities in the
    births data:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`k-means`模型来寻找出生数据中的相似性：
- en: '[PRE49]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Having estimated the model, let''s see if we can find some differences between
    clusters:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在估计了模型之后，让我们看看我们是否能在聚类之间找到一些差异：
- en: '[PRE50]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code produces the following output:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生了以下输出：
- en: '![Finding clusters in the births dataset](img/B05793_06_17.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![在出生数据集中寻找聚类](img/B05793_06_17.jpg)'
- en: Well, the `MOTHER_HEIGHT_IN` is significantly different in cluster 2\. Going
    through the results (which we will not do here for obvious reasons) would most
    likely uncover more differences and allow us to understand the data better.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，`MOTHER_HEIGHT_IN`在聚类2中显著不同。通过查看结果（这里我们不会做，很明显的原因）可能会发现更多差异，并帮助我们更好地理解数据。
- en: Topic mining
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题挖掘
- en: Clustering models are not limited to numeric data only. In the field of NLP,
    problems such as topic extraction rely on clustering to detect documents with
    similar topics. We will go through such an example.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类模型不仅限于数值数据。在自然语言处理（NLP）领域，如主题提取等问题依赖于聚类来检测具有相似主题的文档。我们将通过一个这样的例子来讲解。
- en: 'First, let''s create our dataset. The data is formed from randomly selected
    paragraphs found on the Internet: three of them deal with topics of nature and
    national parks, the remaining three cover technology.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建我们的数据集。数据是从互联网上随机选择的段落中形成的：其中三个涉及自然和国家公园的主题，剩下的三个涉及技术。
- en: Tip
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The code snippet is abbreviated again, for obvious reasons. Refer to the source
    file on GitHub for full representation.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于显而易见的原因，代码片段再次被简化。请参考GitHub上的源文件以获取完整表示。
- en: '[PRE51]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'First, we will once again use the `RegexTokenizer` and the `StopWordsRemover`
    models:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们再次使用`RegexTokenizer`和`StopWordsRemover`模型：
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next in our pipeline is the `CountVectorizer`: a model that counts words in
    a document and returns a vector of counts. The length of the vector is equal to
    the total number of distinct words in all the documents, which can be seen in
    the following snippet:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的管道中接下来的是`CountVectorizer`：这是一个计算文档中单词并返回计数向量的模型。向量的长度等于所有文档中不同单词的总数，这在以下代码片段中可以看到：
- en: '[PRE53]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code will produce the following output:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下输出：
- en: '![Topic mining](img/B05793_06_18.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![主题挖掘](img/B05793_06_18.jpg)'
- en: As you can see, there are 262 distinct words in the text, and each document
    is now represented by a count of each word occurrence.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，文本中有262个不同的单词，现在每个文档都由每个单词出现的计数来表示。
- en: 'It''s now time to start predicting the topics. For that purpose we will use
    the `LDA` model—the **Latent Dirichlet Allocation** model:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始预测主题了。为此，我们将使用`LDA`模型——**潜在狄利克雷分配**模型：
- en: '[PRE54]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The `k` parameter specifies how many topics we expect to see, the `optimizer`
    parameter can be either `'online'` or `'em'` (the latter standing for the Expectation
    Maximization algorithm).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`k`参数指定了我们期望看到多少个主题，`optimizer`参数可以是`''online''`或`''em''`（后者代表期望最大化算法）。'
- en: 'Putting these puzzles together results in, so far, the longest of our pipelines:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些难题组合起来，到目前为止，是我们的管道中最长的：
- en: '[PRE55]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Have we properly uncovered the topics? Well, let''s see:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否正确地揭示了主题？好吧，让我们看看：
- en: '[PRE56]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here''s what we get:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![Topic mining](img/B05793_06_19.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![主题挖掘](img/B05793_06_19.jpg)'
- en: 'Looks like our method discovered all the topics properly! Do not get used to
    seeing such good results though: sadly, real world data is seldom that kind.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的方法正确地发现了所有主题！但是，不要习惯看到这样的好结果：遗憾的是，现实世界的数据很少是这样的。
- en: Regression
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: We could not finish a chapter on a machine learning library without building
    a regression model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在介绍机器学习库的章节中，如果不能构建一个回归模型，就无法完成。
- en: 'In this section, we will try to predict the `MOTHER_WEIGHT_GAIN` given some
    of the features described here; these are contained in the features listed here:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试预测`MOTHER_WEIGHT_GAIN`，给定这里描述的一些特征；这些特征包含在以下列出的特征中：
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'First, since all the features are numeric, we will collate them together and
    use the `ChiSqSelector` to select only the top six most important features:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于所有特征都是数值的，我们将它们收集在一起，并使用`ChiSqSelector`来选择仅前六个最重要的特征：
- en: '[PRE58]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In order to predict the weight gain, we will use the gradient boosted trees
    regressor:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测体重增加，我们将使用梯度提升树回归器：
- en: '[PRE59]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, again, we put it all together into a `Pipeline`:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次，我们将所有这些组合到一个`Pipeline`中：
- en: '[PRE60]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Having created the `weightGain` model, let''s see if it performs well on our
    testing data:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了`weightGain`模型之后，让我们看看它在测试数据上的表现如何：
- en: '[PRE61]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We get the following output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Regression](img/B05793_06_20.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![回归](img/B05793_06_20.jpg)'
- en: Sadly, the model is no better than a flip of a coin. It looks that without additional
    independent features that are better correlated with the `MOTHER_WEIGHT_GAIN`
    label, we will not be able to explain its variance sufficiently.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，模型的表现并不比掷硬币好。看起来，如果没有与`MOTHER_WEIGHT_GAIN`标签更好相关联的附加独立特征，我们将无法充分解释其变异性。
- en: Summary
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we went into details of how to use PySpark ML: the official
    main machine learning library for PySpark. We explained what the `Transformer`
    and `Estimator` are, and showed their role in another concept introduced in the
    ML library: the `Pipeline`. Subsequently, we also presented how to use some of
    the methods to fine-tune the hyper parameters of models. Finally, we gave some
    examples of how to use some of the feature extractors and models from the library.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了如何使用 PySpark ML：PySpark 的官方主要机器学习库。我们解释了 `Transformer` 和 `Estimator`
    是什么，并展示了它们在 ML 库中引入的另一个概念：`Pipeline` 中的作用。随后，我们还介绍了如何使用一些方法来微调模型的超参数。最后，我们给出了一些如何使用库中的一些特征提取器和模型的示例。
- en: In the next chapter, we will delve into graph theory and GraphFrames that help
    in tackling machine learning problems better represented as graphs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨图论和 GraphFrames，这些工具有助于更好地以图的形式表示机器学习问题。
