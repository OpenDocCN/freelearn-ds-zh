- en: Chapter 6. Introducing the ML Package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we worked with the MLlib package in Spark that operated
    strictly on RDDs. In this chapter, we move to the ML part of Spark that operates
    strictly on DataFrames. Also, according to the Spark documentation, the primary
    machine learning API for Spark is now the DataFrame-based set of models contained
    in the `spark.ml` package.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's get to it!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we will reuse a portion of the dataset we played within the
    previous chapter. The data can be downloaded from [http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare transformers, estimators, and pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the chances of infant survival using models available in the ML package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform parameter hyper-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use other machine-learning models available in the package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the top level, the package exposes three main abstract classes: a `Transformer`,
    an `Estimator`, and a `Pipeline`. We will shortly explain each with some short
    examples. We will provide more concrete examples of some of the models in the
    last section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Transformer` class, like the name suggests, *transforms* your data by (normally)
    appending a new column to your DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the high level, when deriving from the `Transformer` abstract class, each
    and every new `Transformer` needs to implement a `.transform(...)` method. The
    method, as a first and normally the only obligatory parameter, requires passing
    a DataFrame to be transformed. This, of course, varies *method-by-method* in the
    ML package: other *popular* parameters are `inputCol` and `outputCol`; these,
    however, frequently default to some predefined values, such as, for example, `''features''`
    for the `inputCol` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many `Transformers` offered in the `spark.ml.feature` and we will
    briefly describe them here (before we use some of them later in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Binarizer`: Given a threshold, the method takes a continuous variable and
    transforms it into a binary one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Bucketizer`: Similar to the `Binarizer`, this method takes a list of thresholds
    (the `splits` parameter) and transforms a continuous variable into a multinomial
    one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChiSqSelector`: For the categorical target variables (think classification
    models), this feature allows you to select a predefined number of features (parameterized
    by the `numTopFeatures` parameter) that explain the variance in the target the
    best. The selection is done, as the name of the method suggests, using a Chi-Square
    test. It is one of the two-step methods: first, you need to `.fit(...)` your data
    (so the method can calculate the Chi-square tests). Calling the `.fit(...)` method
    (you pass your DataFrame as a parameter) returns a `ChiSqSelectorModel` object
    that you can then use to transform your DataFrame using the `.transform(...)`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on Chi-squares can be found here: [http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html](http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`CountVectorizer`: This is useful for a tokenized text (such as `[[''Learning'',
    ''PySpark'', ''with'', ''us''],[''us'', ''us'', ''us'']]`). It is one of two-step
    methods: first, you need to `.fit(...)`, that is, learn the patterns from your
    dataset, before you can `.transform(...)` with the `CountVectorizerModel` returned
    by the `.fit(...)` method. The output from this transformer, for the tokenized
    text presented previously, would look similar to this: `[(4, [0, 1, 2, 3], [1.0,
    1.0, 1.0, 1.0]),(4, [3], [3.0])]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DCT`: The Discrete Cosine Transform takes a vector of real values and returns
    a vector of the same length, but with the sum of cosine functions oscillating
    at different frequencies. Such transformations are useful to extract some underlying
    frequencies in your data or in data compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ElementwiseProduct`: A method that returns a vector with elements that are
    products of the vector passed to the method, and a vector passed as the `scalingVec`
    parameter. For example, if you had a `[10.0, 3.0, 15.0]` vector and your `scalingVec`
    was `[0.99, 3.30, 0.66]`, then the vector you would get would look as follows:
    `[9.9, 9.9, 9.9]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HashingTF`: A hashing trick transformer that takes a list of tokenized text
    and returns a vector (of predefined length) with counts. From PySpark''s documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Since a simple modulo is used to transform the hash function to a column index,
    it is advisable to use a power of two as the numFeatures parameter; otherwise
    the features will not be mapped evenly to the columns."'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`IDF`: This method computes an **Inverse Document Frequency** for a list of
    documents. Note that the documents need to already be represented as a vector
    (for example, using either the `HashingTF` or `CountVectorizer`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndexToString`: A complement to the `StringIndexer` method. It uses the encoding
    from the `StringIndexerModel` object to reverse the string index to original values.
    As an aside, please note that this sometimes does not work and you need to specify
    the values from the `StringIndexer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxAbsScaler`: Rescales the data to be within the `[-1.0, 1.0]` range (thus,
    it does not shift the center of the data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MinMaxScaler`: This is similar to the `MaxAbsScaler` with the difference that
    it scales the data to be in the `[0.0, 1.0]` range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NGram`: This method takes a list of tokenized text and returns *n-grams*:
    pairs, triples, or *n-mores* of subsequent words. For example, if you had a `[''good'',
    ''morning'', ''Robin'', ''Williams'']` vector you would get the following output:
    `[''good morning'', ''morning Robin'', ''Robin Williams'']`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Normalizer`: This method scales the data to be of unit norm using the p-norm
    value (by default, it is L2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OneHotEncoder`: This method encodes a categorical column to a column of binary
    vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCA`: Performs the data reduction using principal component analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PolynomialExpansion`: Performs a polynomial expansion of a vector. For example,
    if you had a vector symbolically written as `[x, y, z]`, the method would produce
    the following expansion: `[x, x*x, y, x*y, y*y, z, x*z, y*z, z*z]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QuantileDiscretizer`: Similar to the `Bucketizer` method, but instead of passing
    the splits parameter, you pass the `numBuckets` one. The method then decides,
    by calculating approximate quantiles over your data, what the splits should be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegexTokenizer`: This is a string tokenizer using regular expressions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RFormula`: For those of you who are avid R users, you can pass a formula such
    as `vec ~ alpha * 3 + beta` (assuming your `DataFrame` has the `alpha` and `beta`
    columns) and it will produce the `vec` column given the expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SQLTransformer`: Similar to the previous, but instead of R-like formulas,
    you can use SQL syntax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `FROM` statement should be selecting from `__THIS__`, indicating you are
    accessing the DataFrame. For example: `SELECT alpha * 3 + beta AS vec FROM __THIS__`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`StandardScaler`: Standardizes the column to have a 0 mean and standard deviation
    equal to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopWordsRemover`: Removes stop words (such as `''the''` or `''a''`) from
    a tokenized text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StringIndexer`: Given a list of all the words in a column, this will produce
    a vector of indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tokenizer`: This is the default tokenizer that converts the string to lower
    case and then splits on space(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorAssembler`: This is a highly useful transformer that collates multiple
    numeric (vectors included) columns into a single column with a vector representation.
    For example, if you had three columns in your DataFrame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of calling:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It would look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`VectorIndexer`: This is a method for indexing categorical columns into a vector
    of indices. It works in a *column-by-column* fashion, selecting distinct values
    from the column, sorting and returning an index of the value from the map instead
    of the original value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorSlicer`: Works on a feature vector, either dense or sparse: given a
    list of indices, it extracts the values from the feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word2Vec`: This method takes a sentence (string) as an input and transforms
    it into a map of `{string, vector}` format, a representation that is useful in
    natural language processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that there are many methods in the ML package that have an E letter next
    to it; this means the method is currently in beta (or Experimental) and it sometimes
    might fail or produce erroneous results. Beware.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Estimators can be thought of as statistical models that need to be estimated
    to make predictions or classify your observations.
  prefs: []
  type: TYPE_NORMAL
- en: If deriving from the abstract `Estimator` class, the new model has to implement
    the `.fit(...)` method that fits the model given the data found in a DataFrame
    and some default or user-specified parameters.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of estimators available in PySpark and we will now shortly describe
    the models available in Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ML package provides a data scientist with seven classification models to
    choose from. These range from the simplest ones (such as logistic regression)
    to more sophisticated ones. We will provide short descriptions of each of them
    in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegression`: The benchmark model for classification. The logistic
    regression uses a logit function to calculate the probability of an observation
    belonging to a particular class. At the time of writing, the PySpark ML supports
    only binary classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeClassifier`: A classifier that builds a decision tree to predict
    a class for an observation. Specifying the `maxDepth` parameter limits the depth
    the tree grows, the `minInstancePerNode` determines the minimum number of observations
    in the tree node required to further split, the `maxBins` parameter specifies
    the maximum number of bins the continuous variables will be split into, and the
    `impurity` specifies the metric to measure and calculate the information gain
    from the split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GBTClassifier`: A **Gradient Boosted Trees** model for classification. The
    model belongs to the family of ensemble models: models that combine multiple weak
    predictive models to form a strong one. At the moment, the `GBTClassifier` model
    supports binary labels, and continuous and categorical features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestClassifier`: This model produces multiple decision trees (hence
    the name—forest) and uses the `mode` output of those decision trees to classify
    observations. The `RandomForestClassifier` supports both binary and multinomial
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NaiveBayes`: Based on the Bayes'' theorem, this model uses conditional probability
    theory to classify observations. The `NaiveBayes` model in PySpark ML supports
    both binary and multinomial labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultilayerPerceptronClassifier`: A classifier that mimics the nature of a
    human brain. Deeply rooted in the Artificial Neural Networks theory, the model
    is a black-box, that is, it is not easy to interpret the internal parameters of
    the model. The model consists, at a minimum, of three, fully connected `layers`
    (a parameter that needs to be specified when creating the model object) of artificial
    neurons: the input layer (that needs to be equal to the number of features in
    your dataset), a number of hidden layers (at least one), and an output layer with
    the number of neurons equal to the number of categories in your label. All the
    neurons in the input and hidden layers have a sigmoid activation function, whereas
    the activation function of the neurons in the output layer is softmax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OneVsRest`: A reduction of a multiclass classification to a binary one. For
    example, in the case of a multinomial label, the model can train multiple binary
    logistic regression models. For example, if `label == 2`, the model will build
    a logistic regression where it will convert the `label == 2` to `1` (all remaining
    label values would be set to `0`) and then train a binary model. All the models
    are then scored and the model with the highest probability wins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are seven models available for regression tasks in the PySpark ML package.
    As with classification, these range from some basic ones (such as the obligatory
    linear regression) to more complex ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AFTSurvivalRegression`: Fits an Accelerated Failure Time regression model.
    It is a parametric model that assumes that a marginal effect of one of the features
    accelerates or decelerates a life expectancy (or process failure). It is highly
    applicable for the processes with well-defined stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeRegressor`: Similar to the model for classification with an obvious
    distinction that the label is continuous instead of binary (or multinomial).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GBTRegressor`: As with the `DecisionTreeRegressor`, the difference is the
    data type of the label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GeneralizedLinearRegression`: A family of linear models with differing kernel
    functions (link functions). In contrast to the linear regression that assumes
    normality of error terms, the GLM allows the label to have different error term
    distributions: the `GeneralizedLinearRegression` model from the PySpark ML package
    supports `gaussian`, `binomial`, `gamma`, and `poisson` families of error distributions
    with a host of different link functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IsotonicRegression`: A type of regression that fits a free-form, non-decreasing
    line to your data. It is useful to fit the datasets with ordered and increasing
    observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinearRegression`: The most simple of regression models, it assumes a linear
    relationship between features and a continuous label, and normality of error terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestRegressor`: Similar to either `DecisionTreeRegressor` or `GBTRegressor`,
    the `RandomForestRegressor` fits a continuous label instead of a discrete one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clustering is a family of unsupervised models that are used to find underlying
    patterns in your data. The PySpark ML package provides the four most popular models
    at the moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BisectingKMeans`: A combination of the k-means clustering method and hierarchical
    clustering. The algorithm begins with all observations in a single cluster and
    iteratively splits the data into `k` clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Check out this website for more information on pseudo-algorithms: [http://minethedata.blogspot.com/2012/08/bisecting-k-means.html](http://minethedata.blogspot.com/2012/08/bisecting-k-means.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`KMeans`: This is the famous k-mean algorithm that separates data into `k`
    clusters, iteratively searching for centroids that minimize the sum of square
    distances between each observation and the centroid of the cluster it belongs
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GaussianMixture`: This method uses `k` Gaussian distributions with unknown
    parameters to dissect the dataset. Using the Expectation-Maximization algorithm,
    the parameters for the Gaussians are found by maximizing the log-likelihood function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Beware that for datasets with many features this model might perform poorly
    due to the curse of dimensionality and numerical issues with Gaussian distributions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`LDA`: This model is used for topic modeling in natural language processing
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also one recommendation model available in PySpark ML, but we will
    refrain from describing it here.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Pipeline` in PySpark ML is a concept of an *end-to-end* transformation-estimation
    process (with distinct stages) that ingests some raw data (in a DataFrame form),
    performs the necessary data carpentry (transformations), and finally estimates
    a statistical model (estimator).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A `Pipeline` can be purely transformative, that is, consisting of `Transformer`s
    only.
  prefs: []
  type: TYPE_NORMAL
- en: A `Pipeline` can be thought of as a chain of multiple discrete stages. When
    a `.fit(...)` method is executed on a `Pipeline` object, all the stages are executed
    in the order they were specified in the `stages` parameter; the `stages` parameter
    is a list of `Transformer` and `Estimator` objects. The `.fit(...)` method of
    the `Pipeline` object executes the `.transform(...)` method for the `Transformer`s
    and the `.fit(...)` method for the `Estimators`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, the output of a preceding stage becomes the input for the following
    stage: when deriving from either the `Transformer` or `Estimator` abstract classes,
    one needs to implement the `.getOutputCol()` method that returns the value of
    the `outputCol` parameter specified when creating an object.'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the chances of infant survival with ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the portion of the dataset from the previous chapter
    to present the ideas of PySpark ML.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you have not yet downloaded the data while reading the previous chapter,
    it can be accessed here: [http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will, once again, attempt to predict the chances of the
    survival of an infant.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we load the data with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We specify the schema of the DataFrame; our severely limited dataset now only
    has 17 columns.
  prefs: []
  type: TYPE_NORMAL
- en: Creating transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can use the dataset to estimate a model, we need to do some transformations.
    Since statistical models can only operate on numeric data, we will have to encode
    the `BIRTH_PLACE` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do any of this, since we will use a number of different feature transformations
    later in this chapter, let''s import them all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To encode the `BIRTH_PLACE` column, we will use the `OneHotEncoder` method.
    However, the method cannot accept `StringType` columns; it can only deal with
    numeric types so first we will cast the column to an `IntegerType`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Having done this, we can now create our first `Transformer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now create a single column with all the features collated together.
    We will use the `VectorAssembler` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `inputCols` parameter passed to the `VectorAssembler` object is a list of
    all the columns to be combined together to form the `outputCol`—the `'features'`.
    Note that we use the output of the encoder object (by calling the `.getOutputCol()`
    method), so we do not have to remember to change this parameter's value should
    we change the name of the output column in the encoder object at any point.
  prefs: []
  type: TYPE_NORMAL
- en: It's now time to create our first estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will (once again) use the logistic regression model. However,
    later in the chapter, we will showcase some more complex models from the `.classification`
    set of PySpark ML models, so we load the whole section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once loaded, let''s create the model by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We would not have to specify the `labelCol` parameter if our target column had
    the name `'label'`. Also, if the output of our `featuresCreator` was not called
    `'features',` we would have to specify the `featuresCol` by (most conveniently)
    calling the `getOutputCol()` method on the `featuresCreator` object.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All that is left now is to create a `Pipeline` and fit the model. First, let''s
    load the `Pipeline` from the ML package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a `Pipeline` is really easy. Here''s how our pipeline should look
    like conceptually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a pipeline](img/B05793_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Converting this structure into a `Pipeline` is a *walk in the park*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Our `pipeline` is now created so we can (finally!) estimate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you fit the model, we need to split our dataset into training and testing
    datasets. Conveniently, the DataFrame API has the `.randomSplit(...)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter is a list of dataset proportions that should end up in,
    respectively, `births_train` and `births_test` subsets. The `seed` parameter provides
    a seed to the randomizer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also split the dataset into more than two subsets as long as the elements
    of the list sum up to 1, and you unpack the output into as many subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we could split the births dataset into three subsets like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code would put a random 70% of the births dataset into the `train`
    object, 20% would go to the `test`, and the `val` DataFrame would hold the remaining
    10%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is about time to finally run our pipeline and estimate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `.fit(...)` method of the pipeline object takes our training dataset as
    an input. Under the hood, the `births_train` dataset is passed first to the `encoder`
    object. The DataFrame that is created at the `encoder` stage then gets passed
    to the `featuresCreator` that creates the `'features'` column. Finally, the output
    from this stage is passed to the `logistic` object that estimates the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.fit(...)` method returns the `PipelineModel` object (the `model` object
    in the preceding snippet) that can then be used for prediction; we attain this
    by calling the `.transform(...)` method and passing the testing dataset created
    earlier. Here''s what the `test_model` looks like in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fitting the model](img/B05793_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we get all the columns from the `Transfomers` and `Estimators`.
    The logistic regression model outputs several columns: the `rawPrediction` is
    the value of the linear combination of features and the β coefficients, the `probability`
    is the calculated probability for each of the classes, and finally, the `prediction`
    is our final class assignment.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Obviously, we would like to now test how well our model did. PySpark exposes
    a number of evaluation methods for classification and regression in the `.evaluation`
    section of the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `BinaryClassficationEvaluator` to test how well our model performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `rawPredictionCol` can either be the `rawPrediction` column produced by
    the estimator or the `probability`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how well our model performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the performance of the model](img/B05793_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The area under the ROC of 74% and area under PR of 71% shows a well-defined
    model, but nothing out of extraordinary; if we had other features, we could drive
    this up, but this is not the purpose of this chapter (nor the book, for that matter).
  prefs: []
  type: TYPE_NORMAL
- en: Saving the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PySpark allows you to save the `Pipeline` definition for later use. It not
    only saves the pipeline structure, but also all the definitions of all the `Transformers`
    and `Estimators`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you can load it up later and use it straight away to `.fit(...)` and predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the same result (as expected):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the model](img/B05793_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you, however, want to save the estimated model, you can also do that; instead
    of saving the `Pipeline`, you need to save the `PipelineModel`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note, that not only the `PipelineModel` can be saved: virtually all the models
    that are returned by calling the `.fit(...)` method on an `Estimator` or `Transformer`
    can be saved and loaded back to be reused.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To save your model, see the following the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script uses the `.load(...)` method, a class method of the `PipelineModel`
    class, to reload the estimated model. You can compare the result of `test_reloadedModel.take(1)`
    with the output of `test_model.take(1)` we presented earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter hyper-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rarely, our first model would be the best we can do. By simply looking at our
    metrics and accepting the model because it passed our pre-conceived performance
    thresholds is hardly a scientific method for finding the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A concept of parameter hyper-tuning is to find the best parameters of the model:
    for example, the maximum number of iterations needed to properly estimate the
    logistic regression model or maximum depth of a decision tree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will explore two concepts that allow us to find the best
    parameters for our models: grid search and train-validation splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grid search is an exhaustive algorithm that loops through the list of defined
    parameter values, estimates separate models, and chooses the best one given some
    evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note of caution should be stated here: if you define too many parameters
    you want to optimize over, or too many values of these parameters, it might take
    a lot of time to select the best model as the number of models to estimate would
    grow very quickly as the number of parameters and parameter values grow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you want to fine-tune two parameters with two parameter values,
    you would have to fit four models. Adding one more parameter with two values would
    require estimating eight models, whereas adding one more additional value to our
    two parameters (bringing it to three values for each) would require estimating
    nine models. As you can see, this can quickly get out of hand if you are not careful.
    See the following chart to inspect this visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grid search](img/B05793_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After this cautionary tale, let''s get to fine-tuning our parameters space.
    First, we load the `.tuning` part of the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s specify our model and the list of parameters we want to loop through:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we specify the model we want to optimize the parameters of. Next, we
    decide which parameters we will be optimizing, and what values for those parameters
    to test. We use the `ParamGridBuilder()` object from the `.tuning` subpackage,
    and keep adding the parameters to the grid with the `.addGrid(...)` method: the
    first parameter is the parameter object of the model we want to optimize (in our
    case, these are `logistic.maxIter` and `logistic.regParam`), and the second parameter
    is a list of values we want to loop through. Calling the `.build()` method on
    the `.ParamGridBuilder` builds the grid.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need some way of comparing the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So, once again, we''ll use the `BinaryClassificationEvaluator`. It is time
    now to create the logic that will do the validation work for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `CrossValidator` needs the `estimator`, the `estimatorParamMaps`, and the
    `evaluator` to do its job. The model loops through the grid of values, estimates
    the models, and compares their performance using the `evaluator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot use the data straight away (as the `births_train` and `births_test`
    still have the `BIRTHS_PLACE` column not encoded) so we create a purely transforming
    `Pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Having done this, we are ready to find the optimal combination of parameters
    for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cvModel` will return the best model estimated. We can now use it to see
    if it performed better than our previous model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grid search](img/B05793_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we got a slightly better result. What parameters does the best
    model have? The answer is a little bit convoluted, but here''s how you can extract
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grid search](img/B05793_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Train-validation splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `TrainValidationSplit` model, to select the best model, performs a random
    split of the input dataset (the training dataset) into two subsets: smaller training
    and validation subsets. The split is only performed once.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will also use the `ChiSqSelector` to select only the top
    five features, thus limiting the complexity of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `numTopFeatures` specifies the number of features to return. We will put
    the selector after the `featuresCreator`, so we call the `.getOutputCol()` on
    the `featuresCreator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered creating the `LogisticRegression` and `Pipeline` earlier, so we
    will not explain how these are created again here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TrainValidationSplit` object gets created in the same fashion as the `CrossValidator`
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we fit our data to the model, and calculate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints out the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Train-validation splitting](img/B05793_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Well, the model with less features certainly performed worse than the full model,
    but the difference was not that great. Ultimately, it is a performance trade-off
    between a more complex model and the less sophisticated one.
  prefs: []
  type: TYPE_NORMAL
- en: Other features of PySpark ML in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we described most of the features of the PySpark
    ML library. In this section, we will provide examples of how to use some of the
    `Transformers` and `Estimators`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have used quite a few models from this submodule of PySpark. In this section,
    we'll show you how to use the most useful ones (in our opinion).
  prefs: []
  type: TYPE_NORMAL
- en: NLP - related feature extractors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described earlier, the `NGram` model takes a list of tokenized text and produces
    pairs (or n-grams) of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will take an excerpt from PySpark''s documentation and
    present how to clean up the text before passing it to the `NGram` model. Here''s
    how our dataset looks like (abbreviated for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the full view of how the following snippet looks like, please download
    the code from our GitHub repository: [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We copied these four paragraphs from the description of the DataFrame usage
    in `Pipelines`: [http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe](http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Each row in our single-column DataFrame is just a bunch of text. First, we
    need to tokenize this text. To do so we will use the `RegexTokenizer` instead
    of just the `Tokenizer` as we can specify the pattern(s) we want the text to be
    broken at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The pattern here splits the text on any number of spaces, but also removes
    commas, full stops, backslashes, and quotation marks. A single row from the output
    of the `tokenizer` looks similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NLP - related feature extractors](img/B05793_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the `RegexTokenizer` not only splits the sentences in to words,
    but also normalizes the text so each word is in small-caps.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is still plenty of junk in our text: words such as `be`, `a`,
    or `to` normally provide us with nothing useful when analyzing a text. Thus, we
    will remove these so called `stopwords` using nothing else other than the `StopWordsRemover(...)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the method looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NLP - related feature extractors](img/B05793_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we only have the useful words. So, let''s build our `NGram` model and the
    `Pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the `pipeline`, we follow in a very similar fashion as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NLP - related feature extractors](img/B05793_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That's it. We have got our n-grams and we can now use them in further NLP processing.
  prefs: []
  type: TYPE_NORMAL
- en: Discretizing continuous variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ever so often, we deal with a continuous feature that is highly non-linear and
    really hard to fit in our model with only one coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: In such a situation, it might be hard to explain the relationship between such
    a feature and the target with just one coefficient. Sometimes, it is useful to
    band the values into discrete buckets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create some fake data with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a DataFrame by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Discretizing continuous variables](img/B05793_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will use the `QuantileDiscretizer` model to split our continuous variable
    into five buckets (the `numBuckets` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we have got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Our function now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discretizing continuous variables](img/B05793_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can now treat this variable as categorical and use the `OneHotEncoder` to
    encode it for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing continuous variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standardizing continuous variables helps not only in better understanding the
    relationships between the features (as interpreting the coefficients becomes easier),
    but it also aids computational efficiency and protects from running into some
    numerical traps. Here's how you do it with PySpark ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a vector representation of our continuous variable
    (as it is only a single float):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build our `normalizer` and the `pipeline`. By setting the `withMean`
    and `withStd` to `True`, the method will remove the mean and scale the variance
    to be of unit length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what the transformed data would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standardizing continuous variables](img/B05793_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the data now oscillates around 0 with the unit variance (the
    green line).
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have only used the `LogisticRegression` model from PySpark ML. In
    this section, we will use the `RandomForestClassfier` to, once again, model the
    chances of survival for an infant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can do that, though, we need to cast the `label` feature to `DoubleType`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the label converted to double, we are ready to build our model.
    We progress in a similar fashion as before with the distinction that we will reuse
    the `encoder` and `featureCreator` from earlier in the chapter. The `numTrees`
    parameter specifies how many decision trees should be in our random forest, and
    the `maxDepth` parameter limits the depth of the trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now see how the `RandomForestClassifier` model performs compared to
    the `LogisticRegression`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification](img/B05793_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, as you can see, the results are better than the logistic regression model
    by roughly 3 percentage points. Let''s test how well would a model with one tree
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification](img/B05793_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Not bad at all! It actually performed better than the random forest model in
    terms of the precision-recall relationship and only slightly worse in terms of
    the area under the ROC. We just might have found a winner!
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is another big part of machine learning: quite often, in the real
    world, we do not have the luxury of having the target feature, so we need to revert
    to an unsupervised learning paradigm, where we try to uncover patterns in the
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding clusters in the births dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will use the `k-means` model to find similarities in the
    births data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Having estimated the model, let''s see if we can find some differences between
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding clusters in the births dataset](img/B05793_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Well, the `MOTHER_HEIGHT_IN` is significantly different in cluster 2\. Going
    through the results (which we will not do here for obvious reasons) would most
    likely uncover more differences and allow us to understand the data better.
  prefs: []
  type: TYPE_NORMAL
- en: Topic mining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering models are not limited to numeric data only. In the field of NLP,
    problems such as topic extraction rely on clustering to detect documents with
    similar topics. We will go through such an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our dataset. The data is formed from randomly selected
    paragraphs found on the Internet: three of them deal with topics of nature and
    national parks, the remaining three cover technology.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code snippet is abbreviated again, for obvious reasons. Refer to the source
    file on GitHub for full representation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will once again use the `RegexTokenizer` and the `StopWordsRemover`
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next in our pipeline is the `CountVectorizer`: a model that counts words in
    a document and returns a vector of counts. The length of the vector is equal to
    the total number of distinct words in all the documents, which can be seen in
    the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic mining](img/B05793_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are 262 distinct words in the text, and each document
    is now represented by a count of each word occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s now time to start predicting the topics. For that purpose we will use
    the `LDA` model—the **Latent Dirichlet Allocation** model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The `k` parameter specifies how many topics we expect to see, the `optimizer`
    parameter can be either `'online'` or `'em'` (the latter standing for the Expectation
    Maximization algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting these puzzles together results in, so far, the longest of our pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Have we properly uncovered the topics? Well, let''s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic mining](img/B05793_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looks like our method discovered all the topics properly! Do not get used to
    seeing such good results though: sadly, real world data is seldom that kind.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could not finish a chapter on a machine learning library without building
    a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will try to predict the `MOTHER_WEIGHT_GAIN` given some
    of the features described here; these are contained in the features listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'First, since all the features are numeric, we will collate them together and
    use the `ChiSqSelector` to select only the top six most important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to predict the weight gain, we will use the gradient boosted trees
    regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, again, we put it all together into a `Pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Having created the `weightGain` model, let''s see if it performs well on our
    testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression](img/B05793_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sadly, the model is no better than a flip of a coin. It looks that without additional
    independent features that are better correlated with the `MOTHER_WEIGHT_GAIN`
    label, we will not be able to explain its variance sufficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we went into details of how to use PySpark ML: the official
    main machine learning library for PySpark. We explained what the `Transformer`
    and `Estimator` are, and showed their role in another concept introduced in the
    ML library: the `Pipeline`. Subsequently, we also presented how to use some of
    the methods to fine-tune the hyper parameters of models. Finally, we gave some
    examples of how to use some of the feature extractors and models from the library.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into graph theory and GraphFrames that help
    in tackling machine learning problems better represented as graphs.
  prefs: []
  type: TYPE_NORMAL
