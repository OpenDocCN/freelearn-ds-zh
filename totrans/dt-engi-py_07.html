<html><head></head><body>
		<div><h1 id="_idParaDest-71"><em class="italic"><a id="_idTextAnchor073"/>Chapter 6</em>: Building a 311 Data Pipeline</h1>
			<p>In the previous three chapters, you learned how to use Python, Airflow, and NiFi to build data pipelines. In this chapter, you will use those skills to create a pipeline that connects to <strong class="bold">SeeClickFix</strong> and downloads all the issues for a city, and then loads it in Elasticsearch. I am currently running this pipeline every 8 hours. I use this pipeline as a source of open source intelligence – using it to monitor quality of life issues in neighborhoods, as well as reports of abandoned vehicles, graffiti, and needles. Also, it's really interesting to see what kinds of things people complain to their city about – during the COVID-19 pandemic, my city has seen several reports of people not social distancing at clubs.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Building the data pipeline</li>
				<li>Building a Kibana dashboard</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor074"/>Building the data pipeline</h1>
			<p>This <a id="_idIndexMarker362"/>data pipeline will be slightly different from the previous pipelines in that we will need to use a trick to start it off. We will have two paths to the same database – one of which we will turn off once it has run the first time, and we will have a processor that connects to itself for the success relationship. The following screenshot shows the completed pipeline:</p>
			<div><div><img src="img/Figure_6.1_B15739.jpg" alt="Figure 6.1 – The complete pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The complete pipeline</p>
			<p>The <a id="_idIndexMarker363"/>preceding screenshot may look complicated, but I assure you that it will make sense by the end of this chapter.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>Mapping a data type</h2>
			<p>Before<a id="_idIndexMarker364"/> you can build the pipeline, you need to map a field in Elasticsearch so that you get the benefit of the coordinates by mapping them as the geopoint data type. To do that, open Kibana at <code>http://localhost:5601</code>. At the toolbar, select <strong class="bold">Dev Tools</strong> (the wrench icon) and enter the code shown in the left panel of the following screenshot, and then click the run arrow. If it was successful, you will see output in the right panel, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.2_B15739.jpg" alt="Figure 6.2 – Adding geopoint mapping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Adding geopoint mapping</p>
			<p>Now <a id="_idIndexMarker365"/>that you have created the <code>scf</code> index with geopoint mapping, when you run your pipeline, the <code>coords</code> field will be converted into spatial coordinates in Elasticsearch.</p>
			<p>Let's start building.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/>Triggering a pipeline</h2>
			<p>In the <a id="_idIndexMarker366"/>previous section, I mentioned that you would need to trick the data pipeline into starting. Remember that this pipeline will connect to an API endpoint, and in order to do that, the NiFi processors for calling HTTP endpoints, as well as the <code>ExecuteScript</code> processer that you will use, require an incoming flowfile to start them. This processor cannot be the first processor in a data pipeline.</p>
			<p>To start the data pipeline, you will use the <code>GenerateFlowFile</code> processor. Drag and drop the processor on the canvas. Double-click on it to change the configuration. In the <code>Start Flow Fake Data</code>. This lets us know that this processor sends fake data just to start the flow. The configuration will use all the defaults and look like the following screenshot:</p>
			<div><div><img src="img/Figure_6.3_B15739.jpg" alt="Figure 6.3 – Configuring the GenerateFlowfile processor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Configuring the GenerateFlowfile processor</p>
			<p>Lastly, in<a id="_idIndexMarker367"/> the <strong class="bold">SCHEDULING</strong> tab, set the processor to run at your desired interval. I use 8 because I do not want to overwhelm the API. </p>
			<p>The processor, when running, will generate a single flowfile with 0 bytes of data. It is empty, but it does contain metadata generated by NiFi. However, this empty flowfile will do the trick and start the next processor. That is where the work begins.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor077"/>Querying SeeClickFix</h2>
			<p>In the previous<a id="_idIndexMarker368"/> NiFi examples, you did not use any code, just configurations to make the processor do what you needed. We could do that in this pipeline. However, now is a good time to introduce coding using Python – Jython – into your pipelines. </p>
			<p>Drag and drop the <code>ExecuteScript</code> processor to the canvas. Double-click on it to edit the configuration. Starting with the <code>Query SCF</code> so that I know it queries <strong class="bold">SeeClickFix</strong>. In the <strong class="bold">Properties</strong> tab, set <strong class="bold">Script Engine</strong> to <strong class="bold">Python</strong>. In the <strong class="bold">Script Body</strong> parameter, you will write the Python<a id="_idIndexMarker369"/> code that the processor will execute. The query steps are as follows:</p>
			<ol>
				<li>You need to import the required libraries. The following code is the libraries that you will always need to include:<pre>import java.io
from org.apache.commons.io import IOUtils
from java.nio.charset import StandardCharsets
from org.apache.nifi.processor.io import StreamCallback
from org.python.core.util import StringUtil</pre></li>
				<li>Next, you will create the class that will be called to handle the work. The <code>process</code> function will contain the code that will perform the task:<pre>class ModJSON(StreamCallback):
  def __init__(self):
        pass
  def process(self, inputStream, outputStream):
    # Task Goes Here</pre></li>
				<li>Lastly, assume that no errors have occurred, and check whether there is a flowfile. If there is one, write the flowfile calling the class. Next, check whether an error occurred. If there was an error, you will send the flowfile to the failure relationship, otherwise, send it to the success relationship:<pre>errorOccurred=False
flowFile = session.get()
if (flowFile != None):
  flowFile = session.write(flowFile, ModJSON())
  #flowFile = session.putAttribute(flowFile)
  if(errorOccurred):
    session.transfer(flowFile, REL_FAILURE)
 else:
    session.transfer(flowFile, REL_SUCCESS)</pre><p>The preceding code is the boiler plate for any Python <code>ExecuteScript</code> processor. The only thing you will need to change will be in the process function, which we will do in the steps that follow.</p><p>Because NiFi <a id="_idIndexMarker370"/>uses Jython, you can add many Python libraries to the Jython environment, but that is beyond the scope of this book. For now, you will use the standard libraries.</p></li>
				<li>To make a call to the SeeClickFix API, you will need to import the <code>urllib</code> libraries and <code>json</code>, as shown:<pre>import urllib
import urllib2
import json</pre></li>
				<li>Next, you will put the code in the <code>process</code> function. The code will be a <code>try</code> <code>except</code> block that makes a request to the HTTP endpoint and writes out the response to <code>outputStream</code>. If there was an error, the <code>except</code> block will set <code>errorOccurred</code> to <code>True</code> and this will trigger the rest of the code to send the flowfile to the <code>Failure</code> relationship. The only line in the <code>try</code> block that is not standard Python for using <code>urllib</code> is <code>outputStream.write()</code>. This is where you write to the flowfile:<pre>    try:
        param = {'place_url':'bernalillo-county',
                'per_page':'100'}
        url = 'https://seeclickfix.com/api/v2/issues?' + 
               urllib.urlencode(param)
        rawreply = urllib2.urlopen(url).read()
        reply = json.loads(rawreply)
        outputStream.write(bytearray(json.dumps(reply,
               indent=4).encode('utf-8')))
    except:
        global errorOccurred
        errorOccurred=True
        
        outputStream.write(bytearray(json.dumps(reply,
               indent=4).encode('utf-8'))) </pre></li>
			</ol>
			<p>The preceding<a id="_idIndexMarker371"/> code, when successful, will output a JSON flowfile. The contents of the flowfile will contain some metadata and an array of issues. The two pieces of metadata we will be interested in are <strong class="bold">page</strong> and <strong class="bold">pages</strong>.</p>
			<p>You have grabbed the first 100 issues for Bernalillo County, and will pass this flowfile to two different processors – <code>GetEveryPage</code> and <code>SplitJson</code>. We will follow the <code>SplitJson</code> path, as this path will send the data to Elasticsearch.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/>Transforming the data for Elasticsearch</h2>
			<p>The following <a id="_idIndexMarker372"/>are the steps for<a id="_idIndexMarker373"/> transforming data for Elasticsearch:</p>
			<ol>
				<li value="1">Drag and drop the <code>SplitJson</code> processor to the canvas. Double-click on it to modify the properties. In the <strong class="bold">Properties</strong> tab, set the <strong class="bold">JsonPath Expression</strong> property to <strong class="bold">$.issues</strong>. This processor will now split the 100 issues into their own flowfiles.</li>
				<li>Next, you need to add coordinates in the format expected by NiFi. We will use an <code>x</code>, <code>y</code> string named <code>coords</code>. To do that, drag and drop an <code>ExecuteScript</code> processor to the canvas. Double-click on it and click the <code>import json</code> statement. </li>
				<li>The <code>process</code> function will convert the input stream to a string. The input stream<a id="_idIndexMarker374"/> is the flowfile contents from the previous<a id="_idIndexMarker375"/> processor. In this case, it is a single issue. Then it will use the <code>json</code> library to load it as <code>json</code>. You then add a field named <code>coords</code> and assign it the value of a concatenated string of the <code>lat</code> and <code>lng</code> fields in the flowfile JSON. Lastly, you write the JSON back to the output stream as a new flowfile:<pre>def process(self, inputStream, outputStream):
    try:
        text = IOUtils.toString(inputStream,
                                StandardCharsets.UTF_8)
        reply=json.loads(text)
        reply['coords']=str(reply['lat'])+',                           '+str(reply['lng'])
        d=reply['created_at'].split('T')
        reply['opendate']=d[0]
        outputStream.write(bytearray(json.dumps(reply, 
                           indent=4).encode('utf-8')))
    except:
        global errorOccurred
        errorOccurred=True        
        outputStream.write(bytearray(json.dumps(reply,
                           indent=4).encode('utf-8')))</pre><p>Now you have a single issue, with a new field called <code>coords</code>, that is a string format that Elasticsearch recognizes as a geopoint. You are almost ready to load the data in Elasticsearch, but first you need a unique identifier.</p></li>
				<li>To create the <a id="_idIndexMarker376"/>equivalent of a primary key in<a id="_idIndexMarker377"/> Elasticsearch, you can specify an ID. The JSON has an ID for each issue that you can use. To do so, drag and drop the <code>EvaluateJsonPath</code> processor on to the canvas. Double-click on it and select the <code>id</code> with the value of <code>$.id</code>. Remember that <code>$.</code> allows you to specify a JSON field to extract. The flowfile now contains a unique ID extracted from the JSON.</li>
				<li>Drag and drop the <code>PutElasticsearchHttp</code> processor on to the canvas. Double-click on it to edit the properties. Set the <code>http://localhost:9200</code>. In the optional <strong class="bold">Identifier Attribute</strong> property, set the value to <strong class="bold">id</strong>. This is the attribute you just extracted in the previous processor. Set the <strong class="bold">Index</strong> to <strong class="bold">SCF</strong> (short for <strong class="bold">SeeClickFix</strong>), and the <strong class="bold">Type</strong> to <strong class="bold">doc</strong>. Lastly, you will set the <strong class="bold">Index Operation</strong> property to <strong class="bold">upsert</strong>. In Elasticsearch, <strong class="bold">upsert</strong> will index the document if the ID does not already exist, and it will update if the ID exists, and the data is different. Otherwise, nothing will happen, and the record will be ignored, which is what you want if the data is already the same.</li>
			</ol>
			<p>The issues are now being loaded in Elasticsearch, and if you were to check, you will have 100 documents in your <code>scf</code> index. But there are a lot more than 100 records in the SeeClickFix data for Bernalillo County; there are 44 pages of records (4,336 issues) according to the metadata from the <code>QuerySCF</code> processor.</p>
			<p>The following section will show you how to grab all the data.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor079"/>Getting every page</h2>
			<p>When you queried SeeClickFix, you <a id="_idIndexMarker378"/>sent the results to two paths. We took the <code>SplitJson</code> path. The reason for this is because on the initial query, you got back 100 issues and how many pages of issues exist (as part of the metadata). You sent the issues to the <code>SplitJson</code> path, because they were ready to process, but now you need to do something with the number of pages. We will do that by following the <code>GetEveryPage</code> path.</p>
			<p>Drag and drop an <code>ExecuteScript</code> processor on to the canvas. Double-click on it to edit the <code>urllib</code> and <code>json</code> libraries. </p>
			<p>The <code>process</code> function will convert the input stream to JSON, and then it will load it using the <code>json</code> library. The main logic of the function states that if the current page is less than or equal to the total number of pages, call the API and request the next page (<code>next_page_url</code>), and then write out the JSON as a flowfile. Otherwise, it stops. The code is as follows:</p>
			<pre>try:
        text = IOUtils.toString(inputStream, 
                                StandardCharsets.UTF_8)
        asjson=json.loads(text)
        if asjson['metadata']['pagination']
        ['page']&lt;=asjson['metadata']['pagination']['pages']:
          url = asjson['metadata']['pagination']
                                  ['next_page_url']
          rawreply = urllib2.urlopen(url).read()
          reply = json.loads(rawreply)
          outputStream.write(bytearray(json.dumps(reply,
                             indent=4).encode('utf-8')))
        else:
          global errorOccurred
          errorOccurred=True        
          outputStream.write(bytearray(json.dumps(asjson,
                             indent=4).encode('utf-8')))        
    except:
        global errorOccurred
        errorOccurred=True       
        outputStream.write(bytearray(json.dumps(asjson, 
                           indent=4).encode('utf-8')))</pre>
			<p>You will connect the relationship success for this processor to the <code>SplitJson</code> processor in the last path we took. The flowfile will be split on issues, coordinates added, the ID extracted, and the issue sent to Elasticsearch. However, we need to do this 42 times.</p>
			<p>To keep <a id="_idIndexMarker379"/>processing pages, you need to connect the success relationship to itself. That's right; you can connect a processor to itself. When you processed the first page through this processor, the next page was 2. The issues were sent to <code>SplitJson</code>, and back to this processor, which said the current page is less than 44 and the next page is 3. </p>
			<p>You now have an Elasticsearch index with all of the current issues from SeeClickFix. However, the number of issues for Bernalillo County is much larger than the set of current issues – there is an archive. And now that you have a pipeline pulling new issues every 8 hours, you will always be up to date, but you can backfill Elasticsearch with all of the archived issues as well. Then you will have the full history of issues.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor080"/>Backfilling data</h2>
			<p>To backfill the <code>SCF</code> index with<a id="_idIndexMarker380"/> historic data only requires the addition of a single parameter to the <code>params</code> object in the <code>QuerySCF</code> processor. To do that, right-click on the <code>QuerySCF</code> processor and select <code>QuerySCFArchive</code>. In the <code>params</code> object to the following code:</p>
			<pre>param = {'place_url':'bernalillo-county', 'per_page': '100', 'status':'Archived'}</pre>
			<p>The <code>status</code> parameter was added with the value <code>Archived</code>. Now, connect the <code>GenerateFlowfile</code> processor to this backfill processor to start it. Then, connect the processor to the <code>SplitJson</code> processor for the success relationship. This will send the issues to Elasticsearch. But you need to loop through all the pages, so connect the processor to the <code>GetEveryPage</code> processor too. This will loop through the archives and send all the issues to Elasticsearch. Once this pipeline finishes, you can stop the <code>QuerySCFArchive</code> processor.</p>
			<p>When you have a system that is constantly adding new records – like a transactional system – you will follow this pattern often. You will build a data pipeline to extract the recent records and extract the new records at a set interval – daily or hourly depending on how often the system updates or how much in real time you need it to be. Once your pipeline is working, you will add a series of processors to grab all the historic data and backfill your warehouse. You<a id="_idIndexMarker381"/> may not need to go back to the beginning of time, but in this case, there were sufficiently few records to make it feasible.</p>
			<p>You will also follow this pattern if something goes wrong or if you need to populate a new warehouse. If your warehouse becomes corrupted or you bring a new warehouse online, you can rerun this backfill pipeline to bring in all the data again, making the new database complete. But it will only contain current state. The next chapter deals with production pipelines and will help you solve this problem by improving your pipelines. For now, let's visualize your new Elasticsearch index in Kibana.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor081"/>Building a Kibana dashboard</h1>
			<p>Now that your <a id="_idIndexMarker382"/>SeeClickFix data pipeline has loaded data in Elasticsearch, it would be nice to see the results of the data, as would an analyst. Using Kibana, you can do just that. In this section, you will build a Kibana dashboard for your data pipeline.</p>
			<p>To open Kibana, browse to <code>http://localhost:5601</code> and you will see the main window. At the bottom of the toolbar (on the left of the screen; you may need to expand it), click the management icon at the bottom. You need to select <code>scf*</code>, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.4_B15739.jpg" alt="Figure 6.4 – Creating the index pattern in Kibana"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Creating the index pattern in Kibana</p>
			<p>When you click the <a id="_idIndexMarker383"/>next step, you will be asked to select a <code>created_at</code>, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.5_B15739.jpg" alt="Figure 6.5 – Selecting the Time Filter field&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Selecting the Time Filter field</p>
			<p>Once you have created the index in Kibana, you can move on to visualizations.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/>Creating visualizations</h2>
			<p>To create<a id="_idIndexMarker384"/> visualizations, select the visualization icon in the toolbar. Select <strong class="bold">Create Visualization</strong> and you will see a variety of types available, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.6_B15739.jpg" alt="Figure 6.6 – Available visualization types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Available visualization types</p>
			<p>You will see<a id="_idIndexMarker385"/> the <code>scf</code> — this will apply to all visualizations in this chapter. Leave the y axis as <code>created_at</code> and the interval will be <strong class="bold">Monthly</strong>. You will see a chart as shown in the following screenshot (yours may vary):</p>
			<div><div><img src="img/Figure_6.7_B15739.jpg" alt="Figure 6.7 – Bar chart of created_at counts by month&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Bar chart of created_at counts by month</p>
			<p>Save the bar <a id="_idIndexMarker386"/>chart and name it <code>scf-bar</code>, or anything that you will be able to associate with the SeeClickFix data.</p>
			<p>Next, select visualization again and choose metric. You will only add a custom label under the <strong class="bold">Metrics</strong> options. I chose <strong class="bold">Issues</strong>. By doing this, you remove the default count that gets placed under the numbers in the metric. This visualization is giving us a count of issues and will change when we apply filters in the dashboard. The configuration is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.8_B15739.jpg" alt="Figure 6.8 – Metrics visualization configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Metrics visualization configuration</p>
			<p>Again, save the<a id="_idIndexMarker387"/> visualization using any convention, or prefix it with <code>scf</code>, as I have done.</p>
			<p>For the next visualization, select a pie chart – which will default to a donut. Under <strong class="bold">Buckets</strong>, select <strong class="bold">Split slices</strong>. For <strong class="bold">Aggregations</strong>, select <strong class="bold">Terms</strong>. And for <strong class="bold">Field</strong>, select <strong class="bold">request_type.title.keyword</strong>. Leave the rest of the defaults set. This will give you the top five titles. The results are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.9_B15739.jpg" alt="Figure 6.9 – Top five issue titles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Top five issue titles</p>
			<p>While not a<a id="_idIndexMarker388"/> visualization, <strong class="bold">Markdown</strong> can add value to your dashboard by providing some context or a description. Select <strong class="bold">Markdown</strong> from the visualization options. You can enter Markdown in the left pane and, by clicking the run symbol, see the preview in the right pane. I have just added an H1, some text, and a bullet list, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.10_B15739.jpg" alt="Figure 6.10 – Using the Markdown editor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Using the Markdown editor</p>
			<p>The last visualization, <code>scf</code>. Once on the map screen, select <strong class="bold">Add Layer</strong> and the source will be <strong class="bold">Documents</strong>. This allows you to select an index. The following screenshot shows what you should see:</p>
			<div><div><img src="img/Figure_6.11_B15739.jpg" alt="Figure 6.11 – Adding a new layer with the source being documents"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – Adding a new layer with the source being documents</p>
			<p>When you<a id="_idIndexMarker390"/> select <code>scf</code> as the index pattern, Kibana will recognize the appropriate field and add the data to the map. Your map will be blank, and you may wonder went wrong. Kibana sets the time filter to the last 15 minutes, and you do not have data newer than the last 8 hours. Set the filter to a longer time frame, and the data will appear if the <code>create_at</code> field is in the window. The results are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.12_B15739.jpg" alt="Figure 6.12 – A map visualization from an Elasticsearch index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – A map visualization from an Elasticsearch index</p>
			<p>Now that <a id="_idIndexMarker391"/>you have created visualizations from your data, you can now move on to combining them into a dashboard. The next section will show you how.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor083"/>Creating a dashboard</h2>
			<p>To build a <a id="_idIndexMarker392"/>dashboard, select the dashboard icon on the toolbar. You will then select <code>scf</code> – or any of the names you used to save your visualizations. Adding them to the dashboard, you can then position them and resize them. Make sure to save your dashboard once it is set up. I have built the dashboard shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.13_B15739.jpg" alt="Figure 6.13 – A SeeClickFix dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – A SeeClickFix dashboard</p>
			<p>The <a id="_idIndexMarker393"/>dashboard has the Markdown, pie chart, map, metric, and bar chart added. I moved them around by grabbing the top of the panel and resized them by grabbing the lower-right corner and dragging. You can also click the gear icon and add a new name for your panels, so that they do not have the name that you used when you save the visualization.</p>
			<p>With your dashboard, you can filter the data and all the visualizations will change. For example, I have clicked on the Graffiti label in the pie chart and the results are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.14_B15739.jpg" alt="Figure 6.14 – Filtering on Graffiti&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Filtering on Graffiti</p>
			<p>Using filters<a id="_idIndexMarker394"/> is where the metric visualization comes in handy. It is nice to know what the number of records are. You can see that the map and the bar chart changed as well. You can also filter on the date range. I have selected the last 7 days in the filter, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.15_B15739.jpg" alt="Figure 6.15 – Filtering by time in a dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – Filtering by time in a dashboard</p>
			<p>The time<a id="_idIndexMarker395"/> filter allows you to select <strong class="bold">Now</strong>, <strong class="bold">Relative</strong>, or <strong class="bold">Absolute</strong>. <strong class="bold">Relative</strong> is a number of days, months, years, and so on from <strong class="bold">Now</strong>, while <strong class="bold">Absolute</strong> allows you to specify a start and end time on a calendar. The results of the seven-day filter are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.16_B15739.jpg" alt="Figure 6.16 – Dashboard with a seven-day filter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Dashboard with a seven-day filter</p>
			<p>The last<a id="_idIndexMarker396"/> filter I will show is the map filter. You can select an area or draw a polygon on the map to filter your dashboard. By clicking on the map tools icon, the options will appear as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.17_B15739.jpg" alt="Figure 6.17 – Tools icon on the map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Tools icon on the map</p>
			<p>Using <a id="_idIndexMarker397"/>the <strong class="bold">Draw</strong> bounds to filter data, I drew a rectangle on the map and the results are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_6.18_B15739.jpg" alt="Figure 6.18 – Filtering data using the map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – Filtering data using the map</p>
			<p>In the preceding dashboard, you can see the perfect rectangle of points. The map filter is one of my favorite filters. </p>
			<p>Kibana<a id="_idIndexMarker398"/> dashboards make your data pipelines useful to non-data engineers. The work you put into moving and transforming data becomes live data that can be used by analysts and mangers to explore and learn from the data. Kibana dashboards are also an excellent way for you, the data engineer, to visualize the data you have extracted, transformed, and loaded to see whether there are any obvious issues in your data pipeline. They can be a type of debugging tool.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>Summary</h1>
			<p>In this chapter, you learned how to build a data pipeline using data from a REST API. You also added a flow to the data pipeline to allow you to backfill the data, or to recreate a database with all of the data using a single pipeline.</p>
			<p>The second half of the chapter provided a basic overview of how to build a dashboard using Kibana. Dashboards will usually be outside the responsibilities of a data engineer. In smaller firms, however, this could very well be your job. Furthermore, being able to quickly build a dashboard can help validate your data pipeline and look for any possible errors in the data. </p>
			<p>In the next chapter, we begin a new section of this book, where you will take the skills you have learned and improve them by making your pipelines ready for production. You will learn about deployment, better validation techniques, and other skills needed when you are running pipelines in a production environment.</p>
		</div>
	</body></html>