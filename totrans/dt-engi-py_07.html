<html><head></head><body>
		<div id="_idContainer096">
			<h1 id="_idParaDest-71"><em class="italic"><a id="_idTextAnchor073"/>Chapter 6</em>: Building a 311 Data Pipeline</h1>
			<p>In the previous three chapters, you learned how to use Python, Airflow, and NiFi to build data pipelines. In this chapter, you will use those skills to create a pipeline that connects to <strong class="bold">SeeClickFix</strong> and downloads all the issues for a city, and then loads it in Elasticsearch. I am currently running this pipeline every 8 hours. I use this pipeline as a source of open source intelligence – using it to monitor quality of life issues in neighborhoods, as well as reports of abandoned vehicles, graffiti, and needles. Also, it's really interesting to see what kinds of things people complain to their city about – during the COVID-19 pandemic, my city has seen several reports of people not social distancing at clubs.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Building the data pipeline</li>
				<li>Building a Kibana dashboard</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor074"/>Building the data pipeline</h1>
			<p>This <a id="_idIndexMarker362"/>data pipeline will be slightly different from the previous pipelines in that we will need to use a trick to start it off. We will have two paths to the same database – one of which we will turn off once it has run the first time, and we will have a processor that connects to itself for the success relationship. The following screenshot shows the completed pipeline:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/Figure_6.1_B15739.jpg" alt="Figure 6.1 – The complete pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The complete pipeline</p>
			<p>The <a id="_idIndexMarker363"/>preceding screenshot may look complicated, but I assure you that it will make sense by the end of this chapter.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>Mapping a data type</h2>
			<p>Before<a id="_idIndexMarker364"/> you can build the pipeline, you need to map a field in Elasticsearch so that you get the benefit of the coordinates by mapping them as the geopoint data type. To do that, open Kibana at <strong class="source-inline">http://localhost:5601</strong>. At the toolbar, select <strong class="bold">Dev Tools</strong> (the wrench icon) and enter the code shown in the left panel of the following screenshot, and then click the run arrow. If it was successful, you will see output in the right panel, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/Figure_6.2_B15739.jpg" alt="Figure 6.2 – Adding geopoint mapping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Adding geopoint mapping</p>
			<p>Now <a id="_idIndexMarker365"/>that you have created the <strong class="source-inline">scf</strong> index with geopoint mapping, when you run your pipeline, the <strong class="source-inline">coords</strong> field will be converted into spatial coordinates in Elasticsearch.</p>
			<p>Let's start building.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/>Triggering a pipeline</h2>
			<p>In the <a id="_idIndexMarker366"/>previous section, I mentioned that you would need to trick the data pipeline into starting. Remember that this pipeline will connect to an API endpoint, and in order to do that, the NiFi processors for calling HTTP endpoints, as well as the <strong class="source-inline">ExecuteScript</strong> processer that you will use, require an incoming flowfile to start them. This processor cannot be the first processor in a data pipeline.</p>
			<p>To start the data pipeline, you will use the <strong class="source-inline">GenerateFlowFile</strong> processor. Drag and drop the processor on the canvas. Double-click on it to change the configuration. In the <strong class="bold">Settings</strong> tab, name the processor. I have named it <strong class="source-inline">Start Flow Fake Data</strong>. This lets us know that this processor sends fake data just to start the flow. The configuration will use all the defaults and look like the following screenshot:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/Figure_6.3_B15739.jpg" alt="Figure 6.3 – Configuring the GenerateFlowfile processor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Configuring the GenerateFlowfile processor</p>
			<p>Lastly, in<a id="_idIndexMarker367"/> the <strong class="bold">SCHEDULING</strong> tab, set the processor to run at your desired interval. I use 8 because I do not want to overwhelm the API. </p>
			<p>The processor, when running, will generate a single flowfile with 0 bytes of data. It is empty, but it does contain metadata generated by NiFi. However, this empty flowfile will do the trick and start the next processor. That is where the work begins.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor077"/>Querying SeeClickFix</h2>
			<p>In the previous<a id="_idIndexMarker368"/> NiFi examples, you did not use any code, just configurations to make the processor do what you needed. We could do that in this pipeline. However, now is a good time to introduce coding using Python – Jython – into your pipelines. </p>
			<p>Drag and drop the <strong class="source-inline">ExecuteScript</strong> processor to the canvas. Double-click on it to edit the configuration. Starting with the <strong class="bold">Settings</strong> tab, name it something that makes sense to you – I named it <strong class="source-inline">Query SCF</strong> so that I know it queries <strong class="bold">SeeClickFix</strong>. In the <strong class="bold">Properties</strong> tab, set <strong class="bold">Script Engine</strong> to <strong class="bold">Python</strong>. In the <strong class="bold">Script Body</strong> parameter, you will write the Python<a id="_idIndexMarker369"/> code that the processor will execute. The query steps are as follows:</p>
			<ol>
				<li>You need to import the required libraries. The following code is the libraries that you will always need to include:<p class="source-code">import java.io</p><p class="source-code">from org.apache.commons.io import IOUtils</p><p class="source-code">from java.nio.charset import StandardCharsets</p><p class="source-code">from org.apache.nifi.processor.io import StreamCallback</p><p class="source-code">from org.python.core.util import StringUtil</p></li>
				<li>Next, you will create the class that will be called to handle the work. The <strong class="source-inline">process</strong> function will contain the code that will perform the task:<p class="source-code">class ModJSON(StreamCallback):</p><p class="source-code">  def __init__(self):</p><p class="source-code">        pass</p><p class="source-code">  def process(self, inputStream, outputStream):</p><p class="source-code">    # Task Goes Here</p></li>
				<li>Lastly, assume that no errors have occurred, and check whether there is a flowfile. If there is one, write the flowfile calling the class. Next, check whether an error occurred. If there was an error, you will send the flowfile to the failure relationship, otherwise, send it to the success relationship:<p class="source-code">errorOccurred=False</p><p class="source-code">flowFile = session.get()</p><p class="source-code">if (flowFile != None):</p><p class="source-code">  flowFile = session.write(flowFile, ModJSON())</p><p class="source-code">  #flowFile = session.putAttribute(flowFile)</p><p class="source-code">  if(errorOccurred):</p><p class="source-code">    session.transfer(flowFile, REL_FAILURE)</p><p class="source-code"> else:</p><p class="source-code">    session.transfer(flowFile, REL_SUCCESS)</p><p>The preceding code is the boiler plate for any Python <strong class="source-inline">ExecuteScript</strong> processor. The only thing you will need to change will be in the process function, which we will do in the steps that follow.</p><p>Because NiFi <a id="_idIndexMarker370"/>uses Jython, you can add many Python libraries to the Jython environment, but that is beyond the scope of this book. For now, you will use the standard libraries.</p></li>
				<li>To make a call to the SeeClickFix API, you will need to import the <strong class="source-inline">urllib</strong> libraries and <strong class="source-inline">json</strong>, as shown:<p class="source-code">import urllib</p><p class="source-code">import urllib2</p><p class="source-code">import json</p></li>
				<li>Next, you will put the code in the <strong class="source-inline">process</strong> function. The code will be a <strong class="source-inline">try</strong> <strong class="source-inline">except</strong> block that makes a request to the HTTP endpoint and writes out the response to <strong class="source-inline">outputStream</strong>. If there was an error, the <strong class="source-inline">except</strong> block will set <strong class="source-inline">errorOccurred</strong> to <strong class="source-inline">True</strong> and this will trigger the rest of the code to send the flowfile to the <strong class="source-inline">Failure</strong> relationship. The only line in the <strong class="source-inline">try</strong> block that is not standard Python for using <strong class="source-inline">urllib</strong> is <strong class="source-inline">outputStream.write()</strong>. This is where you write to the flowfile:<p class="source-code">    try:</p><p class="source-code">        param = {'place_url':'bernalillo-county',</p><p class="source-code">                'per_page':'100'}</p><p class="source-code">        url = 'https://seeclickfix.com/api/v2/issues?' + </p><p class="source-code">               urllib.urlencode(param)</p><p class="source-code">        rawreply = urllib2.urlopen(url).read()</p><p class="source-code">        reply = json.loads(rawreply)</p><p class="source-code">        outputStream.write(bytearray(json.dumps(reply,</p><p class="source-code">               indent=4).encode('utf-8')))</p><p class="source-code">    except:</p><p class="source-code">        global errorOccurred</p><p class="source-code">        errorOccurred=True</p><p class="source-code">        </p><p class="source-code">        outputStream.write(bytearray(json.dumps(reply,</p><p class="source-code">               indent=4).encode('utf-8'))) </p></li>
			</ol>
			<p>The preceding<a id="_idIndexMarker371"/> code, when successful, will output a JSON flowfile. The contents of the flowfile will contain some metadata and an array of issues. The two pieces of metadata we will be interested in are <strong class="bold">page</strong> and <strong class="bold">pages</strong>.</p>
			<p>You have grabbed the first 100 issues for Bernalillo County, and will pass this flowfile to two different processors – <strong class="source-inline">GetEveryPage</strong> and <strong class="source-inline">SplitJson</strong>. We will follow the <strong class="source-inline">SplitJson</strong> path, as this path will send the data to Elasticsearch.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/>Transforming the data for Elasticsearch</h2>
			<p>The following <a id="_idIndexMarker372"/>are the steps for<a id="_idIndexMarker373"/> transforming data for Elasticsearch:</p>
			<ol>
				<li value="1">Drag and drop the <strong class="source-inline">SplitJson</strong> processor to the canvas. Double-click on it to modify the properties. In the <strong class="bold">Properties</strong> tab, set the <strong class="bold">JsonPath Expression</strong> property to <strong class="bold">$.issues</strong>. This processor will now split the 100 issues into their own flowfiles.</li>
				<li>Next, you need to add coordinates in the format expected by NiFi. We will use an <strong class="source-inline">x</strong>, <strong class="source-inline">y</strong> string named <strong class="source-inline">coords</strong>. To do that, drag and drop an <strong class="source-inline">ExecuteScript</strong> processor to the canvas. Double-click on it and click the <strong class="bold">Properties</strong> tab. Set the <strong class="bold">Script Engine</strong> to <strong class="bold">Python</strong>. The <strong class="bold">Script Body</strong> property will contain the standard boiler plate, plus the <strong class="source-inline">import json</strong> statement. </li>
				<li>The <strong class="source-inline">process</strong> function will convert the input stream to a string. The input stream<a id="_idIndexMarker374"/> is the flowfile contents from the previous<a id="_idIndexMarker375"/> processor. In this case, it is a single issue. Then it will use the <strong class="source-inline">json</strong> library to load it as <strong class="source-inline">json</strong>. You then add a field named <strong class="source-inline">coords</strong> and assign it the value of a concatenated string of the <strong class="source-inline">lat</strong> and <strong class="source-inline">lng</strong> fields in the flowfile JSON. Lastly, you write the JSON back to the output stream as a new flowfile:<p class="source-code">def process(self, inputStream, outputStream):</p><p class="source-code">    try:</p><p class="source-code">        text = IOUtils.toString(inputStream,</p><p class="source-code">                                StandardCharsets.UTF_8)</p><p class="source-code">        reply=json.loads(text)</p><p class="source-code">        reply['coords']=str(reply['lat'])+',                           '+str(reply['lng'])</p><p class="source-code">        d=reply['created_at'].split('T')</p><p class="source-code">        reply['opendate']=d[0]</p><p class="source-code">        outputStream.write(bytearray(json.dumps(reply, </p><p class="source-code">                           indent=4).encode('utf-8')))</p><p class="source-code">    except:</p><p class="source-code">        global errorOccurred</p><p class="source-code">        errorOccurred=True        </p><p class="source-code">        outputStream.write(bytearray(json.dumps(reply,</p><p class="source-code">                           indent=4).encode('utf-8')))</p><p>Now you have a single issue, with a new field called <strong class="source-inline">coords</strong>, that is a string format that Elasticsearch recognizes as a geopoint. You are almost ready to load the data in Elasticsearch, but first you need a unique identifier.</p></li>
				<li>To create the <a id="_idIndexMarker376"/>equivalent of a primary key in<a id="_idIndexMarker377"/> Elasticsearch, you can specify an ID. The JSON has an ID for each issue that you can use. To do so, drag and drop the <strong class="source-inline">EvaluateJsonPath</strong> processor on to the canvas. Double-click on it and select the <strong class="bold">Properties</strong> tab. Clicking the plus sign in the upper-right corner, add a property named <strong class="source-inline">id</strong> with the value of <strong class="source-inline">$.id</strong>. Remember that <strong class="source-inline">$.</strong> allows you to specify a JSON field to extract. The flowfile now contains a unique ID extracted from the JSON.</li>
				<li>Drag and drop the <strong class="source-inline">PutElasticsearchHttp</strong> processor on to the canvas. Double-click on it to edit the properties. Set the <strong class="bold">Elasticsearch URL</strong> property to <strong class="source-inline">http://localhost:9200</strong>. In the optional <strong class="bold">Identifier Attribute</strong> property, set the value to <strong class="bold">id</strong>. This is the attribute you just extracted in the previous processor. Set the <strong class="bold">Index</strong> to <strong class="bold">SCF</strong> (short for <strong class="bold">SeeClickFix</strong>), and the <strong class="bold">Type</strong> to <strong class="bold">doc</strong>. Lastly, you will set the <strong class="bold">Index Operation</strong> property to <strong class="bold">upsert</strong>. In Elasticsearch, <strong class="bold">upsert</strong> will index the document if the ID does not already exist, and it will update if the ID exists, and the data is different. Otherwise, nothing will happen, and the record will be ignored, which is what you want if the data is already the same.</li>
			</ol>
			<p>The issues are now being loaded in Elasticsearch, and if you were to check, you will have 100 documents in your <strong class="source-inline">scf</strong> index. But there are a lot more than 100 records in the SeeClickFix data for Bernalillo County; there are 44 pages of records (4,336 issues) according to the metadata from the <strong class="source-inline">QuerySCF</strong> processor.</p>
			<p>The following section will show you how to grab all the data.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor079"/>Getting every page</h2>
			<p>When you queried SeeClickFix, you <a id="_idIndexMarker378"/>sent the results to two paths. We took the <strong class="source-inline">SplitJson</strong> path. The reason for this is because on the initial query, you got back 100 issues and how many pages of issues exist (as part of the metadata). You sent the issues to the <strong class="source-inline">SplitJson</strong> path, because they were ready to process, but now you need to do something with the number of pages. We will do that by following the <strong class="source-inline">GetEveryPage</strong> path.</p>
			<p>Drag and drop an <strong class="source-inline">ExecuteScript</strong> processor on to the canvas. Double-click on it to edit the <strong class="bold">Properties</strong> tab. Set the <strong class="bold">Script Engine</strong> property to <strong class="bold">Python</strong> and the <strong class="bold">Script Body</strong> will include the standard boiler plate – including the imports for the <strong class="source-inline">urllib</strong> and <strong class="source-inline">json</strong> libraries. </p>
			<p>The <strong class="source-inline">process</strong> function will convert the input stream to JSON, and then it will load it using the <strong class="source-inline">json</strong> library. The main logic of the function states that if the current page is less than or equal to the total number of pages, call the API and request the next page (<strong class="source-inline">next_page_url</strong>), and then write out the JSON as a flowfile. Otherwise, it stops. The code is as follows:</p>
			<p class="source-code">try:</p>
			<p class="source-code">        text = IOUtils.toString(inputStream, </p>
			<p class="source-code">                                StandardCharsets.UTF_8)</p>
			<p class="source-code">        asjson=json.loads(text)</p>
			<p class="source-code">        if asjson['metadata']['pagination']</p>
			<p class="source-code">        ['page']&lt;=asjson['metadata']['pagination']['pages']:</p>
			<p class="source-code">          url = asjson['metadata']['pagination']</p>
			<p class="source-code">                                  ['next_page_url']</p>
			<p class="source-code">          rawreply = urllib2.urlopen(url).read()</p>
			<p class="source-code">          reply = json.loads(rawreply)</p>
			<p class="source-code">          outputStream.write(bytearray(json.dumps(reply,</p>
			<p class="source-code">                             indent=4).encode('utf-8')))</p>
			<p class="source-code">        else:</p>
			<p class="source-code">          global errorOccurred</p>
			<p class="source-code">          errorOccurred=True        </p>
			<p class="source-code">          outputStream.write(bytearray(json.dumps(asjson,</p>
			<p class="source-code">                             indent=4).encode('utf-8')))        </p>
			<p class="source-code">    except:</p>
			<p class="source-code">        global errorOccurred</p>
			<p class="source-code">        errorOccurred=True       </p>
			<p class="source-code">        outputStream.write(bytearray(json.dumps(asjson, </p>
			<p class="source-code">                           indent=4).encode('utf-8')))</p>
			<p>You will connect the relationship success for this processor to the <strong class="source-inline">SplitJson</strong> processor in the last path we took. The flowfile will be split on issues, coordinates added, the ID extracted, and the issue sent to Elasticsearch. However, we need to do this 42 times.</p>
			<p>To keep <a id="_idIndexMarker379"/>processing pages, you need to connect the success relationship to itself. That's right; you can connect a processor to itself. When you processed the first page through this processor, the next page was 2. The issues were sent to <strong class="source-inline">SplitJson</strong>, and back to this processor, which said the current page is less than 44 and the next page is 3. </p>
			<p>You now have an Elasticsearch index with all of the current issues from SeeClickFix. However, the number of issues for Bernalillo County is much larger than the set of current issues – there is an archive. And now that you have a pipeline pulling new issues every 8 hours, you will always be up to date, but you can backfill Elasticsearch with all of the archived issues as well. Then you will have the full history of issues.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor080"/>Backfilling data</h2>
			<p>To backfill the <strong class="source-inline">SCF</strong> index with<a id="_idIndexMarker380"/> historic data only requires the addition of a single parameter to the <strong class="source-inline">params</strong> object in the <strong class="source-inline">QuerySCF</strong> processor. To do that, right-click on the <strong class="source-inline">QuerySCF</strong> processor and select <strong class="bold">copy</strong>. Right-click on a blank spot of canvas, and then select <strong class="bold">paste</strong>. Double-click the copied processor and, in the <strong class="bold">Settings</strong> tab, rename it as <strong class="source-inline">QuerySCFArchive</strong>. In the <strong class="bold">Properties</strong> tab, modify the <strong class="bold">Script Body</strong> parameter, changing the <strong class="source-inline">params</strong> object to the following code:</p>
			<p class="source-code">param = {'place_url':'bernalillo-county', 'per_page': '100', 'status':'Archived'}</p>
			<p>The <strong class="source-inline">status</strong> parameter was added with the value <strong class="source-inline">Archived</strong>. Now, connect the <strong class="source-inline">GenerateFlowfile</strong> processor to this backfill processor to start it. Then, connect the processor to the <strong class="source-inline">SplitJson</strong> processor for the success relationship. This will send the issues to Elasticsearch. But you need to loop through all the pages, so connect the processor to the <strong class="source-inline">GetEveryPage</strong> processor too. This will loop through the archives and send all the issues to Elasticsearch. Once this pipeline finishes, you can stop the <strong class="source-inline">QuerySCFArchive</strong> processor.</p>
			<p>When you have a system that is constantly adding new records – like a transactional system – you will follow this pattern often. You will build a data pipeline to extract the recent records and extract the new records at a set interval – daily or hourly depending on how often the system updates or how much in real time you need it to be. Once your pipeline is working, you will add a series of processors to grab all the historic data and backfill your warehouse. You<a id="_idIndexMarker381"/> may not need to go back to the beginning of time, but in this case, there were sufficiently few records to make it feasible.</p>
			<p>You will also follow this pattern if something goes wrong or if you need to populate a new warehouse. If your warehouse becomes corrupted or you bring a new warehouse online, you can rerun this backfill pipeline to bring in all the data again, making the new database complete. But it will only contain current state. The next chapter deals with production pipelines and will help you solve this problem by improving your pipelines. For now, let's visualize your new Elasticsearch index in Kibana.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor081"/>Building a Kibana dashboard</h1>
			<p>Now that your <a id="_idIndexMarker382"/>SeeClickFix data pipeline has loaded data in Elasticsearch, it would be nice to see the results of the data, as would an analyst. Using Kibana, you can do just that. In this section, you will build a Kibana dashboard for your data pipeline.</p>
			<p>To open Kibana, browse to <strong class="source-inline">http://localhost:5601</strong> and you will see the main window. At the bottom of the toolbar (on the left of the screen; you may need to expand it), click the management icon at the bottom. You need to select <strong class="bold">Create new Index Pattern</strong> and enter <strong class="source-inline">scf*</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/Figure_6.4_B15739.jpg" alt="Figure 6.4 – Creating the index pattern in Kibana"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Creating the index pattern in Kibana</p>
			<p>When you click the <a id="_idIndexMarker383"/>next step, you will be asked to select a <strong class="bold">Time Filter field name</strong>. Because there are several fields with times in them, and they are in a format that is already recognizable by Elasticsearch, they will be indexed as such, and you can select a primary time filter. The field selected will be the default field used in screens such as <strong class="bold">Discovery</strong> when a bar chart preview of the data is displayed by time, and when you use a time filter in visualizations or dashboards. I have selected <strong class="source-inline">created_at</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/Figure_6.5_B15739.jpg" alt="Figure 6.5 – Selecting the Time Filter field&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Selecting the Time Filter field</p>
			<p>Once you have created the index in Kibana, you can move on to visualizations.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/>Creating visualizations</h2>
			<p>To create<a id="_idIndexMarker384"/> visualizations, select the visualization icon in the toolbar. Select <strong class="bold">Create Visualization</strong> and you will see a variety of types available, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/Figure_6.6_B15739.jpg" alt="Figure 6.6 – Available visualization types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Available visualization types</p>
			<p>You will see<a id="_idIndexMarker385"/> the <strong class="bold">Lens</strong> type, which is a <strong class="bold">Beta</strong> visualization, as well as <strong class="bold">Controls</strong> and <strong class="bold">Vega</strong>, which are <strong class="bold">Experimentals</strong>. For now, select the <strong class="bold">Vertical Bar</strong> chart. When asked for a source, choose <strong class="source-inline">scf</strong> — this will apply to all visualizations in this chapter. Leave the y axis as <strong class="bold">Count</strong>, but add a new bucket and select the x axis. For <strong class="bold">Aggregations</strong>, choose <strong class="bold">Date Histogram</strong>. The field is <strong class="source-inline">created_at</strong> and the interval will be <strong class="bold">Monthly</strong>. You will see a chart as shown in the following screenshot (yours may vary):</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Figure_6.7_B15739.jpg" alt="Figure 6.7 – Bar chart of created_at counts by month&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Bar chart of created_at counts by month</p>
			<p>Save the bar <a id="_idIndexMarker386"/>chart and name it <strong class="source-inline">scf-bar</strong>, or anything that you will be able to associate with the SeeClickFix data.</p>
			<p>Next, select visualization again and choose metric. You will only add a custom label under the <strong class="bold">Metrics</strong> options. I chose <strong class="bold">Issues</strong>. By doing this, you remove the default count that gets placed under the numbers in the metric. This visualization is giving us a count of issues and will change when we apply filters in the dashboard. The configuration is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/Figure_6.8_B15739.jpg" alt="Figure 6.8 – Metrics visualization configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Metrics visualization configuration</p>
			<p>Again, save the<a id="_idIndexMarker387"/> visualization using any convention, or prefix it with <strong class="source-inline">scf</strong>, as I have done.</p>
			<p>For the next visualization, select a pie chart – which will default to a donut. Under <strong class="bold">Buckets</strong>, select <strong class="bold">Split slices</strong>. For <strong class="bold">Aggregations</strong>, select <strong class="bold">Terms</strong>. And for <strong class="bold">Field</strong>, select <strong class="bold">request_type.title.keyword</strong>. Leave the rest of the defaults set. This will give you the top five titles. The results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/Figure_6.9_B15739.jpg" alt="Figure 6.9 – Top five issue titles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Top five issue titles</p>
			<p>While not a<a id="_idIndexMarker388"/> visualization, <strong class="bold">Markdown</strong> can add value to your dashboard by providing some context or a description. Select <strong class="bold">Markdown</strong> from the visualization options. You can enter Markdown in the left pane and, by clicking the run symbol, see the preview in the right pane. I have just added an H1, some text, and a bullet list, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/Figure_6.10_B15739.jpg" alt="Figure 6.10 – Using the Markdown editor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Using the Markdown editor</p>
			<p>The last visualization, <strong class="bold">Map</strong>, has an arrow because maps have their own place on the toolbar, and you can do a lot more with them than the other visualizations. For now, you can <a id="_idIndexMarker389"/>select <strong class="bold">Map</strong> from either location. You will select <strong class="bold">Create Map</strong>, and when prompted for the index pattern, select <strong class="source-inline">scf</strong>. Once on the map screen, select <strong class="bold">Add Layer</strong> and the source will be <strong class="bold">Documents</strong>. This allows you to select an index. The following screenshot shows what you should see:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/Figure_6.11_B15739.jpg" alt="Figure 6.11 – Adding a new layer with the source being documents"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – Adding a new layer with the source being documents</p>
			<p>When you<a id="_idIndexMarker390"/> select <strong class="source-inline">scf</strong> as the index pattern, Kibana will recognize the appropriate field and add the data to the map. Your map will be blank, and you may wonder went wrong. Kibana sets the time filter to the last 15 minutes, and you do not have data newer than the last 8 hours. Set the filter to a longer time frame, and the data will appear if the <strong class="source-inline">create_at</strong> field is in the window. The results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/Figure_6.12_B15739.jpg" alt="Figure 6.12 – A map visualization from an Elasticsearch index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – A map visualization from an Elasticsearch index</p>
			<p>Now that <a id="_idIndexMarker391"/>you have created visualizations from your data, you can now move on to combining them into a dashboard. The next section will show you how.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor083"/>Creating a dashboard</h2>
			<p>To build a <a id="_idIndexMarker392"/>dashboard, select the dashboard icon on the toolbar. You will then select <strong class="bold">Create a new dashboard</strong> and add visualizations. If this is the first dashboard, you may see text asking whether you want to add an existing item. Add an item and then, in the search bar, type <strong class="source-inline">scf</strong> – or any of the names you used to save your visualizations. Adding them to the dashboard, you can then position them and resize them. Make sure to save your dashboard once it is set up. I have built the dashboard shown in the following screenshot:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/Figure_6.13_B15739.jpg" alt="Figure 6.13 – A SeeClickFix dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – A SeeClickFix dashboard</p>
			<p>The <a id="_idIndexMarker393"/>dashboard has the Markdown, pie chart, map, metric, and bar chart added. I moved them around by grabbing the top of the panel and resized them by grabbing the lower-right corner and dragging. You can also click the gear icon and add a new name for your panels, so that they do not have the name that you used when you save the visualization.</p>
			<p>With your dashboard, you can filter the data and all the visualizations will change. For example, I have clicked on the Graffiti label in the pie chart and the results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/Figure_6.14_B15739.jpg" alt="Figure 6.14 – Filtering on Graffiti&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Filtering on Graffiti</p>
			<p>Using filters<a id="_idIndexMarker394"/> is where the metric visualization comes in handy. It is nice to know what the number of records are. You can see that the map and the bar chart changed as well. You can also filter on the date range. I have selected the last 7 days in the filter, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/Figure_6.15_B15739.jpg" alt="Figure 6.15 – Filtering by time in a dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – Filtering by time in a dashboard</p>
			<p>The time<a id="_idIndexMarker395"/> filter allows you to select <strong class="bold">Now</strong>, <strong class="bold">Relative</strong>, or <strong class="bold">Absolute</strong>. <strong class="bold">Relative</strong> is a number of days, months, years, and so on from <strong class="bold">Now</strong>, while <strong class="bold">Absolute</strong> allows you to specify a start and end time on a calendar. The results of the seven-day filter are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/Figure_6.16_B15739.jpg" alt="Figure 6.16 – Dashboard with a seven-day filter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Dashboard with a seven-day filter</p>
			<p>The last<a id="_idIndexMarker396"/> filter I will show is the map filter. You can select an area or draw a polygon on the map to filter your dashboard. By clicking on the map tools icon, the options will appear as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Figure_6.17_B15739.jpg" alt="Figure 6.17 – Tools icon on the map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Tools icon on the map</p>
			<p>Using <a id="_idIndexMarker397"/>the <strong class="bold">Draw</strong> bounds to filter data, I drew a rectangle on the map and the results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/Figure_6.18_B15739.jpg" alt="Figure 6.18 – Filtering data using the map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – Filtering data using the map</p>
			<p>In the preceding dashboard, you can see the perfect rectangle of points. The map filter is one of my favorite filters. </p>
			<p>Kibana<a id="_idIndexMarker398"/> dashboards make your data pipelines useful to non-data engineers. The work you put into moving and transforming data becomes live data that can be used by analysts and mangers to explore and learn from the data. Kibana dashboards are also an excellent way for you, the data engineer, to visualize the data you have extracted, transformed, and loaded to see whether there are any obvious issues in your data pipeline. They can be a type of debugging tool.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>Summary</h1>
			<p>In this chapter, you learned how to build a data pipeline using data from a REST API. You also added a flow to the data pipeline to allow you to backfill the data, or to recreate a database with all of the data using a single pipeline.</p>
			<p>The second half of the chapter provided a basic overview of how to build a dashboard using Kibana. Dashboards will usually be outside the responsibilities of a data engineer. In smaller firms, however, this could very well be your job. Furthermore, being able to quickly build a dashboard can help validate your data pipeline and look for any possible errors in the data. </p>
			<p>In the next chapter, we begin a new section of this book, where you will take the skills you have learned and improve them by making your pipelines ready for production. You will learn about deployment, better validation techniques, and other skills needed when you are running pipelines in a production environment.</p>
		</div>
	</body></html>