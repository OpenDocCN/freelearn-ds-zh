<html><head></head><body>
        

                            
                    Exploring, Cleaning, Refining, and Blending Datasets
                
            
            
                
<p>In the previous chapter, we learned about the power of data visualizations, and the importance of having good-quality, consistent data defined with dimensions and measures. </p>
<p>Now that we understand <em>why</em> that's important, we are going to focus on the <em>how</em> throughout this chapter by working hands-on with data. Most of the examples provided so far included data that was already <em>prepped</em> (prepared) ahead of time for easier consumption. We are now switching gears by learning the skills that are necessary to be comfortable working with data to increase your data literacy. </p>
<p>A key concept of this chapter is cleaning, filtering, and refining data. In many cases, the reason why you need to perform these actions is the source data does not provide high-quality analytics <em>as is</em>. Throughout my career, high-quality data is not the norm and data gaps are common. As good data analysts, we need to work with what we have available. We will cover some techniques to enrich the quality of the data so you can provide quality insights and answer questions from the data even when the source does not include all of the information required.</p>
<p>In my experience, highlighting the poor quality of the source data is the insight because not enough transparency exists and key stakeholders are unaware of the challenges of using the data. The bottom line is poor quality should not stop you from proceeding with working with data. My goal is to demonstrate a repeatable technique and workflow to improve data quality for analysis.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Retrieving, viewing, and storing tabular data</li>
<li>Learning how to restrict, sort, and sift through data</li>
<li>Cleaning, refining, and purifying data using Python</li>
<li>Combining and binning data</li>
</ul>
<h1 id="uuid-78f0fd2c-df4c-4449-9019-bb5fb9957113">Technical requirements</h1>
<p>Here's the GitHub repository of this book: <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07</a>.</p>
<p>You can download and install the required software from the following link: <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<h1 id="uuid-a462c4bb-3a7a-4e38-a61e-e83346018198">Retrieving, viewing, and storing tabular data</h1>
<p>The ability to retrieve and view tabular data has been covered multiple times in prior chapters; however, those examples were focused on the perspective of the consumer. We learned the skills necessary to understand what structured data is in, the many different forms it can take, and how to answer some questions from data. Our data literacy has increased during this time but we have relied on the producers of data sources to make it easier to read using a few Python commands or SQL commands. In this chapter, we are switching gears from being exclusively a <strong>consumer</strong> to now a <strong>producer</strong> of data by learning skills to manipulate data for analysis.</p>
<p>As a good data analyst, you will need both sides of the consumer and producer spectrum of skills to solve more complicated questions with data. For example, a common measure requested by businesses with web or mobile users is called <strong>usage analytics</strong>. This means counting the number of users over snapshots of time, such as by day, week, month, and year. More importantly, you want to better understand whether those users are new, returning, or lost.</p>
<p>Common questions related to usage analytics are as follows:</p>
<ul>
<li>How many new users have hit the website this day, week, or month?</li>
<li>How many returning users have accessed the website this day, week, or month?</li>
<li>How many users have we lost (inactive for more than 60 days) this week, month, or year?</li>
</ul>
<p>To answer these types of questions, your data source must have, at a minimum, <kbd>timestamp</kbd> and unique <kbd>user_id</kbd> fields available. In many cases, this data will have high volume and velocity, so analyzing this information will require the right combination of people, processes, and technology, which I have had the pleasure of working with. Data engineering teams build out ingestion pipelines to make this data accessible for reporting and analytics.</p>
<p>You may need to work with the data engineering team to apply the business rules and summary levels (also known as aggregates) to the data that include additional fields required to answer the user analytics questions. For our examples, I have provided a much smaller sample of data and we are going to derive new data fields from the existing source data file provided.</p>
<p>I find the best way to learn is to walk through the steps together, so let's create a new Jupyter notebook named <kbd>user_churn_prep</kbd>. We will begin with retrieving data from SQL against a database and loading it into a DataFrame, similar to the steps outlined in <a href="bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml">Chapter 5</a>, <em>Gathering and Loading Data in Python</em>. To keep it simple, we are using another SQLite database to retrieve the source data.</p>
<p>If you would like more details about connecting to SQL data sources, please refer to <a href="bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml">Chapter 5</a>, <em>Gathering and Loading Data</em> <em>in Python</em>.</p>
<h2 id="uuid-098f4cc8-15ab-4dc1-a6af-27317e81b3f1">Retrieving</h2>
<p>To create a connection and use SQLite, we have to import a new library using the code. For this example, I have provided the database file named <kbd>user_hits.db</kbd>, so be sure to download it from my GitHub repository beforehand:</p>
<ol>
<li>To load a SQLite database connection, you just need to add the following command in your Jupyter notebook and run the cell. I have placed a copy on GitHub for reference:</li>
</ol>
<pre style="padding-left: 60px">In[]: import sqlite3</pre>
<ol start="2">
<li>Next, we need to assign a connection to a variable named <kbd>conn</kbd> and point to the location of the database file, which is named <kbd>user_hits.db</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: conn = sqlite3.connect('user_hits.db')</pre>
<p>Be sure that you have copied the <kbd>user_hits.db</kbd> file to the correct Jupyter folder directory to avoid errors with the connection.</p>
<ol start="3">
<li>Import the <kbd>pandas</kbd> library so you can create a DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="4">
<li>Run a SQL statement and assign the results to a DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn = pd.read_sql_query("SELECT * FROM tbl_user_hits;", conn)</pre>
<ol start="5">
<li>Now that we have the results in a DataFrame, we can use all of the available <kbd>pandas</kbd> library commands against this data without going back to the database. Your code should look similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-780 image-border" src="img/9989271e-f748-4d28-915e-049d558dff00.png" style="width:35.00em;height:9.17em;"/></p>
<h2 id="uuid-8e7e89a7-64e5-43ae-9dc7-960dd8087460">Viewing </h2>
<p>Perform the following steps to view the results of the retrieved data:</p>
<ol>
<li>To view the results, we can just run the <kbd>head()</kbd> command against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn.head()</pre>
<p style="padding-left: 60px">The output will look like the following table, where the <kbd>tbl_user_hits</kbd> table has been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="img/8eff19fc-3499-4283-841d-ec0b75b7c443.png" style="width:14.08em;height:13.08em;"/></p>
<p style="padding-left: 60px">Before we move on to the next step, let's verify the data we loaded with a few metadata commands.</p>
<ol start="2">
<li>Type in <kbd>df_user_churn.info()</kbd> in the next <kbd>In[]:</kbd> cell and run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn.info()</pre>
<p style="padding-left: 60px">Verify that the output cell displays <kbd>Out []</kbd>. There will be multiple rows, including data types for all columns, similar to the following screenshot:</p>
<div><img class="alignnone size-full wp-image-782 image-border" src="img/b8de6e38-5c79-4911-bd87-b3cb59aaf02e.png" style="width:20.75em;height:9.08em;"/></div>
<h2 id="uuid-1f1bf175-6dff-43bb-a9d5-44c52f39c362">Storing</h2>
<p>Now that we have the data available to work with as a DataFrame in Jupyter, let's run a few commands to store it as a file for reference. Storing data as a snapshot for analysis is a useful technique to learn, and while our example is simplistic, the concept will help in future data analysis projects.</p>
<p>To store your DataFrame into a CSV file, you just have to run the following command:</p>
<pre>In[]: df_user_churn.to_csv('user_hits_export.csv')</pre>
<p class="mce-root"/>
<p>The results will look similar to the following screenshot, where a new CSV file is created in the same project folder as your current Jupyter notebook. Based on the OS you are using on your workstation, the results will vary:</p>
<div><img src="img/32997d53-84a6-4c1a-9881-ec3357bdf233.png"/></div>
<p>There are other formats you can export your DataFrame to, including Excel. You should also note the file path from which you are exporting the data file. Check out the <em>Further reading</em> section for more information.</p>
<h1 id="uuid-d8957a24-4098-401e-965f-c14c54f8c28e">Learning how to restrict, sort, and sift through data</h1>
<p>Now that we have the data available in a DataFrame, we can walk through how to restrict, sort, and sift through data with a few Python commands. The concepts we are going to walk through using pandas are also common using SQL, so I will also include the equivalent SQL commands for reference.</p>
<h2 id="uuid-fc274032-14a6-4734-8bab-8ac1140f4837">Restricting</h2>
<p>The concept of restricting data, which is also known as filtering data, is all about isolating one or more records based on conditions. Simple examples are when you are only retrieving results based on matching a specific field and value. For example, you only want to see results for one user or a specific point in time. Other requirements for restricting data can be more complicated, including explicit conditions that require elaborate logic, business rules, and multiple steps. I will not be covering complex examples that require complex logic but will add some references in the <em>Further reading</em> section. However, the concepts covered will teach you essential skills to satisfy many common use cases.</p>
<p class="mce-root"/>
<p>For our first example, let's isolate one specific user from our DataFrame. Using <kbd>pandas</kbd> commands, that is pretty easy, so let's start up a new Jupyter notebook named <kbd>user_churn_restricting</kbd>:</p>
<ol>
<li>Import the <kbd>pandas</kbd> library so you can create a DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="2">
<li>Create a new DataFrame by loading the data from the CSV file we created in the prior example:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn = pd.read_csv('user_hits_export.csv');</pre>
<p>The file path and filename must be the same as those you used in the prior example.</p>
<p style="padding-left: 60px">Now that we have all user data loaded into a single DataFrame, we can easily reference the source dataset to restrict results. It is a best practice to keep this source DataFrame intact so you can reference it for other purposes and analysis. It is also common during analysis to need to make adjustments based on changing requirements, or that you will only gain insights by making adjustments.</p>
<p style="padding-left: 60px">In my career, I follow a common practice of <em>you don't know what you don't know</em> while working with data, so having the flexibility to easily reference the source data without undoing your changes is important. This is commonly known as snapshotting your analysis and having the ability to roll back changes as needed.</p>
<p>When working with big data sources where the sources are larger than a billion rows, snapshots will require a large number of resources where RAM and CPU will be impacted. You may be required to snapshot incrementally for a specific date or create a rolling window of time to limit the amount of data you can work with at one time.</p>
<p style="padding-left: 60px">To restrict our data to a specific user, we will be creating a new DataFrame from the source DataFrame. That way, if we need to make adjustments to the filters used to create the new DataFrame, we don't have to rerun all of the steps from the beginning.</p>
<p class="mce-root"/>
<ol start="3">
<li>Create a new DataFrame by loading the data from the source DataFrame. The syntax is nested so you are actually calling the same <kbd>df_user_churn</kbd> DataFrame within itself and filtering results only for the explicit value where <kbd>userid</kbd> is equal to <kbd>1</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_restricted = df_user_churn[df_user_churn['userid']==1]</pre>
<ol start="4">
<li>To view and verify the results, you can run a simple <kbd>head()</kbd> command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_restricted.head()</pre>
<p>The results will look similar to the following screenshot, where the only two rows in the new <kbd>df_user_restricted</kbd> DataFrame have a value where <kbd>userid</kbd> is <kbd>1</kbd>:</p>
<div><img class="alignnone size-full wp-image-786 image-border" src="img/fdc9fa3a-878d-45af-adfd-7c3b6b77e979.png" style="width:32.00em;height:9.92em;"/></div>
<p>Restricting data helps to isolate records for specific types of analysis to help to answer additional questions. In the next step, we can start answering questions related to usage patterns.</p>
<h2 id="uuid-a7ac4ebc-0bb1-4cef-9b95-f42f5a2b16a7">Sorting</h2>
<p>Now that we have isolated a specific user by creating a new DataFrame, which is now available for reference, we can enhance our analysis by asking questions such as the following:</p>
<ul>
<li>When did a specific user start using our website?</li>
<li>How frequently does this user access our website?</li>
<li>When was the last time this user accessed our website?</li>
</ul>
<p class="mce-root"/>
<p>All of these questions can be answered with a few simple Python commands focused on sorting commands. Sorting data is a skill that computer programmers of any programming language are familiar with. It's easily done with SQL by adding an <kbd>order by</kbd> command. Many third-party software, such as Microsoft Excel, Google Sheets, or Qlik Sense, has a sorting feature built in. The concept of sorting data is well known, so I will not go into a detailed definition; rather, I will focus on important features and best practices when performing data analysis.</p>
<p>With structured data, sorting is commonly understood to be row-level by specific columns, which will be defined by ordering the sequence of the values from either low to high or high to low. The default is low to high unless you explicitly change it. If the values have a data type that is numeric, such as integer or float, the sort order sequence will be easy to identify. For textual data, the values are sorted alphabetically, and, depending on the technology used, mixed case text will be handled differently. In Python and pandas, we have specific functions available along with parameters to handle many different use cases and needs.</p>
<p>Let's start answering some of the questions we outlined previously using the <kbd>sort()</kbd> function:</p>
<ol>
<li>To answer the question <em>When did a specific user start using our website?</em>, we just need to run the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_restricted.sort_values(by='date')</pre>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where the results are sorted in ascending order by the <kbd>date</kbd> field: </p>
<div><img class="alignnone size-full wp-image-788 image-border" src="img/68281f91-b36c-4ed4-baae-eb3b505f4694.png" style="width:24.00em;height:7.75em;"/></div>
<ol start="2">
<li>To answer the question "<em>When is the last time this user accessed our website?</em>", we just need to run the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_restricted.sort_values(by='date', ascending=False)</pre>
<p style="padding-left: 60px" class="mce-root">The results will look similar to the following screenshot, where the same records are displayed as the previous one; however, the values are sorted in descending order by last date available for this specific <kbd>userid</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-789 image-border" src="img/e34fe94c-7b2f-4e90-9b5b-e6be0a53018a.png" style="width:30.92em;height:7.25em;"/></p>
<h2 id="uuid-eb26931d-2dce-433e-a2ae-24007f5e963a">Sifting</h2>
<p class="mce-root">The concept of sifting through data means we are isolating specific columns and/or rows from a dataset based on one or more conditions. There are nuanced differences between sifting versus restricting, so I would distinguish sifting as the need to include additional business rules or conditions applied to a population of data to isolate a subset of that data. Sifting data usually requires creating new derived columns from the source data to answer more complex questions. For our next example, a good question about usage would be: <em>Do the same users who hit our website on Monday also return during the same week?</em></p>
<p>To answer this question, we need to isolate the usage patterns for a specific day of the week. This process requires a few steps, which we will outline together from the original DataFrame we created previously:</p>
<ol start="1">
<li>Create a new DataFrame by loading the data from the source file:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned = pd.read_csv('user_hits_binning_import.csv', parse_dates=['date'])</pre>
<p style="padding-left: 60px">Next, we need to extend the DataFrame by adding new derived columns to help to make the analysis easier. Since we have a <kbd>Timestamp</kbd> field available, the pandas library has some very useful functions available to help the process. Standard SQL has built-in features as well and will vary depending on which RDMS is used, so you will have to reference the date/time functions available. For example, a Postgres database uses the syntax of <kbd>select to_char(current_date,'Day');</kbd> to convert a date field into the current day of the week.</p>
<ol start="2">
<li>Import a new <kbd>datetime</kbd> library for easy reference to date and time functions:</li>
</ol>
<pre style="padding-left: 60px">In[]: import datetime</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="3">
<li>Assign a variable to the current <kbd>datetime</kbd> for easier calculation of the <kbd>age</kbd> from today:</li>
</ol>
<pre style="padding-left: 60px">In[]: now = pd.to_datetime('now')</pre>
<ol start="4">
<li>Add a new derived column called <kbd>age</kbd> that is calculated from the current date minus the date value per user:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned['age'] = now - df_user_churn_cleaned['date']<br/></pre>
<p>If you receive <kbd>datetime</kbd> errors in your notebook, you may need to upgrade your <kbd>pandas</kbd> library.</p>
<h1 id="uuid-908a5b5f-485c-4071-9cc3-f2e737ce705c">Cleaning, refining, and purifying data using Python</h1>
<p>Data quality is highly important for any data analysis and analytics. In many cases, you will not understand how good or bad the data quality is until you start working with it. I would define good-quality data as information that is well structured, defined, and consistent, where almost all of the values in each field are defined as expected. In my experience, data warehouses will have high-quality data because it has been reported on across the organization. In my experience, bad data quality occurs where a lack of transparency exists against the data source. Bad data quality examples are a lack of conformity and inconsistency in the expected data type or any consistent pattern of values in delimited datasets. To help to solve these data quality issues, you can begin to understand your data with the concepts and questions we covered in <a href="0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml">Chapter 1</a>,<em> Fundamentals of Data Analysis</em>, with <strong>Know Your Data (KYD)</strong>. Since the quality of data will vary by source, some specific questions you can ask to understand data quality are as follows:</p>
<ul>
<li>Is the data structured or unstructured?</li>
<li>Does the data lineage trace back to a system or application?</li>
<li>Does the data get transformed and stored in a warehouse?</li>
<li>Does the data have a schema with each field having a defined data type?</li>
<li>Do you have a data dictionary available with business rules documented?</li>
</ul>
<p class="mce-root"/>
<p>Receiving answers to these questions ahead of time would be a luxury; uncovering them as you go is more common for a data analyst. During this process, you will still find the need to clean, refine, and purify your data for analysis purposes. How much time you need to spend will vary on many different factors, and the true cost of quality will be the time and effort required to improve data quality.</p>
<p>Cleaning data can take on many different forms and has been a common practice for decades for data engineers and analytic practitioners. There are many different technologies and skillsets required for enterprise and big data cleansing. Data cleaning is an industry within <strong>Information Technology</strong> (<strong>IT</strong>) because good-quality data is worth the price of outsourcing.</p>
<p>A common definition of data cleansing is the process of removing or resolving poor-quality data records from the source, which can vary based on the technology used to persist the data, such as a database table or encoded file. Poor-quality data can be identified as any data that does not match the producers' intended and defined requirements. This can include the following:</p>
<ul>
<li>Missing or null (<kbd>NaN</kbd>) values from the fields of one or more rows</li>
<li>Orphan records where the primary or foreign keys cannot be found in any referenced source tables</li>
<li>Corrupted records where one or more records cannot be read by any reporting or analysis technology</li>
</ul>
<p>For our example, let's look at our usage data again and see whether we can find any issues by profiling it to see whether we can find any anomalies:</p>
<ol>
<li>Import the CSV file and run the <kbd>info()</kbd> command to confirm the data types and row counts and profile the DataFrame for more information:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_usage_patterns = pd.read_csv('user_hits_import.csv')<br/>df_usage_patterns.info()</pre>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where metadata of the DataFrame is presented:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-791 image-border" src="img/a045eaa8-bd04-4e25-9fc9-503c47d0d599.png" style="width:16.33em;height:6.92em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">One anomaly that is uncovered is that the number of values is different between the two fields. For <kbd>userid</kbd>, there are 9 non-null values and for the <kbd>date</kbd> field, there are 12 non-null values. For this dataset, we expect each row to have one value for both fields, but this command is telling us there are missing values. Let's run another command to identify which index/row has the missing data.</p>
<ol start="2">
<li>Run the <kbd>isnull()</kbd> command to confirm the data types and row counts and profile the DataFrame for more information:</li>
</ol>
<pre style="padding-left: 60px">In[]: pd.isnull(df_usage_patterns)</pre>
<p style="padding-left: 60px">The results will look similar to the following table, where a list of <kbd>True</kbd> and <kbd>False</kbd> values is displayed by row and column:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e00c8355-7f06-4f3a-9934-d2868db06239.png" style="width:12.42em;height:22.83em;"/></p>
<p>The record count looks okay but notice that there are null values (NaN) that exist in the <kbd>userid</kbd> field. A unique identifier for each row to help us to identify each user is critical for accurate analysis of this data. The reason why <kbd>userid</kbd> is blank would have to be explained by the producer of this data and may require additional engineering resources to help to investigate and troubleshoot the root cause of the issue. In some cases, it may be a simple technical hiccup during data source creation that requires a minor code change and reprocessing. </p>
<p>I always recommend cleaning data as close to the source as possible, which saves time by avoiding reworking by other data analysts or reporting systems.</p>
<p>Having nulls included in our analysis will impact our summary statistics and metrics. For example, the count of the average daily users would be lower on the dates where the null values exist. For the user churn analysis, the measure of the frequency of reporting users would be skewed because the NaN values could be one of the returning <kbd>user_ids</kbd> or a new user.</p>
<p>With any high volume transaction-based system, there could be a margin of error that you may need to account for. As a good data analyst, ask the question, <em>what is the cost of quality and of being one hundred percent accurate?</em> If the price is too high due to the time and resources required to change it, a good alternative is to exclude and isolate the missing data so it can be investigated later.</p>
<p>Note that if you end up adding isolated data back into your analysis, you will have to restate results and inform any consumers of the change to your metrics.</p>
<p>Let's walk through an example of how to isolate and exclude any missing data by identifying the NaN records and creating a new DataFrame that has them removed:</p>
<ol start="1">
<li>Create a new DataFrame by loading the data from the source DataFrame, except we will exclude the null values by adding the <kbd>dropna()</kbd> command: </li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned = df_usage_patterns.dropna()</pre>
<ol start="2">
<li>To view and verify the results, you can run a simple <kbd>head()</kbd> command and confirm the NaN/null values have been removed:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned.head(10)</pre>
<p style="padding-left: 60px">The results will look similar to the following table, where the new DataFrame has complete records with no missing values in either <kbd>userid</kbd> or <kbd>date</kbd>:</p>
<div><img src="img/0ecc9d12-931f-41d3-b27f-20d94839ffc2.png" style="width:13.25em;height:17.67em;"/></div>
<h1 id="uuid-38a4428c-7527-4ccf-9666-5850c12361cf">Combining and binning data</h1>
<p>Combining multiple data sources is sometimes necessary for multiple reasons, which include the following:</p>
<ul>
<li>The source data is broken up into many different files with the same defined schema (tables and field names), but the number of rows will vary slightly. A common reason is for storage purposes, where it is easier to maintain multiple smaller file sizes versus one large file.</li>
<li>The data is partitioned where one field is used to break apart the data for faster response time reading or writing to the source data. For example, HIVE/HDFS recommends storing data by a single date value so you can easily identify when it was processed and quickly extract data for a specific day.</li>
<li>Historical data is stored in a different technology than more current data. For example, the engineering team changed the technology being used to manage the source data and it was decided not to import historical data beyond a specific date.</li>
</ul>
<p>For any of the reasons defined here, combining data is a common practice in data analysis. I would define the process of combining data as when you are layering two or more data sources into one where the same fields/columns from all sources align. In SQL, this would be known as <kbd>UNION ALL</kbd> and in <kbd>pandas</kbd>, we use the <kbd>concat()</kbd> function to bring all of the data together.</p>
<p>A good visual example of how data is combined is in the following screenshot, where multiple source files are named <kbd>user_data_YYYY.cvs</kbd> and each year is defined as YYYY. These three files, which all have the same field names of <kbd>userid</kbd>, <kbd>date</kbd>, and <kbd>year</kbd>, are imported into one SQL table named <kbd>tbl_user_data_stage</kbd>, which is shown in the following screenshot. The target table that stores this information also includes a new field named <kbd>filesource</kbd> so the data lineage is more transparent to both the producer and the consumer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0868ad5f-464e-4e91-b680-9dc5e1de379e.png" style="width:24.58em;height:26.25em;"/></p>
<p>Once the data has been processed and persisted into a table named <kbd>tbl_user_data_stage</kbd>, all of the records from the three files are preserved as displayed in the following table. In this example, any duplicates would be preserved between what existed in the source files and the target table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-796 image-border" src="img/525acd5e-4dbe-469d-9ab7-5ac7efe74497.png" style="width:29.58em;height:13.92em;"/></p>
<p>One of the reasons data engineering teams create <kbd>stage</kbd> tables is to help to build data ingestion pipelines and create business rules where duplicate records are removed.</p>
<p>To recreate the example in Jupyter, let's create a new notebook and name it <kbd>ch_07_combining_data</kbd>. There are more efficient ways to import multiple files but, in our example, we will import each one in separate DataFrames and then combine them into one:</p>
<ol>
<li>Import the <kbd>pandas</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<p>You will also need to copy the three CSV files to your local folder.</p>
<ol start="2">
<li>Import the first CSV file named <kbd>user_data_2017.csv</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2017 = pd.read_csv('user_data_2017.csv')</pre>
<ol start="3">
<li>Run the <kbd>head()</kbd> command to verify the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2017.head()</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where the rows are displayed with a header row and index added starting with a value of <kbd>0</kbd>:</p>
<div><img src="img/352f93fc-909f-4097-8bb8-514cfcbf05ab.png" style="width:14.67em;height:8.67em;"/></div>
<ol start="4">
<li>Repeat the process for the next CSV file, which is named <kbd>user_data_2018.csv</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2018 = pd.read_csv('user_data_2018.csv')</pre>
<ol start="5">
<li>Run the <kbd>head()</kbd> command to verify the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2018.head()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where the rows are displayed with a header row and index added starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-798 image-border" src="img/f521d692-48b2-4660-83ac-690e2f755de4.png" style="width:29.92em;height:12.00em;"/></p>
<ol start="6">
<li>Repeat the process for the next CSV file, which is named <kbd>user_data_2019.csv</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2019 = pd.read_csv('user_data_2019.csv')</pre>
<ol start="7">
<li>Run the <kbd>head()</kbd> command to verify the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_2019.head()</pre>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where the rows are displayed with a header row and index added starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-799 image-border" src="img/05142418-c3a4-4661-9b16-5d7bdbc3d62b.png" style="width:30.92em;height:12.83em;"/></p>
<ol start="8">
<li>The next step is to merge the DataFrames using the <kbd>concat()</kbd> function. We include the <kbd>ignore_index=True</kbd> parameter to create a new index value for all of the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_combined = pd.concat([df_user_data_2017, df_user_data_2018, df_user_data_2019], ignore_index=True)</pre>
<ol start="9">
<li>Run the <kbd>head()</kbd> command to verify the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_data_combined.head(10)</pre>
<p style="padding-left: 60px">The results will look similar to the following screenshot, where the rows are displayed with a header row and index added starting with a value of <kbd>0</kbd>:</p>
<div><img src="img/b8b88974-fba2-4880-91f5-463fad4056ea.png"/></div>
<p class="mce-root"/>
<h2 id="uuid-808e485f-3fcb-41f7-be14-d7344319b5da">Binning</h2>
<p>Binning is a very common analysis technique that allows you to group numeric data values based on one or more criteria. These groups become named categories; they are ordinal in nature and can have equal widths between the ranges or customized requirements. A good example is age ranges, which you commonly see on surveys such as that seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9726f6e6-d29c-4d90-9c0c-b365e195734f.png" style="width:11.58em;height:10.17em;"/></p>
<p>In this example, a person's age range is the input, but what if we actually had the birthdate of each person available in the data? Then, we could calculate the age as of today and assign an age band based on the criteria in the preceding screenshot. This is the process of binning your data.</p>
<p>Another common example is weather data, where the assigned categories of <em>hot</em>, <em>warm</em>, or <em>cold</em> are assigned to ranges of Fahrenheit or Celsius temperature. Each bin value is defined by a condition that is arbitrarily decided by the data analyst.</p>
<p>For our user data, let's assign age bins based on when the user first appeared in our dataset. We will define three bins based on the requirements, which allows us the flexibility to adjust the assigned ranges. For our example, we define the bins as follows:</p>
<ul>
<li>Less than 1 year</li>
<li>1 to 2 years</li>
<li>Greater than 3 years</li>
</ul>
<p>The specific conditions on how to create the bins will be evident once we walk through the code.</p>
<p class="mce-root"/>
<p>Another cool feature of this type of analysis is the fact that our calculated age is based on the usage data and a point in time that's calculated each time we run the code. For example, if the date of the first time a user hits the website is <em>1/1/2017</em> and we did this analysis on December 3, 2018, the age in days of the user would be 360, which would be assigned to the <em>Less than 1 year</em> bin.</p>
<p>If we rerun this analysis at a later date such as November 18, 2019, the calculated age would change, so the new assigned bin would be <em>1 to 2 years</em>.</p>
<p>The decision on where to add the logic for each bin will vary. The most flexible to make changes to the assigned bins is to add the logic where you deliver the analytics. In our examples, that would be directly in the Jupyter notebook. However, in enterprise environments where many different technologies could be used to deliver the same analysis, it makes sense to move the binning logic closer to the source. In some cases of very large datasets stored in databases, using SQL or even having the schema changed in the table is a better option.</p>
<p>If you have the luxury of a skilled data engineering team and experience of working with big data like I had, the decision to move the binning logic closer to the data tables is easy. In SQL, you could use <kbd>CASE Statement</kbd> or if/then logic. Qlik has a function called <kbd>class()</kbd>, which will bin values based on a linear scale. In Microsoft Excel, a nested formula can be used to assign bins based on a mix of functions.</p>
<p>So, the concept of binning can be applied across different technologies and as a good data analyst, you now have a foundation of understanding how it can be done.</p>
<p>Let's reinforce the knowledge by walking through an example using our usage data and Jupyter Notebook.</p>
<p>Remember to copy any dependency CSV files into the working folder before walking through the following steps.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To recreate the example in Jupyter, let's create a new notebook and name it <kbd>ch_07_sifting_and_binning_data</kbd>:</p>
<ol>
<li>Import the <kbd>pandas</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="2">
<li>Read in the CSV file provided that includes additional data for this example and create a new DataFrame named <kbd>df_user_churn_cleaned</kbd>. We are also converting the <kbd>date</kbd> field found in the source CSV file into a data type of <kbd>datetime64</kbd> while importing using the <kbd>parse_dates</kbd> parameter. This will make it easier to manipulate in the next few steps:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned = pd.read_csv('user_hits_binning_import.csv', parse_dates=['date'])</pre>
<ol start="3">
<li>Verify the DataFrame is valid using the <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned.head(10)<br/></pre>
<p style="padding-left: 60px">The output of the function will look similar to the following table, where the DataFrame is loaded with two fields with the correct data types and is available for analysis:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/26491120-81b8-4bd5-bd08-2a03283b3564.png" style="width:14.08em;height:18.17em;"/></p>
<ol start="4">
<li>Import the <kbd>datetime</kbd> and <kbd>numpy</kbd> libraries for reference later to calculate the <kbd>age</kbd> value of the <kbd>userid</kbd> field:</li>
</ol>
<pre style="padding-left: 60px">In[]: from datetime import datetime<br/>      import numpy as np</pre>
<ol start="5">
<li>Create a new derived column named <kbd>age</kbd> by calculating the difference between the current date and time using the <kbd>now</kbd> function and the <kbd>date</kbd> field. To format the <kbd>age</kbd> field in days, we include the <kbd>dt.days</kbd> function, which will convert the values into a clean <kbd>"%d"</kbd> format:</li>
</ol>
<pre style="padding-left: 60px">In[]: #df_user_churn_cleaned['age'] = (datetime.now() - pd.to_datetime(df_user_churn_cleaned['date'])).dt.days<br/>df_user_churn_cleaned['age'] = (datetime(2020, 2, 28)  - pd.to_datetime(df_user_churn_cleaned['date'])).dt.days<br/></pre>
<p>To match the screenshots, I explicitly defined the date value to 2020-02-28 with a date format of YYYY-MM-DD. You can uncomment the preceding line to calculate the current timestamp. Since the timestamp changes every time you run the function, the results will not match exactly to any image.</p>
<ol start="6">
<li>Verify that the new <kbd>age</kbd> column has been included in your DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_churn_cleaned.head()</pre>
<p style="padding-left: 60px">The output of the function will look similar to the following table, where the DataFrame has been modified from its original import and includes a new field called <kbd>age</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/807c6815-1eab-40a6-8929-4d8ae8a49dcf.png" style="width:17.08em;height:22.08em;"/></p>
<ol start="7">
<li>Create a new DataFrame called <kbd>df_ages</kbd> that groups the dimensions from the existing DataFrame and calculates the max <kbd>age</kbd> value by <kbd>userid</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_ages = df_user_churn_cleaned.groupby('userid').max()</pre>
<p style="padding-left: 60px">The output will look similar to the following screenshot, where the number of rows has decreased from the source DateFrame. Only a distinct list of <kbd>userid</kbd> values will be displayed along with the maximum <kbd>age</kbd> when the first record was created by <kbd>userid</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a2cc716b-cae4-43f8-a6cc-049d9eef0faf.png" style="width:16.42em;height:17.92em;"/></p>
<ol start="8">
<li>Create a new <kbd>age_bin</kbd> column by using the <kbd>pandas</kbd> library's <kbd>cut()</kbd> function. This will thread each value from the <kbd>age</kbd> field between one of the assigned <kbd>bins</kbd> range we have assigned. We use the <kbd>labels</kbd> parameter to make the analysis easier to consume for any audience. Note that the value of <kbd>9999</kbd> was chosen to create a maximum boundary for the <kbd>age</kbd> value:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_ages['age_bin'] = pd.cut(x=df_ages['age'], bins=[1, 365, 730, 9999], labels=['&lt; 1 year', '1 to 2 years', '&gt; 3 years'])<br/></pre>
<ol start="9">
<li>Display the DataFrame and validate the bin values displayed:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_ages</pre>
<p style="padding-left: 60px">The output of the function will look similar to the following screenshot, where the DataFrame has been modified and we now see the values in the <kbd>age_bin</kbd> field:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/54d672fc-1106-43d2-b44a-99fbe93b5a92.png" style="width:19.50em;height:16.33em;"/></p>
<h1 id="uuid-d498d831-a13a-4219-bf1b-63ac07a881b2" class="mce-root">Summary</h1>
<p class="mce-root">Congratulations, you have now increased your data literacy skills by working with data as both a consumer and producer of analytics. We covered some important topics, including essential skills to manipulate data by creating views of data, sorting, and querying tabular data from a SQL source. You now have a repeatable workflow for combining multiple data sources into one refined dataset.</p>
<p class="mce-root">We explored additional features of working with <kbd>pandas</kbd> DataFrames, showing how to restrict and sift data. We walked through real-world practical examples using the concept of <em>u</em><q>ser churn</q> to answer key business questions about usage patterns by isolating specific users and dealing with missing values from the source data.</p>
<p class="mce-root">Our next chapter is <a href="9bdac090-8534-480e-8154-a854115c0b7a.xhtml">Chapter 8</a>, <em>Understanding Joins, Relationships, and Data Aggregates</em>. Along with creating a summary analysis using a concept called aggregation, we will also go into detail on how to join data with defined relationships.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<h1 id="uuid-812cae81-cd60-4eea-afea-85b28e6506c8" class="mce-root">Further reading</h1>
<p class="mce-root">You can refer to the following links for more information on the topics of this chapter:</p>
<ul>
<li>A nice walk-through of filtering and grouping using DataFrames: <a href="https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb">https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb</a></li>
<li>Comparison of SQL features and their equivalent pandas functions: <a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html">https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html</a></li>
<li>Additional information on exporting data to Excel: <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel</a></li>
<li>Examples of SQL date and time functions: <a href="https://www.postgresql.org/docs/8.1/functions-datetime.html">https://www.postgresql.org/docs/8.1/functions-datetime.html</a></li>
</ul>


            

            
        
    </body></html>