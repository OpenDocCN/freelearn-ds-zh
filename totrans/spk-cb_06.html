<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Getting Started with Machine Learning Using MLlib</h1></div></div></div><p>This chapter is divided into the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating vectors</li><li class="listitem" style="list-style-type: disc">Creating a labeled point</li><li class="listitem" style="list-style-type: disc">Creating matrices</li><li class="listitem" style="list-style-type: disc">Calculating summary statistics</li><li class="listitem" style="list-style-type: disc">Calculating correlation</li><li class="listitem" style="list-style-type: disc">Doing hypothesis testing</li><li class="listitem" style="list-style-type: disc">Creating machine learning pipelines using ML</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Introduction</h1></div></div></div><p>The following<a id="id319" class="indexterm"/> is Wikipedia's definition of machine learning:</p><div><blockquote class="blockquote"><p><em>"Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data."</em></p></blockquote></div><p>Essentially, machine learning is making use of past data to make predictions about the future. Machine learning heavily depends upon statistical analysis and methodology.</p><p>In statistics, there are four types of measurement scales:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Scale type</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Nominal Scale</p>
</td><td style="text-align: left" valign="top">
<p>=, ≠</p>
<p>Identifies categories</p>
<p>Can't <a id="id320" class="indexterm"/>be numeric</p>
<p>Example: male, female</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ordinal Scale</p>
</td><td style="text-align: left" valign="top">
<p>=, ≠, &lt;, &gt;</p>
<p>Nominal scale +</p>
<p>Ranks <a id="id321" class="indexterm"/>from least important to most important</p>
<p>Example: corporate hierarchy</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Interval Scale</p>
</td><td style="text-align: left" valign="top">
<p>=, ≠, &lt;, &gt;, +, -</p>
<p>Ordinal <a id="id322" class="indexterm"/>scale + distance between observations</p>
<p>Numbers assigned to observations indicate order</p>
<p>Difference between any consecutive values is same as others</p>
<p>60° temperature is not the double of 30°</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ratio Scale</p>
</td><td style="text-align: left" valign="top">
<p>=, ≠, &lt;, &gt;, +, ×, ÷</p>
<p>Interval<a id="id323" class="indexterm"/> scale +ratios of observations</p>
<p>$20 is twice as costly as $10</p>
</td></tr></tbody></table></div><p>Another distinction that can be made among the data is between the continuous and discrete data. Continuous data can take any value. Most data belonging to the interval and ratio scale is continuous.</p><p>Discrete variables can take on only particular values and there are clear boundaries between the values. For example, a house can have two or three rooms but not 2.75 rooms. Data belonging to nominal and ordinal scale is always discrete.</p><p>MLlib is the Spark's library for machine learning. In this chapter, we will focus on the fundamentals of machine learning.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Creating vectors</h1></div></div></div><p>Before understanding Vectors, let's focus on what is a point. A point is just a set of numbers. This set<a id="id324" class="indexterm"/> of numbers or coordinates defines the point's position in space. The numbers of coordinates determine dimensions of the space.</p><p>We can visualize space with up to three dimensions. Space with more than three dimensions is <a id="id325" class="indexterm"/>called <strong>hyperspace</strong>. Let's put this spatial metaphor to use.</p><p>Let's start with a person. A person has the following dimensions:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Weight</li><li class="listitem" style="list-style-type: disc">Height</li><li class="listitem" style="list-style-type: disc">Age</li></ul></div><p>We are working in three-dimensional space here. Thus, the interpretation of point (160,69,24) would be 160 lb weight, 69 inches height, and 24 years age.</p><div><div><h3 class="title"><a id="note17"/>Note</h3><p>Points and vectors are same thing. Dimensions in vectors are called <strong>features</strong>. In another way, we can define a <a id="id326" class="indexterm"/>feature as an individual measurable property of a phenomenon being observed.</p></div></div><p>Spark has local vectors and matrices and also distributed matrices. Distributed matrix is backed by one <a id="id327" class="indexterm"/>or more RDDs. A local vector has numeric indices and double values, and is stored on a single machine.</p><p>There are two types of local vectors in MLlib: dense and sparse. A dense vector is backed by an array of its values, while a sparse vector is backed by two parallel arrays, one for indices and another for values.</p><p>So, person data (160,69,24) will be represented as [160.0,69.0,24.0] using dense vector and as (3,[0,1,2],[160.0,69.0,24.0]) using sparse vector format.</p><p>Whether to make a vector sparse or dense depends upon how many null values or 0s it has. Let's take a case of a vector with 10,000 values with 9,000 of them being 0. If we use dense vector format, it would be a simple structure, but 90 percent of space would be wasted. Sparse vector format would work out better here as it would only keep indices, which are non-zero.</p><p>Sparse data is very common and Spark supports the <code class="literal">libsvm</code> format for it which stores one feature vector per line.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec66"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import the MLlib vector explicitly (not to confuse with other vector classes):<div><pre class="programlisting">
<strong>Scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong>
</pre></div></li><li class="listitem">Create a dense vector:<div><pre class="programlisting">
<strong>scala&gt; val dvPerson = Vectors.dense(160.0,69.0,24.0)</strong>
</pre></div></li><li class="listitem">Create a sparse vector:<div><pre class="programlisting">
<strong>scala&gt; val svPerson = Vectors.sparse(3,Array(0,1,2),Array(160.0,69.0,24.0))</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec67"/>How it works...</h2></div></div></div><p>The following<a id="id328" class="indexterm"/> is the method signature of <code class="literal">vectors.dense</code>:</p><div><pre class="programlisting">def dense(values: Array[Double]): Vector</pre></div><p>Here, values represent double array of elements in the vector.</p><p>The following is the method signature of <code class="literal">Vectors.sparse</code>:</p><div><pre class="programlisting">def sparse(size: Int, indices: Array[Int], values: Array[Double]): Vector</pre></div><p>Here, <code class="literal">size</code> represents the size of the vector, <code class="literal">indices</code> is an array of indices, and <code class="literal">values</code> is an array of values as doubles. Do make sure you specify <code class="literal">double</code> as datatype or use decimal in at least one value; otherwise it will throw an exception for the dataset, which has only integer.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Creating a labeled point</h1></div></div></div><p>Labeled point is <a id="id329" class="indexterm"/>a local vector (sparse/dense), which has an associated label with it. Labeled data is used in supervised learning to help train algorithms. You<a id="id330" class="indexterm"/> will get to know more about it in the next chapter.</p><p>Label is stored as a double value in <code class="literal">LabeledPoint</code>. It means that when you have categorical labels, they need to be mapped to double values. What value you assign to a category is immaterial and is only a matter of convenience.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Label values</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Binary classification</p>
</td><td style="text-align: left" valign="top">
<p>0 or 1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Multiclass classification</p>
</td><td style="text-align: left" valign="top">
<p>0, 1, 2…</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Regression</p>
</td><td style="text-align: left" valign="top">
<p>Decimal values</p>
</td></tr></tbody></table></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec68"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$spark-shell</strong>
</pre></div></li><li class="listitem">Import the MLlib vector explicitly:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong>
</pre></div></li><li class="listitem">Import the <code class="literal">LabeledPoint</code>:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong>
</pre></div></li><li class="listitem">Create a labeled point with a positive label and dense vector:<div><pre class="programlisting">
<strong>scala&gt; val willBuySUV = LabeledPoint(1.0,Vectors.dense(300.0,80,40))</strong>
</pre></div></li><li class="listitem">Create <a id="id331" class="indexterm"/>a labeled point with a negative label and dense vector:<div><pre class="programlisting">
<strong>scala&gt; val willNotBuySUV = LabeledPoint(0.0,Vectors.dense(150.0,60,25))</strong>
</pre></div></li><li class="listitem">Create a labeled point with a positive label and sparse vector:<div><pre class="programlisting">
<strong>scala&gt; val willBuySUV = LabeledPoint(1.0,Vectors.sparse(3,Array(0,1,2),Array(300.0,80,40)))</strong>
</pre></div></li><li class="listitem">Create a labeled point with a negative label and sparse vector:<div><pre class="programlisting">
<strong>scala&gt; val willNotBuySUV = LabeledPoint(0.0,Vectors.sparse(3,Array(0,1,2),Array(150.0,60,25)))</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">libsvm</code> file with the same data:<div><pre class="programlisting">
<strong>$vi person_libsvm.txt (libsvm indices start with 1)</strong>
<strong>0  1:150 2:60 3:25</strong>
<strong>1  1:300 2:80 3:40</strong>
</pre></div></li><li class="listitem">Upload <code class="literal">person_libsvm.txt</code> to <code class="literal">hdfs</code>:<div><pre class="programlisting">
<strong>$ hdfs dfs -put person_libsvm.txt person_libsvm.txt</strong>
</pre></div></li><li class="listitem">Do a few more imports:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong>
<strong>scala&gt; import org.apache.spark.rdd.RDD</strong>
</pre></div></li><li class="listitem">Load data from <code class="literal">libsvm</code> file:<div><pre class="programlisting">
<strong>scala&gt; val persons = MLUtils.loadLibSVMFile(sc,"person_libsvm.txt")</strong>
</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Creating matrices</h1></div></div></div><p>Matrix is<a id="id332" class="indexterm"/> simply a table to represent multiple feature vectors. A matrix that <a id="id333" class="indexterm"/>can be stored on one machine is called <strong>local matrix</strong> and the <a id="id334" class="indexterm"/>one<a id="id335" class="indexterm"/> that can be distributed<a id="id336" class="indexterm"/> across <a id="id337" class="indexterm"/>the cluster is called <strong>distributed matrix</strong>.</p><p>Local matrices have integer-based indices, while distributed matrices have long-based indices. Both have values as doubles.</p><p>There are three types of distributed matrices:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">RowMatrix</code>: This has <a id="id338" class="indexterm"/>each row as a feature vector.</li><li class="listitem" style="list-style-type: disc"><code class="literal">IndexedRowMatrix</code>: This<a id="id339" class="indexterm"/> also has row indices.</li><li class="listitem" style="list-style-type: disc"><code class="literal">CoordinateMatrix</code>: This<a id="id340" class="indexterm"/> is simply a matrix of <code class="literal">MatrixEntry</code>. A <code class="literal">MatrixEntry</code> represents an entry in the matrix represented by its row and column index.</li></ul></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec69"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the<a id="id341" class="indexterm"/> Spark shell:<div><pre class="programlisting">
<strong>$spark-shell</strong>
</pre></div></li><li class="listitem">Import the matrix-related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Matrix, Matrices}</strong>
</pre></div></li><li class="listitem">Create a dense local matrix:<div><pre class="programlisting">
<strong>scala&gt; val people = Matrices.dense(3,2,Array(150d,60d,25d, 300d,80d,40d))</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">personRDD</code> as RDD of vectors:<div><pre class="programlisting">
<strong>scala&gt; val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))</strong>
</pre></div></li><li class="listitem">Import <code class="literal">RowMatrix</code> and related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix,RowMatrix, CoordinateMatrix, MatrixEntry}</strong>
</pre></div></li><li class="listitem">Create a row matrix of <code class="literal">personRDD</code>:<div><pre class="programlisting">
<strong>scala&gt; val personMat = new RowMatrix(personRDD)</strong>
</pre></div></li><li class="listitem">Print the number of rows:<div><pre class="programlisting">
<strong>scala&gt; print(personMat.numRows)</strong>
</pre></div></li><li class="listitem">Print the number of columns:<div><pre class="programlisting">
<strong>scala&gt; print(personMat.numCols)</strong>
</pre></div></li><li class="listitem">Create an RDD of indexed rows:<div><pre class="programlisting">
<strong>scala&gt; val personRDD = sc.parallelize(List(IndexedRow(0L, Vectors.dense(150,60,25)), IndexedRow(1L, Vectors.dense(300,80,40))))</strong>
</pre></div></li><li class="listitem">Create an indexed row matrix:<div><pre class="programlisting">
<strong>scala&gt; val pirmat = new IndexedRowMatrix(personRDD)</strong>
</pre></div></li><li class="listitem">Print the number of rows:<div><pre class="programlisting">
<strong>scala&gt; print(pirmat.numRows)</strong>
</pre></div></li><li class="listitem">Print the number of columns:<div><pre class="programlisting">
<strong>scala&gt; print(pirmat.numCols)</strong>
</pre></div></li><li class="listitem">Convert the indexed row matrix back to row matrix:<div><pre class="programlisting">
<strong>scala&gt; val personMat = pirmat.toRowMatrix</strong>
</pre></div></li><li class="listitem">Create an RDD of matrix entries:<div><pre class="programlisting">
<strong>scala&gt; val meRDD = sc.parallelize(List(</strong>
<strong>  MatrixEntry(0,0,150),</strong>
<strong>  MatrixEntry(1,0,60),</strong>
<strong>MatrixEntry(2,0,25),</strong>
<strong>MatrixEntry(0,1,300),</strong>
<strong>MatrixEntry(1,1,80),</strong>
<strong>MatrixEntry(2,1,40)</strong>
<strong>))</strong>
</pre></div></li><li class="listitem">Create a <a id="id342" class="indexterm"/>coordinate matrix:<div><pre class="programlisting">
<strong>scala&gt; val pcmat = new CoordinateMatrix(meRDD)</strong>
</pre></div></li><li class="listitem">Print the number of rows:<div><pre class="programlisting">
<strong>scala&gt; print(pcmat.numRows)</strong>
</pre></div></li><li class="listitem">Print the number of columns:<div><pre class="programlisting">
<strong>scala&gt; print(pcmat.numCols)</strong>
</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Calculating summary statistics</h1></div></div></div><p>Summary statistics is<a id="id343" class="indexterm"/> used to summarize observations to get a<a id="id344" class="indexterm"/> collective sense of the data. The summary includes the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Central tendency of data—mean, mode, median</li><li class="listitem" style="list-style-type: disc">Spread of data—variance, standard deviation</li><li class="listitem" style="list-style-type: disc">Boundary conditions—min, max</li></ul></div><p>This recipe covers how to produce summary statistics.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec70"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import the matrix-related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong>
<strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">personRDD</code> as RDD of vectors:<div><pre class="programlisting">
<strong>scala&gt; val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))</strong>
</pre></div></li><li class="listitem">Compute<a id="id345" class="indexterm"/> the column summary statistics:<div><pre class="programlisting">
<strong>scala&gt; val summary = Statistics.colStats(personRDD)</strong>
</pre></div></li><li class="listitem">Print the mean of this summary:<div><pre class="programlisting">
<strong>scala&gt; print(summary.mean)</strong>
</pre></div></li><li class="listitem">Print the variance:<div><pre class="programlisting">
<strong>scala&gt; print(summary.variance)</strong>
</pre></div></li><li class="listitem">Print the non-zero values in each column:<div><pre class="programlisting">
<strong>scala&gt; print(summary.numNonzeros)</strong>
</pre></div></li><li class="listitem">Print the sample size:<div><pre class="programlisting">
<strong>scala&gt; print(summary.count)</strong>
</pre></div></li><li class="listitem">Print the max value of each column:<div><pre class="programlisting">
<strong>scala&gt; print(summary.max)</strong>
</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Calculating correlation</h1></div></div></div><p>Correlation is a<a id="id346" class="indexterm"/> statistical relationship between two variables such that when <a id="id347" class="indexterm"/>one variable changes, it leads to a change in the other variable. Correlation analysis measures the extent to which the two variables are correlated.</p><p>If an increase<a id="id348" class="indexterm"/> in one variable leads to an increase in another, it is <a id="id349" class="indexterm"/>called a <strong>positive correlation</strong>. If an increase in one variable leads to a decrease<a id="id350" class="indexterm"/> in the other, it is<a id="id351" class="indexterm"/> a <strong>negative correlation</strong>.</p><p>Spark supports two correlation algorithms: Pearson and Spearman. Pearson algorithm works with two continuous variables, such as a person's height and weight or house size and house price. Spearman deals with one continuous and one categorical variable, for example, zip code and house price.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec71"/>Getting ready</h2></div></div></div><p>Let's use some real data so that we can calculate correlation more meaningfully. The following are <a id="id352" class="indexterm"/>the size and price of houses in the City of Saratoga, California, in early 2014:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>House size (sq ft)</p>
</th><th style="text-align: left" valign="bottom">
<p>Price</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>2100</p>
</td><td style="text-align: left" valign="top">
<p>$1,620,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2300</p>
</td><td style="text-align: left" valign="top">
<p>$1,690,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2046</p>
</td><td style="text-align: left" valign="top">
<p>$1,400,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4314</p>
</td><td style="text-align: left" valign="top">
<p>$2,000,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1244</p>
</td><td style="text-align: left" valign="top">
<p>$1,060,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4608</p>
</td><td style="text-align: left" valign="top">
<p>$3,830,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2173</p>
</td><td style="text-align: left" valign="top">
<p>$1,230,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2750</p>
</td><td style="text-align: left" valign="top">
<p>$2,400,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4010</p>
</td><td style="text-align: left" valign="top">
<p>$3,380,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1959</p>
</td><td style="text-align: left" valign="top">
<p>$1,480,000</p>
</td></tr></tbody></table></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec72"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import the statistics and related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg._</strong>
<strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong>
</pre></div></li><li class="listitem">Create an RDD of house sizes:<div><pre class="programlisting">
<strong>scala&gt; val sizes = sc.parallelize(List(2100, 2300, 2046, 4314, 1244, 4608, 2173, 2750, 4010, 1959.0))</strong>
</pre></div></li><li class="listitem">Create an RDD of house prices:<div><pre class="programlisting">
<strong>scala&gt; val prices = sc.parallelize(List(1620000 , 1690000, 1400000, 2000000, 1060000, 3830000, 1230000, 2400000, 3380000, 1480000.00))</strong>
</pre></div></li><li class="listitem">Compute the correlation:<div><pre class="programlisting">
<strong>scala&gt; val correlation = Statistics.corr(sizes,prices)</strong>
<strong>correlation: Double = 0.8577177736252577 </strong>
</pre></div><p><code class="literal">0.85</code> means a very strong positive correlation.</p><p>Since we do not have a specific algorithm here, it is, by default, Pearson. The <code class="literal">corr</code> method is overloaded to take the algorithm name as the third parameter.</p></li><li class="listitem">Compute <a id="id353" class="indexterm"/>the correlation with Pearson:<div><pre class="programlisting">
<strong>scala&gt; val correlation = Statistics.corr(sizes,prices)</strong>
</pre></div></li><li class="listitem">Compute the correlation with Spearman:<div><pre class="programlisting">
<strong>scala&gt; val correlation = Statistics.corr(sizes,prices,"spearman")</strong>
</pre></div></li></ol></div><p>Both the variables in the preceding example are continuous, so Spearman assumes the size to be discrete. A better example of Spearman's use would be zip code versus price.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec49"/>Doing hypothesis testing</h1></div></div></div><p>Hypothesis testing<a id="id354" class="indexterm"/> is a way of determining probability that a given hypothesis is true. Let's say a sample data suggests that females tend to vote more for the Democratic Party. This may or may not be true for the larger population. What if this<a id="id355" class="indexterm"/> pattern is there in the sample data just by chance?</p><p>Another way to look at the goal of hypothesis testing is to answer this question: If a sample has a pattern in it, what are the chances of the pattern being there just by chance?</p><p>How do we do it? There is a saying that the best way to prove something is to try to disprove it.</p><p>The hypothesis to <a id="id356" class="indexterm"/>disprove is called <strong>null hypothesis</strong>. Hypothesis testing works with categorical data. Let's look at the example of a gallop poll of party affiliations.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Party</p>
</th><th style="text-align: left" valign="bottom">
<p>Male</p>
</th><th style="text-align: left" valign="bottom">
<p>Female</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Democratic Party</p>
</td><td style="text-align: left" valign="top">
<p>32</p>
</td><td style="text-align: left" valign="top">
<p>41</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Republican Party</p>
</td><td style="text-align: left" valign="top">
<p>28</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Independent</p>
</td><td style="text-align: left" valign="top">
<p>34</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td></tr></tbody></table></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec73"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import the relevant classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong>
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Vector,Vectors}</strong>
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Matrix, Matrices}</strong>
</pre></div></li><li class="listitem">Create a vector for the Democratic Party:<div><pre class="programlisting">
<strong>scala&gt; val dems = Vectors.dense(32.0,41.0)</strong>
</pre></div></li><li class="listitem">Create a <a id="id357" class="indexterm"/>vector for the Republican Party:<div><pre class="programlisting">
<strong>scala&gt; val reps= Vectors.dense(28.0,25.0)</strong>
</pre></div></li><li class="listitem">Create a vector for the Independents:<div><pre class="programlisting">
<strong>scala&gt; val indies = Vectors.dense(34.0,26.0)</strong>
</pre></div></li><li class="listitem">Do the chi-square goodness of fit test of the observed data against uniform distribution:<div><pre class="programlisting">
<strong>scala&gt; val dfit = Statistics.chiSqTest(dems)</strong>
<strong>scala&gt; val rfit = Statistics.chiSqTest(reps)</strong>
<strong>scala&gt; val ifit = Statistics.chiSqTest(indies)</strong>
</pre></div></li><li class="listitem">Print the goodness of fit results:<div><pre class="programlisting">
<strong>scala&gt; print(dfit)</strong>
<strong>scala&gt; print(rfit)</strong>
<strong>scala&gt; print(ifit)</strong>
</pre></div></li><li class="listitem">Create the input matrix:<div><pre class="programlisting">
<strong>scala&gt; val mat = Matrices.dense(2,3,Array(32.0,41.0, 28.0,25.0, 34.0,26.0))</strong>
</pre></div></li><li class="listitem">Do the chi-square independence test:<div><pre class="programlisting">
<strong>scala&gt; val in = Statistics.chiSqTest(mat)</strong>
</pre></div></li><li class="listitem">Print the independence test results:<div><pre class="programlisting">
<strong>scala&gt; print(in)</strong>
</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec50"/>Creating machine learning pipelines using ML</h1></div></div></div><p>Spark ML is a <a id="id358" class="indexterm"/>new library in Spark to<a id="id359" class="indexterm"/> build machine learning pipelines. This library is being developed along with MLlib. It helps to combine multiple machine learning algorithms into a single pipeline, and uses DataFrame as dataset.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec74"/>Getting ready</h2></div></div></div><p>Let's first understand some of the basic concepts in Spark ML. It uses transformers to transform one DataFrame into another DataFrame. One example of simple transformations can be to append a column. You can think of it as being equivalent to "alter table" in relational world.</p><p>Estimator, on<a id="id360" class="indexterm"/> the other hand, represents a machine learning algorithm, which learns from the data. Input to an estimator is a DataFrame and output is a transformer. Every Estimator has a <code class="literal">fit()</code> method, which does the job of training the algorithm.</p><p>A machine<a id="id361" class="indexterm"/> learning pipeline is <a id="id362" class="indexterm"/>defined as a sequence of stages; each stage can be either an estimator or a transformer.</p><p>The example we are going to use in this recipe is whether someone is a basketball player or not a basketball player. For this, we are going to have a pipeline of one estimator and one transformer.</p><p>Estimator gets training data to train the algorithms and then transformer makes predictions.</p><p>For now, assume <code class="literal">LogisticRegression</code> to be the machine learning algorithm we are using. We will explain the details about <code class="literal">LogisticRegression</code> along with other algorithms in the subsequent chapters.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec75"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Do the imports:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.{Vector,Vectors}</strong>
<strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong>
<strong>scala&gt; import org.apache.spark.ml.classification.LogisticRegression</strong>
</pre></div></li><li class="listitem">Create a labeled point for Lebron who is a basketball player, is 80 inches tall height and weighs 250 lbs:<div><pre class="programlisting">
<strong>scala&gt; val lebron = LabeledPoint(1.0,Vectors.dense(80.0,250.0))</strong>
</pre></div></li><li class="listitem">Create a labeled point for Tim who is not a basketball player, is 70 inches tall height and weighs 150 lbs:<div><pre class="programlisting">
<strong>scala&gt; val tim = LabeledPoint(0.0,Vectors.dense(70.0,150.0))</strong>
</pre></div></li><li class="listitem">Create a labeled point for Brittany who is a basketball player, is 80 inches tall height and weighs 207 lbs:<div><pre class="programlisting">
<strong>scala&gt; val brittany = LabeledPoint(1.0,Vectors.dense(80.0,207.0))</strong>
</pre></div></li><li class="listitem">Create a labeled point for Stacey who is not a basketball player, is 65 inches tall, and weighs 120 lbs:<div><pre class="programlisting">
<strong>scala&gt; val stacey = LabeledPoint(0.0,Vectors.dense(65.0,120.0))</strong>
</pre></div></li><li class="listitem">Create a training RDD:<div><pre class="programlisting">
<strong>scala&gt; val trainingRDD = sc.parallelize(List(lebron,tim,brittany,stacey))</strong>
</pre></div></li><li class="listitem">Create a training DataFrame:<div><pre class="programlisting">
<strong>scala&gt; val trainingDF = trainingRDD.toDF</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">LogisticRegression</code> estimator:<div><pre class="programlisting">
<strong>scala&gt; val estimator = new LogisticRegression</strong>
</pre></div></li><li class="listitem">Create <a id="id363" class="indexterm"/>a transformer<a id="id364" class="indexterm"/> by fitting the estimator with training DataFrame:<div><pre class="programlisting">
<strong>scala&gt; val transformer = estimator.fit(trainingDF)</strong>
</pre></div></li><li class="listitem">Now, let's create a test data—John is 90 inches tall and weighs 270 lbs, and is a basketball player:<div><pre class="programlisting">
<strong>scala&gt; val john = Vectors.dense(90.0,270.0)</strong>
</pre></div></li><li class="listitem">Create another test data—Tom is 62 inches tall and weighs 150 lbs, and is not a basketball player:<div><pre class="programlisting">
<strong>scala&gt; val tom = Vectors.dense(62.0,120.0)</strong>
</pre></div></li><li class="listitem">Create a training RDD:<div><pre class="programlisting">
<strong>scala&gt; val testRDD = sc.parallelize(List(john,tom))</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">Features</code> case class:<div><pre class="programlisting">
<strong>scala&gt; case class Feature(v:Vector)</strong>
</pre></div></li><li class="listitem">Map the <code class="literal">testRDD</code> to an RDD for <code class="literal">Features</code>:<div><pre class="programlisting">
<strong>scala&gt; val featuresRDD = testRDD.map( v =&gt; Feature(v))</strong>
</pre></div></li><li class="listitem">Convert <code class="literal">featuresRDD</code> into a DataFrame with column name <code class="literal">"features"</code>:<div><pre class="programlisting">
<strong>scala&gt; val featuresDF = featuresRDD.toDF("features")</strong>
</pre></div></li><li class="listitem">Transform <code class="literal">featuresDF</code> by adding the <code class="literal">predictions</code> column to it:<div><pre class="programlisting">
<strong>scala&gt; val predictionsDF = transformer.transform(featuresDF)</strong>
</pre></div></li><li class="listitem">Print the <code class="literal">predictionsDF</code>:<div><pre class="programlisting">
<strong>scala&gt; predictionsDF.foreach(println)</strong>
</pre></div></li><li class="listitem"><code class="literal">PredictionDF</code>, as you can see, creates three columns—<code class="literal">rawPrediction</code>, <code class="literal">probability</code>, and <code class="literal">prediction</code>—besides keeping features. Let's select only <code class="literal">features</code> and <code class="literal">prediction</code>:<div><pre class="programlisting">
<strong>scala&gt; val shorterPredictionsDF = predictionsDF.select("features","prediction")</strong>
</pre></div></li><li class="listitem">Rename <a id="id365" class="indexterm"/>the prediction <a id="id366" class="indexterm"/>to <code class="literal">isBasketBallPlayer</code>:<div><pre class="programlisting">
<strong>scala&gt; val playerDF = shorterPredictionsDF.toDF("features","isBasketBallPlayer")</strong>
</pre></div></li><li class="listitem">Print the schema for <code class="literal">playerDF</code>:<div><pre class="programlisting">
<strong>scala&gt; playerDF.printSchema</strong>
</pre></div></li></ol></div></div></div></body></html>