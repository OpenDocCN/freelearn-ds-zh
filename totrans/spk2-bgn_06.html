<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0; Spark Stream Processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6.   Spark Stream Processing  </h1></div></div></div><p>Data processing use cases can be mainly divided into two types. The first type is the use cases where the data is static and processing is done in its entirety as one unit of work, or by dividing it into smaller batches. While doing the data processing, the underlying data set does not change nor do new data sets get added to the processing units. This is batch processing.</p><p>The second type is the use cases where the data is getting generated like a stream, and the processing is done as and when the data is generated. This is stream processing. In the previous chapters of this book, all the data processing use cases were pertaining to the former type. This chapter is going to focus on the latter.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data stream processing</li><li class="listitem" style="list-style-type: disc">Micro batch data processing</li><li class="listitem" style="list-style-type: disc">A log event processor</li><li class="listitem" style="list-style-type: disc">Windowed data processing and other options</li><li class="listitem" style="list-style-type: disc">Kafka stream processing</li><li class="listitem" style="list-style-type: disc">Streaming jobs with Spark</li></ul></div><div class="section" title="Data stream processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Data stream processing</h1></div></div></div><p>Data sources generate data like a stream, and many real-world use cases require them to be processed in real time. The meaning of <span class="emphasis"><em>real time</em></span> can change from use case to use case. The main parameter that defines what is meant by real time for a given use case is how soon the ingested data or the frequent interval in which all the data ingested since the last interval needs to be processed. For example, when a major sports event is happening, the application that consumes the score events and sends them to the subscribed users should be processing the data as fast as it can. The faster can be sent, the better it is.</p><p>But what is the definition of <span class="emphasis"><em>fast</em></span> here? Is it fine to process the score data within, say, an hour of the score event happening? Probably not. Is it fine to process the data within a minute of the score event happening? It is definitely better than processing after an hour. Is it fine to process the data within a second of the score event happening? Probably yes, and much better than the earlier data processing time intervals.</p><p>In any data stream processing use cases, this time interval is very important. The data processing framework should have the capability to process the data stream in an appropriate time interval of choice to deliver good business value.</p><p>When processing stream data in regular intervals of choice, the data is collected from the beginning of the time interval to the end of the time interval, grouped in a micro batch, and data processing is done on that batch of data. Over an extended period of time, the data processing application would have processed many such micro batches of data. In this type of processing, the data processing application will have visibility of only the specific micro batch that is getting processed at a given point in time. In other words, the application will not have any visibility or access to the already processed micro batches of data.</p><p>Now, there is another dimension to this type of processing. Suppose a given use case mandates the need to process the data every minute, but at the same time, while processing the data of a given micro batch, there is a need to peek into the data that was already processed in the last 15 minutes. A fraud detection module of a retail banking transaction processing application is a good example of this particular business requirement. There is no doubt that the retail banking transactions are to be processed within milliseconds of their occurrence. When processing an ATM cash withdrawal transaction, it is a good idea to see whether somebody is trying to continuously withdraw cash and, if found, send the proper alert. For this, when processing a given cash withdrawal transaction, the application checks whether there are any other cash withdrawals from the same ATM using the same card that was used in the last 15 minutes. The business rule is to send an alert when such transactions are more than two in the last 15 minutes. In this use case, the fraud detection application should have visibility of all the transactions that happened in a window of 15 minutes.</p><p>A good stream data processing framework should have the ability to processing the data in any given interval of time, as well as the ability to peek into the data ingested within a sliding window of time. The Spark Streaming library that is working on top of Spark is one of the best data stream processing frameworks that has both of these capabilities.</p><p>Look again at the bigger picture of the Spark library stack as given in <span class="emphasis"><em>Figure 1</em></span> to set the context and see what is being discussed here before getting into and taking up the use cases.</p><p>
</p><div class="mediaobject"><img alt="Data stream processing" src="graphics/image_06_001.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p></div></div>
<div class="section" title="Micro batch data processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Micro batch data processing</h1></div></div></div><p>Every Spark Streaming data processing application will be running continuously till it is terminated. This application will be constantly <span class="emphasis"><em>listening</em></span> to the data source to receive the incoming stream of data. The Spark Streaming data processing application would have a configured batch interval. At the end of every batch interval, it will produce a data abstraction named <span class="strong"><strong>Discretized Stream</strong></span> (<span class="strong"><strong>DStream</strong></span>) which works very similar to Spark's RDD. Just like RDD, a DStream supports an equivalents method for the commonly used Spark transformations and Spark actions. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip41"/>Tip</h3><p>Just like RDD, a DStream is also immutable and distributed.</p></div></div><p>
<span class="emphasis"><em>Figure 2</em></span> shows how DStreams are being produced in a Spark Streaming data processing application.</p><p>
</p><div class="mediaobject"><img alt="Micro batch data processing" src="graphics/image_06_004.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p><p>
<span class="emphasis"><em>Figure 2</em></span> depicts the most important elements of a Spark Streaming application. For the configured batch interval, the application produces one DStream. Each DStream is a collection of RDDs consisting of the data collected within that batch interval. The number of RDDs within a DStream for a given batch interval will vary. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip42"/>Tip</h3><p>Since Spark Streaming applications are continuously running applications collecting data, in this chapter, rather than running the code in REPL, the complete application is discussed, including the instructions to compile, package and run.</p></div></div><p>The Spark programming model was discussed in <a class="link" href="ch02.html" title="Chapter 2. Spark Programming Model">Chapter 2</a>, <span class="emphasis"><em>Spark Programming Model</em></span>.</p><div class="section" title="Programming with DStreams"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec29"/>Programming with DStreams</h2></div></div></div><p>Programming with DStreams in a Spark Streaming data processing application also follows a very similar model, as DStreams consist of one or more RDDs. When methods such as Spark transformations or Spark actions are invoked on a DStream, the equivalent operation is applied to all the RDDs that constitute the DStream.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>An important point to note here is that not all the Spark transformations and Spark actions that work on RDD are unsupported on DStreams. The other notable change is the differences in capability across programming languages.</p><p>The Scala and Java APIs for Spark Streaming are ahead of the Python API in terms of the number of features supported for Spark Streaming data processing application development.</p></div></div><p>
<span class="emphasis"><em>Figure 3</em></span> depicts how methods applied on a DStream are applied on the underlying RDDs. The Spark Streaming programming guide is to be consulted before using any of the methods on DStreams. The Spark Streaming programming guide is marked with special callouts containing the text <span class="emphasis"><em>Python API</em></span> wherever the Python API deviates from its Scala or Java counterparts.</p><p>Assume that, for a given batch interval in a Spark Streaming data processing application, a DStream is generated consisting of multiple RDDs. When a filter method is applied on that DStream, here is how it gets translated into the underlying RDDs.<span class="emphasis"><em> Figure 3</em></span> shows a filter transformation applied on a DStream with two RDDs, resulting in another DStream containing only one RDD because of the filter condition: </p><p>
</p><div class="mediaobject"><img alt="Programming with DStreams" src="graphics/image_06_003.jpg"/><div class="caption"><p>Figure 3</p></div></div><p>
</p></div></div>
<div class="section" title="A log event processor"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec48"/>A log event processor</h1></div></div></div><p>These days, it is very common to have a central repository of application log events in many enterprises. Also, the log events are streamed live to data processing applications in order to monitor the performance of the running applications on a real-time basis so that timely remediation measures can be taken. Such a use case is discussed here to demonstrate the real-time processing of log events using a Spark Streaming data processing application. In this use case, the live application log events are written to a TCP socket. The Spark Streaming data processing application constantly listens to a given port on a given host to collect the stream of log events. </p><div class="section" title="Getting ready with the Netcat server"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec30"/>Getting ready with the Netcat server</h2></div></div></div><p>The Netcat utility that comes with most UNIX installations is used here as the data server. To make sure that Netcat is installed in the system, type the manual command as given in the following scripts, and, after coming out of it, run it and make sure that there is no error message. Once the server is up and running, whatever is typed in the standard input of the Netcat server console is considered as the application logs events for simplicity and demonstration purposes. The following commands run from a terminal prompt will start the Netcat data server on localhost port <code class="literal">9999</code>:</p><pre class="programlisting">
<span class="strong"><strong>$ man nc</strong></span>
<span class="strong"><strong>
NC(1)          BSD General Commands Manual
NC(1) 
NAME
     nc -- arbitrary TCP and UDP connections and listens 
SYNOPSIS
     nc [-46AcDCdhklnrtUuvz] [-b boundif] [-i interval] [-p source_port] [-s source_ip_address] [-w timeout] [-X proxy_protocol] [-x proxy_address[:port]]
        [hostname] [port[s]]
 DESCRIPTION
     The nc (or netcat) utility is used for just about anything under the sun involving TCP or UDP.  It can open TCP connections, send UDP packets, listen on
     arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6.  Unlike telnet(1), nc scripts nicely, and separates error messages onto
     standard error instead of sending them to standard output, as telnet(1) does with some. 
     Common uses include: 
           o   simple TCP proxies
           o   shell-script based HTTP clients and servers
           o   network daemon testing
           o   a SOCKS or HTTP ProxyCommand for ssh(1)
           o   and much, much more
$ nc -lk 9999</strong></span>
</pre><p>Once the preceding steps are completed, the Netcat server is ready and the Spark Streaming data processing application will process all the lines that are typed in the previous console window. Leave this console window alone; all the following shell commands will be run in a different terminal window.</p><p>Since there is a lack of parity of Spark Streaming features between different programming languages, the Scala code is used to explain all the Spark Streaming concepts and use cases. After that, the Python code is given, and, if there is a lack of support for any of the features being discussed in Python, that is also captured.</p><p>The Scala and Python code are organized in the way demonstrated in <span class="emphasis"><em>Figure 4</em></span>. For the compilation, packaging and running of the code, bash scripts are used so that it is easy for the readers to run them to produce consistent results. Each of these script file contents are discussed here.</p></div><div class="section" title="Organizing files"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec31"/>Organizing files</h2></div></div></div><p>In the following folder tree, the <code class="literal">project</code>and <code class="literal">target</code>folders are created at runtime. The source code that comes with this book can be copied directly to a convenient folder in the system: </p><p>
</p><div class="mediaobject"><img alt="Organizing files" src="graphics/image_06_007.jpg"/><div class="caption"><p>Figure 4</p></div></div><p>
</p><p>For compiling and packaging, the <span class="strong"><strong>Scala build tool</strong></span> (<span class="strong"><strong>sbt</strong></span>) is used. In order to make sure that sbt is working properly, run the following commands from the <code class="literal">Scala</code> folder of the tree in <span class="emphasis"><em>Figure 4</em></span> in the terminal window. This is to make sure that sbt is working fine and the code is compiling:</p><pre class="programlisting">
<span class="strong"><strong>$ cd Scala
$ sbt
&gt; compile</strong></span>
<span class="strong"><strong>
      [success] Total time: 1 s, completed 24 Jul, 2016 8:39:04 AM   </strong></span>
<span class="strong"><strong>
	  &gt; exit
	  $</strong></span>
</pre><p>The following table captures the representative sample list of files and the purpose of each of them in the context of the Spark Streaming data processing application that is being discussed here.</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>File Name</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">README.txt</code>
</p>
</td><td>
<p>Instructions to run the application. One for the Scala application and the other one for the Python application.</p>
</td></tr><tr><td>
<p>
<code class="literal">submitPy.sh</code>
</p>
</td><td>
<p>Bash script to submit the Python job to the Spark cluster.</p>
</td></tr><tr><td>
<p>
<code class="literal">compile.sh</code>
</p>
</td><td>
<p>Bash script to compile the Scala code.</p>
</td></tr><tr><td>
<p>
<code class="literal">submit.sh</code>
</p>
</td><td>
<p>Bash script to submit the Scala job to the Spark cluster.</p>
</td></tr><tr><td>
<p>
<code class="literal">config.sbt</code>
</p>
</td><td>
<p>The sbt configuration file.</p>
</td></tr><tr><td>
<p>
<code class="literal">*.scala</code>
</p>
</td><td>
<p>Spark Streaming data processing application code in Scala.</p>
</td></tr><tr><td>
<p>
<code class="literal">*.py</code>
</p>
</td><td>
<p>Spark Streaming data processing application code in Python.</p>
</td></tr><tr><td>
<p>
<code class="literal">*.jar</code>
</p>
</td><td>
<p>The Spark Streaming and Kafka integration JAR file that needs to be downloaded and placed under the <code class="literal">lib</code> folder for the proper functioning of the applications. This is being used in <code class="literal">submit.sh</code> as well as in <code class="literal">submitPy.sh</code> for submitting the job to the cluster.</p>
</td></tr></tbody></table></div></div><div class="section" title="Submitting the jobs to the Spark cluster"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec32"/>Submitting the jobs to the Spark cluster</h2></div></div></div><p>To properly run the application, some of the configurations depend on the system in which it is being run. They are to be edited in the <code class="literal">submit.sh</code> file and the <code class="literal">submitPy.sh</code> file. Wherever such edits are required, comments are given with the<code class="literal"> [FILLUP]</code> tag. The most important of these are the setting of the Spark installation directory and the Spark master configuration, which can differ from system to system. The source of the preceding script <code class="literal">submit.sh</code> file is given as follows:</p><pre class="programlisting">
<span class="strong"><strong>#!/bin/bash
	  #-----------
	  # submit.sh
	  #-----------
	  # IMPORTANT - Assumption is that the $SPARK_HOME and $KAFKA_HOME environment variables are already set in the system that is running the application
	  # [FILLUP] Which is your Spark master. If monitoring is needed, use the desired Spark master or use local
	  # When using the local mode. It is important to give more than one cores in square brackets
	  #SPARK_MASTER=spark://Rajanarayanans-MacBook-Pro.local:7077
	  SPARK_MASTER=local[4]
	  # [OPTIONAL] Your Scala version
	  SCALA_VERSION="2.11"
	  # [OPTIONAL] Name of the application jar file. You should be OK to leave it like that
	  APP_JAR="spark-for-beginners_$SCALA_VERSION-1.0.jar"
	  # [OPTIONAL] Absolute path to the application jar file
	  PATH_TO_APP_JAR="target/scala-$SCALA_VERSION/$APP_JAR"
	  # [OPTIONAL] Spark submit commandSPARK_SUBMIT="$SPARK_HOME/bin/spark-submit"
	  # [OPTIONAL] Pass the application name to run as the parameter to this script
	  APP_TO_RUN=$1
	  sbt package
	  if [ $2 -eq 1 ]
	  then
	  $SPARK_SUBMIT --class $APP_TO_RUN --master $SPARK_MASTER --jars $KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar,$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar,$KAFKA_HOME/libs/metrics-core-2.2.0.jar,$KAFKA_HOME/libs/zkclient-0.3.jar,./lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar $PATH_TO_APP_JAR
	  else
	  $SPARK_SUBMIT --class $APP_TO_RUN --master $SPARK_MASTER --jars $PATH_TO_APP_JAR $PATH_TO_APP_JAR
	  fi</strong></span>
</pre><p>
	  The source of the preceding script file <code class="literal">submitPy.sh</code> is given as follows:
	  </p><pre class="programlisting">
<span class="strong"><strong>
	  #!/usr/bin/env bash
	  #------------
	  # submitPy.sh
	  #------------
	  # IMPORTANT - Assumption is that the $SPARK_HOME and $KAFKA_HOME environment variables are already set in the system that is running the application
	  # Disable randomized hash in Python 3.3+ (for string) Otherwise the following exception will occur
	  # raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")
	  # Exception: Randomness of hash of string should be disabled via PYTHONHASHSEED
	  export PYTHONHASHSEED=0
	  # [FILLUP] Which is your Spark master. If monitoring is needed, use the desired Spark master or use local
	  # When using the local mode. It is important to give more than one cores in square brackets
	  #SPARK_MASTER=spark://Rajanarayanans-MacBook-Pro.local:7077
	  SPARK_MASTER=local[4]
	  # [OPTIONAL] Pass the application name to run as the parameter to this script
	  APP_TO_RUN=$1
	  # [OPTIONAL] Spark submit command
	  SPARK_SUBMIT="$SPARK_HOME/bin/spark-submit"
	  if [ $2 -eq 1 ]
	  then
	  $SPARK_SUBMIT --master $SPARK_MASTER --jars $KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar,$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar,$KAFKA_HOME/libs/metrics-core-2.2.0.jar,$KAFKA_HOME/libs/zkclient-0.3.jar,./lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar $APP_TO_RUN
	  else
	  $SPARK_SUBMIT --master $SPARK_MASTER $APP_TO_RUN
	  fi</strong></span>
</pre></div><div class="section" title="Monitoring running applications"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec33"/>Monitoring running applications</h2></div></div></div><p>As described in <a class="link" href="ch02.html" title="Chapter 2. Spark Programming Model">Chapter 2</a>, <span class="emphasis"><em>Spark Programming Model</em></span>, Spark installation comes with a powerful Spark web UI for monitoring the Spark applications that are running.</p><p>There are additional visualizations available specifically for the Spark Streaming jobs that are running.</p><p>The following scripts start the Spark master and workers, and enable monitoring. The assumption here is that the reader has made all the configuration changes suggested in <a class="link" href="ch02.html" title="Chapter 2. Spark Programming Model">Chapter 2</a>, <span class="emphasis"><em>Spark Programming Model</em></span> to enable Spark application monitoring. If that is not done, the applications can still be run. The only change to be made is to put the cases in the <code class="literal">submit.sh</code> file and the <code class="literal">submitPy.sh</code> file to make sure that instead of the Spark master URL, something like <code class="literal">local[4]</code> is used. Run the following commands on the terminal window:</p><pre class="programlisting">
<span class="strong"><strong>
	  $ cd $SPARK_HOME
	  $ ./sbin/start-all.sh
       starting org.apache.spark.deploy.master.Master, logging to /Users/RajT/source-code/spark-source/spark-2.0/logs/spark-RajT-org.apache.spark.deploy.master.Master-1-Rajanarayanans-MacBook-Pro.local.out    </strong></span>
<span class="strong"><strong>
	   localhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/RajT/source-code/spark-source/spark-2.0/logs/spark-RajT-org.apache.spark.deploy.worker.Worker-1-Rajanarayanans-MacBook-Pro.local.out</strong></span>
</pre><p>
	  Make sure that the Spark web UI is up and running by visiting <code class="literal">http://localhost:8080/</code>.</p></div><div class="section" title="Implementing the application in Scala"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec34"/>Implementing the application in Scala</h2></div></div></div><p>The following code snippet is the Scala code for the log event processing application:</p><pre class="programlisting">
<span class="strong"><strong>
	  /**
	  The following program can be compiled and run using SBT
	  Wrapper scripts have been provided with this
	  The following script can be run to compile the code
	  ./compile.sh
	  The following script can be used to run this application in Spark
	  ./submit.sh com.packtpub.sfb.StreamingApps
	  **/
	  package com.packtpub.sfb
	  import org.apache.spark.sql.{Row, SparkSession}
	  import org.apache.spark.streaming.{Seconds, StreamingContext}
	  import org.apache.spark.storage.StorageLevel
	  import org.apache.log4j.{Level, Logger}
	  object StreamingApps{
	  def main(args: Array[String]) 
	  {
	  // Log level settings
	  	  LogSettings.setLogLevels()
	  	  // Create the Spark Session and the spark context	  
	  	  val spark = SparkSession
	  	  .builder
	  	  .appName(getClass.getSimpleName)
	  	  .getOrCreate()
	     // Get the Spark context from the Spark session for creating the streaming context
	  	  val sc = spark.sparkContext   
	      // Create the streaming context
	      val ssc = new StreamingContext(sc, Seconds(10))
	      // Set the check point directory for saving the data to recover when 
       there is a crash   ssc.checkpoint("/tmp")
	      println("Stream processing logic start")
	      // Create a DStream that connects to localhost on port 9999
	      // The StorageLevel.MEMORY_AND_DISK_SER indicates that the data will be 
       stored in memory and if it overflows, in disk as well
	      val appLogLines = ssc.socketTextStream("localhost", 9999, 
       StorageLevel.MEMORY_AND_DISK_SER)
	      // Count each log message line containing the word ERROR
	      val errorLines = appLogLines.filter(line =&gt; line.contains("ERROR"))
	      // Print the elements of each RDD generated in this DStream to the 
        console   errorLines.print()
		   // Count the number of messages by the windows and print them
		   errorLines.countByWindow(Seconds(30), Seconds(10)).print()
		   println("Stream processing logic end")
		   // Start the streaming   ssc.start()   
		   // Wait till the application is terminated             
		   ssc.awaitTermination()    }
		}object LogSettings{
		  /** 
		   Necessary log4j logging level settings are done 
		  */  def setLogLevels() {
		    val log4jInitialized = 
         Logger.getRootLogger.getAllAppenders.hasMoreElements
		     if (!log4jInitialized) {
		        // This is to make sure that the console is clean from other INFO 
            messages printed by Spark
			       Logger.getRootLogger.setLevel(Level.WARN)
			    }
			  }
			}</strong></span>
</pre><p>In the previous code snippet, there are two Scala objects. One is for setting the proper logging levels, to make sure that unwanted messages are not displayed on the console. The <code class="literal">StreamingApps</code> Scala object holds the logic of the stream processing. The following list captures the essence of the functionality:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A Spark configuration is created with the application name.</li><li class="listitem" style="list-style-type: disc">A Spark <code class="literal">StreamingContext</code> object is created, which is the heart of the stream processing. The second parameter of the <code class="literal">StreamingContext</code> constructor is the batch interval, which is 10 seconds. The line containing <code class="literal">ssc.socketTextStream</code> creates DStreams at every batch interval, which is 10 seconds here, containing the lines typed in the Netcat console.</li><li class="listitem" style="list-style-type: disc">A filter transformation is applied next on the DStream, to have only the lines containing the word <code class="literal">ERROR</code>. The filter transformation creates new DStreams containing only the filtered lines.</li><li class="listitem" style="list-style-type: disc">The next line prints the DStream contents to the console. In other words, for every batch interval, if there are lines containing the word <code class="literal">ERROR</code>, that get displayed in the console.</li><li class="listitem" style="list-style-type: disc">At the end of this data processing logic, the given <code class="literal">StreamingContext</code> is started and will run until it is terminated.</li></ul></div><p>In the previous code snippet, there is no loop construct telling the application to repeat till the running application is terminated. This is achieved by the Spark Streaming library itself. From the beginning till the termination of the data processing application, all the statements are run once. All the operations on the DStreams are repeated (internally) for every batch. If the output of the previous application is closely examined, the output from the println() statements are seen only once in the console, even though these statements are between the initialization and termination of the <code class="literal">StreamingContext</code>. That is because the <span class="emphasis"><em>magic loop</em></span> is repeating only for the statements containing original and derived DStreams.</p><p>Because of the special nature of the looping implemented in Spark Streaming applications, it is futile to give print statements and log statements within the streaming logic in the application code, like the one that is given in the code snippet. If that is a must, then these logging statements are to be instrumented within the functions that are passed to DStreams for the transformations and actions.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip44"/>Tip</h3><p>If persistence is required for the processed data, there are many output operations available for DStreams, just as there are for RDDs.</p></div></div></div><div class="section" title="Compiling and running the application"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec35"/>Compiling and running the application</h2></div></div></div><p>The following commands are run on the terminal window to compile and run the application. Instead of using <code class="literal">./compile.sh</code>, a simple sbt compile command can also be used.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>Note that, as discussed previously, the Netcat server must be running before these commands are executed.</p></div></div><pre class="programlisting">
<span class="strong"><strong>
			$ cd Scala
			$ ./compile.sh
			
      [success] Total time: 1 s, completed 24 Jan, 2016 2:34:48 PM
    
	$ ./submit.sh com.packtpub.sfb.StreamingApps
	
      Stream processing logic start    
	  
      Stream processing logic end  
	  
      -------------------------------------------                                     
    
      Time: 1469282910000 ms
    
      -------------------------------------------
    
      -------------------------------------------
    
      Time: 1469282920000 ms
    
      -------------------------------------------    </strong></span>
</pre><p>If no error messages are shown, and the results are showing in line with the previous output, the Spark Streaming data processing application has started properly.</p></div><div class="section" title="Handling the output"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Handling the output</h2></div></div></div><p>Note that the output of the print statements comes before the DStream output print. So far, nothing has been typed in the Netcat console and, therefore, there is nothing to process.</p><p>Now go to the Netcat console that was started earlier and enter the following lines of log event messages by giving a gap of few seconds to make sure that the output goes to more than one batch, where the batch size is 10 seconds:</p><pre class="programlisting">
<span class="strong"><strong>
	  [Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/
	  [Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/
	  [Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test
	  [Fri Dec 20 01:54:34 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test
	  [Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] Client sent malformed Host header
	  [Fri Dec 20 02:25:55 2015] [WARN] [client 1.2.3.4.5.6] Client sent malformed Host header
	  [Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] user test: authentication failure for "/~raj/test": Password Mismatch
	  [Mon Dec 20 23:02:01 2015] [WARN] [client 1.2.3.4.5.6] user test: authentication failure for "/~raj/test": Password Mismatch
	  </strong></span>
</pre><p>Once the log event messages are entered into the Netcat console window, the following results will start showing up in the Spark Streaming data processing application, filtering only the log event messages containing the keyword ERROR.</p><pre class="programlisting">
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283110000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index
      forbidden by rule: /home/raj/</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283190000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283200000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index
      forbidden by rule: /apache/web/test</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283250000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283260000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>[Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] Client sent 
      malformed Host header</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1469283310000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>[Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] user test:
      authentication failure for "/~raj/test": Password Mismatch</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
	  <span class="strong"><strong>Time: 1453646710000 ms</strong></span>
	  <span class="strong"><strong>-------------------------------------------</strong></span>
</pre><p>The Spark web UI (<code class="literal">http://localhost:8080/</code>) was already enabled, and Figures 5 and 6 show the Spark applications and statistics.</p><p>From the main page (after visiting the URL <code class="literal">http://localhost:8080/</code>), click the running Spark Streaming data processing application's name link to bring up the regular monitoring page. From that page, click the <span class="strong"><strong>Streaming</strong></span> tab, to reveal the page containing the streaming statistics.</p><p>The link and tab to be clicked are circled in red:</p><p>
</p><div class="mediaobject"><img alt="Handling the output" src="graphics/image_06_008.jpg"/><div class="caption"><p>Figure 5</p></div></div><p>
</p><p>From the page shown in <span class="emphasis"><em>Figure 5</em></span>, click on the circled application link; it will take you to the relevant page. From that page, once the <span class="strong"><strong>Streaming</strong></span> tab is clicked, the page containing the streaming statistics will show up as captured in <span class="emphasis"><em>Figure 6</em></span>:</p><p>
</p><div class="mediaobject"><img alt="Handling the output" src="graphics/image_06_009.jpg"/><div class="caption"><p>Figure 6</p></div></div><p>
</p><p>There are a whole lot of application statistics available from these Spark web UI pages, and exploring them extensively is a good idea to gain a deeper understanding of the behavior of the Spark Streaming data processing applications submitted. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip46"/>Tip</h3><p>Care must be taken while enabling the monitoring of streaming applications as it should not affect the performance of the application itself.</p></div></div></div><div class="section" title="Implementing the application in Python"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec37"/>Implementing the application in Python</h2></div></div></div><p>The same use case is implemented in Python, and the following code snippet saved in <code class="literal">StreamingApps.py</code>is used to do this: </p><pre class="programlisting">
<span class="strong"><strong>
	  # The following script can be used to run this application in Spark
	  # ./submitPy.sh StreamingApps.py
	  from __future__ import print_function
	  import sys
	  from pyspark import SparkContext
	  from pyspark.streaming import StreamingContext
	  if __name__ == "__main__":
	      # Create the Spark context
	      sc = SparkContext(appName="PythonStreamingApp")
	      # Necessary log4j logging level settings are done 
	      log4j = sc._jvm.org.apache.log4j
	      log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)
	      # Create the Spark Streaming Context with 10 seconds batch interval
	      ssc = StreamingContext(sc, 10)
	      # Set the check point directory for saving the data to recover when
        there is a crash
		    ssc.checkpoint("\tmp")
		    # Create a DStream that connects to localhost on port 9999
		    appLogLines = ssc.socketTextStream("localhost", 9999)
		    # Count each log messge line containing the word ERROR
		    errorLines = appLogLines.filter(lambda appLogLine: "ERROR" in appLogLine)
		    # // Print the elements of each RDD generated in this DStream to the console
		</strong></span>
		    errorLines.pprint()
		    # Count the number of messages by the windows and print them
		    errorLines.countByWindow(30,10).pprint()
		    # Start the streaming
		    ssc.start()
		    # Wait till the application is terminated   
		    ssc.awaitTermination()</pre><p>The following commands are run on the terminal window to run the Python Spark Streaming data processing application from the directory where the code is downloaded. Before running the application, in the same way that the modifications are made to the script that is used to run the Scala application, the <code class="literal">submitPy.sh</code> file also has to be changed to point to the right Spark installation directory and configure the Spark master. If monitoring is enabled, and if the submission is pointing to the right Spark master, the same Spark web UI will capture the statistics of the Python Spark Streaming data processing applications as well.</p><p>The following commands are run on the terminal window to run the Python application:</p><pre class="programlisting">
<span class="strong"><strong>
		$ cd Python
		$ ./submitPy.sh StreamingApps.py
		</strong></span>
</pre><p>Once the same log event messages used in the Scala implementation are entered into the Netcat console window, the following results will start showing up in the streaming application, filtering only the log event messages containing the keyword <code class="literal">ERROR</code>:</p><pre class="programlisting">
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:21:50</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:22:00</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] 
		Directory index forbidden by rule: /home/raj/</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:23:50</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] 
		Directory index forbidden by rule: /apache/web/test</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:25:10</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:25:20</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>[Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] 
		Client sent malformed Host header</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:26:50</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>[Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] 
		user test: authentication failure for "/~raj/test": Password Mismatch</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
		<span class="strong"><strong>Time: 2016-07-23 15:26:50</strong></span>
		<span class="strong"><strong>-------------------------------------------</strong></span>
</pre><p>If you look at the outputs from both the Scala and Python programs, you can clearly see whether there are any log event messages containing the word <code class="literal">ERROR</code> in a given batch interval. Once the data is processed, the application discards the processed data without retaining them for any future use.</p><p>In other words, the application never retains or remembers any of the log event messages from the previous batch intervals. If there is a need to capture the number of error messages, say in the last 5 minutes or so, then the previous approach will not work. We will discuss that in the next section.</p></div></div>
<div class="section" title="Windowed data processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec49"/>Windowed data processing</h1></div></div></div><p>In the Spark Streaming data processing application discussed in the previous section, assume that there is a need to count the number of log event messages containing the keyword ERROR in the previous three batches. In other words, there should be the ability to count the number of such event messages across a window of three batches. At any given point in time, the window should be sliding along with time as and when a new batch of data is available. Three important terms have been discussed here, and <span class="emphasis"><em>Figure 7</em></span> explains them. They are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Batch interval: The time interval at which a DStream is produced</li><li class="listitem" style="list-style-type: disc">Window length: The duration of the number of batch intervals where there is a need to peek into all the DStreams produced in those batch intervals</li><li class="listitem" style="list-style-type: disc">Sliding interval:  The interval at which the window operation, such as counting the event messages, is performed</li></ul></div><p>
</p><div class="mediaobject"><img alt="Windowed data processing" src="graphics/image_06_011.jpg"/><div class="caption"><p>Figure 7</p></div></div><p>
</p><p>In <span class="emphasis"><em>Figure 7</em></span>, at a given point in time, the DStreams used for the operation to be performed are enclosed in a rectangle. </p><p>In every batch interval, a new DStream is generated. Here, the window length is three and the operation to be performed in a window is counting the number of event messages in that window. The sliding interval is kept the same as the batch interval so that the counting operation is done as and when a new DStream is generated, so that the count is correct all the time. </p><p>At time <span class="strong"><strong>t2</strong></span>, the counting operation is done on the DStreams generated at times <span class="strong"><strong>t0</strong></span>, <span class="strong"><strong>t1</strong></span>, and <span class="strong"><strong>t2</strong></span>. At time <span class="strong"><strong>t3</strong></span>, the counting operation is done again since the sliding window is kept the same as the batch interval, and this time counting the events is done on the DStreams generated at time <span class="strong"><strong>t1</strong></span>, <span class="strong"><strong>t2</strong></span>, and <span class="strong"><strong>t3</strong></span>. At time <span class="strong"><strong>t4</strong></span>, the counting operation is done again, counting the events done on the DStreams generated at time <span class="strong"><strong>t2</strong></span>, <span class="strong"><strong>t3</strong></span>, and <span class="strong"><strong>t4</strong></span>. The operations continue in that fashion till the application is terminated. </p><div class="section" title="Counting the number of log event messages processed in Scala"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec38"/>Counting the number of log event messages processed in Scala</h2></div></div></div><p>In the preceding section, the processing of the log event messages is discussed. In the same application code after the printing of the log event messages containing the word <code class="literal">ERROR</code>, include the following lines of code in the Scala application:</p><pre class="programlisting">errorLines.print()errorLines.countByWindow(Seconds(30), Seconds(10)).print()</pre><p>The first parameter is the window length and the second one is the sliding window interval. This single magic line will print a count of log event messages processed once the following lines are typed in the Netcat console:</p><pre class="programlisting">
<span class="strong"><strong>[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test</strong></span>
</pre><p>The same Spark Streaming data processing application in Scala, with the additional lines of code, produces the following output:</p><pre class="programlisting">
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284630000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index 
      forbidden by rule: /home/raj/</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284630000 ms
      -------------------------------------------</strong></span>
<span class="strong"><strong>1</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284640000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index 
      forbidden by rule: /apache/web/test</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284640000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284650000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284660000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>1</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1469284670000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>0</strong></span>
</pre><p>If the output is studied properly, it can be noticed that, in the first batch interval, one log event message is processed. Obviously the count displayed is <code class="literal">1</code> for that batch interval. In the next batch interval, one more log event message is processed. The count displayed for that batch interval is <code class="literal">2</code>. In the next batch interval, no log event message is processed. But the count for that window is still <code class="literal">2</code>. For one more window, the count is displayed as <code class="literal">2</code>. Then it reduces to <code class="literal">1</code>, and then 0.</p><p>The most important point to be noted here is that, in the application codes for both Scala and Python, immediately after StreamingContext creation, the following line of code needs to be inserted to specify the checkpoint directory:</p><pre class="programlisting">ssc.checkpoint("/tmp") 
</pre></div><div class="section" title="Counting the number of log event messages processed in Python"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Counting the number of log event messages processed in Python</h2></div></div></div><p>In the Python application code, after the printing of the log event messages containing the word ERROR, include the following lines of code in the Scala application:</p><pre class="programlisting">
errorLines.pprint()
errorLines.countByWindow(30,10).pprint()</pre><p>The first parameter is the window length and the second one is the sliding window interval. This single magic line will print a count of log event messages processed once the following lines are typed in the Netcat console:</p><pre class="programlisting">
[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] 
Directory index forbidden by rule: /home/raj/
[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] 
Directory index forbidden by rule: /home/raj/
[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] 
Directory index forbidden by rule: /apache/web/test
</pre><p>The same Spark Streaming data processing application in Python, with the additional lines of code, produces the following output:</p><pre class="programlisting">
<span class="strong"><strong>------------------------------------------- 
Time: 2016-07-23 15:29:40 
------------------------------------------- 
[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ 
------------------------------------------- 
Time: 2016-07-23 15:29:40 
------------------------------------------- 
1 
------------------------------------------- 
Time: 2016-07-23 15:29:50 
------------------------------------------- 
[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test 
------------------------------------------- 
Time: 2016-07-23 15:29:50 
------------------------------------------- 
2 
------------------------------------------- 
Time: 2016-07-23 15:30:00 
------------------------------------------- 
------------------------------------------- 
Time: 2016-07-23 15:30:00 
------------------------------------------- 
2 
------------------------------------------- 
Time: 2016-07-23 15:30:10 
------------------------------------------- 
------------------------------------------- 
Time: 2016-07-23 15:30:10 
------------------------------------------- 
1 
------------------------------------------- 
Time: 2016-07-23 15:30:20 
------------------------------------------- 
------------------------------------------- 
Time: 2016-07-23 15:30:20 
-------------------------------------------</strong></span>
</pre><p>The output pattern of the Python application is also very similar to the Scala application.</p></div></div>
<div class="section" title="More processing options"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec50"/>More processing options</h1></div></div></div><p>Apart from the count operation in a window, there are more operations that can be done on DStreams in conjunction with windowing. The following table captures the important transformations. All these transformations are acting on the selected window and return a DStream.</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Transformation</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Description</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">window(windowLength, slideInterval)</code>
</p>
</td><td>
<p>Returns DStreams computed in the window</p>
</td></tr><tr><td>
<p>
<code class="literal">countByWindow(windowLength, slideInterval)</code>
</p>
</td><td>
<p>Returns the count of elements</p>
</td></tr><tr><td>
<p>
<code class="literal">reduceByWindow(func, windowLength, slideInterval)</code>
</p>
</td><td>
<p>Returns one element by applying the aggregation function</p>
</td></tr><tr><td>
<p>
<code class="literal">reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code>
</p>
</td><td>
<p>Returns one key/value pair per key after applying the aggregation function over  multiple values per key</p>
</td></tr><tr><td>
<p>
<code class="literal">countByValueAndWindow(windowLength, slideInterval, [numTasks])</code>
</p>
</td><td>
<p>Returns one key/count pair per key after applying the count of multiple values per key</p>
</td></tr></tbody></table></div><p>One of the most important steps of stream processing is the persisting of the stream data into secondary storage. Since the velocity of the data in Spark Streaming data processing applications is going to be very high, any kind of persistence mechanism that introduces additional latency into the whole process is not an advisable solution.</p><p>In batch processing scenarios, it is fine to write to the HDFS and other file system based storage. But when it comes to the storage of stream output, depending on the use case, an ideal stream data storage mechanism should be chosen.</p><p>NoSQL data stores such as Cassandra support fast writes of temporal data. It is also ideal to read the data that is stored for any further analysis purposes. Spark Streaming library supports many output methods on DStreams. They include options to save the stream data as text file, object file, Hadoop files, and so on. As well as this, there are many third-party drivers available to save the data into various data stores.</p></div>
<div class="section" title="Kafka stream processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec51"/>Kafka stream processing</h1></div></div></div><p>The log event processor example covered in this chapter was listening to a TCP socket for the stream of messages to be processed by the Spark Streaming data processing application. But in real-world use cases, this is not going to be the case.</p><p>Message queueing systems with publish-subscribe capability are generally used for processing messages. The traditional message queueing systems failed to perform because of the huge volume of messages to be processed per second for the needs of large-scale data processing applications.</p><p>Kafka is a publish-subscribe messaging system used by many IoT applications to process a huge number of messages. The following capabilities of Kafka made it one of the most widely used messaging systems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extremely fast: Kafka can process huge amounts of data by handling reading and writing in short intervals of time from many application clients</li><li class="listitem" style="list-style-type: disc">Highly scalable: Kafka is designed to scale up and scale out to form a cluster using commodity hardware</li><li class="listitem" style="list-style-type: disc">Persists a huge number of messages: Messages reaching Kafka topics are persisted into the secondary storage, while at the same time it is handling huge number of messages flowing through</li></ul></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>A detailed treatment of Kafka is outside the scope of this book. It is assumed that the reader is familiar with and has working knowledge of Kafka. From a Spark Streaming data processing application perspective, it doesn't really make a difference whether it is a TCP socket or Kafka that is being used as a message source. But working on a teaser use case with Kafka as the message producer will give a good appreciation of the toolsets enterprises are using heavily. <span class="emphasis"><em>Learning Apache Kafka</em></span>
<span class="emphasis"><em>- Second Edition </em></span>by <span class="emphasis"><em>Nishant Garg</em></span> (<a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition">https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition</a>) is a good reference book to learn more about Kafka.</p></div></div><p>The following are some of the important elements of Kafka, and are terms to be understood before proceeding further:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Producer: The real source of the messages, such as weather sensors or mobile phone network</li><li class="listitem" style="list-style-type: disc">Broker: The Kafka cluster, which receives and persists the messages published to its topics by various producers</li><li class="listitem" style="list-style-type: disc">Consumer: The data processing applications subscribed to the Kafka topics that consume the messages published to the topics</li></ul></div><p>The same log event processing application use case discussed in the preceding section is used again here to elucidate the usage of Kafka with Spark Streaming. Instead of collecting the log event messages from the TCP socket, here the Spark Streaming data processing application will act as a consumer of a Kafka topic and the messages published to the topic will be consumed.</p><p>The Spark Streaming data processing application uses the version 0.8.2.2 of Kafka as the message broker, and the assumption is that the reader has already installed Kafka, at least in a standalone mode. The following activities are to be performed to make sure that Kafka is ready to process the messages produced by the producers and that the Spark Streaming data processing application can consume those messages:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Zookeeper that comes with Kafka installation.</li><li class="listitem">Start the Kafka server.</li><li class="listitem">Create a topic for the producers to send the messages to.</li><li class="listitem">Pick up one Kafka producer and start publishing log event messages to the newly created topic.</li><li class="listitem">Use the Spark Streaming data processing application to process the log events published to the newly created topic.</li></ol></div><div class="section" title="Starting Zookeeper and Kafka"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec40"/>Starting Zookeeper and Kafka</h2></div></div></div><p>The following scripts are run from separate terminal windows in order to start Zookeeper and the Kafka broker, and to create the required Kafka topics:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $KAFKA_HOME 
$ $KAFKA_HOME/bin/zookeeper-server-start.sh 
$KAFKA_HOME/config/zookeeper.properties  
[2016-07-24 09:01:30,196] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory) 
$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties  
 
[2016-07-24 09:05:06,381] INFO 0 successfully elected as leader 
(kafka.server.ZookeeperLeaderElector) 
[2016-07-24 09:05:06,455] INFO [Kafka Server 0], started 
(kafka.server.KafkaServer) 
$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 
--replication-factor 1 --partitions 1 --topic sfb 
Created topic "sfb". 
$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list 
localhost:9092 --topic sfb</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip48"/>Tip</h3><p>Make sure that the environment variable <code class="literal">$KAFKA_HOME</code> is pointing to the directory where Kafka is installed. Also, it is very important to start Zookeeper, Kafka server, Kafka producer, and Spark Streaming log event data processing application in separate terminal windows.</p></div></div><p>The Kafka message producer can be any application capable of publishing messages to the Kafka topics. Here, the <code class="literal">kafka-console-producer</code> that comes with Kafka is used as the producer of choice. Once the producer starts running, whatever is typed into its console window will be treated as a message that is published to the chosen Kafka topic. The Kafka topic is given as a command line argument when starting the <code class="literal">kafka-console-producer</code>.</p><p>The submission of the Spark Streaming data processing application that consumes log event messages produced by the Kafka producer is slightly different from the application covered in the preceding section. Here, many Kafka jar files are required for the data processing. Since they are not part of the Spark infrastructure, they have to be submitted to the Spark cluster. The following jar files are required for the successful running of this application:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">$KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">$KAFKA_HOME/libs/metrics-core-2.2.0.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">$KAFKA_HOME/libs/zkclient-0.3.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Code/Scala/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Code/Python/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar</code></li></ul></div><p>In the preceding list of jar files, the maven repository co-ordinate for <code class="literal">spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar</code> is <code class="literal">"org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.0.0-preview"</code>. This particular jar file has to be downloaded and placed in the lib folder of the directory structure given in Figure 4. It is being used in the <code class="literal">submit.sh</code> and the <code class="literal">submitPy.sh</code> scripts, which  submit the application to the Spark cluster. The download URL for this jar file is given in the reference section of this chapter.</p><p>In the <code class="literal">submit.sh</code> and <code class="literal">submitPy.sh</code> files, the last few lines contain a conditional statement looking for the second parameter value of 1 to identify this application and ship the required jar files to the Spark cluster.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip49"/>Tip</h3><p>Instead of shipping these individual jar files separately to the Spark cluster when the job is submitted, an assembly jar can be used by creating it using sbt.</p></div></div></div><div class="section" title="Implementing the application in Scala"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec41"/>Implementing the application in Scala</h2></div></div></div><p>The following code snippet is the Scala code for the log event processing application that processes the messages produced by the Kafka producer. The use case of this application is the same as the one discussed in the preceding section concerning windowing operations:</p><pre class="programlisting">
<span class="strong"><strong>/** 
The following program can be compiled and run using SBT 
Wrapper scripts have been provided with this 
The following script can be run to compile the code 
./compile.sh 
 
The following script can be used to run this application in Spark. The second command line argument of value 1 is very important. This is to flag the shipping of the kafka jar files to the Spark cluster 
./submit.sh com.packtpub.sfb.KafkaStreamingApps 1 
**/ 
package com.packtpub.sfb 
 
import java.util.HashMap 
import org.apache.spark.streaming._ 
import org.apache.spark.sql.{Row, SparkSession} 
import org.apache.spark.streaming.kafka._ 
import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord} 
 
object KafkaStreamingApps { 
  def main(args: Array[String]) { 
   // Log level settings 
   LogSettings.setLogLevels() 
   // Variables used for creating the Kafka stream 
   //The quorum of Zookeeper hosts 
    val zooKeeperQuorum = "localhost" 
   // Message group name 
   val messageGroup = "sfb-consumer-group" 
   //Kafka topics list separated by coma if there are multiple topics to be listened on 
   val topics = "sfb" 
   //Number of threads per topic 
   val numThreads = 1 
   // Create the Spark Session and the spark context            
   val spark = SparkSession 
         .builder 
         .appName(getClass.getSimpleName) 
         .getOrCreate() 
   // Get the Spark context from the Spark session for creating the streaming context 
   val sc = spark.sparkContext    
   // Create the streaming context 
   val ssc = new StreamingContext(sc, Seconds(10)) 
    // Set the check point directory for saving the data to recover when there is a crash 
   ssc.checkpoint("/tmp") 
   // Create the map of topic names 
    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap 
   // Create the Kafka stream 
    val appLogLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2) 
   // Count each log messge line containing the word ERROR 
    val errorLines = appLogLines.filter(line =&gt; line.contains("ERROR")) 
   // Print the line containing the error 
   errorLines.print() 
   // Count the number of messages by the windows and print them 
   errorLines.countByWindow(Seconds(30), Seconds(10)).print() 
   // Start the streaming 
    ssc.start()    
   // Wait till the application is terminated             
    ssc.awaitTermination()  
  } 
} 
 
</strong></span>
</pre><p>Compared to the Scala code in the preceding section, the major difference is in the way the stream is created.</p></div><div class="section" title="Implementing the application in Python"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec42"/>Implementing the application in Python</h2></div></div></div><p>The following code snippet is the Python code for the log event processing application that processes the message produced by the Kafka producer. The use case of this application is also the same as the one discussed in the preceding section concerning windowing operations:</p><pre class="programlisting">
<span class="strong"><strong>
# The following script can be used to run this application in Spark 
# ./submitPy.sh KafkaStreamingApps.py 1 
 
from __future__ import print_function 
import sys 
from pyspark import SparkContext 
from pyspark.streaming import StreamingContext 
from pyspark.streaming.kafka import KafkaUtils 
 
if __name__ == "__main__": 
    # Create the Spark context 
    sc = SparkContext(appName="PythonStreamingApp") 
    # Necessary log4j logging level settings are done  
    log4j = sc._jvm.org.apache.log4j 
    log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN) 
    # Create the Spark Streaming Context with 10 seconds batch interval 
    ssc = StreamingContext(sc, 10) 
    # Set the check point directory for saving the data to recover when there is a crash 
    ssc.checkpoint("\tmp") 
    # The quorum of Zookeeper hosts 
    zooKeeperQuorum="localhost" 
    # Message group name 
    messageGroup="sfb-consumer-group" 
    # Kafka topics list separated by coma if there are multiple topics to be listened on 
    topics = "sfb" 
    # Number of threads per topic 
    numThreads = 1     
    # Create a Kafka DStream 
    kafkaStream = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, {topics: numThreads}) 
    # Create the Kafka stream 
    appLogLines = kafkaStream.map(lambda x: x[1]) 
    # Count each log messge line containing the word ERROR 
    errorLines = appLogLines.filter(lambda appLogLine: "ERROR" in appLogLine) 
    # Print the first ten elements of each RDD generated in this DStream to the console 
    errorLines.pprint() 
    errorLines.countByWindow(30,10).pprint() 
    # Start the streaming 
    ssc.start() 
    # Wait till the application is terminated    
    ssc.awaitTermination()</strong></span>
</pre><p>The following commands are run on the terminal window to run the Scala application:</p><pre class="programlisting">
<span class="strong"><strong>
	$ cd Scala
	$ ./submit.sh com.packtpub.sfb.KafkaStreamingApps 1</strong></span>
</pre><p>The following commands are run on the terminal window to run the Python application:</p><pre class="programlisting">
<span class="strong"><strong>
	$ cd Python
	$ 
	./submitPy.sh KafkaStreamingApps.py 1</strong></span>
</pre><p>When both of the preceding programs are running, whatever log event messages are typed into the console window of the Kafka console producer, and invoked using the following command and inputs, will be processed by the application. The outputs of this program will be very similar to the ones that are given in the preceding section:</p><pre class="programlisting">
	<span class="strong"><strong>$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 
	--topic sfb 
	[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ 
	[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ 
	[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: 
	/apache/web/test 
</strong></span>
</pre><p>Spark provides two approaches to process Kafka streams. The first one is the receiver-based approach that was discussed previously and the second one is the direct approach. </p><p>This direct approach to processing Kafka messages is a simplified method in which Spark Streaming is using all the possible capabilities of Kafka just like any of the Kafka topic consumers, and polls for the messages in the specific topic, and the partition by the offset number of the messages. Depending on the batch interval of the Spark Streaming data processing application, it picks up a certain number of offsets from the Kafka cluster, and this range of offsets is processed as a batch. This is highly efficient and ideal for processing messages with a requirement to have exactly-once processing. This method also reduces the Spark Streaming library's need to do additional work to implement the exactly-once semantics of the message processing and delegates that responsibility to Kafka. The programming constructs of this approach are slightly different in the APIs used for the data processing. Consult the appropriate reference material for the details. </p><p>The preceding sections introduced the concept of a Spark Streaming library and discussed some of the real-world use cases. There is a big difference between Spark data processing applications developed to process static batch data and those developed to process dynamic stream data in a deployment perspective. The availability of data processing applications to process a stream of data must be constant. In other words, such applications should not have components that are single points of failure. The following section is going to discuss this topic.</p></div></div>
<div class="section" title="Spark Streaming jobs in production"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec52"/>Spark Streaming jobs in production</h1></div></div></div><p>When a Spark Streaming application is processing the incoming data, it is very important to have uninterrupted data processing capability so that all the data that is getting ingested is processed. In business-critical streaming applications, most of the time missing even one piece of data can have a huge business impact. To deal with such situations, it is important to avoid single points of failure in the application infrastructure. </p><p>From a Spark Streaming application perspective, it is good to understand how the underlying components in the ecosystem are laid out so that the appropriate measures can be taken to avoid single points of failure.</p><p>A Spark Streaming application deployed in a cluster such as Hadoop YARN, Mesos or Spark Standalone mode has two main components very similar to any other type of Spark application:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark driver</strong></span>: This contains the application code written by the user</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Executors</strong></span>: The executors that execute the jobs submitted by the Spark driver</li></ul></div><p>But the executors have an additional component called a receiver that receives the data getting ingested as a stream and saves it as blocks of data in memory. When one receiver is receiving the data and forming the data blocks, they are replicated to another executor for fault-tolerance. In other words, in-memory replication of the data blocks is done onto a different executor. At the end of every batch interval, these data blocks are combined to form a DStream and sent out for further processing downstream. </p><p>
<span class="emphasis"><em>Figure 8</em></span> depicts the components working together in a Spark Streaming application infrastructure deployed in a cluster:</p><p>
</p><div class="mediaobject"><img alt="Spark Streaming jobs in production" src="graphics/image_06_013.jpg"/><div class="caption"><p>Figure 8</p></div></div><p>
</p><p>In <span class="emphasis"><em>Figure 8</em></span>, there are two executors. The receiver component is deliberately not displayed in the second executor to show that it is not using the receiver and instead just collects the replicated data blocks from the other executor. But when needed, such as on the failure of the first executor, the receiver in the second executor can start functioning.</p><div class="section" title="Implementing fault-tolerance in Spark Streaming data processing applications"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec43"/>Implementing fault-tolerance in Spark Streaming data processing applications</h2></div></div></div><p>Spark Streaming data processing application infrastructure has many moving parts. Failures can happen to any one of them, resulting in the interruption of the data processing. Typically failures can happen to the Spark driver or the executors. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note50"/>Note</h3><p>This section is not intended to give detailed treatment to running the Spark Streaming applications in production with fault-tolerance. The intention is to make the reader appreciate the precautions to be taken when deploying Spark Streaming data processing applications in production.</p></div></div><p>When an executor fails, since the replication of data is happening on a regular basis, the task of receiving the data stream will be taken over by the executor on which the data was getting replicated. There is a situation in which when an executor fails, all the data that is unprocessed will be lost. To circumvent this problem, there is a way to persist the data blocks into HDFS or Amazon S3 in the form of write-ahead logs. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip51"/>Tip</h3><p>There is no need to have both the in-memory replication of the data blocks and write-ahead logs together in one infrastructure. Keep only one of them, depending on the need.</p></div></div><p>When the Spark driver fails, the driven program is stopped, all the executors lose connection, and they stop functioning. This is the most dangerous situation. To deal with this situation, some configuration and code changes are necessary. </p><p>The Spark driver has to be configured to have an automatic driver restart, which is supported by the cluster managers. This includes a change in the Spark job submission method to have the cluster mode in whichever may be the cluster manager. When a restart of the driver happens, to start from the place when it crashed, a checkpointing mechanism has to be implemented in the driver program. This has already been done in the code samples that are used. The following lines of code do that job: </p><pre class="programlisting">
<span class="strong"><strong>    ssc = StreamingContext(sc, 10) 
    ssc.checkpoint("\tmp")</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip52"/>Tip</h3><p>In a sample application, it is fine to use a local system directory as the checkpoint directory. But in a production environment, it is better to keep this checkpoint directory as an HDFS location in the case of Hadoop or an S3 location in the case of an Amazon cloud. </p></div></div><p>From an application coding perspective, the way the <code class="literal">StreamingContext</code>is created is slightly different. Instead of creating a new <code class="literal">StreamingContext</code>every time, the factory method <code class="literal">getOrCreate</code>of the <code class="literal">StreamingContext</code>is to be used with a function, as shown in the following code segment. If that is done, when the driver is restarted, the factory method will check the checkpoint directory to see whether an earlier <code class="literal">StreamingContext</code>was in use, and, if found in the checkpoint data, it is created. Otherwise, a new <code class="literal">StreamingContext</code>is created. </p><p>The following code snippet gives the definition of a function that can be used with the <code class="literal">getOrCreate</code>factory method of the <code class="literal">StreamingContext</code>. As mentioned earlier, a detailed treatment of these aspects is beyond the scope of this book:</p><pre class="programlisting">
	<span class="strong"><strong> /** 
  * The following function has to be used when the code is being restructured to have checkpointing and driver recovery 
  * The way it should be used is to use the StreamingContext.getOrCreate with this function and do a start of that 
  */ 
  def sscCreateFn(): StreamingContext = { 
   // Variables used for creating the Kafka stream 
   // The quorum of Zookeeper hosts 
    val zooKeeperQuorum = "localhost" 
   // Message group name 
   val messageGroup = "sfb-consumer-group" 
   //Kafka topics list separated by coma if there are multiple topics to be listened on 
   val topics = "sfb" 
   //Number of threads per topic 
   val numThreads = 1      
   // Create the Spark Session and the spark context            
   val spark = SparkSession 
         .builder 
         .appName(getClass.getSimpleName) 
         .getOrCreate() 
   // Get the Spark context from the Spark session for creating the streaming context 
   val sc = spark.sparkContext    
   // Create the streaming context 
   val ssc = new StreamingContext(sc, Seconds(10)) 
   // Create the map of topic names 
    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap 
   // Create the Kafka stream 
    val appLogLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2) 
   // Count each log messge line containing the word ERROR 
    val errorLines = appLogLines.filter(line =&gt; line.contains("ERROR")) 
   // Print the line containing the error 
   errorLines.print() 
   // Count the number of messages by the windows and print them 
   errorLines.countByWindow(Seconds(30), Seconds(10)).print() 
   // Set the check point directory for saving the data to recover when there is a crash 
   ssc.checkpoint("/tmp") 
   // Return the streaming context 
   ssc 
  } 
</strong></span>
</pre><p>At a data source level, it is a good idea to build parallelism for faster data processing and, depending on the source of data, this can be accomplished in different ways. Kafka inherently supports partition at the topic level, and that kind of scaling out mechanism supports a good amount of parallelism. As a consumer of Kafka topics, the Spark Streaming data processing application can have multiple receivers by creating multiple streams, and the data generated by those streams can be combined by the union operation on the Kafka streams.</p><p>The production deployment of Spark Streaming data processing applications is to be done purely based on the type of application that is being used. Some of the guidelines given previously are just introductory and conceptual in nature. There is no silver bullet approach to solving production deployment problems, and they have to evolve along with the application development.</p></div><div class="section" title="Structured streaming"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec44"/>Structured streaming </h2></div></div></div><p>In the data streaming use cases that have been covered so far, there are many developer tasks in terms of building of the structure data and implementing fault tolerance for the application. The data that has been dealt with so far in data streaming applications is unstructured data. Just like the batch data processing use cases, even in streaming use cases, if there is the capability to process structured data, that is a great advantage, and lots of pre-processing can be avoided. Data stream processing applications are continuously running applications and they are bound to develop failures or interruptions. In such situations, it is imperative to build fault tolerance in the data streaming applications.</p><p>In any data streaming application, the data is getting ingested continuously, and if there is a need to interrogate the data received at any given point in time, the application developers have to persist the data processed into a data store that supports querying. In Spark 2.0, the structured streaming concept is built around these aspects, and the whole idea behind building this brand new feature from the ground up is to relieve application developers of these pain areas. There is a feature with the reference number SPARK-8360 being built at the time of writing this chapter, and its progress can be monitored by visiting the corresponding page.</p><p>The structured streaming concept can be explained using a real-world use case, such as the banking transaction use case we looked at before. Assume that the comma-separated transaction records containing the account number and transaction amount are coming in a stream. In the structured stream processing method, all these data items get ingested into an unbounded table or DataFrame that supports querying using Spark SQL. In other words, since the data is accumulated in a DataFrame, whatever data processing is possible using a DataFrame will be possible with the stream data as well. This reduces the burden on application developers and they can focus on the business logic of the application rather than the infrastructure related-aspects. </p></div></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec53"/>References</h1></div></div></div><p>For more information, visit the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://kafka.apache.org/">http://kafka.apache.org/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">http://spark.apache.org/docs/latest/streaming-kafka-integration.html</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition">https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar">http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-836">https://issues.apache.org/jira/browse/SPARK-836</a></li></ul></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec54"/>Summary</h1></div></div></div><p>Spark provides a very powerful library on top of the Spark core to process the stream of data getting ingested at a high velocity. This chapter introduced the basics of the Spark Streaming library, and a simple log event message processing system has been developed with two types of data source: one uses a TCP data server and the other uses Kafka. At the end of the chapter, a brief look at the production deployment of Spark Streaming data processing applications is provided and the possible ways of implementing fault-tolerance in Spark Streaming data processing applications as discussed.</p><p>Spark 2.0 brings the capability to process and query structured data in streaming applications, and the concept has been introduced, which relieves application developers from pre-processing the unstructured data, building fault-tolerance and querying the data that is being ingested on a near-real-time basis.</p><p>Applied mathematicians and statisticians have come up with ways and means to answer questions related to a new piece of data based on the <span class="emphasis"><em>learning</em></span> that has already been done on an existing bank of data. Typically these questions include, but are not limited to: does this piece of data fit a given model, can this piece of data be classified in a certain way, and does this piece of data belong to any group or cluster?</p><p>There are lots of algorithms available to <span class="emphasis"><em>train</em></span> a data model and ask questions to this <span class="emphasis"><em>model</em></span> about the new piece of data. This rapidly evolving branch of data science has huge applicability in data processing, and is popularly known as machine learning. The next chapter is going to discuss the machine learning library of Spark.</p></div></body></html>