<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of a Deep Neural Network</h1>
                </header>
            
            <article>
                
<p>We will now use our accumulated knowledge of GPU programming to implement our very own deep neural network (DNN) with PyCUDA. DNNs have attracted a lot of interest in the last decade, as they provide a robust and elegant model for machine learning (ML). DNNs was also one of the first applications (outside of rendering graphics) that were able to show the true power of GPUs by leveraging their massive parallel throughput, which ultimately helped NVIDIA rise to become a major player in the field of artificial intelligence.</p>
<p>In the course of this book, we have mostly been covering individual topics in a <em>bubble</em> on a chapter-by-chapter basis—here, we will build on many of the subjects we have learned about thus far for our very own implementation of a DNN. While there are several open source frameworks for GPU-based DNNs currently available to the general public—for example, Google's TensorFlow and Keras, Microsoft's CNTK, Facebook's Caffe2, and PyTorch—it is very instructive to go through an implementation of one from scratch, which will give us a greater insight and appreciation of the underlying technologies required for DNNs. We have a lot of material to cover here, so we'll cut right to the chase after a brief introduction to some of the basic concepts.</p>
<p>In this chapter, we will be looking at the following:</p>
<ul>
<li>Understanding what an <strong>artificial neuron</strong> (<strong>AN</strong>) is</li>
<li>Understanding how many ANs can be combined together in a <strong>deep neural network</strong> (<strong>DNN</strong>)</li>
<li>Implementing a DNN from scratch in CUDA and Python</li>
<li>Understanding how cross-entropy loss can be used to evaluate the output of a neural network</li>
<li>Implementing gradient descent to train an NN</li>
<li>Learning how to train and test an NN on a small dataset</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>A Linux or Windows 10 PC with a modern <span>NVIDIA </span>GPU (2016—onward) is required for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward) installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with the PyCUDA module is also required.</p>
<p>This chapter's code is also available on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.</p>
<div class="packt_infobox">For more information about the prerequisites for this chapter, check out the preface of this book. For the software and hardware requirements, check out the README file in <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial neurons and neural networks</h1>
                </header>
            
            <article>
                
<p><span>Let's briefly go over some of the basics of</span> <strong>machine learning (ML)</strong> <span>and</span> <strong>neural networks (NNs)</strong><span>. In Machine Learning, our goal is to take a collection of data with a particular set of labeled classes or characteristics and use these examples to train our system to predict the values of future data. We call a program or function that predicts classes or labels of future data based on prior training data a <strong>classifier</strong>.</span></p>
<p>There are many types of classifiers, but here we will be focusing on NNs. The idea behind NNs is that they (allegedly) work in a way that is similar to the human brain, in that they learn and classify data using a collection of <strong>artificial neurons (ANs)</strong>, all connected together to form a particular structure. Let's step back for a moment, though, and look at what an individual AN is. In mathematics, this is just an <em>affine</em> function from the linear space <strong>R<sup>n</sup></strong> to <strong>R</strong>, like so:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/25478608-c882-49f1-85a9-9e3f6b0d472d.png" style="width:22.42em;height:1.67em;" width="3750" height="280"/></div>
<p>We can see that this can be characterized as a dot product between a constant weight vector <strong><em>w</em></strong> and an input vector <strong><em>x</em></strong>, with an additional bias constant <em>b</em> added to the end. (Again, the only <em>input</em> into this function here is <em>x</em>; the other values are constants!)</p>
<p>Now, individuall<em>y</em> a single AN is fairly useless (and stupid), as their <em>intelligence</em> only emerges when acting in cooperation with a large number of other ANs. Our first step is to stack a collection of <em>m</em> similar ANs on top of each other so as to form what we will call a <strong>dense layer (DL)</strong>. This is dense because each neuron will process every single input value from <em>x <span>– </span></em>each AN will take in an array or vector value from <strong>R<sup>n</sup></strong> and output a single value in <strong>R.</strong> Since there are <em>m</em> neurons, this means that we can say their output collectively is in the space <strong>R<sup>m</sup></strong>. We will notice that if we stack the weights for each neuron in our layer, so as to form an <em>m x n</em> matrix of weights, we can then just calculate the output of each neuron with a matrix multiplication followed by the addition of the appropriate biases:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png" style="width:39.00em;height:5.75em;" width="6730" height="990"/></div>
<p>Now, let's suppose that we want to build an NN classifier that can classify <em>k</em> different classes; we can create a new additional dense layer that takes in the <em>m</em> values from the prior dense layer, and outputs <em>k</em> values. Supposing that we have the appropriate weight and bias values for each layer (which are certainly not trivial to find), and that we also have the appropriate <strong>activation function</strong> set up after each layer (which we will define later), this will act as a classifier between our <em>k</em> distinct classes, giving us the probability of <em>x</em> falling into each respective class based on the outputs of the final layer. Of course, we're getting way ahead of ourselves here, but that is, in a nutshell, how an NN works.</p>
<p class="mce-root">Now, it seems like we can just keep connecting <span>dense layer</span>s to each other into long chains to achieve classifications. This is what is known as a DNN. When we have a layer that is not directly connected to the inputs or outputs, that is known as a hidden layer. The strength of a DNN is that the additional layers allow the NN to capture abstractions and subtleties of the data that a shallow NN could not pick up on.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing a dense layer of artificial neurons</h1>
                </header>
            
            <article>
                
<p>Now, let's implement the most important building block of an NN, the <strong>dense layer</strong>. Let's start by declaring a CUDA kernel, like so:</p>
<pre>__global__ void dense_eval(int num_outputs, int num_inputs, int relu, int sigmoid, float * w, float * b, float * x, float *y, int batch_size, int w_t, int b_t, float delta)</pre>
<p>Let's go over the inputs, one by one. <kbd>num_outputs</kbd>, of course, indicates the total number of outputs this layer has; this is exactly the number of neurons in the layer. <kbd>num_inputs</kbd> tells us the size of the input data. Setting a positive value for <kbd>relu</kbd> and <kbd>sigmoid</kbd> will indicate that we should use the corresponding activation function on the output of this layer, which we will define later. <kbd>w</kbd> and <kbd>b</kbd> are arrays containing the weights and biases of this layer, while <kbd>x</kbd> and <kbd>y</kbd> will act as our inputs and outputs. Oftentimes, we wish to classify more than one piece of data at a time. We can indicate this by setting <kbd>batch_size</kbd> to be the number of points we wish to predict. Finally, <kbd>w_t</kbd>, <kbd>b_t</kbd>, and <kbd>delta</kbd> will be used in the training process to determine the appropriate weights and biases for this layer by means of <strong>gradient descent</strong>. (We will see more on gradient descent in a later section.)</p>
<p>Now, let's start writing our kernel. We will parallelize the computations over each output, so we will set an integer <kbd>i</kbd> to be the global thread ID to this end, and have any unnecessary extra threads which happen to be running this kernel to just not do anything with the appropriate <kbd>if</kbd> statement:</p>
<pre>{<br/> int i = blockDim.x*blockIdx.x + threadIdx.x;<br/><br/> if (i &lt; num_outputs)<br/> {</pre>
<p>Now, let's iterate over each data point in the batch with the appropriate <kbd>for</kbd> loop:</p>
<pre>for(int k=0; k &lt; batch_size; k++)<br/> { </pre>
<p>We will multiply and accumulate the 32-bit floats from the weights and inputs into a 64-bit double <kbd>temp</kbd> and then add the appropriate bias point. We will then typecast this back to a 32-bit float and put the value in the output array, and then close off the loop over <kbd>k</kbd>:</p>
<pre>double temp = 0.0f;<br/> for (int j = 0; j &lt; num_inputs; j++)<br/> {<br/>   temp += ((double) w[(num_inputs)*i + j ] ) * ( (double) x[k*num_inputs + j]);<br/> }<br/> temp += (double) b[i];<br/> y[k * num_outputs + i] = (float) temp;  <br/>}</pre>
<div class="mce-root packt_tip"><em>Multiply and accumulate</em> types of operations are generally subject to a great loss of numerical precision. This can be mitigated by using a temporary variable of higher precision to store values in the course of the operation, and then typecasting this variable back to the original precision after the operation is completed.</div>
<p>To train an NN, we will ultimately have to calculate the derivative (from calculus) of our NN with respect to each weight and bias within each individual layer, which is with respect to a particular batch of inputs. Remember that the derivative of a mathematical function <em>f</em> at the value <em>x</em> can be estimated as <em>f</em><em>(x + δ) - f(x) / δ</em>, where delta (δ) is some sufficiently small positive value. We will use the input values <kbd>w_t</kbd> and <kbd>b_t</kbd> to indicate to the kernel whether we want to calculate the derivative with respect to a particular weight or bias; otherwise, we will set these input values to a negative value to evaluate only for this layer. We will also set delta to be an appropriately small value for the calculation of the derivative, and use this to increment the value of the appropriate bias or weight:</p>
<pre>if( w_t &gt;= 0 &amp;&amp; i == (w_t / num_inputs))<br/> {<br/> int j = w_t % num_inputs;<br/> for(int k=0; k &lt; batch_size; k++)<br/>  y[k*num_outputs + i] += delta*x[k*num_inputs+j];<br/>}<br/>if( b_t &gt;= 0 &amp;&amp; i == b_t )<br/> {<br/>  for(int k=0; k &lt; batch_size; k++)<br/>  y[k*num_outputs + i] += delta;<br/> }</pre>
<p>Now, we will add some code for what is known as the <strong>rectified linear unit</strong> (or <strong>ReLU</strong>) and <strong>sigmoid activation functions</strong>. These are used for processing the immediate output of a dense neural layer. ReLU just sets all negative values to 0, while acting as an identity for positive inputs, while sigmoid just computes the value of the <kbd>sigmoid</kbd> function on each value ( <em>1 / (1 + e<sup>-x</sup>)</em> ). ReLU (or any other activation function) is used between hidden layers in an NN as a means to make the entire NN act as a nonlinear function; otherwise, the entire NN would constitute a trivial (and inefficiently computed) matrix operation. (While there are many other nonlinear activation functions that can be used between layers, ReLU has been found to be a particularly effective function for training.) Sigmoid is used as a final layer in an NN intended for <strong>labeling</strong>, that is, one that may assign multiple labels for a given input, as opposed to assigning an input to a single class.</p>
<p>Let's go up a little bit in the file, before we even begin to define this CUDA kernel, and define these operations as C macros. We will also remember to put in the CUDA-C code we've just written while we are at it:</p>
<pre>DenseEvalCode = '''<br/>#define _RELU(x) ( ((x) &gt; 0.0f) ? (x) : 0.0f )<br/>#define _SIGMOID(x) ( 1.0f / (1.0f + expf(-(x)) ))</pre>
<p>Now, we will use the kernel inputs <kbd>relu</kbd> and <kbd>sigmoid</kbd> to indicate whether we should use these additional layers; we will take a positive input from these to indicate that they should be used, respectively. We can add this, close off our kernel, and compile it into a usable Python function:</p>
<pre>if(relu &gt; 0 || sigmoid &gt; 0)<br/>for(int k=0; k &lt; batch_size; k++)<br/> { <br/>   float temp = y[k * num_outputs + i];<br/>   if (relu &gt; 0)<br/>    temp = _RELU(temp);<br/>   if (sigmoid &gt; 0)<br/>    temp = _SIGMOID(temp);<br/>   y[k * num_outputs + i] = temp; <br/>  }<br/> }<br/> return;<br/>}<br/>'''<br/>eval_mod = SourceModule(DenseEvalCode)<br/>eval_ker = eval_mod.get_function('dense_eval')</pre>
<p>Now, let's go to the beginning of the file and set up the appropriate import statements. Notice that we will include the <kbd>csv</kbd> module, which will be used for processing data inputs for testing and training:</p>
<pre>from __future__ import division<br/>import pycuda.autoinit<br/>import pycuda.driver as drv<br/>from pycuda import gpuarray<br/>from pycuda.compiler import SourceModule<br/>from pycuda.elementwise import ElementwiseKernel<br/>import numpy as np<br/>from Queue import Queue<br/>import csv<br/>import time</pre>
<p>Now, let's continue setting up our <span>dense layer</span>; we will want to wrap this within a Python class for ease of use, which will make our lives much easier when we start connecting these <span>dense layer</span>s together into a full-blown NN. We'll call <kbd>class DenseLayer</kbd> and start by writing a constructor. Most of the inputs and setup here should be self-explanatory: we should definitely add an option to load weights and biases from a pre-trained network, and we'll also include the option to specify a default <em>delta </em>value as well as a default stream. (If no weights or biases are given, weights are initialized to random values, while all biases are set to 0.) We will also specify whether to use ReLU or sigmoid layers here, as well. Toward the end, notice how we set up the block and grid sizes:</p>
<pre>class DenseLayer:<br/>    def __init__(self, num_inputs=None, num_outputs=None, weights=None, b=None, stream=None, relu=False, sigmoid=False, delta=None):<br/>        self.stream = stream<br/> <br/>        if delta is None:<br/>            self.delta = np.float32(0.001)<br/>        else:<br/>            self.delta = np.float32(delta)<br/><br/>        if weights is None:<br/>            weights = np.random.rand(num_outputs, num_inputs) - .5<br/>            self.num_inputs = np.int32(num_inputs)<br/>        self.num_outputs = np.int32(num_outputs) <br/> <br/>        if type(weights) != pycuda.gpuarray.GPUArray:<br/>            self.weights = gpuarray.to_gpu_async(np.array(weights, <br/>            dtype=np.float32) , stream = self.stream)<br/>        else:<br/>            self.weights = weights<br/> <br/>        if num_inputs is None or num_outputs is None:<br/>            self.num_inputs = np.int32(self.weights.shape[1])<br/>            self.num_outputs = np.int32(self.weights.shape[0])<br/> <br/>        else:<br/>            self.num_inputs = np.int32(num_inputs)<br/>            self.num_outputs = np.int32(num_outputs)<br/><br/>        if b is None:<br/>            b = gpuarray.zeros((self.num_outputs,),dtype=np.float32)<br/> <br/>        if type(b) != pycuda.gpuarray.GPUArray:<br/>            self.b = gpuarray.to_gpu_async(np.array(b, <br/>            dtype=np.float32) , stream = self.stream)<br/>        else:<br/>            self.b = b <br/> <br/>        self.relu = np.int32(relu)<br/>        self.sigmoid = np.int32(sigmoid)<br/> <br/>        self.block = (32,1,1)<br/>        self.grid = (int(np.ceil(self.num_outputs / 32)), 1,1)</pre>
<p>Now, we will set up a function in this class to evaluate inputs from this layer; we will meticulously check the input (x) to determine if it is already on the GPU (transferring it over to a <kbd>gpuarray</kbd> if not), and we will let the user specify a preallocated <kbd>gpuarray</kbd> for output (y), manually allocating an output array if one is not specified. We will also check the delta and <kbd>w_t</kbd>/<kbd>b_t</kbd> values for the case of training, as well as <kbd>batch_size</kbd>. We will then run the kernel on the <kbd>x</kbd> input with outputs going into <kbd>y</kbd>, and finally return <kbd>y</kbd> as the output value:</p>
<pre>def eval_(self, x, y=None, batch_size=None, stream=None, delta=None, w_t = None, b_t = None):<br/><br/>if stream is None:<br/>    stream = self.stream<br/><br/>if type(x) != pycuda.gpuarray.GPUArray:<br/>    x = gpuarray.to_gpu_async(np.array(x,dtype=np.float32), stream=self.stream)<br/> <br/>if batch_size is None:<br/>    if len(x.shape) == 2:<br/>        batch_size = np.int32(x.shape[0])<br/>    else:<br/>        batch_size = np.int32(1)<br/> <br/>if delta is None:<br/>    delta = self.delta<br/><br/>delta = np.float32(delta)<br/> <br/>if w_t is None:<br/>    w_t = np.int32(-1)<br/> <br/>if b_t is None:<br/>    b_t = np.int32(-1)<br/> <br/>if y is None:<br/>    if batch_size == 1:<br/>        y = gpuarray.empty((self.num_outputs,), dtype=np.float32)<br/>    else:<br/>        y = gpuarray.empty((batch_size, self.num_outputs), dtype=np.float32)<br/><br/>    eval_ker(self.num_outputs, self.num_inputs, self.relu, self.sigmoid, self.weights, self.b, x, y, np.int32(batch_size), w_t, b_t, delta , block=self.block, grid=self.grid , stream=stream)<br/> <br/> return y</pre>
<p>There we go. We have fully implemented a <span>dense layer</span>!</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of the softmax layer</h1>
                </header>
            
            <article>
                
<p>We will now look at how we can implement a <strong>softmax layer</strong>. As we have already discussed, a sigmoid layer is used for assigning labels to a class—that is, if you want to have multiple nonexclusive characteristics that you want to infer from an input, you should use a sigmoid layer. A <strong>softmax layer</strong> is used when you only want to assign a single class to a sample by inference—this is done by computing a probability for each possible class (with probabilities over all classes, of course, summing to 100%). We can then select the class with the highest probability to give the final classification.</p>
<p>Now, let's see exactly what the softmax layer does—given a set of a collection of <em>N</em> real numbers (<em>c<sub>0</sub>, ..., c<sub>N-1</sub></em>) , we first compute the sum of the exponential function on each number (<img class="fm-editor-equation" src="assets/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png" style="width:9.67em;height:1.00em;" width="1640" height="170"/>), and then calculate the exponential of each number divided by this sum to yield the softmax:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/ce20fc54-36ee-46a9-86a9-40387fb73545.png" style="width:10.75em;height:1.50em;" width="1580" height="220"/></div>
<p>Let's start with our implementation. We will start by writing two very short CUDA kernels: one that takes the exponential of each input, and another that takes the mean over all of the points:</p>
<pre>SoftmaxExpCode='''<br/>__global__ void softmax_exp( int num, float *x, float *y, int batch_size)<br/>{<br/> int i = blockIdx.x * blockDim.x + threadIdx.x;<br/><br/> if (i &lt; num)<br/> {<br/>  for (int k=0; k &lt; batch_size; k++)<br/>  {<br/>   y[num*k + i] = expf(x[num*k+i]);<br/>  }<br/> }<br/>}<br/>'''<br/>exp_mod = SourceModule(SoftmaxExpCode)<br/>exp_ker = exp_mod.get_function('softmax_exp')<br/><br/>SoftmaxMeanCode='''<br/>__global__ void softmax_mean( int num, float *x, float *y, int batch_size)<br/>{<br/> int i = blockDim.x*blockIdx.x + threadIdx.x;<br/> <br/> if (i &lt; batch_size)<br/> {<br/>  float temp = 0.0f;<br/><br/>  for(int k=0; k &lt; num; k++)<br/>   temp += x[i*num + k];<br/> <br/> <br/>  for(int k=0; k &lt; num; k++)<br/>   y[i*num+k] = x[i*num+k] / temp;<br/> }<br/> <br/> return;<br/>}'''<br/><br/>mean_mod = SourceModule(SoftmaxMeanCode)<br/>mean_ker = mean_mod.get_function('softmax_mean')</pre>
<p>Now, let's write a Python wrapper class, like we did previously. First, we will start with the constructor, and we will indicate the number of both inputs and outputs with <kbd>num</kbd>. We can also specify a default stream, if we wish:</p>
<pre>class SoftmaxLayer:<br/>    def __init__(self, num=None, stream=None):<br/>     self.num = np.int32(num)<br/>     self.stream = stream</pre>
<p>Now, let's write <kbd>eval_ function</kbd> in a way that is similar to the <span>dense layer</span>:</p>
<pre>def eval_(self, x, y=None, batch_size=None, stream=None):<br/> if stream is None:<br/> stream = self.stream<br/> <br/> if type(x) != pycuda.gpuarray.GPUArray:<br/>  temp = np.array(x,dtype=np.float32)<br/>  x = gpuarray.to_gpu_async( temp , stream=stream)<br/> <br/> if batch_size==None:<br/>  if len(x.shape) == 2:<br/>   batch_size = np.int32(x.shape[0])<br/>  else:<br/>   batch_size = np.int32(1)<br/> else:<br/>  batch_size = np.int32(batch_size)<br/> <br/> if y is None:<br/>  if batch_size == 1:<br/>   y = gpuarray.empty((self.num,), dtype=np.float32)<br/> else:<br/>  y = gpuarray.empty((batch_size, self.num), dtype=np.float32)<br/><br/> exp_ker(self.num, x, y, batch_size, block=(32,1,1), grid=(int( np.ceil( self.num / 32) ), 1, 1), stream=stream)<br/> <br/> mean_ker(self.num, y, y, batch_size, block=(32,1,1), grid=(int( np.ceil( batch_size / 32)), 1,1), stream=stream)<br/> <br/> return y<br/><br/></pre>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of Cross-Entropy loss</h1>
                </header>
            
            <article>
                
<p>Now, let's implement what is known as the <strong>cross-entropy loss</strong> function. This is used to measure how accurate an NN is on a small subset of data points during the training process; the bigger the value that is output by our loss function, the more inaccurate our NN is at properly classifying the given data. We do this by calculating a standard mean log-entropy difference between the expected output and the actual output of the NN. For numerical stability, we will limit the value of the output to <kbd>1</kbd>:</p>
<pre>MAX_ENTROPY = 1<br/><br/>def cross_entropy(predictions=None, ground_truth=None):<br/> <br/> if predictions is None or ground_truth is None:<br/>  raise Exception("Error! Both predictions and ground truth must be float32 arrays")<br/> <br/> p = np.array(predictions).copy()<br/> y = np.array(ground_truth).copy()<br/> <br/> if p.shape != y.shape:<br/>  raise Exception("Error! Both predictions and ground_truth must have same shape.")<br/> <br/> if len(p.shape) != 2:<br/>  raise Exception("Error! Both predictions and ground_truth must be 2D arrays.")<br/> <br/> total_entropy = 0<br/> <br/> for i in range(p.shape[0]):<br/>  for j in range(p.shape[1]):<br/>   if y[i,j] == 1: <br/>    total_entropy += min( np.abs( np.nan_to_num( np.log( p[i,j] ) ) ) , MAX_ENTROPY) <br/>   else: <br/>    total_entropy += min( np.abs( np.nan_to_num( np.log( 1 - p[i,j] ) ) ), MAX_ENTROPY)<br/> <br/> return total_entropy / p.size</pre>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of a sequential network</h1>
                </header>
            
            <article>
                
<p>Now, let's implement one final class that will combine multiple <span>dense layer</span> and softmax layer objects into a single coherent feed-forward sequential neural network. This will be implemented as another class, which will subsume the other classes. Let's first start by writing the constructor—we will be able to set the max batch size here, which will affect how much memory is allocated for the use of this network <span>– </span>we'll store some allocated memory used for weights and input/output for each layer in the list variable, <kbd>network_mem</kbd>. We will also store the <kbd>DenseLayer</kbd> and <kbd>SoftmaxLayer</kbd> objects in the list network, and information about each layer in the NN in <kbd>network_summary</kbd>. Notice how we can also set up some training parameters here, including the delta, how many streams to use for gradient descent (we'll see this later), as well as the number of training epochs.</p>
<p>We can also see one other input at the beginning called layers. Here, we can indicate the construction of the NN by describing each layer, which the constructor will create by iterating through each element of layers and calling the <kbd>add_layer</kbd> method, which we will implement next:</p>
<pre>class SequentialNetwork:<br/> def __init__(self, layers=None, delta=None, stream = None, max_batch_size=32, max_streams=10, epochs = 10):<br/><br/> self.network = []<br/> self.network_summary = []<br/> self.network_mem = []<br/> <br/> if stream is not None:<br/>  self.stream = stream<br/> else:<br/>  self.stream = drv.Stream()<br/> <br/> if delta is None:<br/>  delta = 0.0001<br/> <br/> self.delta = delta<br/> self.max_batch_size=max_batch_size<br/> self.max_streams = max_streams<br/> self.epochs = epochs<br/> <br/> if layers is not None:<br/>  for layer in layers:<br/>   add_layer(self, layer)</pre>
<p>Now, let's implement the <kbd>add_layer</kbd> method. We will use a dictionary data type to pass all of the relevant information about the layer to the sequential network—including the type of layer (dense, softmax, and so on), the number of inputs/outputs, weights, and biases. This will append the appropriate object and information to the object's network and <kbd>network_summary</kbd> list variables, as well as appropriately allocate <kbd>gpuarray</kbd> objects to the <kbd>network_mem</kbd> list:</p>
<pre>def add_layer(self, layer):<br/> if layer['type'] == 'dense':<br/>  if len(self.network) == 0:<br/>   num_inputs = layer['num_inputs']<br/>  else:<br/>   num_inputs = self.network_summary[-1][2]<br/> <br/>  num_outputs = layer['num_outputs']<br/>  sigmoid = layer['sigmoid']<br/>  relu = layer['relu']<br/>  weights = layer['weights']<br/>  b = layer['bias']<br/> <br/>  self.network.append(DenseLayer(num_inputs=num_inputs, num_outputs=num_outputs, sigmoid=sigmoid, relu=relu, weights=weights, b=b))<br/>  self.network_summary.append( ('dense', num_inputs, num_outputs))<br/> <br/>  if self.max_batch_size &gt; 1:<br/>   if len(self.network_mem) == 0:<br/>self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][1]), dtype=np.float32))<br/> self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32 ) ) <br/> else:<br/> if len(self.network_mem) == 0:<br/> self.network_mem.append( gpuarray.empty( (self.network_summary[-1][1], ), dtype=np.float32 ) )<br/> self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32 ) ) <br/> <br/> elif layer['type'] == 'softmax':<br/> <br/>  if len(self.network) == 0:<br/>   raise Exception("Error! Softmax layer can't be first!")<br/> <br/>  if self.network_summary[-1][0] != 'dense':<br/>   raise Exception("Error! Need a dense layer before a softmax layer!")<br/> <br/> <br/>  num = self.network_summary[-1][2]<br/>  self.network.append(SoftmaxLayer(num=num))<br/>  self.network_summary.append(('softmax', num, num))<br/> <br/>  if self.max_batch_size &gt; 1:<br/>   self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32)) <br/>  else:<br/>   self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32))</pre>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of inference methods</h1>
                </header>
            
            <article>
                
<p>We will now add two methods for inference to our <kbd>SequentialNetwork</kbd> class—that is, for predicting an output given for a particular input. The first method we will just call <kbd>predict</kbd>, which will be used by the end user. In the course of the training process, we will have to make predictions based on a partial result from only some of the layers, and we will make another method to this end called <kbd>partial_predict</kbd>.</p>
<p>Let's start by implementing <em>predict</em>.  This will take two inputs—a collection of samples in the form of a one- or two-dimensional NumPy array, and possibly a user-defined CUDA stream. We will start by doing some type-checks and formatting on the samples (here, called <kbd>x</kbd>), remembering that the samples will be stored row-wise:</p>
<pre>def predict(self, x, stream=None):<br/> <br/> if stream is None:<br/>  stream = self.stream<br/> <br/> if type(x) != np.ndarray:<br/>  temp = np.array(x, dtype = np.float32)<br/>  x = temp<br/> <br/> if(x.size == self.network_mem[0].size):<br/>  self.network_mem[0].set_async(x, stream=stream)<br/> else:<br/> <br/>  if x.size &gt; self.network_mem[0].size:<br/>   raise Exception("Error: batch size too large for input.")<br/> <br/>  x0 = np.zeros((self.network_mem[0].size,), dtype=np.float32)<br/>  x0[0:x.size] = x.ravel()<br/>  self.network_mem[0].set_async(x0.reshape( self.network_mem[0].shape), stream=stream)<br/> <br/> if(len(x.shape) == 2):<br/>  batch_size = x.shape[0]<br/> else:<br/>  batch_size = 1</pre>
<p>Now, let's perform the actual inference step. We just have to iterate through our entire neural network, performing an <kbd>eval_</kbd> on each layer:</p>
<pre>for i in xrange(len(self.network)):<br/> self.network[i].eval_(x=self.network_mem[i], y= self.network_mem[i+1], batch_size=batch_size, stream=stream)</pre>
<p>We will now pull the final output of the NN, the GPU, and return it to the user. If the number of samples in <kbd>x</kbd> is actually smaller than the maximum batch size, we will slice the output array appropriately before it is returned:</p>
<pre>y = self.network_mem[-1].get_async(stream=stream)<br/> <br/>if len(y.shape) == 2:<br/> y = y[0:batch_size, :]<br/> <br/>return y</pre>
<p>Now, with that done, let's implement <kbd>partial_predict</kbd>. Let's briefly discuss the idea behind this. When we are in the training process, we will evaluate a collection of samples, and then look at how a subtle change of adding <em>delta</em> to each weight and bias individually will affect the outputs. To save time, we can calculate the outputs of each layer and store them for a given collection of samples, and then only recompute the output for the layer where we change the weight, as well as for all subsequent layers. We'll see the idea behind this in a little more depth soon, but for now, we can implement this like so:</p>
<pre>def partial_predict(self, layer_index=None, w_t=None, b_t=None, partial_mem=None, stream=None, batch_size=None, delta=None):<br/> <br/> self.network[layer_index].eval_(x=self.network_mem[layer_index], y = partial_mem[layer_index+1], batch_size=batch_size, stream = stream, w_t=w_t, b_t=b_t, delta=delta)<br/> <br/> for i in xrange(layer_index+1, len(self.network)):<br/>  self.network[i].eval_(x=partial_mem[i], y =partial_mem[i+1], batch_size=batch_size, stream = stream)</pre>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article>
                
<p>We will now make a full implementation of the training method for our NN in the form of <strong>batch-stochastic gradient descent (BSGD)</strong>. Let's think about what this means, word by word. <strong>Batch</strong> means that this training algorithm will operate on a collection of training samples at once, rather than all of the samples simultaneously, while <strong>stochastic</strong> indicates that each batch is chosen randomly. <strong>Gradient</strong> means that we will be using a gradient from calculus—which, here, is the collection of derivatives for each weight and bias on the loss function. Finally, <strong>descent</strong> means that we are trying to reduce the loss function—we do this by iteratively making subtle changes on the weights and biases by <em>subtracting</em> the Gradient. </p>
<div class="packt_infobox">Remember from calculus that the gradient of a point always points in the direction of the greatest <em>increase</em>, with its opposite direction being that of the greatest <em>decrease</em>. Since we want a <em>decrease</em>, we subtract the gradient.</div>
<p>We will now implement BSGD as the <kbd>bsgd</kbd> method in our <kbd>SequentialNetwork</kbd> class. Let's go over the input parameters of <kbd>bsgd</kbd>, one by one: </p>
<ul>
<li><kbd>training</kbd> will be a two-dimensional NumPy array of training samples</li>
<li><kbd>labels</kbd> will be the desired output of the final layer of the NN corresponding to each training sample</li>
<li><kbd>delta</kbd> will indicate how much we should increase a weight for the calculation of derivatives by</li>
<li><kbd>max_streams</kbd> will indicate the maximum number of concurrent CUDA streams that BSGD will perform calculations over</li>
<li><kbd>batch_size</kbd> will indicate how large we want the batches that we will calculate the loss function on for each update of the weights</li>
<li><kbd>epochs</kbd> will indicate how many times we shuffle the order of the current set of samples, break into a collection of batches, and then perform BSGD on</li>
<li><kbd>training_rate</kbd> will indicate the rate at which we will update our weights and biases with our gradient calculations</li>
</ul>
<p>We'll start out this method as usual and perform some checks and typecasting, set up the collection of CUDA stream objects into a Python list, and allocate some additional needed GPU memory in another list:</p>
<pre>def bsgd(self, training=None, labels=None, delta=None, max_streams = None, batch_size = None, epochs = 1, training_rate=0.01):<br/> <br/> training_rate = np.float32(training_rate)<br/> <br/> training = np.float32(training)<br/> labels = np.float32(labels)<br/> <br/> if( training.shape[0] != labels.shape[0] ):<br/>  raise Exception("Number of training data points should be same as labels!")<br/><br/> if max_streams is None:<br/>  max_streams = self.max_streams<br/> <br/> if epochs is None:<br/> epochs = self.epochs<br/> <br/> if delta is None:<br/> delta = self.delta<br/> <br/> streams = []<br/> bgd_mem = []<br/> <br/> # create the streams needed for training<br/> for _ in xrange(max_streams):<br/>  streams.append(drv.Stream())<br/>  bgd_mem.append([])<br/> <br/> <br/> # allocate memory for each stream<br/> for i in xrange(len(bgd_mem)):<br/>  for mem_bank in self.network_mem:<br/>   bgd_mem[i].append( gpuarray.empty_like(mem_bank) )</pre>
<p>Now, we can begin training. We will start by doing an iteration of the entire BSGD for each <kbd>epoch</kbd>, performing a random shuffle of the entire dataset for each epoch. We'll print some information to the terminal as well so that the user will have some status updates in the training process:</p>
<pre>num_points = training.shape[0]<br/> <br/>if batch_size is None:<br/> batch_size = self.max_batch_size<br/> <br/>index = range(training.shape[0])<br/> <br/>for k in xrange(epochs): <br/> <br/> print '-----------------------------------------------------------'<br/> print 'Starting training epoch: %s' % k<br/> print 'Batch size: %s , Total number of training samples: %s' % (batch_size, num_points)<br/> print '-----------------------------------------------------------'<br/> <br/> all_grad = []<br/> <br/> np.random.shuffle(index)</pre>
<p>Now, we will make a loop that iterates over each batch in the shuffled dataset. We start by calculating the entropy from the current batch, and we will print this as well. If the user sees decreases in entropy, then they will know that gradient descent is working here:</p>
<pre>for r in xrange(int(np.floor(training.shape[0]/batch_size))):<br/> <br/> batch_index = index[r*batch_size:(r+1)*batch_size] <br/> <br/> batch_training = training[batch_index, :]<br/> batch_labels = labels[batch_index, :]<br/> <br/> batch_predictions = self.predict(batch_training)<br/> <br/> cur_entropy = cross_entropy(predictions=batch_predictions, ground_truth=batch_labels)<br/> <br/> print 'entropy: %s' % cur_entropy</pre>
<p>We will now iterate through each dense layer of our NN, calculating the gradient for the entire set of weights and biases. We will store these derivatives for the weights and biases in <em>flattened</em> (one-dimensional) arrays, which will correspond to the <kbd>w_t</kbd> and <kbd>b_t</kbd> indices in our CUDA kernels, which are also flattened. Since we will have multiple streams process different outputs for different weights, we will use a Python Queue container to store the set of weights and biases that are yet to be processed for this batch: we can then just pop values off the top of this container to the next available stream (we'll store these as tuples, with the first element indicating whether this is a weight or bias, in particular):</p>
<pre>for i in xrange(len(self.network)):<br/> <br/> if self.network_summary[i][0] != 'dense':<br/>  continue<br/> <br/> all_weights = Queue()<br/> <br/> grad_w = np.zeros((self.network[i].weights.size,), dtype=np.float32)<br/> grad_b = np.zeros((self.network[i].b.size,), dtype=np.float32)<br/> <br/> for w in xrange( self.network[i].weights.size ):<br/>  all_weights.put( ('w', np.int32(w) ) )<br/> <br/> for b in xrange( self.network[i].b.size ):<br/>  all_weights.put(('b', np.int32(b) ) )</pre>
<p>Now, we need to iterate over each and every weight and bias, which we can do with a <kbd>while</kbd> loop that checks if the <kbd>queue</kbd> object we just set up is empty. We will set up another queue, <kbd>stream_weights</kbd>, that will help us organize which weights and biases each stream has processed. After setting up the weight and bias inputs appropriately, we can now use <kbd>partial_predict</kbd> by using the current stream and corresponding GPU memory arrays:</p>
<div class="packt_infobox">Notice that we already performed a <kbd>predict</kbd> for this batch of samples to calculate the entropy, so we are now able to perform <kbd>partial_predict</kbd> on this batch, provided we are careful about which memory and layers we use.</div>
<pre>while not all_weights.empty():<br/> <br/> stream_weights = Queue()<br/> <br/> for j in xrange(max_streams):<br/> <br/>  if all_weights.empty():<br/>    break<br/> <br/>  wb = all_weights.get()<br/> <br/>  if wb[0] == 'w':<br/>   w_t = wb[1]<br/>   b_t = None<br/>  elif wb[0] == 'b':<br/>   b_t = wb[1]<br/>   w_t = None<br/> <br/>  stream_weights.put( wb )<br/> <br/>  self.partial_predict(layer_index=i, w_t=w_t, b_t=b_t, partial_mem=bgd_mem[j], stream=streams[j], batch_size=batch_size, delta=delta)</pre>
<p>We have only computed the prediction of the output for alterations of a small set of weights and biases. We will have to compute the entropy for each, and then store the value of the derivative in the flattened arrays:</p>
<pre>for j in xrange(max_streams):<br/> <br/> if stream_weights.empty():<br/>  break<br/> <br/> wb = stream_weights.get()<br/> <br/> w_predictions = bgd_mem[j][-1].get_async(stream=streams[j])<br/> <br/> w_entropy = cross_entropy(predictions=w_predictions[ :batch_size,:], ground_truth=batch_labels)<br/> <br/> <br/> <br/> if wb[0] == 'w':<br/>  w_t = wb[1]<br/>  grad_w[w_t] = -(w_entropy - cur_entropy) / delta<br/> <br/> elif wb[0] == 'b':<br/>  b_t = wb[1]<br/>  grad_b[b_t] = -(w_entropy - cur_entropy) / delta</pre>
<p>We have now finished the <kbd>while</kbd> loop. Once we reach the outside of this, we will know that we've calculated the derivatives for all weights and biases for this particular layer. Before we iterate to the next layer, we will append the calculated values for the gradient of the current set of weights and biases into the <kbd>all_grad</kbd> list. We will also reshape the flattened list of weights back into the original shape while we're at it:</p>
<pre>all_grad.append([np.reshape(grad_w,self.network[i].weights.shape) , grad_b])</pre>
<p>After we are done iterating over every layer, we can perform the optimization of the weights and biases of our NN on this batch. Notice how if the <kbd>training_rate</kbd> variable is far less than <kbd>1</kbd>, this will reduce how fast the weights are updated:</p>
<pre>for i in xrange(len(self.network)):<br/> if self.network_summary[i][0] == 'dense':<br/>  new_weights = self.network[i].weights.get()<br/>  new_weights += training_rate*all_grad[i][0]<br/>  new_bias = self.network[i].b.get()<br/>  new_bias += training_rate*all_grad[i][1]<br/>  self.network[i].weights.set(new_weights)<br/>  self.network[i].b.set(new_bias)</pre>
<p>We have fully implemented a (very simple) GPU-based DNN!</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conditioning and normalizing data</h1>
                </header>
            
            <article>
                
<p>Before we move on to training and testing our brand-new NN, we need to step back for a moment and talk about <strong>conditioning</strong> and <strong>normalizing</strong> data. NNs are highly susceptible to numerical error, especially when inputs have a large variance in scale. This can be mitigated by properly <strong>conditioning</strong> our training data; this means that for each point in an input sample, we will calculate the mean and variance of each point over all samples, and then subtract the mean and divide by the standard deviation for each point in each sample before it is input into the NN for either training or inference (prediction). This method is known as n<strong>ormalization</strong>. Let's put together a small Python function that can do this for us:</p>
<pre>def condition_data(data, means=None, stds=None):<br/> <br/> if means is None:<br/>  means = np.mean(data, axis=0)<br/> <br/> if stds is None:<br/>  stds = np.std(data, axis = 0)<br/> <br/> conditioned_data = data.copy()<br/> conditioned_data -= means<br/> conditioned_data /= stds<br/> <br/> return (conditioned_data, means, stds)</pre>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Iris dataset</h1>
                </header>
            
            <article>
                
<p>We will now construct our very own DNN for a real-life problem: classification of flower types based on the measurements of petals. We will be working with the well-known <em>Iris dataset</em> for this. This dataset is stored as a comma-separated value (CSV) text file, with each line containing four different numerical values (petal measurements), followed by the flower type (here, there are three classes—<em>Irissetosa</em>, <em>Irisversicolor</em>, and <em>Irisvirginica</em>). We will now design a small DNN that will classify the type of iris, based on this set.</p>
<div class="packt_infobox">Before we continue, please download the Iris dataset and put it into your working directory.  This is available from the UC Irvine Machine Learning repository, which can be found here: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>.</div>
<p>We will start by processing this file into appropriate data arrays that we can use for training and validating our DNN. Let's start by opening up our main function; we will need to translate the names of the flowers into actual classes that a DNN can output, so let's make a small dictionary that will give us a corresponding label for each class. We will also set up some empty lists to store our training data and labels:</p>
<pre>if __name__ == '__main__':<br/> to_class = { 'Iris-setosa' : [1,0,0] , 'Iris-versicolor' : [0,1,0], 'Iris-virginica' : [0,0,1]}<br/> <br/> iris_data = []<br/> iris_labels = []</pre>
<p>Now, let's read from the CSV file. We will use the <kbd>reader</kbd> function from Python's <kbd>csv</kbd> module, which we imported earlier:</p>
<pre>with open('C:/Users/btuom/examples/9/iris.data', 'rb') as csvfile:<br/> csvreader = csv.reader(csvfile, delimiter=',')<br/> for row in csvreader:<br/>  newrow = []<br/>  if len(row) != 5:<br/>   break<br/>  for i in range(4):<br/>   newrow.append(row[i])<br/>  iris_data.append(newrow)<br/>  iris_labels.append(to_class[row[4]])</pre>
<p>We will now randomly shuffle the data and use two-third of these samples as training data. The remaining one-third will be used for test (validation) data:</p>
<pre>iris_len = len(iris_data)<br/>shuffled_index = list(range(iris_len))<br/>np.random.shuffle(shuffled_index)<br/>  <br/>iris_data = np.float32(iris_data)<br/>iris_labels = np.float32(iris_labels)<br/>iris_data = iris_data[shuffled_index, :]<br/>iris_labels = iris_labels[shuffled_index,:]<br/> <br/>t_len = (2*iris_len) // 3<br/> <br/>iris_train = iris_data[:t_len, :]<br/>label_train = iris_labels[:t_len, :]<br/> <br/>iris_test = iris_data[t_len:,:]<br/>label_test = iris_labels[t_len:, :]</pre>
<p>Now, finally, we can begin building our DNN! First, let's create a <kbd>SequentialNetwork</kbd> object. We'll set the <kbd>max_batch_size</kbd> to <kbd>32</kbd>:</p>
<pre>sn = SequentialNetwork( max_batch_size=32 )</pre>
<p class="mce-root">Now, let's create our NN. This will consist of four dense layers (two hidden) and a softmax layer. We will increment the number of neurons in each layer until the final layer, which will only have three outputs (one for each class). This increasing amount of neurons per layer allows us to capture some of the subtleties of the data:</p>
<pre class="mce-root"><br/>sn.add_layer({'type' : 'dense', 'num_inputs' : 4, 'num_outputs' : 10, 'relu': True, 'sigmoid': False, 'weights' : None, 'bias' : None} ) <br/>sn.add_layer({'type' : 'dense', 'num_inputs' : 10, 'num_outputs' : 15, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) <br/>sn.add_layer({'type' : 'dense', 'num_inputs' : 15, 'num_outputs' : 20, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) <br/>sn.add_layer({'type' : 'dense', 'num_inputs' : 20, 'num_outputs' : 3, 'relu': True, 'sigmoid': False, 'weights': None , 'bias': None } ) <br/>sn.add_layer({'type' : 'softmax'})</pre>
<p>We will now condition our training data and begin the training with our BSGD method that we just implemented. We will train with <kbd>batch_size</kbd> set to <kbd>16</kbd>, <kbd>max_streams</kbd> set to <kbd>10</kbd>, the number of <kbd>epochs</kbd> set to 100, the <kbd>delta</kbd> set to 0.0001, and the <kbd>training_rate</kbd> set to 1—these will be admissible parameters for virtually any modern GPU. We will also time the training procedure while we're at it, which can be rather time-consuming:</p>
<pre>ctrain, means, stds = condition_data(iris_train)<br/><br/>t1 = time()<br/>sn.bsgd(training=ctrain, labels=label_train, batch_size=16, max_streams=20, epochs=100 , delta=0.0001, training_rate=1)<br/>training_time = time() - t1</pre>
<p>Now, our DNN is fully trained. We are ready to begin the validation process! Let's set up a Python variable called <kbd>hits</kbd> to count the total number of correct classifications. We will also need to condition the validation/testing data too. One more thing—we determine the class by the index corresponding to the largest value of the softmax layer of our DNN. We can check whether this gives us the correct classification by using NumPy's <kbd>argmax</kbd> function, like so:</p>
<pre>hits = 0<br/>ctest, _, _ = condition_data(iris_test, means=means, stds=stds)<br/>for i in range(ctest.shape[0]):<br/> if np.argmax(sn.predict(ctest[i,:])) == np.argmax( label_test[i,:]):<br/>  hits += 1</pre>
<p>Now, we are ready to check how well our DNN actually works. Let's output the accuracy as well as the total training time:</p>
<pre>print 'Percentage Correct Classifications: %s' % (float(hits ) / ctest.shape[0])<br/>print 'Total Training Time: %s' % training_time</pre>
<p>Now, we are done. We can now fully implement a <span>DNN</span> with Python and CUDA! Generally speaking, you can expect an accuracy ranging from 80%-97% for this particular problem, with a training time of 10-20 minutes on any Pascal-level GPU. </p>
<div class="packt_infobox">The code for this chapter is available in the <kbd>deep_neural_network.py</kbd> file, under the appropriate directory in this book's GitHub repository.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started by giving the definition of an artificial neural network, and showed you how individual ANs can be combined into dense layers, which combine together into a full-on deep neural network. We then implemented a dense layer in CUDA-C and made an appropriate corresponding Python wrapper class. We also included functionality to add ReLU and sigmoid layers on the outputs of a dense layer. We saw the definition and motivation of using a softmax layer, which is used for classification problems, and then implemented this in CUDA-C and Python. Finally, we implemented a Python class so that we could build a sequential feed-forward DNN from the prior classes;  we implemented a cross-entropy loss function, and then used this in our loss function in our implementation of gradient descent to train the weights and biases in our DNN. Finally, we used our implementation to construct, train, and test a DNN on a real-life dataset.</p>
<p>We now have a great deal of self-confidence in our CUDA programming abilities, since we can write our own GPU-based DNN! We will now move on to some very advanced material in the next two chapters, where we will look at how we can write our own interfaces to compiled CUDA code, as well as some of the very technical ins and outs of <span>NVIDIA </span>GPUs.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Suppose you construct a DNN and after training it, it yields only garbage. After inspection, you find that all of the weights and biases are either huge numbers or NaNs. What might the problem be?</li>
<li>Name one possible problem with a small <kbd>training_rate</kbd> value.</li>
<li>Name one possible problem with a large <kbd>training_rate</kbd> value.</li>
<li>Suppose we want to train a DNN that will assign multiple labels to an image of an animal ("slimey", "furry", "red", "brown", and so on). Should we use a sigmoid or softmax layer at the end of the DNN?</li>
<li>Suppose we want to classify an image of a single animal as either a cat or dog. Do we use sigmoid or softmax?</li>
<li>If we decrease the batch size, will there be more or less updates to the weights and biases during gradient descent training?</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>