<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Juggling Data with Spark</h1></div></div></div><p>As per the batch and streaming architecture laid out in the previous chapter, we need data to fuel our applications. We will harvest data focused on Apache Spark from Twitter. The objective of this chapter is to prepare data to be further used by the machine learning and streaming applications. This chapter focuses on how to exchange code and data across the distributed network. We will get practical insights into serialization, persistence, marshaling, and caching. We will get to grips with on Spark SQL, the key Spark module to interactively explore structured and semi-structured data. The fundamental data structure powering Spark SQL is the Spark dataframe. The Spark dataframe is inspired by the Python Pandas dataframe and the R dataframe. It is a powerful data structure, well understood and appreciated by data scientists with a background in R or Python.</p><p>In this chapter, we will cover the following points:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Connect to Twitter, collect the relevant data, and then persist it in various formats such as JSON and CSV and data stores such as MongoDB</li><li class="listitem" style="list-style-type: disc">Analyze the data using Blaze and Odo, a spin-off library from Blaze, in order to connect and transfer data from various sources and destinations</li><li class="listitem" style="list-style-type: disc">Introduce Spark dataframes as the foundation for data interchange between the various Spark modules and explore data interactively using Spark SQL</li></ul></div><div><div><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Revisiting the data-intensive app architecture</h1></div></div></div><p>Let's <a id="id146" class="indexterm"/>first put in context the focus of this chapter with respect to the data-intensive app architecture. We will concentrate our attention on the integration layer and essentially run through iterative cycles of the acquisition, refinement, and persistence of the data. This cycle was termed the five Cs. The five Cs stand for <em>connect</em>, <em>collect</em>, <em>correct</em>, <em>compose</em>, and <em>consume</em>. They are the essential processes we run through in the integration layer in order to get to the right quality and quantity of data retrieved from Twitter. We will also delve deeper in the persistence layer and set up a data store such as MongoDB to collect our data for processing later.</p><p>We will <a id="id147" class="indexterm"/>explore the data with Blaze, a Python library for data manipulation, and Spark SQL, the interactive module of Spark for data discovery powered by the Spark dataframe. The dataframe paradigm is shared by Python Pandas, Python Blaze, and Spark SQL. We will get a feel for the nuances of the three dataframe flavors.</p><p>The following diagram sets the context of the chapter's focus, highlighting the integration layer and the persistence layer:</p><div><img src="img/B03986_03_01.jpg" alt="Revisiting the data-intensive app architecture"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Serializing and deserializing data</h1></div></div></div><p>As we<a id="id148" class="indexterm"/> are harvesting data from web APIs under rate limit constraints, we<a id="id149" class="indexterm"/> need to store them. As the data is processed on a distributed cluster, we need consistent ways to save state and retrieve it for later usage.</p><p>Let's now define serialization, persistence, marshaling, and caching or memorization.</p><p>Serializing a Python object converts it into a stream of bytes. The Python object needs to be retrieved beyond the scope of its existence, when the program is shut. The serialized Python object can be transferred over a network or stored in a persistent storage. Deserialization is the opposite and converts the stream of bytes into the original Python object so the program can carry on from the saved state. The most popular serialization library in Python is Pickle. As a matter of fact, the PySpark commands are transferred over the wire to the worker nodes via pickled data.</p><p>Persistence saves a program's state data to disk or memory so that it can carry on where it left off upon restart. It saves a Python object from memory to a file or a database and loads it later with the same state.</p><p>Marshalling sends Python code or data over a network TCP connection in a multicore or distributed system.</p><p>Caching converts a Python object to a string in memory so that it can be used as a dictionary key later on. Spark supports pulling a dataset into a cluster-wide, in-memory cache. This is very useful when data is accessed repeatedly such as when querying a small reference dataset or running an iterative algorithm such as Google PageRank.</p><p>Caching is a crucial concept for Spark as it allows us to save RDDs in memory or with a spillage to disk. The caching strategy can be selected based on the lineage of the data or the <strong>DAG</strong> (short for <strong>Directed Acyclic Graph</strong>) of transformations applied to the RDDs in order to<a id="id150" class="indexterm"/> minimize shuffle or cross network heavy data exchange. In order to achieve good performance with Spark, beware of data shuffling. A good partitioning policy and use of RDD caching, coupled with avoiding unnecessary action operations, leads to better performance with Spark.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Harvesting and storing data</h1></div></div></div><p>Before <a id="id151" class="indexterm"/>delving into database persistent storage such as MongoDB, we <a id="id152" class="indexterm"/>will look at some useful file storages that are widely used: <strong>CSV</strong> (short <a id="id153" class="indexterm"/>for <strong>comma-separated values</strong>) and <a id="id154" class="indexterm"/>
<strong>JSON</strong> (short for <strong>JavaScript Object Notation</strong>) file storage. The enduring popularity of these two file formats lies in a few key reasons: they are human readable, simple, relatively lightweight, and easy to use.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec28"/>Persisting data in CSV</h2></div></div></div><p>The CSV<a id="id155" class="indexterm"/> format is lightweight, human readable, and easy to use. It has delimited text columns with an inherent tabular schema.</p><p>Python offers a robust <code class="literal">csv</code> library that can serialize a <code class="literal">csv</code> file into a Python dictionary. For the purpose of our program, we have written a <code class="literal">python</code> class that manages to persist data in CSV format and read from a given CSV.</p><p>Let's run through the code of the class <code class="literal">IO_csv</code> object. The <code class="literal">__init__</code> section of the class basically instantiates the file path, the filename, and the file suffix (in this case, <code class="literal">.csv</code>):</p><div><pre class="programlisting">class IO_csv(object):

    def __init__(self, filepath, filename, filesuffix='csv'):
        self.filepath = filepath       # /path/to/file without the /' at the end
        self.filename = filename       # FILE_NAME
        self.filesuffix = filesuffix</pre></div><p>The <code class="literal">save</code> method of the class uses a Python named tuple and the header fields of the <code class="literal">csv</code> file in order to impart a schema while persisting the rows of the CSV. If the <code class="literal">csv</code> file already exists, it will be appended and not overwritten otherwise; it will be created:</p><div><pre class="programlisting">    def save(self, data, NTname, fields):
        # NTname = Name of the NamedTuple
        # fields = header of CSV - list of the fields name
        NTuple = namedtuple(NTname, fields)
        
        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):
            # Append existing file
            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:
                writer = csv.writer(f)
                # writer.writerow(fields) # fields = header of CSV
                writer.writerows([row for row in map(NTuple._make, data)])
                # list comprehension using map on the NamedTuple._make() iterable and the data file to be saved
                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file
        else:
            # Create new file
            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(fields) # fields = header of CSV - list of the fields name
                writer.writerows([row for row in map(NTuple._make, data)])
                #  list comprehension using map on the NamedTuple._make() iterable and the data file to be saved
                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file</pre></div><p>The <code class="literal">load</code> method <a id="id156" class="indexterm"/>of the class also uses a Python named tuple and the header fields of the <code class="literal">csv</code> file in order to retrieve the data using a consistent schema. The <code class="literal">load</code> method is a memory-efficient generator to avoid loading a huge file in memory: hence we use <code class="literal">yield</code> in place of <code class="literal">return</code>:</p><div><pre class="programlisting">    def load(self, NTname, fields):
        # NTname = Name of the NamedTuple
        # fields = header of CSV - list of the fields name
        NTuple = namedtuple(NTname, fields)
        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'rU') as f:
            reader = csv.reader(f)
            for row in map(NTuple._make, reader):
                # Using map on the NamedTuple._make() iterable and the reader file to be loaded
                yield row </pre></div><p>Here's the named tuple. We are using it to parse the tweet in order to save or retrieve them to and from the <code class="literal">csv</code> file:</p><div><pre class="programlisting">fields01 = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text', 'url']
Tweet01 = namedtuple('Tweet01',fields01)

def parse_tweet(data):
    """
    Parse a ``tweet`` from the given response data.
    """
    return Tweet01(
        id=data.get('id', None),
        created_at=data.get('created_at', None),
        user_id=data.get('user_id', None),
        user_name=data.get('user_name', None),
        tweet_text=data.get('tweet_text', None),
        url=data.get('url')
    )</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec29"/>Persisting data in JSON</h2></div></div></div><p>JSON is <a id="id157" class="indexterm"/>one of the most popular data formats for Internet-based applications. All the APIs we are dealing with, Twitter, GitHub, and Meetup, deliver their data in JSON format. The JSON format is relatively lightweight compared to XML and human readable, and the schema is embedded in JSON. As opposed to the CSV format, where all records follow exactly the same tabular structure, JSON records can vary in their structure. JSON is semi-structured. A JSON record can be mapped into a Python dictionary of dictionaries.</p><p>Let's run<a id="id158" class="indexterm"/> through the code of the class <code class="literal">IO_json</code> object. The <code class="literal">__init__</code> section of the class basically instantiates the file path, the filename, and the file suffix (in this case, <code class="literal">.json</code>):</p><div><pre class="programlisting">class IO_json(object):
    def __init__(self, filepath, filename, filesuffix='json'):
        self.filepath = filepath        # /path/to/file without the /' at the end
        self.filename = filename        # FILE_NAME
        self.filesuffix = filesuffix
        # self.file_io = os.path.join(dir_name, .'.join((base_filename, filename_suffix)))</pre></div><p>The <code class="literal">save</code> method of the class uses <code class="literal">utf-8</code> encoding in order to ensure read and write compatibility of the data. If the JSON file already exists, it will be appended and not overwritten; otherwise it will be created:</p><div><pre class="programlisting">    def save(self, data):
        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):
            # Append existing file
            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:
                f.write(unicode(json.dumps(data, ensure_ascii= False))) # In python 3, there is no "unicode" function 
                # f.write(json.dumps(data, ensure_ascii= False)) # create a \" escape char for " in the saved file        
        else:
            # Create new file
            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:
                f.write(unicode(json.dumps(data, ensure_ascii= False)))
                # f.write(json.dumps(data, ensure_ascii= False))</pre></div><p>The <code class="literal">load</code> method of the class just returns the file that has been read. A further <code class="literal">json.loads</code> function needs to be applied in order to retrieve the <code class="literal">json</code> out of the file read:</p><div><pre class="programlisting">    def load(self):
        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:
            return f.read()</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec30"/>Setting up MongoDB</h2></div></div></div><p>It is <a id="id159" class="indexterm"/>crucial to store the information harvested. Thus, we set <a id="id160" class="indexterm"/>up MongoDB as our main document data store. As all the<a id="id161" class="indexterm"/> information collected is in JSON format and MongoDB stores information in <strong>BSON</strong> (short for <strong>Binary JSON</strong>), it is therefore a natural choice.</p><p>We will run through the following steps now:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing the MongoDB server and client</li><li class="listitem" style="list-style-type: disc">Running the MongoDB server</li><li class="listitem" style="list-style-type: disc">Running the Mongo client</li><li class="listitem" style="list-style-type: disc">Installing the PyMongo driver</li><li class="listitem" style="list-style-type: disc">Creating the Python Mongo client</li></ul></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec03"/>Installing the MongoDB server and client</h3></div></div></div><p>In order<a id="id162" class="indexterm"/> to install the MongoDB package, perform through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the public key used by the package management system (in our case, Ubuntu's <code class="literal">apt</code>). To import the MongoDB public key, we issue the following command:<div><pre class="programlisting">
<strong>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</strong>
</pre></div></li><li class="listitem">Create a list file for MongoDB. To create the list file, we use the following command:<div><pre class="programlisting">
<strong>echo "deb http://repo.mongodb.org/apt/ubuntu "$("lsb_release -sc)"/ mongodb-org/3.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list</strong>
</pre></div></li><li class="listitem">Update the local package database as <code class="literal">sudo</code>:<div><pre class="programlisting">
<strong>sudo apt-get update</strong>
</pre></div></li><li class="listitem">Install the MongoDB packages. We install the latest stable version of MongoDB with the following command:<div><pre class="programlisting">
<strong>sudo apt-get install -y mongodb-org</strong>
</pre></div></li></ol></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec04"/>Running the MongoDB server</h3></div></div></div><p>Let's <a id="id163" class="indexterm"/>start the MongoDB server:</p><div><ol class="orderedlist arabic"><li class="listitem">To start MongoDB server, we issue the following command to start <code class="literal">mongod</code>:<div><pre class="programlisting">
<strong>sudo service mongodb start</strong>
</pre></div></li><li class="listitem">To check whether <code class="literal">mongod</code> has started properly, we issue the command: <div><pre class="programlisting">
<strong>an@an-VB:/usr/bin$ ps -ef | grep mongo</strong>
<strong>mongodb    967     1  4 07:03 ?        00:02:02 /usr/bin/mongod --config /etc/mongod.conf</strong>
<strong>an        3143  3085  0 07:45 pts/3    00:00:00 grep --color=auto mongo</strong>
</pre></div><p>In this case, we see that <code class="literal">mongodb</code> is running in process <code class="literal">967</code>.</p></li><li class="listitem">The <code class="literal">mongod</code> server sends a message to the effect that it is waiting for connection on <code class="literal">port 27017</code>. This <a id="id164" class="indexterm"/>is the default port for MongoDB. It can be changed in the configuration file.</li><li class="listitem">We can check the contents of the log file at <code class="literal">/var/log/mongod/mongod.log</code>:<div><pre class="programlisting">
<strong>an@an-VB:/var/lib/mongodb$ ls -lru</strong>
<strong>total 81936</strong>
<strong>drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 _tmp</strong>
<strong>-rw-r--r-- 1 mongodb nogroup       69 Apr 25 11:19 storage.bson</strong>
<strong>-rwxr-xr-x 1 mongodb nogroup        5 Apr 25 11:19 mongod.lock</strong>
<strong>-rw------- 1 mongodb nogroup 16777216 Apr 25 11:19 local.ns</strong>
<strong>-rw------- 1 mongodb nogroup 67108864 Apr 25 11:19 local.0</strong>
<strong>drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 journal</strong>
</pre></div></li><li class="listitem">In order to stop the <code class="literal">mongodb</code> server, just issue the following command:<div><pre class="programlisting">
<strong>sudo service mongodb stop</strong>
</pre></div></li></ol></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec05"/>Running the Mongo client</h3></div></div></div><p>Running the Mongo client in the console is as easy as calling <code class="literal">mongo</code>, as highlighted in the following command:</p><div><pre class="programlisting">
<strong>an@an-VB:/usr/bin$ mongo</strong>
<strong>MongoDB shell version: 3.0.2</strong>
<strong>connecting to: test</strong>
<strong>Server has startup warnings: </strong>
<strong>2015-05-30T07:03:49.387+0200 I CONTROL  [initandlisten] </strong>
<strong>2015-05-30T07:03:49.388+0200 I CONTROL  [initandlisten] </strong>
</pre></div><p>At the mongo client console prompt, we can see the databases with the following commands:</p><div><pre class="programlisting">&gt; show dbs
local  0.078GB
test   0.078GB</pre></div><p>We select the test database using <code class="literal">use test</code>:</p><div><pre class="programlisting">&gt; use test
switched to db test</pre></div><p>We display <a id="id165" class="indexterm"/>the collections within the test database:</p><div><pre class="programlisting">&gt; show collections
restaurants
system.indexes</pre></div><p>We check a sample record in the restaurant collection listed previously:</p><div><pre class="programlisting">&gt; db.restaurants.find()
{ "_id" : ObjectId("553b70055e82e7b824ae0e6f"), "address : { "building : "1007", "coord" : [ -73.856077, 40.848447 ], "street : "Morris Park Ave", "zipcode : "10462 }, "borough : "Bronx", "cuisine : "Bakery", "grades : [ { "grade : "A", "score" : 2, "date" : ISODate("2014-03-03T00:00:00Z") }, { "date" : ISODate("2013-09-11T00:00:00Z"), "grade : "A", "score" : 6 }, { "score" : 10, "date" : ISODate("2013-01-24T00:00:00Z"), "grade : "A }, { "date" : ISODate("2011-11-23T00:00:00Z"), "grade : "A", "score" : 9 }, { "date" : ISODate("2011-03-10T00:00:00Z"), "grade : "B", "score" : 14 } ], "name : "Morris Park Bake Shop", "restaurant_id : "30075445" }</pre></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec06"/>Installing the PyMongo driver</h3></div></div></div><p>Installing<a id="id166" class="indexterm"/> the Python driver with anaconda is easy. Just run the following command at the terminal:</p><div><pre class="programlisting">conda install pymongo</pre></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec07"/>Creating the Python client for MongoDB</h3></div></div></div><p>We are<a id="id167" class="indexterm"/> creating a <code class="literal">IO_mongo</code> class that will be used in our harvesting and processing programs to store the data collected and retrieved saved information. In order to create the <code class="literal">mongo</code> client, we will import the <code class="literal">MongoClient</code> module from <code class="literal">pymongo</code>. We connect to the <code class="literal">mongodb</code> server on localhost at port 27017. The command is as follows:</p><div><pre class="programlisting">from pymongo import MongoClient as MCli

class IO_mongo(object):
    conn={'host':'localhost', 'ip':'27017'}</pre></div><p>We initialize our class with the client connection, the database (in this case, <code class="literal">twtr_db</code>), and the collection (in this case, <code class="literal">twtr_coll</code>) to be accessed:</p><div><pre class="programlisting">    def __init__(self, db='twtr_db', coll='twtr_coll', **conn ):
        # Connects to the MongoDB server 
        self.client = MCli(**conn)
        self.db = self.client[db]
        self.coll = self.db[coll]</pre></div><p>The <code class="literal">save</code> method inserts new records in the preinitialized collection and database:</p><div><pre class="programlisting">    def save(self, data):
        # Insert to collection in db  
        return self.coll.insert(data)</pre></div><p>The <code class="literal">load</code> method <a id="id168" class="indexterm"/>allows the retrieval of specific records according to criteria and projection. In the case of large amount of data, it returns a cursor:</p><div><pre class="programlisting">    def load(self, return_cursor=False, criteria=None, projection=None):

            if criteria is None:
                criteria = {}

            if projection is None:
                cursor = self.coll.find(criteria)
            else:
                cursor = self.coll.find(criteria, projection)

            # Return a cursor for large amounts of data
            if return_cursor:
                return cursor
            else:
                return [ item for item in cursor ]</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec31"/>Harvesting data from Twitter</h2></div></div></div><p>Each <a id="id169" class="indexterm"/>social network poses its limitations and challenges. One of the main obstacles for harvesting data is an imposed rate limit. While running repeated or long-running connections between rates limit pauses, we have to be careful to avoid collecting duplicate data.</p><p>We have redesigned our connection programs outlined in the previous chapter to take care of the rate limits.</p><p>In this <code class="literal">TwitterAPI</code> class that connects and collects the tweets according to the search query we specify, we have added the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Logging capability using the Python logging library with the aim of collecting any errors or warning in the case of program failure</li><li class="listitem" style="list-style-type: disc">Persistence capability using MongoDB, with the <code class="literal">IO_mongo</code> class exposed previously as well as JSON file using the <code class="literal">IO_json</code> class</li><li class="listitem" style="list-style-type: disc">API rate limit and error management capability, so we can ensure more resilient calls to Twitter without getting barred for tapping into the firehose</li></ul></div><p>Let's go through the steps:</p><div><ol class="orderedlist arabic"><li class="listitem">We initialize by instantiating the Twitter API with our credentials:<div><pre class="programlisting">class TwitterAPI(object):
    """
    TwitterAPI class allows the Connection to Twitter via OAuth
    once you have registered with Twitter and receive the 
    necessary credentials 
    """

    def __init__(self): 
        consumer_key = 'get_your_credentials'
        consumer_secret = get your_credentials'
        access_token = 'get_your_credentials'
        access_secret = 'get your_credentials'
        self.consumer_key = consumer_key
        self.consumer_secret = consumer_secret
        self.access_token = access_token
        self.access_secret = access_secret
        self.retries = 3
        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)
        self.api = twitter.Twitter(auth=self.auth)</pre></div></li><li class="listitem">We<a id="id170" class="indexterm"/> initialize the logger by providing the log level:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">logger.debug</code>(debug message)</li><li class="listitem" style="list-style-type: disc"><code class="literal">logger.info</code>(info message)</li><li class="listitem" style="list-style-type: disc"><code class="literal">logger.warn</code>(warn message)</li><li class="listitem" style="list-style-type: disc"><code class="literal">logger.error</code>(error message)</li><li class="listitem" style="list-style-type: disc"><code class="literal">logger.critical</code>(critical message)</li></ul></div></li><li class="listitem">We set the log path and the message format:<div><pre class="programlisting">        # logger initialisation
        appName = 'twt150530'
        self.logger = logging.getLogger(appName)
        #self.logger.setLevel(logging.DEBUG)
        # create console handler and set level to debug
        logPath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'
        fileName = appName
        fileHandler = logging.FileHandler("{0}/{1}.log".format(logPath, fileName))
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fileHandler.setFormatter(formatter)
        self.logger.addHandler(fileHandler) 
        self.logger.setLevel(logging.DEBUG)</pre></div></li><li class="listitem">We<a id="id171" class="indexterm"/> initialize the JSON file persistence instruction:<div><pre class="programlisting">        # Save to JSON file initialisation
        jsonFpath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'
        jsonFname = 'twtr15053001'
        self.jsonSaver = IO_json(jsonFpath, jsonFname)</pre></div></li><li class="listitem">We initialize the MongoDB database and collection for persistence:<div><pre class="programlisting">        # Save to MongoDB Intitialisation
        self.mongoSaver = IO_mongo(db='twtr01_db', coll='twtr01_coll')</pre></div></li><li class="listitem">The method <code class="literal">searchTwitter</code> launches the search according to the query specified:<div><pre class="programlisting">    def searchTwitter(self, q, max_res=10,**kwargs):
        search_results = self.api.search.tweets(q=q, count=10, **kwargs)
        statuses = search_results['statuses']
        max_results = min(1000, max_res)
        
        for _ in range(10):
            try:
                next_results = search_results['search_metadata']['next_results']
                # self.logger.info('info' in searchTwitter - next_results:%s'% next_results[1:])
            except KeyError as e:
                self.logger.error('error' in searchTwitter: %s', %(e))
                break
            
            # next_results = urlparse.parse_qsl(next_results[1:]) # python 2.7
            next_results = urllib.parse.parse_qsl(next_results[1:])
            # self.logger.info('info' in searchTwitter - next_results[max_id]:', next_results[0:])
            kwargs = dict(next_results)
            # self.logger.info('info' in searchTwitter - next_results[max_id]:%s'% kwargs['max_id'])
            search_results = self.api.search.tweets(**kwargs)
            statuses += search_results['statuses']
            self.saveTweets(search_results['statuses'])
            
            if len(statuses) &gt; max_results:
                self.logger.info('info' in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))
                break
        return statuses</pre></div></li><li class="listitem">The <code class="literal">saveTweets</code> method <a id="id172" class="indexterm"/>actually saves the collected tweets in JSON and in MongoDB:<div><pre class="programlisting">    def saveTweets(self, statuses):
        # Saving to JSON File
        self.jsonSaver.save(statuses)
        
        # Saving to MongoDB
        for s in statuses:
            self.mongoSaver.save(s)</pre></div></li><li class="listitem">The <code class="literal">parseTweets</code> method allows us to extract the key tweet information from the vast amount of information provided by the Twitter API:<div><pre class="programlisting">    def parseTweets(self, statuses):
        return [ (status['id'], 
                  status['created_at'], 
                  status['user']['id'],
                  status['user']['name'] 
                  status['text''text'], 
                  url['expanded_url']) 
                        for status in statuses 
                            for url in status['entities']['urls'] ]</pre></div></li><li class="listitem">The <code class="literal">getTweets</code> method calls the <code class="literal">searchTwitter</code> method described previously. The <code class="literal">getTweets</code> method ensures that API calls are made reliably whilst respecting the imposed rate limit. The code is as follows:<div><pre class="programlisting">    def getTweets(self, q,  max_res=10):
        """
        Make a Twitter API call whilst managing rate limit and errors.
        """
        def handleError(e, wait_period=2, sleep_when_rate_limited=True):
            if wait_period &gt; 3600: # Seconds
                self.logger.error('Too many retries in getTweets: %s', %(e))
                raise e
            if e.e.code == 401:
                self.logger.error('error 401 * Not Authorised * in getTweets: %s', %(e))
                return None
            elif e.e.code == 404:
                self.logger.error('error 404 * Not Found * in getTweets: %s', %(e))
                return None
            elif e.e.code == 429: 
                self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', %(e))
                if sleep_when_rate_limited:
                    self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', %(e))
                    sys.stderr.flush()
                    time.sleep(60*15 + 5)
                    self.logger.info('error 429 * Retrying now * in getTweets: %s', %(e))
                    return 2
                else:
                    raise e # Caller must handle the rate limiting issue
            elif e.e.code in (500, 502, 503, 504):
                self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))
                time.sleep(wait_period)
                wait_period *= 1.5
                return wait_period
            else:
                self.logger.error('Exit - aborting - %s', %(e))
                raise e</pre></div></li><li class="listitem">Here, we<a id="id173" class="indexterm"/> are calling the <code class="literal">searchTwitter</code> API with the relevant query based on the parameters specified. If we encounter any error such as rate limitation from the provider, this will be processed by the <code class="literal">handleError</code> method:<div><pre class="programlisting">        while True:
            try:
                self.searchTwitter( q, max_res=10)
            except twitter.api.TwitterHTTPError as e:
                error_count = 0 
                wait_period = handleError(e, wait_period)
                if wait_period is None:
                    return</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Exploring data using Blaze</h1></div></div></div><p>Blaze is <a id="id174" class="indexterm"/>an open source Python library, primarily developed by<a id="id175" class="indexterm"/> Continuum.io, leveraging Python Numpy arrays and Pandas dataframe. Blaze extends to out-of-core computing, while Pandas and Numpy are single-core.</p><p>Blaze offers an adaptable, unified, and consistent user interface across various backends. Blaze orchestrates the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Data</strong>: Seamless exchange of data across storages such as CSV, JSON, HDF5, HDFS, and Bcolz files.</li><li class="listitem" style="list-style-type: disc"><strong>Computation</strong>: Using the same query processing against computational backends such as Spark, MongoDB, Pandas, or SQL Alchemy.</li><li class="listitem" style="list-style-type: disc"><strong>Symbolic expressions</strong>: Abstract expressions such as join, group-by, filter, selection, and projection with a syntax similar to Pandas but limited in scope. Implements the split-apply-combine methods pioneered by the R language.</li></ul></div><p>Blaze expressions are lazily evaluated and in that respect share a similar processing paradigm with Spark RDDs transformations.</p><p>Let's dive into Blaze by first importing the necessary libraries: <code class="literal">numpy</code>, <code class="literal">pandas</code>, <code class="literal">blaze</code> and <code class="literal">odo</code>. Odo is a spin-off of Blaze and ensures data migration from various backends. The commands are as follows:</p><div><pre class="programlisting">import numpy as np
import pandas as pd
from blaze import Data, by, join, merge
from odo import odo
BokehJS successfully loaded.</pre></div><p>We create a Pandas <code class="literal">Dataframe</code> by reading the parsed tweets saved in a CSV file, <code class="literal">twts_csv</code>:</p><div><pre class="programlisting">twts_pd_df = pd.DataFrame(twts_csv_read, columns=Tweet01._fields)
twts_pd_df.head()

Out[65]:
id    created_at    user_id    user_name    tweet_text    url
1   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
2   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
3   98808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://www.webex.com/ciscospark/
4   598808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/</pre></div><p>We run<a id="id176" class="indexterm"/> the Tweets Panda <code class="literal">Dataframe</code> to the <code class="literal">describe()</code> function<a id="id177" class="indexterm"/> to get some overall information on the dataset:</p><div><pre class="programlisting">twts_pd_df.describe()
Out[66]:
id    created_at    user_id    user_name    tweet_text    url
count  19  19  19  19  19  19
unique    7  7   6   6     6   7
top    598808944719593472    2015-05-14 11:15:52    14755521 raulsaeztapia    RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://bit.ly/1Hfd0Xm
freq    6    6    9    9    6    6</pre></div><p>We convert the Pandas <code class="literal">dataframe</code> into a Blaze <code class="literal">dataframe</code> by simply passing it through the <code class="literal">Data()</code> function:</p><div><pre class="programlisting">#
# Blaze dataframe
#
twts_bz_df = Data(twts_pd_df)</pre></div><p>We can retrieve the schema representation of the Blaze <code class="literal">dataframe</code> by passing the <code class="literal">schema</code> function:</p><div><pre class="programlisting">twts_bz_df.schema
Out[73]:
dshape("""{
  id: ?string,
  created_at: ?string,
  user_id: ?string,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>The <code class="literal">.dshape</code> function gives a record count and the schema:</p><div><pre class="programlisting">twts_bz_df.dshape
Out[74]: 
dshape("""19 * {
  id: ?string,
  created_at: ?string,
  user_id: ?string,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>We can <a id="id178" class="indexterm"/>print the Blaze <code class="literal">dataframe</code> content:</p><div><pre class="programlisting">twts_bz_df.data
Out[75]:
id    created_at    user_id    user_name    tweet_text    url
1    598831111406510082    2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...
2    598831111406510082    2015-05-14 12:43:57    14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...
... 
18   598782970082807808    2015-05-14 09:32:39    1377652806 embeddedcomputer.nl    RT @BigDataTechCon: Moving Rating Prediction w...    http://buff.ly/1QBpk8J
19   598777933730160640     2015-05-14 09:12:38   294862170    Ellen Friedman   I'm still on Euro time. If you are too check o...http://bit.ly/1Hfd0Xm</pre></div><p>We<a id="id179" class="indexterm"/> extract the column <code class="literal">tweet_text</code> and take the unique values:</p><div><pre class="programlisting">twts_bz_df.tweet_text.distinct()
Out[76]:
    tweet_text
0   RT @pacoid: Great recap of @StrataConf EU in L...
1   RT @alvaroagea: Simply @ApacheSpark http://t.c...
2   RT @PrabhaGana: What exactly is @ApacheSpark a...
3   RT @Ellen_Friedman: I'm still on Euro time. If...
4   RT @BigDataTechCon: Moving Rating Prediction w...
5   I'm still on Euro time. If you are too check o...</pre></div><p>We extract multiple columns <code class="literal">['id', 'user_name','tweet_text']</code> from the <code class="literal">dataframe</code> and take the unique records:</p><div><pre class="programlisting">twts_bz_df[['id', 'user_name','tweet_text']].distinct()
Out[78]:
  id   user_name   tweet_text
0   598831111406510082   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...
1   598808944719593472   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...
2   598796205091500032   John Humphreys   RT @PrabhaGana: What exactly is @ApacheSpark a...
3   598788561127735296   Leonardo D'Ambrosi   RT @Ellen_Friedman: I'm still on Euro time. If...
4   598785545557438464   Alexey Kosenkov   RT @Ellen_Friedman: I'm still on Euro time. If...
5   598782970082807808   embeddedcomputer.nl   RT @BigDataTechCon: Moving Rating Prediction w...
6   598777933730160640   Ellen Friedman   I'm still on Euro time. If you are too check o...</pre></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec32"/>Transferring data using Odo</h2></div></div></div><p>Odo is <a id="id180" class="indexterm"/>a spin-off project of Blaze. Odo allows the interchange of data. Odo ensures the migration<a id="id181" class="indexterm"/> of data across different<a id="id182" class="indexterm"/> formats (CSV, JSON, HDFS, and more) and across different databases (SQL databases, MongoDB, and so on) using a very simple predicate:</p><div><pre class="programlisting">Odo(source, target)</pre></div><p>To transfer to a database, the address is specified using a URL. For example, for a MongoDB database, it would look like this: </p><div><pre class="programlisting">mongodb://username:password@hostname:port/database_name::collection_name</pre></div><p>Let's run some examples of using Odo. Here, we illustrate <code class="literal">odo</code> by reading a CSV file and creating a Blaze <code class="literal">dataframe</code>:</p><div><pre class="programlisting">filepath   = csvFpath
filename   = csvFname
filesuffix = csvSuffix
twts_odo_df = Data('{0}/{1}.{2}'.format(filepath, filename, filesuffix))</pre></div><p>Count the number of records in the <code class="literal">dataframe</code>:</p><div><pre class="programlisting">twts_odo_df.count()
Out[81]:
19</pre></div><p>Display the five initial records of the <code class="literal">dataframe</code>:</p><div><pre class="programlisting">twts_odo_df.head(5)
Out[82]:
  id   created_at   user_id   user_name   tweet_text   url
0   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
1   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
2   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://www.webex.com/ciscospark/
3   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/
4   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   https://www.sparkfun.com/</pre></div><p>Get<a id="id183" class="indexterm"/> <code class="literal">dshape</code> information from the <code class="literal">dataframe</code>, which gives <a id="id184" class="indexterm"/>us the number of records and the schema:</p><div><pre class="programlisting">twts_odo_df.dshape
Out[83]:
dshape("var * {
  id: int64,
  created_at: ?datetime,
  user_id: int64,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>Save a processed Blaze <code class="literal">dataframe</code> into JSON:</p><div><pre class="programlisting">odo(twts_odo_distinct_df, '{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix))
Out[92]:
&lt;odo.backends.json.JSONLines at 0x7f77f0abfc50&gt;</pre></div><p>Convert a JSON file to a CSV file:</p><div><pre class="programlisting">odo('{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix), '{0}/{1}.{2}'.format(csvFpath, csvFname, csvSuffix))
Out[94]:
&lt;odo.backends.csv.CSV at 0x7f77f0abfe10&gt;</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Exploring data using Spark SQL</h1></div></div></div><p>Spark SQL is a <a id="id185" class="indexterm"/>relational query engine built on top<a id="id186" class="indexterm"/> of <a id="id187" class="indexterm"/>Spark Core. Spark SQL uses a <a id="id188" class="indexterm"/>query optimizer called <strong>Catalyst</strong>.</p><p>Relational queries can be expressed using SQL or HiveQL and executed against JSON, CSV, and various databases. Spark SQL gives us the full expressiveness of declarative programing <a id="id189" class="indexterm"/>with Spark dataframes on top of <a id="id190" class="indexterm"/>functional programming with RDDs.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Understanding Spark dataframes</h2></div></div></div><p>Here's a<a id="id191" class="indexterm"/> tweet from <code class="literal">@bigdata</code> announcing Spark 1.3.0, the advent of Spark SQL and dataframes. It also highlights the various data sources in the lower part of the diagram. On the top part, we can notice R as the new language that will be gradually supported on top of Scala, Java, and Python. Ultimately, the Data Frame philosophy is pervasive between R, Python, and Spark.</p><div><img src="img/B03986_03_02.jpg" alt="Understanding Spark dataframes"/></div><p>Spark <a id="id192" class="indexterm"/>dataframes originate from SchemaRDDs. It combines RDD with a schema that can be inferred by Spark, if requested, when registering the dataframe. It allows us to query complex nested JSON data with plain SQL. Lazy evaluation, lineage, partitioning, and persistence apply to dataframes.</p><p>Let's query the data with Spark SQL, by first importing <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code>:</p><div><pre class="programlisting">from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext, Row
In [95]:
sc
Out[95]:
&lt;pyspark.context.SparkContext at 0x7f7829581890&gt;
In [96]:
sc.master
Out[96]:
u'local[*]'
''In [98]:
# Instantiate Spark  SQL context
sqlc =  SQLContext(sc)</pre></div><p>We read in<a id="id193" class="indexterm"/> the JSON file we saved with Odo:</p><div><pre class="programlisting">twts_sql_df_01 = sqlc.jsonFile ("/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401_distinct.json")
In [101]:
twts_sql_df_01.show()
created_at           id                 tweet_text           user_id    user_name          
2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia      
2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521   raulsaeztapia      
2015-05-14T10:25:15Z 598796205091500032 RT @PrabhaGana: W... 48695135   John Humphreys     
2015-05-14T09:54:52Z 598788561127735296 RT @Ellen_Friedma... 2385931712 Leonardo D'Ambrosi
2015-05-14T09:42:53Z 598785545557438464 RT @Ellen_Friedma... 461020977  Alexey Kosenkov    
2015-05-14T09:32:39Z 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl
2015-05-14T09:12:38Z 598777933730160640 I'm still on Euro... 294862170  Ellen Friedman     </pre></div><p>We print the schema of the Spark dataframe:</p><div><pre class="programlisting">twts_sql_df_01.printSchema()
root
 |-- created_at: string (nullable = true)
 |-- id: long (nullable = true)
 |-- tweet_text: string (nullable = true)
 |-- user_id: long (nullable = true)
 |-- user_name: string (nullable = true)</pre></div><p>We select the <code class="literal">user_name</code> column from the dataframe:</p><div><pre class="programlisting">twts_sql_df_01.select('user_name').show()
user_name          
raulsaeztapia      
raulsaeztapia      
John Humphreys     
Leonardo D'Ambrosi
Alexey Kosenkov    
embeddedcomputer.nl
Ellen Friedman     </pre></div><p>We register the dataframe as a table, so we can execute a SQL query on it:</p><div><pre class="programlisting">twts_sql_df_01.registerAsTable('tweets_01')</pre></div><p>We<a id="id194" class="indexterm"/> execute a SQL statement against the dataframe:</p><div><pre class="programlisting">twts_sql_df_01_selection = sqlc.sql("SELECT * FROM tweets_01 WHERE user_name = 'raulsaeztapia'")
In [109]:
twts_sql_df_01_selection.show()
created_at           id                 tweet_text           user_id  user_name    
2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia
2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia</pre></div><p>Let's process some more complex JSON; we read the original Twitter JSON file:</p><div><pre class="programlisting">tweets_sqlc_inf = sqlc.jsonFile(infile)</pre></div><p>Spark SQL is able to infer the schema of a complex nested JSON file:</p><div><pre class="programlisting">tweets_sqlc_inf.printSchema()
root
 |-- contributors: string (nullable = true)
 |-- coordinates: string (nullable = true)
 |-- created_at: string (nullable = true)
 |-- entities: struct (nullable = true)
 |    |-- hashtags: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- indices: array (nullable = true)
 |    |    |    |    |-- element: long (containsNull = true)
 |    |    |    |-- text: string (nullable = true)
 |    |-- media: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- display_url: string (nullable = true)
 |    |    |    |-- expanded_url: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- id_str: string (nullable = true)
 |    |    |    |-- indices: array (nullable = true)
... (snip) ...
|    |-- statuses_count: long (nullable = true)
 |    |-- time_zone: string (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- utc_offset: long (nullable = true)
 |    |-- verified: boolean (nullable = true)</pre></div><p>We extract<a id="id195" class="indexterm"/> the key information of interest from the wall of data by selecting specific columns in the dataframe (in this case, <code class="literal">['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']</code>):</p><div><pre class="programlisting">tweets_extract_sqlc = tweets_sqlc_inf[['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']].distinct()
In [145]:
tweets_extract_sqlc.show()
created_at           id                 text                 id         name                expanded_url        
Thu May 14 09:32:... 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl ArrayBuffer(http:...
Thu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia       ArrayBuffer(http:...
Thu May 14 12:18:... 598824733086523393 @rabbitonweb spea... 

...   
Thu May 14 12:28:... 598827171168264192 RT @baandrzejczak... 20909005   Paweł Szulc         ArrayBuffer()       </pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Understanding the Spark SQL query optimizer</h2></div></div></div><p>We<a id="id196" class="indexterm"/> execute a SQL statement against the dataframe:</p><div><pre class="programlisting">tweets_extract_sqlc_sel = sqlc.sql("SELECT * from Tweets_xtr_001 WHERE name='raulsaeztapia'")</pre></div><p>We get a detailed view of the query plans executed by Spark SQL:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Parsed logical plan</li><li class="listitem" style="list-style-type: disc">Analyzed logical plan</li><li class="listitem" style="list-style-type: disc">Optimized logical plan</li><li class="listitem" style="list-style-type: disc">Physical plan</li></ul></div><p>The query plan uses Spark SQL's Catalyst optimizer. In order to generate the compiled bytecode from the query parts, the Catalyst optimizer runs through logical plan parsing and optimization followed by physical plan evaluation and optimization based on cost.</p><p>This is illustrated in the following tweet:</p><div><img src="img/B03986_03_03.jpg" alt="Understanding the Spark SQL query optimizer"/></div><p>Looking<a id="id197" class="indexterm"/> back at our code, we call the <code class="literal">.explain</code> function on the Spark SQL query we just executed, and it delivers the full details of the steps taken by the Catalyst optimizer in order to assess and optimize the logical plan and the physical plan and get to the result RDD:</p><div><pre class="programlisting">tweets_extract_sqlc_sel.explain(extended = True)
== Parsed Logical Plan ==
'Project [*]
 'Filter ('name = raulsaeztapia)'name'  'UnresolvedRelation' [Tweets_xtr_001], None
== Analyzed Logical Plan ==
Project [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82]
 Filter (name#81 = raulsaeztapia)
  Distinct 
   Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
    Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)
== Optimized Logical Plan ==
Filter (name#81 = raulsaeztapia)
 Distinct 
  Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
   Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)
== Physical Plan ==
Filter (name#81 = raulsaeztapia)
 Distinct false
  Exchange (HashPartitioning [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82], 200)
   Distinct true
    Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
     PhysicalRDD [contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29], MapPartitionsRDD[165] at map at JsonRDD.scala:41
Code Generation: false
== RDD ==</pre></div><p>Finally, here's the <a id="id198" class="indexterm"/>result of the query:</p><div><pre class="programlisting">tweets_extract_sqlc_sel.show()
created_at           id                 text                 id       name          expanded_url        
Thu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia ArrayBuffer(http:...
Thu May 14 11:15:... 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia ArrayBuffer(http:...
In [148]:</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Loading and processing CSV files with Spark SQL</h2></div></div></div><p>We will <a id="id199" class="indexterm"/>use the Spark package <code class="literal">spark-csv_2.11:1.2.0</code>. The command to be used to launch PySpark with the IPython Notebook <a id="id200" class="indexterm"/>and the <code class="literal">spark-csv</code> package should explicitly state the <code class="literal">–packages</code> argument:</p><div><pre class="programlisting">
<strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong>
</pre></div><p>This will trigger the following output; we can see that the <code class="literal">spark-csv</code> package is installed with all its dependencies:</p><div><pre class="programlisting">
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong>
</pre></div><div><pre class="programlisting">... (snip) ...
Ivy Default Cache set to: /home/an/.ivy2/cache
The jars for the packages stored in: /home/an/.ivy2/jars
:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.databricks#spark-csv_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
  confs: [default]
  found com.databricks#spark-csv_2.11;1.2.0 in central
  found org.apache.commons#commons-csv;1.1 in central
  found com.univocity#univocity-parsers;1.5.1 in central
:: resolution report :: resolve 835ms :: artifacts dl 48ms
  :: modules in use:
  com.databricks#spark-csv_2.11;1.2.0 from central in [default]
  com.univocity#univocity-parsers;1.5.1 from central in [default]
  org.apache.commons#commons-csv;1.1 from central in [default]
  ----------------------------------------------------------------
  |               |          modules            ||   artifacts   |
  |    conf     | number| search|dwnlded|evicted|| number|dwnlded|
  ----------------------------------------------------------------
  |    default     |   3   |   0   |   0   |   0   ||   3   |   0   
  ----------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  0 artifacts copied, 3 already retrieved (0kB/45ms)</pre></div><p>We are now ready to load our <code class="literal">csv</code> file and process it. Let's first import the <code class="literal">SQLContext</code>:</p><div><pre class="programlisting">#
# Read csv in a Spark DF
#
sqlContext = SQLContext(sc)
spdf_in = sqlContext.read.format('com.databricks.spark.csv')\
                                    .options(delimiter=";").options(header="true")\
                                    .options(header='true').load(csv_in)</pre></div><p>We access<a id="id201" class="indexterm"/> the schema of the dataframe created <a id="id202" class="indexterm"/>from the loaded <code class="literal">csv</code>:</p><div><pre class="programlisting">In [10]:
spdf_in.printSchema()
root
 |-- : string (nullable = true)
 |-- id: string (nullable = true)
 |-- created_at: string (nullable = true)
 |-- user_id: string (nullable = true)
 |-- user_name: string (nullable = true)
 |-- tweet_text: string (nullable = true)</pre></div><p>We check the columns of the dataframe:</p><div><pre class="programlisting">In [12]:
spdf_in.columns
Out[12]:
['', 'id', 'created_at', 'user_id', 'user_name', 'tweet_text']</pre></div><p>We introspect the dataframe content:</p><div><pre class="programlisting">In [13]:
spdf_in.show()
+---+------------------+--------------------+----------+------------------+--------------------+
|   |                id|          created_at|   user_id|         user_name|          tweet_text|
+---+------------------+--------------------+----------+------------------+--------------------+
|  0|638830426971181057|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
|  1|638830426727911424|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
|  2|638830425402556417|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
... (snip) ...
| 41|638830280988426250|Tue Sep 01 21:46:...| 951081582|      Jack Baldwin|RT @cloudaus: We ...|
| 42|638830276626399232|Tue Sep 01 21:46:...|   6525302|Masayoshi Nakamura|PynamoDB使いやすいです  |
+---+------------------+--------------------+----------+------------------+--------------------+
only showing top 20 rows</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Querying MongoDB from Spark SQL</h2></div></div></div><p>There<a id="id203" class="indexterm"/> are two major ways to interact with MongoDB from Spark: the first is through the Hadoop MongoDB connector, and the second one is directly from Spark to MongoDB.</p><p>The first approach to interact with MongoDB from Spark is to set up a Hadoop environment <a id="id204" class="indexterm"/>and query through the Hadoop MongoDB connector. The connector details are hosted on GitHub at <a class="ulink" href="https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage">https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage</a>. An actual use case is<a id="id205" class="indexterm"/> described in the series of blog posts from MongoDB:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>Using MongoDB with Hadoop &amp; Spark: Part 1 - Introduction &amp; Setup</em> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup</a>)</li><li class="listitem" style="list-style-type: disc"><em>Using MongoDB with Hadoop and Spark: Part 2 - Hive Example</em> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example</a>)</li><li class="listitem" style="list-style-type: disc"><em>Using MongoDB with Hadoop &amp; Spark: Part 3 - Spark Example &amp; Key Takeaways</em> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways</a>)</li></ul></div><p>Setting up a full Hadoop environment is bit elaborate. We will favor the second approach. We will use the <code class="literal">spark-mongodb</code> connector developed and maintained by Stratio. We are using the <code class="literal">Stratio spark-mongodb</code> package hosted at <code class="literal">spark.packages.org</code>. The packages information and version can be found in <code class="literal">spark.packages.org</code>:</p><div><div><h3 class="title"><a id="note02"/>Note</h3><p>
<strong>Releases</strong>
</p><p>Version: 0.10.1 ( 8263c8 | zip | jar ) / Date: 2015-11-18 / License: Apache-2.0 / Scala <a id="id206" class="indexterm"/>version: 2.10</p><p>(<a class="ulink" href="http://spark-packages.org/package/Stratio/spark-mongodb">http://spark-packages.org/package/Stratio/spark-mongodb</a>)</p></div></div><p>The command to launch PySpark with the IPython Notebook and the <code class="literal">spark-mongodb</code> package should explicitly state the packages argument:</p><div><pre class="programlisting">
<strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1</strong>
</pre></div><p>This will trigger the following output; we can see that the <code class="literal">spark-mongodb</code> package is installed with all its dependencies:</p><div><pre class="programlisting">an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1
... (snip) ... 
Ivy Default Cache set to: /home/an/.ivy2/cache
The jars for the packages stored in: /home/an/.ivy2/jars
:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.stratio.datasource#spark-mongodb_2.10 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
  confs: [default]
  found com.stratio.datasource#spark-mongodb_2.10;0.10.1 in central
[W 22:10:50.910 NotebookApp] Timeout waiting for kernel_info reply from 764081d3-baf9-4978-ad89-7735e6323cb6
  found org.mongodb#casbah-commons_2.10;2.8.0 in central
  found com.github.nscala-time#nscala-time_2.10;1.0.0 in central
  found joda-time#joda-time;2.3 in central
  found org.joda#joda-convert;1.2 in central
  found org.slf4j#slf4j-api;1.6.0 in central
  found org.mongodb#mongo-java-driver;2.13.0 in central
  found org.mongodb#casbah-query_2.10;2.8.0 in central
  found org.mongodb#casbah-core_2.10;2.8.0 in central
downloading https://repo1.maven.org/maven2/com/stratio/datasource/spark-mongodb_2.10/0.10.1/spark-mongodb_2.10-0.10.1.jar ...
  [SUCCESSFUL ] com.stratio.datasource#spark-mongodb_2.10;0.10.1!spark-mongodb_2.10.jar (3130ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-commons_2.10/2.8.0/casbah-commons_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-commons_2.10;2.8.0!casbah-commons_2.10.jar (2812ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-query_2.10/2.8.0/casbah-query_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-query_2.10;2.8.0!casbah-query_2.10.jar (1432ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-core_2.10/2.8.0/casbah-core_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-core_2.10;2.8.0!casbah-core_2.10.jar (2785ms)
downloading https://repo1.maven.org/maven2/com/github/nscala-time/nscala-time_2.10/1.0.0/nscala-time_2.10-1.0.0.jar ...
  [SUCCESSFUL ] com.github.nscala-time#nscala-time_2.10;1.0.0!nscala-time_2.10.jar (2725ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.6.0/slf4j-api-1.6.0.jar ...
  [SUCCESSFUL ] org.slf4j#slf4j-api;1.6.0!slf4j-api.jar (371ms)
downloading https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/2.13.0/mongo-java-driver-2.13.0.jar ...
  [SUCCESSFUL ] org.mongodb#mongo-java-driver;2.13.0!mongo-java-driver.jar (5259ms)
downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar ...
  [SUCCESSFUL ] joda-time#joda-time;2.3!joda-time.jar (6949ms)
downloading https://repo1.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar ...
  [SUCCESSFUL ] org.joda#joda-convert;1.2!joda-convert.jar (548ms)
:: resolution report :: resolve 11850ms :: artifacts dl 26075ms
  :: modules in use:
  com.github.nscala-time#nscala-time_2.10;1.0.0 from central in [default]
  com.stratio.datasource#spark-mongodb_2.10;0.10.1 from central in [default]
  joda-time#joda-time;2.3 from central in [default]
  org.joda#joda-convert;1.2 from central in [default]
  org.mongodb#casbah-commons_2.10;2.8.0 from central in [default]
  org.mongodb#casbah-core_2.10;2.8.0 from central in [default]
  org.mongodb#casbah-query_2.10;2.8.0 from central in [default]
  org.mongodb#mongo-java-driver;2.13.0 from central in [default]
  org.slf4j#slf4j-api;1.6.0 from central in [default]
  ---------------------------------------------------------------------
  |                  |            modules            ||   artifacts   |
  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
  ---------------------------------------------------------------------
  |      default     |   9   |   9   |   9   |   0   ||   9   |   9   |
  ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  9 artifacts copied, 0 already retrieved (2335kB/51ms)
... (snip) ... </pre></div><p>We<a id="id207" class="indexterm"/> are now ready to query MongoDB on <code class="literal">localhost:27017</code> from the collection <code class="literal">twtr01_coll</code> in the database <code class="literal">twtr01_db</code>.</p><p>We first import the <code class="literal">SQLContext</code>:</p><div><pre class="programlisting">In [5]:
from pyspark.sql import SQLContext
sqlContext.sql("CREATE TEMPORARY TABLE tweet_table USING com.stratio.datasource.mongodb OPTIONS (host 'localhost:27017', database 'twtr01_db', collection 'twtr01_coll')")
sqlContext.sql("SELECT * FROM tweet_table where id=598830778269769728 ").collect()</pre></div><p>Here's the<a id="id208" class="indexterm"/> output of our query:</p><div><pre class="programlisting">Out[5]:
[Row(text=u'@spark_io is now @particle - awesome news - now I can enjoy my Particle Cores/Photons + @sparkfun sensors + @ApacheSpark analytics :-)', _id=u'55aa640fd770871cba74cb88', contributors=None, retweeted=False, user=Row(contributors_enabled=False, created_at=u'Mon Aug 25 14:01:26 +0000 2008', default_profile=True, default_profile_image=False, description=u'Building open source tools for and teaching enterprise software developers', entities=Row(description=Row(urls=[]), url=Row(urls=[Row(url=u'http://t.co/TSHp13EWeu', indices=[0, 22], 

... (snip) ...

 9], name=u'Spark is Particle', screen_name=u'spark_io'), Row(id=487010011, id_str=u'487010011', indices=[17, 26], name=u'Particle', screen_name=u'particle'), Row(id=17877351, id_str=u'17877351', indices=[88, 97], name=u'SparkFun Electronics', screen_name=u'sparkfun'), Row(id=1551361069, id_str=u'1551361069', indices=[108, 120], name=u'Apache Spark', screen_name=u'ApacheSpark')]), is_quote_status=None, lang=u'en', quoted_status_id_str=None, quoted_status_id=None, created_at=u'Thu May 14 12:42:37 +0000 2015', retweeted_status=None, truncated=False, place=None, id=598830778269769728, in_reply_to_user_id=3187046084, retweet_count=0, in_reply_to_status_id=None, in_reply_to_screen_name=u'spark_io', in_reply_to_user_id_str=u'3187046084', source=u'&lt;a href="http://twitter.com" rel="nofollow"&gt;Twitter Web Client&lt;/a&gt;', id_str=u'598830778269769728', coordinates=None, metadata=Row(iso_language_code=u'en', result_type=u'recent'), quoted_status=None)]
#</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Summary</h1></div></div></div><p>In this chapter, we harvested data from Twitter. Once the data was acquired, we explored the information using <code class="literal">Continuum.io's</code> Blaze and Odo libraries. Spark SQL is an important module for interactive data exploration, analysis, and transformation, leveraging the Spark dataframe datastructure. The dataframe concept originates from R and then was adopted by Python Pandas with great success. The dataframe is the workhorse of the data scientist. The combination of Spark SQL and dataframe creates a powerful engine for data processing.</p><p>We are now gearing up for extracting the insights from the datasets using machine learning from Spark MLlib.</p></div></body></html>