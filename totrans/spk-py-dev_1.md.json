["```py\n$ conda update\n\n```", "```py\n$ conda list\n\n```", "```py\n    # install anaconda 2.x.x\n    bash Anaconda-2.x.x-Linux-x86[_64].sh\n\n    ```", "```py\n    # add anaconda to PATH\n    bash Anaconda-2.x.x-Linux-x86[_64].sh\n\n    ```", "```py\n    # install oracle java 8\n    $ sudo apt-get install software-properties-common\n    $ sudo add-apt-repository ppa:webupd8team/java\n    $ sudo apt-get update\n    $ sudo apt-get install oracle-java8-installer\n\n    ```", "```py\n    # \n    $ echo JAVA_HOME\n\n    ```", "```py\n# download spark\n$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz\n\n```", "```py\n# extract, clean up, move the unzipped files under the spark directory\n$ tar -xf spark-1.5.2-bin-hadoop2.6.tgz\n$ rm spark-1.5.2-bin-hadoop2.6.tgz\n$ sudo mv spark-* spark\n\n```", "```py\n# run spark\n$ cd ~/spark\n./bin/pyspark\n\n```", "```py\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n /_/\nUsing Python version 2.7.6 (default, Mar 22 2014 22:59:56)\nSparkContext available as sc.\n>>> \n\n```", "```py\n>>> print(sc)\n<pyspark.context.SparkContext object at 0x7f34b61c4e50>\n\n```", "```py\n$ IPYTHON_OPTS=\"notebook --pylab inline\"  ./bin/pyspark\n\n```", "```py\n# cd to  /home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark\n# launch command using python 2.7 and the spark-csv package:\n$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0\n\n# launch command using python 3.4 and the spark-csv package:\n$ IPYTHON_OPTS='notebook' PYSPARK_PYTHON=python3\n /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0\n\n```", "```py\n# Word count on 1st Chapter of the Book using PySpark\n\n# import regex module\nimport re\n# import add from operator module\nfrom operator import add\n\n# read input file\nfile_in = sc.textFile('/home/an/Documents/A00_Documents/Spark4Py 20150315')\n\n# count lines\nprint('number of lines in file: %s' % file_in.count())\n\n# add up lengths of each line\nchars = file_in.map(lambda s: len(s)).reduce(add)\nprint('number of characters in file: %s' % chars)\n\n# Get words from the input file\nwords =file_in.flatMap(lambda line: re.split('\\W+', line.lower().strip()))\n# words of more than 3 characters\nwords = words.filter(lambda x: len(x) > 3)\n# set count 1 per word\nwords = words.map(lambda w: (w,1))\n# reduce phase - sum count all the words\nwords = words.reduceByKey(add)\n```", "```py\n# create tuple (count, word) and sort in descending\nwords = words.map(lambda x: (x[1], x[0])).sortByKey(False)\n\n# take top 20 words by frequency\nwords.take(20)\n```", "```py\n# create function for histogram of most frequent words\n\n% matplotlib inline\nimport matplotlib.pyplot as plt\n#\n\ndef histogram(words):\n    count = map(lambda x: x[1], words)\n    word = map(lambda x: x[0], words)\n    plt.barh(range(len(count)), count,color = 'grey')\n    plt.yticks(range(len(count)), word)\n\n# Change order of tuple (word, count) from (count, word) \nwords = words.map(lambda x:(x[1], x[0]))\nwords.take(25)\n\n# display histogram\nhistogram(words.take(25))\n```", "```py\nC:\\Programs\\spark\\edx1001\\mooc-setup-master>vagrant up\nBringing machine 'sparkvm' up with 'virtualbox' provider...\n==> sparkvm: Checking if box 'sparkmooc/base' is up to date...\n==> sparkvm: Clearing any previously set forwarded ports...\n==> sparkvm: Clearing any previously set network interfaces...\n==> sparkvm: Preparing network interfaces based on configuration...\n sparkvm: Adapter 1: nat\n==> sparkvm: Forwarding ports...\n sparkvm: 8001 => 8001 (adapter 1)\n sparkvm: 4040 => 4040 (adapter 1)\n sparkvm: 22 => 2222 (adapter 1)\n==> sparkvm: Booting VM...\n==> sparkvm: Waiting for machine to boot. This may take a few minutes...\n sparkvm: SSH address: 127.0.0.1:2222\n sparkvm: SSH username: vagrant\n sparkvm: SSH auth method: private key\n sparkvm: Warning: Connection timeout. Retrying...\n sparkvm: Warning: Remote connection disconnect. Retrying...\n==> sparkvm: Machine booted and ready!\n==> sparkvm: Checking for guest additions in VM...\n==> sparkvm: Setting hostname...\n==> sparkvm: Mounting shared folders...\n sparkvm: /vagrant => C:/Programs/spark/edx1001/mooc-setup-master\n==> sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`\n==> sparkvm: to force provisioning. Provisioners marked to run always will still run.\n\nC:\\Programs\\spark\\edx1001\\mooc-setup-master>\n\n```", "```py\n    export AWS_ACCESS_KEY_ID=accesskeyid\n    export AWS_SECRET_ACCESS_KEY=secretaccesskey\n\n    ```", "```py\n    ~$ cd $SPARK_HOME/ec2\n    ec2$ ./spark-ec2 -k <keypair> -i <key-file> -s <num-slaves> launch <cluster-name>\n\n    ```", "```py\n    ec2$ ./spark-ec2 -k <keypair> -i <key-file> login <cluster-name>\n\n    ```", "```py\n    ec2$ ./spark-ec2 destroy <cluster-name>\n\n    ```", "```py\n$ docker pull thisgokeboysef/pyspark-docker\n\n```"]