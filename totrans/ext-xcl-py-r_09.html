<html><head></head><body>
		<div id="_idContainer096">
			<h1 id="_idParaDest-160" class="chapter-number"><a id="_idTextAnchor178"/>9</h1>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor179"/>Statistical Analysis: Linear and Logistic Regression</h1>
			<p>Welcome to our comprehensive guide on linear and logistic regression using R and Python, where we will explore these essential statistical techniques using two popular frameworks: <strong class="source-inline">tidymodels</strong> and base R and Python. Whether you’re a data science enthusiast or a professional looking to sharpen your skills, this tutorial will help you gain a deep understanding of <strong class="bold">linear</strong> and <strong class="bold">logistic regression</strong> and how to implement them in R and Python. Now, it is possible to perform linear and logistic regression. The issue here is that linear regression can only be performed on a single series of ungrouped data, and performing logistic regression is cumbersome and may require the use of external solver add-ins. Also, the process can only be performed against ungrouped or non-nested data. In R and Python, we do not have <span class="No-Break">such limitations.</span></p>
			<p>In this chapter, we will cover the following topics in both base R and Python and using the <span class="No-Break"><strong class="source-inline">tidymodels</strong></span><span class="No-Break"> framework:</span></p>
			<ul>
				<li>Performing linear regression in both base R and Python and the <strong class="source-inline">tidymodels</strong> frameworks as well as <span class="No-Break">in Python</span></li>
				<li>Performing logistic regression in both base R and Python and the <strong class="source-inline">tidymodels</strong> frameworks as well as <span class="No-Break">in Python</span></li>
			</ul>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor180"/>Technical requirements</h1>
			<p>All code for this chapter can be found on GitHub at this URL:  <a href="https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter9">https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter9</a>. You will need the following R packages installed to <span class="No-Break">follow along:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">readxl 1.4.3</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">performance 0.10.8</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">tidymodels 1.1.1</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">purrr 1.0.2</strong></span></li>
			</ul>
			<p>We will begin by learning about what linear and logistic regression are and then move into the details <span class="No-Break">of everything.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor181"/>Linear regression</h1>
			<p>Linear regression is a fundamental statistical method<a id="_idIndexMarker665"/> used for modeling the relationship between a dependent variable (usually denoted as “Y”) and one or more independent variables (often denoted as “X”). It aims to find the best-fitting linear equation that describes how changes in the independent variables<a id="_idIndexMarker666"/> affect the dependent variable. Many of you may know this as the <strong class="bold">ordinary least squares</strong> (<span class="No-Break"><strong class="bold">OLS</strong></span><span class="No-Break">) method.</span></p>
			<p>In simpler terms, linear regression helps us predict a continuous numeric outcome based on one or more input features. For this to work, if you are unaware, many assumptions must be held true. If you would like to understand these more, then a simple search will bring you a lot of good information on them. In this tutorial, we will delve into both simple linear regression (one independent variable) and multiple linear regression (multiple <span class="No-Break">independent variables).</span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor182"/>Logistic regression</h1>
			<p>Logistic regression is another crucial statistical<a id="_idIndexMarker667"/> technique, which is primarily used for binary classification problems. Instead of predicting continuous outcomes, logistic regression predicts the probability of an event occurring, typically expressed as a “yes” or “no” outcome. This method is particularly useful for scenarios where we need to model the likelihood of an event, such as whether a customer will churn or not or whether an email is spam or not. Logistic regression models the relationship between the independent variables and the log odds of the <span class="No-Break">binary outcome.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor183"/>Frameworks</h2>
			<p>We will explore two approaches<a id="_idIndexMarker668"/> to implementing linear and logistic regression in R. First, we will use the base R framework, which is an excellent starting point to understand the underlying concepts and functions. Then, we will dive into <strong class="source-inline">tidymodels</strong>, a modern and tidy approach to modeling and machine learning in R. <strong class="source-inline">tidymodels</strong> provides a consistent and efficient way to build, tune, and evaluate models, making it a valuable tool for data scientists. In Python, we will parallel this exploration with two prominent libraries: <strong class="source-inline">sklearn</strong> and <strong class="source-inline">statsmodels</strong>. <strong class="source-inline">sklearn</strong>, or Scikit-learn, offers a wide array of simple and efficient tools for predictive data analysis that are accessible to everybody and reusable in various contexts. <strong class="source-inline">statsmodels</strong> is more focused on statistical models and hypothesis tests. Together, these Python libraries offer a robust framework for implementing linear and logistic regression, catering to both machine learning and <span class="No-Break">statistical needs.</span></p>
			<p>Throughout this chapter, we will provide step-by-step instructions, code examples, and practical insights to ensure that you can confidently apply linear and logistic regression techniques to your own data analysis projects. </p>
			<p>Let’s embark on this learning journey and unlock the power of regression analysis in R! With this in place, we move to the first example in base R using the <strong class="source-inline">iris</strong> dataset we saved in <a href="B19142_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor184"/>Performing linear regression in R</h1>
			<p>For this section, we are<a id="_idIndexMarker669"/> going to perform<a id="_idIndexMarker670"/> linear regression in R, both in base R and by way of the <strong class="source-inline">tidymodels</strong> framework. In this section, you will learn how to do this on a dataset that has different groups in it. We will do this because if you can learn to do it this way, then doing it in a single group becomes simpler as there is no need to group data and perform actions by group. The thought process here is that by doing it on grouped data, we hope you can learn an <span class="No-Break">extra skill.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor185"/>Linear regression in base R</h2>
			<p>The first example we are going<a id="_idIndexMarker671"/> to show is using<a id="_idIndexMarker672"/> the <strong class="source-inline">lm()</strong> function to perform a linear regression in base R. Let’s dive right into it with the <span class="No-Break"><strong class="source-inline">iris</strong></span><span class="No-Break"> dataset.</span></p>
			<p>We will break the code down into chunks and discuss what is happening at each step. The first step for us is to use the <strong class="source-inline">library</strong> command to bring in the necessary packages into our <span class="No-Break">development environment:</span></p>
			<pre class="source-code">
library(readxl)</pre>			<p>In this section, we’re loading a library called <strong class="source-inline">readxl</strong>. Libraries are collections of pre-written R functions and code that we can use in our own R scripts. In this case, we’re loading the <strong class="source-inline">readxl</strong> library, which is commonly used for reading data from Excel files. The path assumes you have a <strong class="source-inline">chapter1</strong> folder and a data file in it <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">iris_data.xlsx</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
df &lt;- read_xlsx(
  path = "chapter1/iris_data.xlsx",
  sheet = "iris"
)
head(df)</pre>			<p>Here, we’re reading data from an Excel file named <strong class="source-inline">iris_data.xlsx</strong>, located in the <strong class="source-inline">chapter1</strong> folder. We’re specifically reading the <strong class="source-inline">iris</strong> sheet from that Excel file. The <strong class="source-inline">read_xlsx</strong> function is used for this purpose. The resulting data is stored in a variable called <strong class="source-inline">df</strong>. The <strong class="source-inline">head(df)</strong> function displays the first few rows of this data frame (<strong class="source-inline">df</strong>) so we can see what it <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
iris_split &lt;- split(df, df$species)</pre>			<p>This code splits the <strong class="source-inline">df</strong> dataset<a id="_idIndexMarker673"/> into multiple subsets based on the unique values<a id="_idIndexMarker674"/> in the <strong class="source-inline">species</strong> column. The result is a list of data frames where each data frame contains only the rows that correspond to a specific species <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">iris</strong></span><span class="No-Break">.</span></p>
			<p>Now, we are going to define what will be the dependent and independent variables along with the <span class="No-Break"><strong class="source-inline">formula</strong></span><span class="No-Break"> object:</span></p>
			<pre class="source-code">
dependent_variable &lt;- "petal_length"
independent_variables &lt;- c("petal_width", "sepal_length", "sepal_width")
f_x &lt;- formula(
  paste(
dependent_variable,
"~",
paste(independent_variables, collapse = " + ")
)
)</pre>			<p>Here, we’re defining the variables needed for linear regression. <strong class="source-inline">dependent_variable</strong> is <strong class="source-inline">petal_length</strong>, which is the variable we want to predict. <strong class="source-inline">independent_variables</strong> are <strong class="source-inline">petal_width</strong>, <strong class="source-inline">sepal_length</strong>, and <strong class="source-inline">sepal_width</strong>, which are the variables we’ll use to predict the <span class="No-Break">dependent variable.</span></p>
			<p>The code then creates an <strong class="source-inline">f_x</strong> formula that represents the linear regression model. It essentially says that we want to predict <strong class="source-inline">petal_length</strong> using the other variables listed, separated by a <span class="No-Break">plus sign:</span></p>
			<pre class="source-code">
perform_linear_regression &lt;- function(data) {
  lm_model &lt;- lm(f_x, data = data)
  return(lm_model)
}</pre>			<p>In this part, we’re defining a custom R function called <strong class="source-inline">perform_linear_regression</strong>. This function takes one <strong class="source-inline">data</strong> argument, which is a data frame. Inside the function, we use the <strong class="source-inline">lm</strong> function to perform linear regression, using the <strong class="source-inline">f_x</strong> formula we defined earlier and the provided data frame. The resulting linear model is stored in <strong class="source-inline">lm_model</strong>, and we return it as the output of <span class="No-Break">the function:</span></p>
			<pre class="source-code">
results &lt;- lapply(iris_split, perform_linear_regression)</pre>			<p>Here, we’re applying the <strong class="source-inline">perform_linear_regression</strong> function<a id="_idIndexMarker675"/> to each subset<a id="_idIndexMarker676"/> of the <strong class="source-inline">iris</strong> dataset using the <strong class="source-inline">lapply</strong> function. This means that we’re running linear regression separately for each species of iris, and the results are stored in the <span class="No-Break"><strong class="source-inline">results</strong></span><span class="No-Break"> list:</span></p>
			<pre class="source-code">
lapply(results, summary)</pre>			<p>This code uses <strong class="source-inline">lapply</strong> again, but this time we’re applying the <strong class="source-inline">summary</strong> function to each linear regression model in the <strong class="source-inline">results</strong> list. The <strong class="source-inline">summary</strong> function provides statistical information about the linear regression model, such as coefficients and <span class="No-Break">R-squared values:</span></p>
			<pre class="source-code">
par(mfrow = c(2,2))
lapply(results, plot)
par(mfrow = c(1, 1))</pre>			<p>These lines of code are used to create a set of four plots to visualize the model performance. We first set the layout of the plots to be a 2x2 grid using <strong class="source-inline">par(mfrow = c(2,2))</strong>, so that 4 plots will be displayed in a 2x2 grid. Then, we use <strong class="source-inline">lapply</strong> to plot each linear regression model in the <strong class="source-inline">results</strong> list. Finally, we reset the plot layout to the default with <strong class="source-inline">par(mfrow = </strong><span class="No-Break"><strong class="source-inline">c(1, 1))</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
lm_models &lt;- lapply(
iris_split,
function(df) lm(f_x, data = df)
)</pre>			<p>This part accomplishes the same<a id="_idIndexMarker677"/> linear regression analysis as before but combines<a id="_idIndexMarker678"/> the linear model creation and summarization into a more concise form using anonymous functions. It first applies the <strong class="source-inline">lm</strong> function to each species subset within <strong class="source-inline">iris_split</strong>, creating a list of linear models stored in <strong class="source-inline">lm_models</strong>. Then, it uses <strong class="source-inline">lapply</strong> to obtain summaries for each of these <span class="No-Break">linear models.</span></p>
			<p>In summary, this R code reads iris data from an Excel file, performs linear regression for each species of <strong class="source-inline">iris</strong>, summarizes the results, and creates visualizations to assess the model’s performance. It provides a detailed analysis of how the dependent variable (<strong class="source-inline">petal_length</strong>) is influenced by independent variables (<strong class="source-inline">petal_width</strong>, <strong class="source-inline">sepal_length</strong>, and <strong class="source-inline">sepal_width</strong>) for each species <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">iris</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor186"/>Linear regression with tidymodels and purrr</h2>
			<p>Now that we have gone over <a id="_idIndexMarker679"/>how to perform<a id="_idIndexMarker680"/> a simple linear regression<a id="_idIndexMarker681"/> in R on the <strong class="source-inline">iris</strong><a id="_idIndexMarker682"/> dataset, we will do the same with the <strong class="source-inline">tidymodels</strong> framework. Let’s dive right <span class="No-Break">into it:</span></p>
			<pre class="source-code">
f_x &lt;- formula(paste("petal_width", "~", "petal_length + sepal_width + sepal_length"))</pre>			<p>This block defines a formula for the linear regression model. The <strong class="source-inline">formula()</strong> function takes two arguments: the response variable and the predictor variables. The response variable is the variable that we want to predict, and the predictor variables are the variables that we think can help us predict the response variable. In this case, the response variable is <strong class="source-inline">petal_width</strong> and the predictor variables are <strong class="source-inline">petal_length</strong>, <strong class="source-inline">sepal_width</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">sepal_length</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
library(dplyr)
library(tidyr)
library(purrr)
library(tidymodels)
nested_lm &lt;- df |&gt;
 nest(data = -species) |&gt;
 mutate(split = map(data, ~ initial_split(., prop = 8/10)),
        train = map(split, ~ training(.)),
        test = map(split, ~ testing(.)),
        fit  = map(train, ~ lm(f_x, data = .)),
        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))</pre>			<p>This block creates a nested linear regression model using the <strong class="source-inline">nest()</strong> function from the <strong class="source-inline">tidyr</strong> package. The <strong class="source-inline">nest()</strong> function groups the data by a specified variable, in this case, the <span class="No-Break"><strong class="source-inline">species</strong></span><span class="No-Break"> variable.</span></p>
			<p>For each group, the <strong class="source-inline">nest()</strong> function creates a list containing the data for that group. The <strong class="source-inline">mutate()</strong> function is then used to add new columns to the nested <span class="No-Break">data frame.</span></p>
			<p>The <strong class="source-inline">split()</strong> function is used to randomly split the data in each group into a training set and a test set. The <strong class="source-inline">training()</strong> and <strong class="source-inline">testing()</strong> functions are then used to select the training and test sets, respectively. With <strong class="source-inline">map()</strong> and <strong class="source-inline">map2()</strong>, we can iterate over a vector or list or two vectors or lists and apply a function <span class="No-Break">to them.</span></p>
			<p>The <strong class="source-inline">lm()</strong> function is used to fit<a id="_idIndexMarker683"/> a linear regression model<a id="_idIndexMarker684"/> to the training <a id="_idIndexMarker685"/>data in each group. The <strong class="source-inline">predict()</strong> function<a id="_idIndexMarker686"/> is then used to predict the response variable for the test data in each group using the fitted linear <span class="No-Break">regression model:</span></p>
			<pre class="source-code">
nested_lm |&gt;
 select(species, pred) |&gt;
 unnest(pred)</pre>			<p>This block selects the <strong class="source-inline">species</strong> and <strong class="source-inline">pred</strong> columns from the nested data frame and unnests the <strong class="source-inline">pred</strong> column. The <strong class="source-inline">unnest()</strong> function converts the nested data frame to a regular data frame, with one row for <span class="No-Break">each observation.</span></p>
			<p>The resulting data frame is a nested linear regression model, with one fitted linear regression model for <span class="No-Break">each species.</span></p>
			<p>Let’s take a look at an example. We are<a id="_idIndexMarker687"/> going to use the <strong class="source-inline">f_x</strong> formula that was created<a id="_idIndexMarker688"/> earlier along with the <strong class="source-inline">df</strong> <strong class="source-inline">tibble</strong> variable we created<a id="_idIndexMarker689"/> at the beginning. The following code<a id="_idIndexMarker690"/> shows an example of how to use the nested linear regression model to predict the petal width for a new <span class="No-Break">iris flower:</span></p>
			<pre class="source-code">
library(dplyr)
library(tidyr)
library(purrr)
library(tidymodels)
# Create a nested linear regression model
nested_lm &lt;- df |&gt;
 nest(data = -species) |&gt;
 mutate(split = map(data, ~ initial_split(., prop = 8/10)),
        train = map(split, ~ training(.)),
        test = map(split, ~ testing(.)),
        fit  = map(train, ~ lm(f_x, data = .)),
        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))
# Predict the petal width for a new iris flower
new_iris &lt;- data.frame(sepal_length = 5.2, sepal_width = 2.7, 
    petal_length = 3.5)
# Predict the petal width
predicted_petal_width &lt;- predict(nested_lm[[1]]$fit, 
    newdata = new_iris))
# Print the predicted petal width
print(predicted_petal_width)</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="console">
1.45</pre>			<p>The predicted petal width <a id="_idIndexMarker691"/>is 1.45 cm. We have now finished<a id="_idIndexMarker692"/> going over linear<a id="_idIndexMarker693"/> regression in R<a id="_idIndexMarker694"/> with a basic example. We will now continue the chapter in the next section on performing logistic regression <span class="No-Break">in R.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor187"/>Performing logistic regression in R</h1>
			<p>As we did in the section<a id="_idIndexMarker695"/> on linear regression, in this section, we<a id="_idIndexMarker696"/> will also perform logistic regression in base R and with the <strong class="source-inline">tidymodels</strong> framework. We are going to only perform a simple binary classification regression problem using the <strong class="source-inline">Titanic</strong> dataset, where we will be deciding if someone is going to survive or not. Let’s dive right <span class="No-Break">into it.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor188"/>Logistic regression with base R</h2>
			<p>In order to get<a id="_idIndexMarker697"/> going, we are going<a id="_idIndexMarker698"/> to start with a base R implementation of logistic regression on the <strong class="source-inline">Titanic</strong> dataset where we will be modeling the response of <strong class="source-inline">Survived</strong>. So, let’s get straight <span class="No-Break">into it.</span></p>
			<p>The following is the code that will perform the data modeling along with explanations of what <span class="No-Break">is happening:</span></p>
			<pre class="source-code">
library(tidyverse)
df &lt;- Titanic |&gt;
       as.data.frame() |&gt;
       uncount(Freq)</pre>			<p>This block of code starts by loading<a id="_idIndexMarker699"/> a library called <strong class="source-inline">tidyverse</strong>, which contains various<a id="_idIndexMarker700"/> data manipulation and visualization tools. It then creates a data frame called <strong class="source-inline">df</strong> by taking the <strong class="source-inline">Titanic</strong> dataset (assuming it’s available in your environment) and performing three operations on it using the <strong class="source-inline">|&gt;</strong> operator, where we then use  <strong class="source-inline">as.data.frame()</strong>, which converts the dataset into a data frame, followed by <strong class="source-inline">uncount(Freq)</strong>, which repeats each row in the dataset according to the value in the <strong class="source-inline">Freq</strong> column. This is often done to expand <span class="No-Break">summarized data:</span></p>
			<pre class="source-code">
set.seed(123)
train_index &lt;- sample(nrow(df), floor(nrow(df) * 0.8), replace = FALSE)
train &lt;- df[train_index, ]
test &lt;- df[-train_index, ]</pre>			<p>This section is about splitting the data into a training set and a test set, which is a common practice in <span class="No-Break">machine learning:</span></p>
			<ul>
				<li><strong class="source-inline">set.seed(123)</strong>: This sets a random seed for reproducibility, ensuring that random operations produce the same results <span class="No-Break">each time.</span></li>
				<li><strong class="source-inline">sample(nrow(df), floor(nrow(df) * 0.8), replace = FALSE)</strong>: This randomly selects 80% of the rows in the <strong class="source-inline">df</strong> data frame (the training set) without replacement and stores their indices <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">train_index</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">train &lt;- df[train_index, ]</strong>: This creates the training set by selecting the rows from <strong class="source-inline">df</strong> using the <span class="No-Break"><strong class="source-inline">train_index</strong></span><span class="No-Break"> indices.</span></li>
				<li><strong class="source-inline">test &lt;- df[-train_index, ]</strong>: This creates the test set by selecting the rows from <strong class="source-inline">df</strong> that are not in the training set. We next create <span class="No-Break">the model.</span><pre class="source-code">
model &lt;- glm(Survived ~ Sex + Age + Class, data = train, family = "binomial")</pre></li>			</ul>
			<p>Now let’s discuss the model code <span class="No-Break">as follows:</span></p>
			<ul>
				<li>This block trains a logistic regression model using the <span class="No-Break"><strong class="source-inline">glm</strong></span><span class="No-Break"> function.</span></li>
				<li>The model is trained to predict the <strong class="source-inline">Survived</strong> variable based on the <strong class="source-inline">Sex</strong>, <strong class="source-inline">Age</strong>, and <strong class="source-inline">Class</strong> variables in the training data. Here, <strong class="source-inline">Age</strong> is <span class="No-Break">actually discrete.</span></li>
				<li>The <strong class="source-inline">family = "binomial"</strong> argument specifies that this is a binary classification problem, where the outcome is either <strong class="source-inline">Yes</strong> or <strong class="source-inline">No</strong>. The following link helps in choosing an appropriate <span class="No-Break">family: </span><a href="https://stats.stackexchange.com/a/303592/35448"><span class="No-Break">https://stats.stackexchange.com/a/303592/35448</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>Now, let’s set up the model predictions and <span class="No-Break">response variable:</span></p>
			<pre class="source-code">
predictions &lt;- predict(model, newdata = test, type = "response")
pred_resp &lt;- ifelse(predictions &lt;= 0.5, "No", "Yes")</pre>			<p>Now, let’s go over what we <span class="No-Break">just did:</span></p>
			<ul>
				<li>Here, we use<a id="_idIndexMarker701"/> the trained model to make predictions<a id="_idIndexMarker702"/> on the <span class="No-Break">test set.</span></li>
				<li><strong class="source-inline">predict(model, newdata = test, type = "response")</strong> calculates the predicted probabilities of survival for each passenger in the <span class="No-Break">test set.</span></li>
				<li><strong class="source-inline">ifelse(predictions &lt;= 0.5, "No", "Yes")</strong> converts these probabilities into binary predictions: <strong class="source-inline">"No"</strong> if the probability is less than or equal to <strong class="source-inline">0.5</strong>, and <strong class="source-inline">"Yes"</strong> otherwise. This is common practice, but you must know your project first in order to determine if this is correct or not. Now, onto the <span class="No-Break"><strong class="source-inline">accuracy</strong></span><span class="No-Break"> variable:</span><pre class="source-code">
accuracy &lt;- mean(pred_resp == test$Survived)</pre></li>			</ul>
			<p>We created the <strong class="source-inline">accuracy</strong> variable by doing <span class="No-Break">the following:</span></p>
			<ul>
				<li>This line calculates the accuracy of the model’s predictions by comparing <strong class="source-inline">pred_resp</strong> (the model’s predictions) to the actual survival status in the test <span class="No-Break">set (</span><span class="No-Break"><strong class="source-inline">test$Survived</strong></span><span class="No-Break">).</span></li>
				<li>It computes the mean of the resulting logical values, where <strong class="source-inline">TRUE</strong> represents a correct prediction, and <strong class="source-inline">FALSE</strong> represents an incorrect prediction. Let’s now go over the rest of <span class="No-Break">the code:</span><pre class="source-code">
print(accuracy)
table(pred_resp, test$Survived)</pre></li>			</ul>
			<p>The code prints <span class="No-Break">two things:</span></p>
			<ul>
				<li>The accuracy of the model on the <span class="No-Break">test set.</span></li>
				<li>A confusion matrix that shows how many predictions were correct and how many were incorrect. If you would like to understand confusion matrices more, here is a good <span class="No-Break">link: </span><a href="https://www.v7labs.com/blog/confusion-matrix-guide"><span class="No-Break">https://www.v7labs.com/blog/confusion-matrix-guide</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>In summary, this code loads a dataset, splits<a id="_idIndexMarker703"/> it into a training and test set, trains a logistic regression model<a id="_idIndexMarker704"/> to predict survival, evaluates the model’s accuracy, and displays the results. It’s a basic example of a binary classification machine learning workflow. Now that we have covered performing logistic regression for a classification problem in base R, we will try our hand at the same but this time using the <span class="No-Break"><strong class="source-inline">tidymodels</strong></span><span class="No-Break"> framework.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor189"/>Performing logistic regression using tidymodels</h2>
			<p>In this section, we will use<a id="_idIndexMarker705"/> the <strong class="source-inline">tidymodels</strong> framework to perform<a id="_idIndexMarker706"/> the logistic regression on the <strong class="source-inline">Titanic</strong> dataset. Since we have done this in base R already, let’s get right <span class="No-Break">into it:</span></p>
			<pre class="source-code">
library(tidymodels)
library(healthyR.ai)</pre>			<p>This code loads the two libraries that we will need for our analysis: <strong class="source-inline">tidymodels</strong> and <strong class="source-inline">healthyR.ai</strong>. <strong class="source-inline">tidymodels</strong> is a library that provides a common interface for many machine learning algorithms, while <strong class="source-inline">healthyR.ai</strong> provides a set of tools for evaluating the performance of machine <span class="No-Break">learning models:</span></p>
			<pre class="source-code">
df &lt;- Titanic |&gt;
    as_tibble() |&gt;
    uncount(n) |&gt;
    mutate(across(where(is.character), as.factor))</pre>			<p>This code converts the <strong class="source-inline">Titanic</strong> dataset to a <strong class="source-inline">tibble</strong>, which is a data structure that is compatible with <strong class="source-inline">tidymodels</strong>. It also uncounts the <strong class="source-inline">n</strong> column, which is a column that contains the number of times each row appears in the dataset and is created by the <strong class="source-inline">uncount()</strong> function. Finally, it converts all the character variables in the dataset <span class="No-Break">to factors:</span></p>
			<pre class="source-code">
# Set seed for reproducibility
set.seed(123)
# Split the data into training and test sets
split &lt;- initial_split(df, prop = 0.8)
train &lt;- training(split)
test &lt;- testing(split)</pre>			<p>This code splits the <strong class="source-inline">df</strong> dataset<a id="_idIndexMarker707"/> into training and test sets. The training<a id="_idIndexMarker708"/> set is used to train the model, while the test set is used to evaluate the performance of the model on unseen data. The <strong class="source-inline">initial_split()</strong> function from <strong class="source-inline">tidymodels</strong> is used to perform the split. The <strong class="source-inline">prop</strong> argument specifies the proportion of the data that should be used for training. In this case, we are using 80% of the data for training and 20% of the data <span class="No-Break">for testing:</span></p>
			<pre class="source-code">
# Create a recipe for pre-processing
recipe &lt;- recipe(Survived ~ Sex + Age + Class, data = train)
# Specify logistic regression as the model
log_reg &lt;- logistic_reg() |&gt; set_engine("glm", family = "binomial")
# Combine the recipe and model into a workflow
workflow &lt;- workflow() %&gt;% add_recipe(recipe) %&gt;% add_model(log_reg)
# Train the logistic regression model
fit &lt;- fit(workflow, data = train)</pre>			<p>This code trains a logistic regression model to predict survival on the Titanic. The <strong class="source-inline">recipe()</strong> function from <strong class="source-inline">tidymodels</strong> is used to pre-process the data. The <strong class="source-inline">logistic_reg()</strong> function from <strong class="source-inline">tidymodels</strong> is used to specify the logistic regression model. The <strong class="source-inline">workflow()</strong> function from <strong class="source-inline">tidymodels</strong> is used to combine the recipe and model into a workflow. Finally, the <strong class="source-inline">fit()</strong> function from <strong class="source-inline">tidymodels</strong> is used to train the model on the <span class="No-Break">training data:</span></p>
			<pre class="source-code">
# Predict on the test set
predictions &lt;- predict(fit, new_data = test) |&gt; bind_cols(test) |&gt; select(Class:Survived, .pred_class)
# Better method
pred_fit_tbl &lt;- fit |&gt; augment(new_data = test)</pre>			<p>This code predicts the survival<a id="_idIndexMarker709"/> probability for each passenger<a id="_idIndexMarker710"/> in the test set. The <strong class="source-inline">predict()</strong> function from <strong class="source-inline">tidymodels</strong> is used to make the predictions. The <strong class="source-inline">new_data</strong> argument specifies the data that we want to make predictions on. In this case, we are making predictions on the test set. The <strong class="source-inline">bind_cols()</strong> function is used to bind the predictions to the test set data. The <strong class="source-inline">select()</strong> function is used to select the columns that we want to keep. The <strong class="source-inline">pred_fit_tbl</strong> object is a <strong class="source-inline">tibble</strong> instance that contains the predictions from the model, as well as the ground truth survival labels. This object will be used to evaluate the performance of <span class="No-Break">the model:</span></p>
			<pre class="source-code">
# Accuracy metrics for the model to be scored against from the healthyR.ai package
perf &lt;- hai_default_classification_metric_set()
# Calculate the accuracy metrics
perf(pred_fit_tbl, truth = Survived, estimate = .pred_class)
# Print the confusion matrix
predictions |&gt; conf_mat(truth = Survived, estimate = .pred_class)</pre>			<p>The accuracy check code block evaluates the performance of the model on the test set. It does this by using the <strong class="source-inline">hai_default_classification_metric_set()</strong> function from the healthyR.ai package to create a set of default classification metrics. These metrics include accuracy, precision, recall, and <span class="No-Break">F1 score.</span></p>
			<p>The <strong class="source-inline">perf()</strong> function is then used to calculate the accuracy metrics on the test set. The <strong class="source-inline">pred_fit_tbl</strong> object is the data frame that contains the predictions from the model, as well as the ground truth survival labels. The <strong class="source-inline">truth</strong> and <strong class="source-inline">estimate</strong> arguments specify the columns in the data frame that contain the ground truth and predicted <span class="No-Break">labels, respectively.</span></p>
			<p>The <strong class="source-inline">conf_mat()</strong> function is then used to print the confusion matrix for the model. The confusion matrix is a table that shows how many observations were correctly and incorrectly predicted by <span class="No-Break">the model.</span></p>
			<p>Finally, the <strong class="source-inline">tidy()</strong> and <strong class="source-inline">glance()</strong> functions from the <strong class="source-inline">broom</strong> package can be used to tidy and summarize the fitted<a id="_idIndexMarker711"/> model. The <strong class="source-inline">tidy()</strong> function converts the model object to a <strong class="source-inline">tibble</strong> instance, which<a id="_idIndexMarker712"/> is a data structure that is easy to work with. The <strong class="source-inline">glance()</strong> function prints a summary of the model, including the coefficients, standard errors, and p-values for all of the variables in <span class="No-Break">the model.</span></p>
			<p>Here is a simple explanation of each of the accuracy metrics that are calculated in the accuracy check <span class="No-Break">code block:</span></p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: The accuracy of a model is the proportion<a id="_idIndexMarker713"/> of observations that are correctly predicted by <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Precision</strong>: The precision of a model is the<a id="_idIndexMarker714"/> proportion of positive predictions that <span class="No-Break">are correct.</span></li>
				<li><strong class="bold">Recall</strong>: The recall of a model <a id="_idIndexMarker715"/>is the proportion of actual positive observations that are correctly predicted by <span class="No-Break">the model.</span></li>
				<li><strong class="bold">F1 score</strong>: The F1 score is a harmonic<a id="_idIndexMarker716"/> mean of the precision and recall metrics. It is a good overall measure of the performance of <span class="No-Break">a model.</span></li>
			</ul>
			<p>The confusion matrix is a helpful tool for understanding how the model is performing. The ideal confusion matrix would have all of the observations on the diagonal, indicating that all of the observations were correctly predicted. However, in practice, no model is perfect and there will be some observations that are <span class="No-Break">incorrectly predicted.</span></p>
			<p>Lastly, we will <a id="_idIndexMarker717"/>visualize the model with a <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) curve. To read <a id="_idIndexMarker718"/>more about this type<a id="_idIndexMarker719"/> of curve, you can see the following link: <a href="https://www.tmwr.org/performance">https://www.tmwr.org/performance</a>. Here is the code<a id="_idIndexMarker720"/> that creates the <span class="No-Break">ROC curve:</span></p>
			<pre class="source-code">
roc_curve(
  pred_fit_tbl, truth = Survived, .pred_Yes,
  event_level = "second"
) |&gt;
  autoplot()</pre>			<p>Here is <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B19142_09_1.jpg" alt="Figure 9.1 – ROC curve for the logistic regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – ROC curve for the logistic regression model</p>
			<p>Now, we have learned how to perform both linear and logistic regression in both base R and via the <strong class="source-inline">tidymodels</strong> modeling framework. We did this with the <strong class="source-inline">Titanic</strong> and <strong class="source-inline">iris</strong> datasets. Now, it’s time to do the same <span class="No-Break">in Python!</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor190"/>Performing linear regression in Python using Excel data</h1>
			<p>Linear regression in Python<a id="_idIndexMarker721"/> can be carried out with the help<a id="_idIndexMarker722"/> of libraries<a id="_idIndexMarker723"/> such as <strong class="source-inline">pandas</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">statsmodels</strong>, and <strong class="source-inline">matplotlib</strong>. The following is a step-by-step <span class="No-Break">code example:</span></p>
			<ol>
				<li>First, import the <span class="No-Break">necessary libraries:</span><pre class="source-code">
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from statsmodels.graphics.regressionplots import plot_regress_exog
from statsmodels.graphics.gofplots import qqplot</pre></li>				<li>Then, we create an Excel file with test data. Of course, in a real-life scenario, you would not need the mock data – you would skip this step and load the data from Excel (see the next step) after loading the <span class="No-Break">necessary libraries:</span><pre class="source-code">
# Step 0: Generate sample data and save as Excel file
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 2)  # Two features
y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(n_samples)
# Linear relationship with noise
# Create a pandas DataFrame
data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}
df = pd.DataFrame(data)
# Save the <a id="_idTextAnchor191"/>data to Excel
df.to_excel("linear_regression_input.xlsx")</pre></li>				<li>Next, import the data<a id="_idIndexMarker724"/> from the Excel file<a id="_idIndexMarker725"/> with test data<a id="_idIndexMarker726"/> and prepare it for analysis using tools you have learned in the <span class="No-Break">previous chapter:</span><pre class="source-code">
# Step 1: Import Excel data into a pandas DataFrame
excel_file = "linear_regression_input.xlsx"
df = pd.read_excel(excel_file)
# Step 2: Explore the data
# Use the tools learned in the previous chapter on EDA
# Step 3: Data Preparation (if needed)
# Use the tools learned in the previous chapter on data cleaning</pre></li>				<li>Now, we are ready to carry out the actual analysis. Split the data into training and test data so we can evaluate<a id="_idIndexMarker727"/> the model on a dedicated data (sub)set, then fit the <strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) linear model on the <span class="No-Break">training data:</span><pre class="source-code">
# Step 4: Split data into training and testing sets
X = df[['Feature1', 'Feature2']] # Independent variables
y = df['Target'] # Dependent variable
# Split the data into training and test set using a fixed random seed for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, 
    test_size=0.2, random_state=42)
# Step 5: Fit the Linear Regression model
# Add a constant (intercept) to the independent variables
X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)
# Fit the linear model
model = sm.OLS(y_train, X_train).fit()</pre><p class="list-inset">Note that doing imputation<a id="_idIndexMarker728"/> as part of the data<a id="_idIndexMarker729"/> cleaning process<a id="_idIndexMarker730"/> before splitting the test and training sets may lead to pollution of the test set from the training set. Be conscious of this when performing the data cleaning and <span class="No-Break">preparation steps!</span></p></li>				<li>Next, evaluate the trained model on the <span class="No-Break">test data:</span><pre class="source-code">
# Step 6: Model Evaluation
y_pred = model.predict(X_test)
# Print the model summary
print(model.summary())</pre><p class="list-inset">This will create summary statistics as the output that provides valuable insights into the relationships within <span class="No-Break">your dataset:</span></p></li>			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B19142_09_2.jpg" alt="Figure 9.2 – Summary statistics of the model fitted"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Summary statistics of the model fitted</p>
			<p>Actual interpretation<a id="_idIndexMarker731"/> of model results is a topic<a id="_idIndexMarker732"/> that is beyond the scope<a id="_idIndexMarker733"/> of this book, but here are some hints to get <span class="No-Break">you started:</span></p>
			<ul>
				<li><strong class="bold">Coefficients</strong>: The coefficients associated<a id="_idIndexMarker734"/> with each independent variable (predictor) in the model tell you about the strength and direction of the relationship. A positive coefficient indicates a positive correlation, meaning that as the predictor increases, the target variable tends to increase as well. Conversely, a negative coefficient signifies a <span class="No-Break">negative correlation.</span></li>
				<li><strong class="bold">Intercept</strong>: The intercept represents<a id="_idIndexMarker735"/> the predicted value of the target variable when all predictor variables are set to zero. It’s essential to consider the intercept’s value in the context of <span class="No-Break">your analysis.</span></li>
				<li><strong class="bold">R-squared (</strong><strong class="bold">R</strong><span class="superscript">2</span><strong class="bold">)</strong>: The R-squared value measures<a id="_idIndexMarker736"/> the goodness of fit of the model. It tells you the proportion of variance in the target variable that can be explained by the predictors. Higher R-squared values (closer to 1) indicate a better fit. Note that adding more variables will always increase this measure. A “better” fit might result in “overfitting,” which is something we don’t want. You may want to check model-fit selection criteria such as Mallow’s Cp, AIC, BIC, and adjusted R-squared, which penalizes the number of parameters used to fit <span class="No-Break">the model.</span></li>
				<li><strong class="bold">P-values</strong>: P-values associated with coefficients<a id="_idIndexMarker737"/> help determine the statistical significance of each<a id="_idIndexMarker738"/> predictor. Lower p-values<a id="_idIndexMarker739"/> suggest greater <a id="_idIndexMarker740"/>significance (in the sense that it is stronger evidence to reject the null hypothesis). If a p-value is less than a chosen significance level (for example, 0.05), you can conclude that the predictor has a statistically significant effect on the target variable. Please be aware that there are good reasons to not rely on p-values alone; see the ongoing debate on p-hacking and related topics in <span class="No-Break">statistical science.</span></li>
				<li><strong class="bold">Residuals</strong>: Examining the residuals (the differences between the observed and predicted values) is crucial<a id="_idIndexMarker741"/> for assessing model performance. Ideally, residuals should be random, with no apparent patterns. Patterns in residuals may indicate <span class="No-Break">model misspecification.</span></li>
				<li><strong class="bold">Confidence intervals</strong>: Confidence intervals around coefficients<a id="_idIndexMarker742"/> provide a range within which the true population parameter is likely to lie. Wider intervals indicate <span class="No-Break">greater uncertainty.</span></li>
				<li><strong class="bold">F-statistic</strong>: The F-statistic tests the overall significance <a id="_idIndexMarker743"/>of the model. A small F-statistic suggests that the model doesn’t explain much variance in the target variable, while a large value indicates a better <span class="No-Break">overall fit.</span></li>
				<li><strong class="bold">Adjusted R-squared</strong>: Adjusted R-squared adjusts the R-squared<a id="_idIndexMarker744"/> value for the number of predictors in the model. It helps you determine whether adding more predictors improves the <span class="No-Break">model’s fit.</span></li>
			</ul>
			<p>By carefully examining these <a id="_idIndexMarker745"/>elements, you can gain<a id="_idIndexMarker746"/> insights into how well the linear<a id="_idIndexMarker747"/> model fits your data, the significance of predictor variables, and the overall quality of the model. This information is invaluable for making informed decisions and drawing meaningful conclusions from <span class="No-Break">your analysis.</span></p>
			<p>With the model trained and the fit evaluated, we can visualize the results to help with interpretation. The following code creates a scatterplot of predicted versus <span class="No-Break">observed values:</span></p>
			<pre class="source-code">
plt.scatter(X_test['Feature1'], y_test, color='blue', label='Actual')
plt.scatter(X_test['Feature1'], y_pred, color='red', 
    label='Predicted')
plt.xlabel('Feature1')
plt.ylabel('Target')
plt.title('Linear Regression Prediction')
plt.legend()
plt.show()</pre>			<p>Here is the scatterplot <span class="No-Break">for it:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B19142_09_3.jpg" alt="Figure 9.3 – Linear regression prediction plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Linear regression prediction plot</p>
			<p>In addition, we can create diagnostic<a id="_idIndexMarker748"/> plots and visualizations<a id="_idIndexMarker749"/> such as residual<a id="_idIndexMarker750"/> plots and Q-Q plots, which can help you identify potential issues with the model, such as heteroscedasticity <span class="No-Break">or outliers:</span></p>
			<pre class="source-code">
# Set the backend to 'TkAgg' before generating the plots if needed – comment out this line if in WSL or other non-interactive environment
plt.switch_backend('TkAgg')
# Residuals
fig, ax = plt.subplots(figsize=(12, 8))
plot_regress_exog(model, "Feature1", fig=fig)
plt.show()
# Q-Q plot:
qqplot(model.resid, line="s")
plt.show()</pre>			<p>The two <a id="_idIndexMarker751"/>preceding<a id="_idIndexMarker752"/> plots look<a id="_idIndexMarker753"/> <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B19142_09_4.jpg" alt="Figure 9.4 – Residuals plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Residuals plot</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B19142_09_5.jpg" alt="Figure 9.5 – Residuals Q-Q plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Residuals Q-Q plot</p>
			<p>Finally, we can export the results to Excel. This will be covered in detail in the <span class="No-Break">next subsection.</span></p>
			<p>As a side note, <strong class="source-inline">scikit-learn</strong> also has an in-built linear model but that does not come with the handy summary statistics we have used in the <span class="No-Break">preceding code.</span></p>
			<p>This code demonstrated a basic<a id="_idIndexMarker754"/> linear regression<a id="_idIndexMarker755"/> workflow using Python<a id="_idIndexMarker756"/> and Excel data. Let’s move on to <span class="No-Break">logistic regression!</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor192"/>Logistic regression in Python using Excel data</h1>
			<p>In the following code, we generate<a id="_idIndexMarker757"/> random sample<a id="_idIndexMarker758"/> data with two<a id="_idIndexMarker759"/> features (<strong class="source-inline">Feature1</strong> and <strong class="source-inline">Feature2</strong>) and a binary target variable (<strong class="source-inline">Target</strong>) based on a simple condition. We perform logistic regression, evaluate the model using accuracy, the confusion matrix, and a classification<a id="_idIndexMarker760"/> report, visualize the results for binary<a id="_idIndexMarker761"/> classification, and interpret<a id="_idIndexMarker762"/> <span class="No-Break">the coefficients.</span></p>
			<p>The following is a step-by-step <span class="No-Break">code example:</span></p>
			<ol>
				<li>Again, we start with importing the <span class="No-Break">necessary libraries:</span><pre class="source-code">
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report</pre><p class="list-inset">For this example, we will use a different <span class="No-Break">sample dataset:</span></p><pre class="source-code"># Step 0: Generate sample data
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 2)  # Two features
y = (X[:, 0] + X[:, 1] &gt; 1).astype(int)  # Binary classification based on a condition
# Create a pandas DataFrame
data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}
df = pd.DataFrame(data)
df.to_excel("logistic_regression_input.xlsx")</pre></li>				<li>With your data available in Excel, we can read it and prepare it for the <span class="No-Break">modeling step:</span><pre class="source-code">
# Step 1: Import Excel data into a pandas DataFrame
excel_file = "logistic_regression_input.xlsx"
df = pd.read_excel(excel_file)
# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2, random_state=42)</pre></li>				<li>Now, we can create<a id="_idIndexMarker763"/> and fit a<a id="_idIndexMarker764"/> model. We will use the <strong class="source-inline">scikit-learn</strong> library<a id="_idIndexMarker765"/> <span class="No-Break">this time:</span><pre class="source-code">
# Step 3: Create and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)</pre></li>				<li>With a model fit, we can now visualize <span class="No-Break">the results:</span><pre class="source-code">
# Step 4: Visualization
# Visualization for binary classification
plt.scatter(X_test[y_test == 1][:, 0], 
    X_test[y_test == 1][:, 1], color='blue', 
    label='Class 1 (Actual)')
plt.scatter(X_test[y_test == 0][:, 0], 
    X_test[y_test == 0][:, 1], color='red', 
    label='Class 0 (Actual)')
plt.xlabel('Feature1')
plt.ylabel('Feature2')
plt.title('Logistic Regression Prediction')
plt.legend()
plt.show()</pre></li>				<li>Unlike with linear<a id="_idIndexMarker766"/> regression, we need<a id="_idIndexMarker767"/> different goodness-of-fit <a id="_idIndexMarker768"/>metrics because we are using logistic regression for a <span class="No-Break">binary classification:</span><pre class="source-code">
# Step 5: Model Evaluation and Interpretation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)</pre><p class="list-inset">The result looks <span class="No-Break">like this:</span></p></li>			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B19142_09_6.jpg" alt="Figure 9.6 – Logistic regression prediction plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Logistic regression prediction plot</p>
			<p class="list-inset">This is <a id="_idIndexMarker769"/>the<a id="_idIndexMarker770"/> <span class="No-Break">code</span><span class="No-Break"><a id="_idIndexMarker771"/></span><span class="No-Break"> result:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B19142_09_7.jpg" alt="Figure 9.7 – Model summary statistics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Model summary statistics</p>
			<p class="list-inset">To interpret the preceding results, you can start with <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">Accuracy</strong> is a fundamental<a id="_idIndexMarker772"/> metric, representing <a id="_idIndexMarker773"/>the ratio of correctly<a id="_idIndexMarker774"/> predicted instances to the total number of instances. While easy to understand, accuracy can be misleading if there’s an imbalance between <span class="No-Break">the classes.</span></li>
				<li><strong class="source-inline">Confusion Matrix</strong> offers a more detailed view. It breaks down predictions into four categories: true positives, true negatives, false positives, and false negatives. This matrix provides a clear understanding of how well the model performs in terms of correctly classifying positive and <span class="No-Break">negative instances.</span></li>
				<li><strong class="source-inline">Classification Report</strong> provides a comprehensive summary. It includes metrics such as <strong class="source-inline">precision</strong>, <strong class="source-inline">recall</strong>, <strong class="source-inline">f1-score</strong>, and <strong class="source-inline">support</strong> for both classes. <strong class="source-inline">Precision</strong> measures how many predicted positives were actually positive, while <strong class="source-inline">recall</strong> quantifies how many actual positives were correctly predicted. The <strong class="source-inline">F1-score</strong> balances <strong class="source-inline">precision</strong> and <strong class="source-inline">recall</strong>. <strong class="source-inline">Support</strong> denotes the number of instances for each class. Together, these metrics offer a more nuanced evaluation of the model’s performance in binary <span class="No-Break">classification tasks.</span></li>
			</ul>
			<p>You can use this sample data and code for testing and experimenting with <span class="No-Break">logistic regression.</span></p>
			<p>Please note that contrary to the popular (but incorrect) assertion, logistic regression can be used as a regression as well – what makes it a classifier is an arbitrary cut-off point for the predicted probability. For some use cases, you might want to use the raw regression output (if, for example, you are interested in the predicted probability of the data point belonging to a class, and not the more likely class only) and for others, you might want to play with the cut-off point (if, for example, there is pre-existing domain information that implies that 50% is not the right <span class="No-Break">cut-off point).</span></p>
			<p>That’s it! Logistic regression<a id="_idIndexMarker775"/> is a relatively simple<a id="_idIndexMarker776"/> model with lots<a id="_idIndexMarker777"/> of benefits. It’s performant, easy to fit, easy to interpret, and very versatile. It’s most often used for classification with a domain-knowledge-driven cut-off point, but under the hood, it remains a regression method that can be used for predicting <span class="No-Break">class probabilities.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor193"/>Summary</h1>
			<p>In this chapter, we explored the powerful world of linear and logistic regression using Excel data. Linear regression, a fundamental statistical technique, allows us to model relationships between dependent and independent variables. We discussed its assumptions and applications, and walked through the entire process of loading data from Excel, preparing it for analysis, and fitting linear regression models using both R (using base R and <strong class="source-inline">tidymodels</strong>) and Python (with the <strong class="source-inline">scikit-learn</strong> and <span class="No-Break"><strong class="source-inline">statsmodels</strong></span><span class="No-Break"> libraries).</span></p>
			<p>Through comprehensive code examples, you learned how to perform regression analysis, assess model accuracy, and generate valuable statistics and metrics to interpret model results. We gained insights into creating diagnostic plots, such as residual plots and Q-Q plots, which aid in identifying issues such as heteroscedasticity <span class="No-Break">and outliers.</span></p>
			<p>Additionally, we delved into logistic regression, a powerful tool for class probability prediction and binary classification tasks. We established its importance and applications and outlined the process of data preparation, model fitting, and metrics evaluation. With practical code examples, we observed how logistic regression models can be constructed using <strong class="source-inline">tidymodels</strong> in R and the <strong class="source-inline">scikit-learn</strong> library <span class="No-Break">in Python.</span></p>
			<p>By the end of this chapter, you should have a strong grasp of linear and logistic regression, from theory to practical application, and the ability to harness these techniques to analyze your <span class="No-Break">data efficiently.</span></p>
			<p>With these skills, you are well-equipped to conduct regression analyses and extract valuable insights from your data <span class="No-Break">in Python.</span></p>
			<p>On to the next chapter, where you will learn about time series analysis and its applications to <span class="No-Break">Excel data.</span></p>
		</div>
	</body></html>