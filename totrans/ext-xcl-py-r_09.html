<html><head></head><body>
		<div><h1 id="_idParaDest-160" class="chapter-number"><a id="_idTextAnchor178"/>9</h1>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor179"/>Statistical Analysis: Linear and Logistic Regression</h1>
			<p>Welcome to our comprehensive guide on linear and logistic regression using R and Python, where we will explore these essential statistical techniques using two popular frameworks: <code>tidymodels</code> and base R and Python. Whether you’re a data science enthusiast or a professional looking to sharpen your skills, this tutorial will help you gain a deep understanding of <strong class="bold">linear</strong> and <strong class="bold">logistic regression</strong> and how to implement them in R and Python. Now, it is possible to perform linear and logistic regression. The issue here is that linear regression can only be performed on a single series of ungrouped data, and performing logistic regression is cumbersome and may require the use of external solver add-ins. Also, the process can only be performed against ungrouped or non-nested data. In R and Python, we do not have such limitations.</p>
			<p>In this chapter, we will cover the following topics in both base R and Python and using the <code>tidymodels</code> framework:</p>
			<ul>
				<li>Performing linear regression in both base R and Python and the <code>tidymodels</code> frameworks as well as in Python</li>
				<li>Performing logistic regression in both base R and Python and the <code>tidymodels</code> frameworks as well as in Python</li>
			</ul>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor180"/>Technical requirements</h1>
			<p>All code for this chapter can be found on GitHub at this URL:  <a href="https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter9">https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter9</a>. You will need the following R packages installed to follow along:</p>
			<ul>
				<li><code>readxl 1.4.3</code></li>
				<li><code>performance 0.10.8</code></li>
				<li><code>tidymodels 1.1.1</code></li>
				<li><code>purrr 1.0.2</code></li>
			</ul>
			<p>We will begin by learning about what linear and logistic regression are and then move into the details of everything.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor181"/>Linear regression</h1>
			<p>Linear regression is a fundamental statistical method<a id="_idIndexMarker665"/> used for modeling the relationship between a dependent variable (usually denoted as “Y”) and one or more independent variables (often denoted as “X”). It aims to find the best-fitting linear equation that describes how changes in the independent variables<a id="_idIndexMarker666"/> affect the dependent variable. Many of you may know this as the <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) method.</p>
			<p>In simpler terms, linear regression helps us predict a continuous numeric outcome based on one or more input features. For this to work, if you are unaware, many assumptions must be held true. If you would like to understand these more, then a simple search will bring you a lot of good information on them. In this tutorial, we will delve into both simple linear regression (one independent variable) and multiple linear regression (multiple independent variables).</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor182"/>Logistic regression</h1>
			<p>Logistic regression is another crucial statistical<a id="_idIndexMarker667"/> technique, which is primarily used for binary classification problems. Instead of predicting continuous outcomes, logistic regression predicts the probability of an event occurring, typically expressed as a “yes” or “no” outcome. This method is particularly useful for scenarios where we need to model the likelihood of an event, such as whether a customer will churn or not or whether an email is spam or not. Logistic regression models the relationship between the independent variables and the log odds of the binary outcome.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor183"/>Frameworks</h2>
			<p>We will explore two approaches<a id="_idIndexMarker668"/> to implementing linear and logistic regression in R. First, we will use the base R framework, which is an excellent starting point to understand the underlying concepts and functions. Then, we will dive into <code>tidymodels</code>, a modern and tidy approach to modeling and machine learning in R. <code>tidymodels</code> provides a consistent and efficient way to build, tune, and evaluate models, making it a valuable tool for data scientists. In Python, we will parallel this exploration with two prominent libraries: <code>sklearn</code> and <code>statsmodels</code>. <code>sklearn</code>, or Scikit-learn, offers a wide array of simple and efficient tools for predictive data analysis that are accessible to everybody and reusable in various contexts. <code>statsmodels</code> is more focused on statistical models and hypothesis tests. Together, these Python libraries offer a robust framework for implementing linear and logistic regression, catering to both machine learning and statistical needs.</p>
			<p>Throughout this chapter, we will provide step-by-step instructions, code examples, and practical insights to ensure that you can confidently apply linear and logistic regression techniques to your own data analysis projects. </p>
			<p>Let’s embark on this learning journey and unlock the power of regression analysis in R! With this in place, we move to the first example in base R using the <code>iris</code> dataset we saved in <a href="B19142_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor184"/>Performing linear regression in R</h1>
			<p>For this section, we are<a id="_idIndexMarker669"/> going to perform<a id="_idIndexMarker670"/> linear regression in R, both in base R and by way of the <code>tidymodels</code> framework. In this section, you will learn how to do this on a dataset that has different groups in it. We will do this because if you can learn to do it this way, then doing it in a single group becomes simpler as there is no need to group data and perform actions by group. The thought process here is that by doing it on grouped data, we hope you can learn an extra skill.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor185"/>Linear regression in base R</h2>
			<p>The first example we are going<a id="_idIndexMarker671"/> to show is using<a id="_idIndexMarker672"/> the <code>lm()</code> function to perform a linear regression in base R. Let’s dive right into it with the <code>iris</code> dataset.</p>
			<p>We will break the code down into chunks and discuss what is happening at each step. The first step for us is to use the <code>library</code> command to bring in the necessary packages into our development environment:</p>
			<pre class="source-code">
library(readxl)</pre>			<p>In this section, we’re loading a library called <code>readxl</code>. Libraries are collections of pre-written R functions and code that we can use in our own R scripts. In this case, we’re loading the <code>readxl</code> library, which is commonly used for reading data from Excel files. The path assumes you have a <code>chapter1</code> folder and a data file in it called <code>iris_data.xlsx</code>:</p>
			<pre class="source-code">
df &lt;- read_xlsx(
  path = "chapter1/iris_data.xlsx",
  sheet = "iris"
)
head(df)</pre>			<p>Here, we’re reading data from an Excel file named <code>iris_data.xlsx</code>, located in the <code>chapter1</code> folder. We’re specifically reading the <code>iris</code> sheet from that Excel file. The <code>read_xlsx</code> function is used for this purpose. The resulting data is stored in a variable called <code>df</code>. The <code>head(df)</code> function displays the first few rows of this data frame (<code>df</code>) so we can see what it looks like:</p>
			<pre class="source-code">
iris_split &lt;- split(df, df$species)</pre>			<p>This code splits the <code>df</code> dataset<a id="_idIndexMarker673"/> into multiple subsets based on the unique values<a id="_idIndexMarker674"/> in the <code>species</code> column. The result is a list of data frames where each data frame contains only the rows that correspond to a specific species of <code>iris</code>.</p>
			<p>Now, we are going to define what will be the dependent and independent variables along with the <code>formula</code> object:</p>
			<pre class="source-code">
dependent_variable &lt;- "petal_length"
independent_variables &lt;- c("petal_width", "sepal_length", "sepal_width")
f_x &lt;- formula(
  paste(
dependent_variable,
"~",
paste(independent_variables, collapse = " + ")
)
)</pre>			<p>Here, we’re defining the variables needed for linear regression. <code>dependent_variable</code> is <code>petal_length</code>, which is the variable we want to predict. <code>independent_variables</code> are <code>petal_width</code>, <code>sepal_length</code>, and <code>sepal_width</code>, which are the variables we’ll use to predict the dependent variable.</p>
			<p>The code then creates an <code>f_x</code> formula that represents the linear regression model. It essentially says that we want to predict <code>petal_length</code> using the other variables listed, separated by a plus sign:</p>
			<pre class="source-code">
perform_linear_regression &lt;- function(data) {
  lm_model &lt;- lm(f_x, data = data)
  return(lm_model)
}</pre>			<p>In this part, we’re defining a custom R function called <code>perform_linear_regression</code>. This function takes one <code>data</code> argument, which is a data frame. Inside the function, we use the <code>lm</code> function to perform linear regression, using the <code>f_x</code> formula we defined earlier and the provided data frame. The resulting linear model is stored in <code>lm_model</code>, and we return it as the output of the function:</p>
			<pre class="source-code">
results &lt;- lapply(iris_split, perform_linear_regression)</pre>			<p>Here, we’re applying the <code>perform_linear_regression</code> function<a id="_idIndexMarker675"/> to each subset<a id="_idIndexMarker676"/> of the <code>iris</code> dataset using the <code>lapply</code> function. This means that we’re running linear regression separately for each species of iris, and the results are stored in the <code>results</code> list:</p>
			<pre class="source-code">
lapply(results, summary)</pre>			<p>This code uses <code>lapply</code> again, but this time we’re applying the <code>summary</code> function to each linear regression model in the <code>results</code> list. The <code>summary</code> function provides statistical information about the linear regression model, such as coefficients and R-squared values:</p>
			<pre class="source-code">
par(mfrow = c(2,2))
lapply(results, plot)
par(mfrow = c(1, 1))</pre>			<p>These lines of code are used to create a set of four plots to visualize the model performance. We first set the layout of the plots to be a 2x2 grid using <code>par(mfrow = c(2,2))</code>, so that 4 plots will be displayed in a 2x2 grid. Then, we use <code>lapply</code> to plot each linear regression model in the <code>results</code> list. Finally, we reset the plot layout to the default with <code>par(mfrow = </code><code>c(1, 1))</code>:</p>
			<pre class="source-code">
lm_models &lt;- lapply(
iris_split,
function(df) lm(f_x, data = df)
)</pre>			<p>This part accomplishes the same<a id="_idIndexMarker677"/> linear regression analysis as before but combines<a id="_idIndexMarker678"/> the linear model creation and summarization into a more concise form using anonymous functions. It first applies the <code>lm</code> function to each species subset within <code>iris_split</code>, creating a list of linear models stored in <code>lm_models</code>. Then, it uses <code>lapply</code> to obtain summaries for each of these linear models.</p>
			<p>In summary, this R code reads iris data from an Excel file, performs linear regression for each species of <code>iris</code>, summarizes the results, and creates visualizations to assess the model’s performance. It provides a detailed analysis of how the dependent variable (<code>petal_length</code>) is influenced by independent variables (<code>petal_width</code>, <code>sepal_length</code>, and <code>sepal_width</code>) for each species of <code>iris</code>.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor186"/>Linear regression with tidymodels and purrr</h2>
			<p>Now that we have gone over <a id="_idIndexMarker679"/>how to perform<a id="_idIndexMarker680"/> a simple linear regression<a id="_idIndexMarker681"/> in R on the <code>iris</code><a id="_idIndexMarker682"/> dataset, we will do the same with the <code>tidymodels</code> framework. Let’s dive right into it:</p>
			<pre class="source-code">
f_x &lt;- formula(paste("petal_width", "~", "petal_length + sepal_width + sepal_length"))</pre>			<p>This block defines a formula for the linear regression model. The <code>formula()</code> function takes two arguments: the response variable and the predictor variables. The response variable is the variable that we want to predict, and the predictor variables are the variables that we think can help us predict the response variable. In this case, the response variable is <code>petal_width</code> and the predictor variables are <code>petal_length</code>, <code>sepal_width</code>, and <code>sepal_length</code>:</p>
			<pre class="source-code">
library(dplyr)
library(tidyr)
library(purrr)
library(tidymodels)
nested_lm &lt;- df |&gt;
 nest(data = -species) |&gt;
 mutate(split = map(data, ~ initial_split(., prop = 8/10)),
        train = map(split, ~ training(.)),
        test = map(split, ~ testing(.)),
        fit  = map(train, ~ lm(f_x, data = .)),
        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))</pre>			<p>This block creates a nested linear regression model using the <code>nest()</code> function from the <code>tidyr</code> package. The <code>nest()</code> function groups the data by a specified variable, in this case, the <code>species</code> variable.</p>
			<p>For each group, the <code>nest()</code> function creates a list containing the data for that group. The <code>mutate()</code> function is then used to add new columns to the nested data frame.</p>
			<p>The <code>split()</code> function is used to randomly split the data in each group into a training set and a test set. The <code>training()</code> and <code>testing()</code> functions are then used to select the training and test sets, respectively. With <code>map()</code> and <code>map2()</code>, we can iterate over a vector or list or two vectors or lists and apply a function to them.</p>
			<p>The <code>lm()</code> function is used to fit<a id="_idIndexMarker683"/> a linear regression model<a id="_idIndexMarker684"/> to the training <a id="_idIndexMarker685"/>data in each group. The <code>predict()</code> function<a id="_idIndexMarker686"/> is then used to predict the response variable for the test data in each group using the fitted linear regression model:</p>
			<pre class="source-code">
nested_lm |&gt;
 select(species, pred) |&gt;
 unnest(pred)</pre>			<p>This block selects the <code>species</code> and <code>pred</code> columns from the nested data frame and unnests the <code>pred</code> column. The <code>unnest()</code> function converts the nested data frame to a regular data frame, with one row for each observation.</p>
			<p>The resulting data frame is a nested linear regression model, with one fitted linear regression model for each species.</p>
			<p>Let’s take a look at an example. We are<a id="_idIndexMarker687"/> going to use the <code>f_x</code> formula that was created<a id="_idIndexMarker688"/> earlier along with the <code>df</code> <code>tibble</code> variable we created<a id="_idIndexMarker689"/> at the beginning. The following code<a id="_idIndexMarker690"/> shows an example of how to use the nested linear regression model to predict the petal width for a new iris flower:</p>
			<pre class="source-code">
library(dplyr)
library(tidyr)
library(purrr)
library(tidymodels)
# Create a nested linear regression model
nested_lm &lt;- df |&gt;
 nest(data = -species) |&gt;
 mutate(split = map(data, ~ initial_split(., prop = 8/10)),
        train = map(split, ~ training(.)),
        test = map(split, ~ testing(.)),
        fit  = map(train, ~ lm(f_x, data = .)),
        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))
# Predict the petal width for a new iris flower
new_iris &lt;- data.frame(sepal_length = 5.2, sepal_width = 2.7, 
    petal_length = 3.5)
# Predict the petal width
predicted_petal_width &lt;- predict(nested_lm[[1]]$fit, 
    newdata = new_iris))
# Print the predicted petal width
print(predicted_petal_width)</pre>			<p>Here’s the output:</p>
			<pre class="console">
1.45</pre>			<p>The predicted petal width <a id="_idIndexMarker691"/>is 1.45 cm. We have now finished<a id="_idIndexMarker692"/> going over linear<a id="_idIndexMarker693"/> regression in R<a id="_idIndexMarker694"/> with a basic example. We will now continue the chapter in the next section on performing logistic regression in R.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor187"/>Performing logistic regression in R</h1>
			<p>As we did in the section<a id="_idIndexMarker695"/> on linear regression, in this section, we<a id="_idIndexMarker696"/> will also perform logistic regression in base R and with the <code>tidymodels</code> framework. We are going to only perform a simple binary classification regression problem using the <code>Titanic</code> dataset, where we will be deciding if someone is going to survive or not. Let’s dive right into it.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor188"/>Logistic regression with base R</h2>
			<p>In order to get<a id="_idIndexMarker697"/> going, we are going<a id="_idIndexMarker698"/> to start with a base R implementation of logistic regression on the <code>Titanic</code> dataset where we will be modeling the response of <code>Survived</code>. So, let’s get straight into it.</p>
			<p>The following is the code that will perform the data modeling along with explanations of what is happening:</p>
			<pre class="source-code">
library(tidyverse)
df &lt;- Titanic |&gt;
       as.data.frame() |&gt;
       uncount(Freq)</pre>			<p>This block of code starts by loading<a id="_idIndexMarker699"/> a library called <code>tidyverse</code>, which contains various<a id="_idIndexMarker700"/> data manipulation and visualization tools. It then creates a data frame called <code>df</code> by taking the <code>Titanic</code> dataset (assuming it’s available in your environment) and performing three operations on it using the <code>|&gt;</code> operator, where we then use  <code>as.data.frame()</code>, which converts the dataset into a data frame, followed by <code>uncount(Freq)</code>, which repeats each row in the dataset according to the value in the <code>Freq</code> column. This is often done to expand summarized data:</p>
			<pre class="source-code">
set.seed(123)
train_index &lt;- sample(nrow(df), floor(nrow(df) * 0.8), replace = FALSE)
train &lt;- df[train_index, ]
test &lt;- df[-train_index, ]</pre>			<p>This section is about splitting the data into a training set and a test set, which is a common practice in machine learning:</p>
			<ul>
				<li><code>set.seed(123)</code>: This sets a random seed for reproducibility, ensuring that random operations produce the same results each time.</li>
				<li><code>sample(nrow(df), floor(nrow(df) * 0.8), replace = FALSE)</code>: This randomly selects 80% of the rows in the <code>df</code> data frame (the training set) without replacement and stores their indices in <code>train_index</code>.</li>
				<li><code>train &lt;- df[train_index, ]</code>: This creates the training set by selecting the rows from <code>df</code> using the <code>train_index</code> indices.</li>
				<li><code>test &lt;- df[-train_index, ]</code>: This creates the test set by selecting the rows from <code>df</code> that are not in the training set. We next create the model.<pre class="source-code">
model &lt;- glm(Survived ~ Sex + Age + Class, data = train, family = "binomial")</pre></li>			</ul>
			<p>Now let’s discuss the model code as follows:</p>
			<ul>
				<li>This block trains a logistic regression model using the <code>glm</code> function.</li>
				<li>The model is trained to predict the <code>Survived</code> variable based on the <code>Sex</code>, <code>Age</code>, and <code>Class</code> variables in the training data. Here, <code>Age</code> is actually discrete.</li>
				<li>The <code>family = "binomial"</code> argument specifies that this is a binary classification problem, where the outcome is either <code>Yes</code> or <code>No</code>. The following link helps in choosing an appropriate family: <a href="https://stats.stackexchange.com/a/303592/35448">https://stats.stackexchange.com/a/303592/35448</a>.</li>
			</ul>
			<p>Now, let’s set up the model predictions and response variable:</p>
			<pre class="source-code">
predictions &lt;- predict(model, newdata = test, type = "response")
pred_resp &lt;- ifelse(predictions &lt;= 0.5, "No", "Yes")</pre>			<p>Now, let’s go over what we just did:</p>
			<ul>
				<li>Here, we use<a id="_idIndexMarker701"/> the trained model to make predictions<a id="_idIndexMarker702"/> on the test set.</li>
				<li><code>predict(model, newdata = test, type = "response")</code> calculates the predicted probabilities of survival for each passenger in the test set.</li>
				<li><code>ifelse(predictions &lt;= 0.5, "No", "Yes")</code> converts these probabilities into binary predictions: <code>"No"</code> if the probability is less than or equal to <code>0.5</code>, and <code>"Yes"</code> otherwise. This is common practice, but you must know your project first in order to determine if this is correct or not. Now, onto the <code>accuracy</code> variable:<pre class="source-code">
accuracy &lt;- mean(pred_resp == test$Survived)</pre></li>			</ul>
			<p>We created the <code>accuracy</code> variable by doing the following:</p>
			<ul>
				<li>This line calculates the accuracy of the model’s predictions by comparing <code>pred_resp</code> (the model’s predictions) to the actual survival status in the test set (<code>test$Survived</code>).</li>
				<li>It computes the mean of the resulting logical values, where <code>TRUE</code> represents a correct prediction, and <code>FALSE</code> represents an incorrect prediction. Let’s now go over the rest of the code:<pre class="source-code">
print(accuracy)
table(pred_resp, test$Survived)</pre></li>			</ul>
			<p>The code prints two things:</p>
			<ul>
				<li>The accuracy of the model on the test set.</li>
				<li>A confusion matrix that shows how many predictions were correct and how many were incorrect. If you would like to understand confusion matrices more, here is a good link: <a href="https://www.v7labs.com/blog/confusion-matrix-guide">https://www.v7labs.com/blog/confusion-matrix-guide</a>.</li>
			</ul>
			<p>In summary, this code loads a dataset, splits<a id="_idIndexMarker703"/> it into a training and test set, trains a logistic regression model<a id="_idIndexMarker704"/> to predict survival, evaluates the model’s accuracy, and displays the results. It’s a basic example of a binary classification machine learning workflow. Now that we have covered performing logistic regression for a classification problem in base R, we will try our hand at the same but this time using the <code>tidymodels</code> framework.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor189"/>Performing logistic regression using tidymodels</h2>
			<p>In this section, we will use<a id="_idIndexMarker705"/> the <code>tidymodels</code> framework to perform<a id="_idIndexMarker706"/> the logistic regression on the <code>Titanic</code> dataset. Since we have done this in base R already, let’s get right into it:</p>
			<pre class="source-code">
library(tidymodels)
library(healthyR.ai)</pre>			<p>This code loads the two libraries that we will need for our analysis: <code>tidymodels</code> and <code>healthyR.ai</code>. <code>tidymodels</code> is a library that provides a common interface for many machine learning algorithms, while <code>healthyR.ai</code> provides a set of tools for evaluating the performance of machine learning models:</p>
			<pre class="source-code">
df &lt;- Titanic |&gt;
    as_tibble() |&gt;
    uncount(n) |&gt;
    mutate(across(where(is.character), as.factor))</pre>			<p>This code converts the <code>Titanic</code> dataset to a <code>tibble</code>, which is a data structure that is compatible with <code>tidymodels</code>. It also uncounts the <code>n</code> column, which is a column that contains the number of times each row appears in the dataset and is created by the <code>uncount()</code> function. Finally, it converts all the character variables in the dataset to factors:</p>
			<pre class="source-code">
# Set seed for reproducibility
set.seed(123)
# Split the data into training and test sets
split &lt;- initial_split(df, prop = 0.8)
train &lt;- training(split)
test &lt;- testing(split)</pre>			<p>This code splits the <code>df</code> dataset<a id="_idIndexMarker707"/> into training and test sets. The training<a id="_idIndexMarker708"/> set is used to train the model, while the test set is used to evaluate the performance of the model on unseen data. The <code>initial_split()</code> function from <code>tidymodels</code> is used to perform the split. The <code>prop</code> argument specifies the proportion of the data that should be used for training. In this case, we are using 80% of the data for training and 20% of the data for testing:</p>
			<pre class="source-code">
# Create a recipe for pre-processing
recipe &lt;- recipe(Survived ~ Sex + Age + Class, data = train)
# Specify logistic regression as the model
log_reg &lt;- logistic_reg() |&gt; set_engine("glm", family = "binomial")
# Combine the recipe and model into a workflow
workflow &lt;- workflow() %&gt;% add_recipe(recipe) %&gt;% add_model(log_reg)
# Train the logistic regression model
fit &lt;- fit(workflow, data = train)</pre>			<p>This code trains a logistic regression model to predict survival on the Titanic. The <code>recipe()</code> function from <code>tidymodels</code> is used to pre-process the data. The <code>logistic_reg()</code> function from <code>tidymodels</code> is used to specify the logistic regression model. The <code>workflow()</code> function from <code>tidymodels</code> is used to combine the recipe and model into a workflow. Finally, the <code>fit()</code> function from <code>tidymodels</code> is used to train the model on the training data:</p>
			<pre class="source-code">
# Predict on the test set
predictions &lt;- predict(fit, new_data = test) |&gt; bind_cols(test) |&gt; select(Class:Survived, .pred_class)
# Better method
pred_fit_tbl &lt;- fit |&gt; augment(new_data = test)</pre>			<p>This code predicts the survival<a id="_idIndexMarker709"/> probability for each passenger<a id="_idIndexMarker710"/> in the test set. The <code>predict()</code> function from <code>tidymodels</code> is used to make the predictions. The <code>new_data</code> argument specifies the data that we want to make predictions on. In this case, we are making predictions on the test set. The <code>bind_cols()</code> function is used to bind the predictions to the test set data. The <code>select()</code> function is used to select the columns that we want to keep. The <code>pred_fit_tbl</code> object is a <code>tibble</code> instance that contains the predictions from the model, as well as the ground truth survival labels. This object will be used to evaluate the performance of the model:</p>
			<pre class="source-code">
# Accuracy metrics for the model to be scored against from the healthyR.ai package
perf &lt;- hai_default_classification_metric_set()
# Calculate the accuracy metrics
perf(pred_fit_tbl, truth = Survived, estimate = .pred_class)
# Print the confusion matrix
predictions |&gt; conf_mat(truth = Survived, estimate = .pred_class)</pre>			<p>The accuracy check code block evaluates the performance of the model on the test set. It does this by using the <code>hai_default_classification_metric_set()</code> function from the healthyR.ai package to create a set of default classification metrics. These metrics include accuracy, precision, recall, and F1 score.</p>
			<p>The <code>perf()</code> function is then used to calculate the accuracy metrics on the test set. The <code>pred_fit_tbl</code> object is the data frame that contains the predictions from the model, as well as the ground truth survival labels. The <code>truth</code> and <code>estimate</code> arguments specify the columns in the data frame that contain the ground truth and predicted labels, respectively.</p>
			<p>The <code>conf_mat()</code> function is then used to print the confusion matrix for the model. The confusion matrix is a table that shows how many observations were correctly and incorrectly predicted by the model.</p>
			<p>Finally, the <code>tidy()</code> and <code>glance()</code> functions from the <code>broom</code> package can be used to tidy and summarize the fitted<a id="_idIndexMarker711"/> model. The <code>tidy()</code> function converts the model object to a <code>tibble</code> instance, which<a id="_idIndexMarker712"/> is a data structure that is easy to work with. The <code>glance()</code> function prints a summary of the model, including the coefficients, standard errors, and p-values for all of the variables in the model.</p>
			<p>Here is a simple explanation of each of the accuracy metrics that are calculated in the accuracy check code block:</p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: The accuracy of a model is the proportion<a id="_idIndexMarker713"/> of observations that are correctly predicted by the model.</li>
				<li><strong class="bold">Precision</strong>: The precision of a model is the<a id="_idIndexMarker714"/> proportion of positive predictions that are correct.</li>
				<li><strong class="bold">Recall</strong>: The recall of a model <a id="_idIndexMarker715"/>is the proportion of actual positive observations that are correctly predicted by the model.</li>
				<li><strong class="bold">F1 score</strong>: The F1 score is a harmonic<a id="_idIndexMarker716"/> mean of the precision and recall metrics. It is a good overall measure of the performance of a model.</li>
			</ul>
			<p>The confusion matrix is a helpful tool for understanding how the model is performing. The ideal confusion matrix would have all of the observations on the diagonal, indicating that all of the observations were correctly predicted. However, in practice, no model is perfect and there will be some observations that are incorrectly predicted.</p>
			<p>Lastly, we will <a id="_idIndexMarker717"/>visualize the model with a <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) curve. To read <a id="_idIndexMarker718"/>more about this type<a id="_idIndexMarker719"/> of curve, you can see the following link: <a href="https://www.tmwr.org/performance">https://www.tmwr.org/performance</a>. Here is the code<a id="_idIndexMarker720"/> that creates the ROC curve:</p>
			<pre class="source-code">
roc_curve(
  pred_fit_tbl, truth = Survived, .pred_Yes,
  event_level = "second"
) |&gt;
  autoplot()</pre>			<p>Here is the output:</p>
			<div><div><img src="img/B19142_09_1.jpg" alt="Figure 9.1 – ROC curve for the logistic regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – ROC curve for the logistic regression model</p>
			<p>Now, we have learned how to perform both linear and logistic regression in both base R and via the <code>tidymodels</code> modeling framework. We did this with the <code>Titanic</code> and <code>iris</code> datasets. Now, it’s time to do the same in Python!</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor190"/>Performing linear regression in Python using Excel data</h1>
			<p>Linear regression in Python<a id="_idIndexMarker721"/> can be carried out with the help<a id="_idIndexMarker722"/> of libraries<a id="_idIndexMarker723"/> such as <code>pandas</code>, <code>scikit-learn</code>, <code>statsmodels</code>, and <code>matplotlib</code>. The following is a step-by-step code example:</p>
			<ol>
				<li>First, import the necessary libraries:<pre class="source-code">
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from statsmodels.graphics.regressionplots import plot_regress_exog
from statsmodels.graphics.gofplots import qqplot</pre></li>				<li>Then, we create an Excel file with test data. Of course, in a real-life scenario, you would not need the mock data – you would skip this step and load the data from Excel (see the next step) after loading the necessary libraries:<pre class="source-code">
# Step 0: Generate sample data and save as Excel file
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 2)  # Two features
y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(n_samples)
# Linear relationship with noise
# Create a pandas DataFrame
data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}
df = pd.DataFrame(data)
# Save the <a id="_idTextAnchor191"/>data to Excel
df.to_excel("linear_regression_input.xlsx")</pre></li>				<li>Next, import the data<a id="_idIndexMarker724"/> from the Excel file<a id="_idIndexMarker725"/> with test data<a id="_idIndexMarker726"/> and prepare it for analysis using tools you have learned in the previous chapter:<pre class="source-code">
# Step 1: Import Excel data into a pandas DataFrame
excel_file = "linear_regression_input.xlsx"
df = pd.read_excel(excel_file)
# Step 2: Explore the data
# Use the tools learned in the previous chapter on EDA
# Step 3: Data Preparation (if needed)
# Use the tools learned in the previous chapter on data cleaning</pre></li>				<li>Now, we are ready to carry out the actual analysis. Split the data into training and test data so we can evaluate<a id="_idIndexMarker727"/> the model on a dedicated data (sub)set, then fit the <strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) linear model on the training data:<pre class="source-code">
# Step 4: Split data into training and testing sets
X = df[['Feature1', 'Feature2']] # Independent variables
y = df['Target'] # Dependent variable
# Split the data into training and test set using a fixed random seed for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, 
    test_size=0.2, random_state=42)
# Step 5: Fit the Linear Regression model
# Add a constant (intercept) to the independent variables
X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)
# Fit the linear model
model = sm.OLS(y_train, X_train).fit()</pre><p class="list-inset">Note that doing imputation<a id="_idIndexMarker728"/> as part of the data<a id="_idIndexMarker729"/> cleaning process<a id="_idIndexMarker730"/> before splitting the test and training sets may lead to pollution of the test set from the training set. Be conscious of this when performing the data cleaning and preparation steps!</p></li>				<li>Next, evaluate the trained model on the test data:<pre class="source-code">
# Step 6: Model Evaluation
y_pred = model.predict(X_test)
# Print the model summary
print(model.summary())</pre><p class="list-inset">This will create summary statistics as the output that provides valuable insights into the relationships within your dataset:</p></li>			</ol>
			<div><div><img src="img/B19142_09_2.jpg" alt="Figure 9.2 – Summary statistics of the model fitted"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Summary statistics of the model fitted</p>
			<p>Actual interpretation<a id="_idIndexMarker731"/> of model results is a topic<a id="_idIndexMarker732"/> that is beyond the scope<a id="_idIndexMarker733"/> of this book, but here are some hints to get you started:</p>
			<ul>
				<li><strong class="bold">Coefficients</strong>: The coefficients associated<a id="_idIndexMarker734"/> with each independent variable (predictor) in the model tell you about the strength and direction of the relationship. A positive coefficient indicates a positive correlation, meaning that as the predictor increases, the target variable tends to increase as well. Conversely, a negative coefficient signifies a negative correlation.</li>
				<li><strong class="bold">Intercept</strong>: The intercept represents<a id="_idIndexMarker735"/> the predicted value of the target variable when all predictor variables are set to zero. It’s essential to consider the intercept’s value in the context of your analysis.</li>
				<li><strong class="bold">R-squared (</strong><strong class="bold">R</strong>2<strong class="bold">)</strong>: The R-squared value measures<a id="_idIndexMarker736"/> the goodness of fit of the model. It tells you the proportion of variance in the target variable that can be explained by the predictors. Higher R-squared values (closer to 1) indicate a better fit. Note that adding more variables will always increase this measure. A “better” fit might result in “overfitting,” which is something we don’t want. You may want to check model-fit selection criteria such as Mallow’s Cp, AIC, BIC, and adjusted R-squared, which penalizes the number of parameters used to fit the model.</li>
				<li><strong class="bold">P-values</strong>: P-values associated with coefficients<a id="_idIndexMarker737"/> help determine the statistical significance of each<a id="_idIndexMarker738"/> predictor. Lower p-values<a id="_idIndexMarker739"/> suggest greater <a id="_idIndexMarker740"/>significance (in the sense that it is stronger evidence to reject the null hypothesis). If a p-value is less than a chosen significance level (for example, 0.05), you can conclude that the predictor has a statistically significant effect on the target variable. Please be aware that there are good reasons to not rely on p-values alone; see the ongoing debate on p-hacking and related topics in statistical science.</li>
				<li><strong class="bold">Residuals</strong>: Examining the residuals (the differences between the observed and predicted values) is crucial<a id="_idIndexMarker741"/> for assessing model performance. Ideally, residuals should be random, with no apparent patterns. Patterns in residuals may indicate model misspecification.</li>
				<li><strong class="bold">Confidence intervals</strong>: Confidence intervals around coefficients<a id="_idIndexMarker742"/> provide a range within which the true population parameter is likely to lie. Wider intervals indicate greater uncertainty.</li>
				<li><strong class="bold">F-statistic</strong>: The F-statistic tests the overall significance <a id="_idIndexMarker743"/>of the model. A small F-statistic suggests that the model doesn’t explain much variance in the target variable, while a large value indicates a better overall fit.</li>
				<li><strong class="bold">Adjusted R-squared</strong>: Adjusted R-squared adjusts the R-squared<a id="_idIndexMarker744"/> value for the number of predictors in the model. It helps you determine whether adding more predictors improves the model’s fit.</li>
			</ul>
			<p>By carefully examining these <a id="_idIndexMarker745"/>elements, you can gain<a id="_idIndexMarker746"/> insights into how well the linear<a id="_idIndexMarker747"/> model fits your data, the significance of predictor variables, and the overall quality of the model. This information is invaluable for making informed decisions and drawing meaningful conclusions from your analysis.</p>
			<p>With the model trained and the fit evaluated, we can visualize the results to help with interpretation. The following code creates a scatterplot of predicted versus observed values:</p>
			<pre class="source-code">
plt.scatter(X_test['Feature1'], y_test, color='blue', label='Actual')
plt.scatter(X_test['Feature1'], y_pred, color='red', 
    label='Predicted')
plt.xlabel('Feature1')
plt.ylabel('Target')
plt.title('Linear Regression Prediction')
plt.legend()
plt.show()</pre>			<p>Here is the scatterplot for it:</p>
			<div><div><img src="img/B19142_09_3.jpg" alt="Figure 9.3 – Linear regression prediction plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Linear regression prediction plot</p>
			<p>In addition, we can create diagnostic<a id="_idIndexMarker748"/> plots and visualizations<a id="_idIndexMarker749"/> such as residual<a id="_idIndexMarker750"/> plots and Q-Q plots, which can help you identify potential issues with the model, such as heteroscedasticity or outliers:</p>
			<pre class="source-code">
# Set the backend to 'TkAgg' before generating the plots if needed – comment out this line if in WSL or other non-interactive environment
plt.switch_backend('TkAgg')
# Residuals
fig, ax = plt.subplots(figsize=(12, 8))
plot_regress_exog(model, "Feature1", fig=fig)
plt.show()
# Q-Q plot:
qqplot(model.resid, line="s")
plt.show()</pre>			<p>The two <a id="_idIndexMarker751"/>preceding<a id="_idIndexMarker752"/> plots look<a id="_idIndexMarker753"/> like this:</p>
			<div><div><img src="img/B19142_09_4.jpg" alt="Figure 9.4 – Residuals plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Residuals plot</p>
			<div><div><img src="img/B19142_09_5.jpg" alt="Figure 9.5 – Residuals Q-Q plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Residuals Q-Q plot</p>
			<p>Finally, we can export the results to Excel. This will be covered in detail in the next subsection.</p>
			<p>As a side note, <code>scikit-learn</code> also has an in-built linear model but that does not come with the handy summary statistics we have used in the preceding code.</p>
			<p>This code demonstrated a basic<a id="_idIndexMarker754"/> linear regression<a id="_idIndexMarker755"/> workflow using Python<a id="_idIndexMarker756"/> and Excel data. Let’s move on to logistic regression!</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor192"/>Logistic regression in Python using Excel data</h1>
			<p>In the following code, we generate<a id="_idIndexMarker757"/> random sample<a id="_idIndexMarker758"/> data with two<a id="_idIndexMarker759"/> features (<code>Feature1</code> and <code>Feature2</code>) and a binary target variable (<code>Target</code>) based on a simple condition. We perform logistic regression, evaluate the model using accuracy, the confusion matrix, and a classification<a id="_idIndexMarker760"/> report, visualize the results for binary<a id="_idIndexMarker761"/> classification, and interpret<a id="_idIndexMarker762"/> the coefficients.</p>
			<p>The following is a step-by-step code example:</p>
			<ol>
				<li>Again, we start with importing the necessary libraries:<pre class="source-code">
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report</pre><p class="list-inset">For this example, we will use a different sample dataset:</p><pre class="source-code"># Step 0: Generate sample data
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 2)  # Two features
y = (X[:, 0] + X[:, 1] &gt; 1).astype(int)  # Binary classification based on a condition
# Create a pandas DataFrame
data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}
df = pd.DataFrame(data)
df.to_excel("logistic_regression_input.xlsx")</pre></li>				<li>With your data available in Excel, we can read it and prepare it for the modeling step:<pre class="source-code">
# Step 1: Import Excel data into a pandas DataFrame
excel_file = "logistic_regression_input.xlsx"
df = pd.read_excel(excel_file)
# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2, random_state=42)</pre></li>				<li>Now, we can create<a id="_idIndexMarker763"/> and fit a<a id="_idIndexMarker764"/> model. We will use the <code>scikit-learn</code> library<a id="_idIndexMarker765"/> this time:<pre class="source-code">
# Step 3: Create and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)</pre></li>				<li>With a model fit, we can now visualize the results:<pre class="source-code">
# Step 4: Visualization
# Visualization for binary classification
plt.scatter(X_test[y_test == 1][:, 0], 
    X_test[y_test == 1][:, 1], color='blue', 
    label='Class 1 (Actual)')
plt.scatter(X_test[y_test == 0][:, 0], 
    X_test[y_test == 0][:, 1], color='red', 
    label='Class 0 (Actual)')
plt.xlabel('Feature1')
plt.ylabel('Feature2')
plt.title('Logistic Regression Prediction')
plt.legend()
plt.show()</pre></li>				<li>Unlike with linear<a id="_idIndexMarker766"/> regression, we need<a id="_idIndexMarker767"/> different goodness-of-fit <a id="_idIndexMarker768"/>metrics because we are using logistic regression for a binary classification:<pre class="source-code">
# Step 5: Model Evaluation and Interpretation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)</pre><p class="list-inset">The result looks like this:</p></li>			</ol>
			<div><div><img src="img/B19142_09_6.jpg" alt="Figure 9.6 – Logistic regression prediction plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Logistic regression prediction plot</p>
			<p class="list-inset">This is <a id="_idIndexMarker769"/>the<a id="_idIndexMarker770"/> code<a id="_idIndexMarker771"/> result:</p>
			<div><div><img src="img/B19142_09_7.jpg" alt="Figure 9.7 – Model summary statistics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Model summary statistics</p>
			<p class="list-inset">To interpret the preceding results, you can start with the following:</p>
			<ul>
				<li><code>Accuracy</code> is a fundamental<a id="_idIndexMarker772"/> metric, representing <a id="_idIndexMarker773"/>the ratio of correctly<a id="_idIndexMarker774"/> predicted instances to the total number of instances. While easy to understand, accuracy can be misleading if there’s an imbalance between the classes.</li>
				<li><code>Confusion Matrix</code> offers a more detailed view. It breaks down predictions into four categories: true positives, true negatives, false positives, and false negatives. This matrix provides a clear understanding of how well the model performs in terms of correctly classifying positive and negative instances.</li>
				<li><code>Classification Report</code> provides a comprehensive summary. It includes metrics such as <code>precision</code>, <code>recall</code>, <code>f1-score</code>, and <code>support</code> for both classes. <code>Precision</code> measures how many predicted positives were actually positive, while <code>recall</code> quantifies how many actual positives were correctly predicted. The <code>F1-score</code> balances <code>precision</code> and <code>recall</code>. <code>Support</code> denotes the number of instances for each class. Together, these metrics offer a more nuanced evaluation of the model’s performance in binary classification tasks.</li>
			</ul>
			<p>You can use this sample data and code for testing and experimenting with logistic regression.</p>
			<p>Please note that contrary to the popular (but incorrect) assertion, logistic regression can be used as a regression as well – what makes it a classifier is an arbitrary cut-off point for the predicted probability. For some use cases, you might want to use the raw regression output (if, for example, you are interested in the predicted probability of the data point belonging to a class, and not the more likely class only) and for others, you might want to play with the cut-off point (if, for example, there is pre-existing domain information that implies that 50% is not the right cut-off point).</p>
			<p>That’s it! Logistic regression<a id="_idIndexMarker775"/> is a relatively simple<a id="_idIndexMarker776"/> model with lots<a id="_idIndexMarker777"/> of benefits. It’s performant, easy to fit, easy to interpret, and very versatile. It’s most often used for classification with a domain-knowledge-driven cut-off point, but under the hood, it remains a regression method that can be used for predicting class probabilities.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor193"/>Summary</h1>
			<p>In this chapter, we explored the powerful world of linear and logistic regression using Excel data. Linear regression, a fundamental statistical technique, allows us to model relationships between dependent and independent variables. We discussed its assumptions and applications, and walked through the entire process of loading data from Excel, preparing it for analysis, and fitting linear regression models using both R (using base R and <code>tidymodels</code>) and Python (with the <code>scikit-learn</code> and <code>statsmodels</code> libraries).</p>
			<p>Through comprehensive code examples, you learned how to perform regression analysis, assess model accuracy, and generate valuable statistics and metrics to interpret model results. We gained insights into creating diagnostic plots, such as residual plots and Q-Q plots, which aid in identifying issues such as heteroscedasticity and outliers.</p>
			<p>Additionally, we delved into logistic regression, a powerful tool for class probability prediction and binary classification tasks. We established its importance and applications and outlined the process of data preparation, model fitting, and metrics evaluation. With practical code examples, we observed how logistic regression models can be constructed using <code>tidymodels</code> in R and the <code>scikit-learn</code> library in Python.</p>
			<p>By the end of this chapter, you should have a strong grasp of linear and logistic regression, from theory to practical application, and the ability to harness these techniques to analyze your data efficiently.</p>
			<p>With these skills, you are well-equipped to conduct regression analyses and extract valuable insights from your data in Python.</p>
			<p>On to the next chapter, where you will learn about time series analysis and its applications to Excel data.</p>
		</div>
	</body></html>