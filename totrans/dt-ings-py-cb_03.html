<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-108"><a id="_idTextAnchor108"/>3</h1>
<h1 id="_idParaDest-109"><a id="_idTextAnchor109"/>Data Discovery – Understanding Our Data before Ingesting It</h1>
<p>As you may already have noticed, <strong class="bold">data ingestion</strong> is not just retrieving data from a source and inserting it in another place. It involves understanding some business concepts, secure access to the data, and how to store it, and now it is essential to discover our data.</p>
<p><strong class="bold">Data discovery</strong> is the process of understanding our data’s patterns and behaviors, ensuring the whole <a id="_idIndexMarker239"/>data pipeline will be successful. In this process, we will understand how our data is modeled and used, so we can set up and plan our ingestion using the best fit.</p>
<p>In this chapter, you will learn about the following:</p>
<ul>
<li>Documenting the data discovery process</li>
<li>Configuring OpenMetadata</li>
<li>Connecting OpenMetadata to our database</li>
</ul>
<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/>Technical requirements</h1>
<p>You can also find the code from this chapter in its GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Documenting the data discovery process</h1>
<p>In recent years, manual <a id="_idIndexMarker240"/>data discovery has been rapidly deprecated, giving rise to <strong class="bold">machine learning</strong> and other automated solutions, bringing fast insights into data in storage or online <a id="_idIndexMarker241"/>spreadsheets, such as Google Sheets.</p>
<p>Nevertheless, many small companies are just starting out their businesses or data areas, so implementing a paid or cost-related solution might not be a good idea right away. As data professionals, we also need to be malleable when applying the first solution to a problem – there will always be space to improve it later.</p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Getting ready</h2>
<p>This recipe will cover the steps to start the data discovery process effectively. Even though, here, the process is more related to the manual discovery steps, you will see it also applies to the automated ones.</p>
<p>Let’s start by downloading the datasets.</p>
<p>For this recipe, we are going to use the <em class="italic">The evolution of genes in viruses and bacteria</em> dataset (<a href="https://www.kaggle.com/datasets/thedevastator/the-evolution-of-genes-in-viruses-and-bacteria">https://www.kaggle.com/datasets/thedevastator/the-evolution-of-genes-in-viruses-and-bacteria</a>), and another one containing <em class="italic">hospital administration</em> information (<a href="https://www.kaggle.com/datasets/girishvutukuri/hospital-administration">https://www.kaggle.com/datasets/girishvutukuri/hospital-administration</a>).</p>
<p class="callout-heading">Note</p>
<p class="callout">This recipe does not require the use of the exact datasets mentioned – it covers generically how to apply the methodology to datasets or any data sources. Feel free to use any data you want.</p>
<p>The next stage is <a id="_idIndexMarker242"/>creating the documentation. You can use any software or online application that suits you – the important thing is to have a place to detail and catalog the information.</p>
<p>I will use <strong class="bold">Notion</strong> (<a href="https://www.notion.so/">https://www.notion.so/</a>). Its home page is shown in <em class="italic">Figure 3</em><em class="italic">.1</em>. It offers a free plan <a id="_idIndexMarker243"/>and allows you to create separate places for different types of documentation. However, some companies use <strong class="bold">Confluence by Atlassian</strong> to document their data. It will always depend on the scenario you are in.</p>
<div><div><img alt="Figure 3.1 – Notion home page" height="669" src="img/Figure_3.01_B19453.jpg" width="1460"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Notion home page</p>
<p>This is an optional stage where we are creating a Notion account. On the main page, click on <strong class="bold">Get </strong><strong class="bold">Notion free</strong>.</p>
<p>Another page <a id="_idIndexMarker244"/>will appear and you can use your Google or Apple email to create an account, as follows:</p>
<div><div><img alt="Figure 3.2 – Notion Sign up page" height="869" src="img/Figure_3.02_B19453.jpg" width="860"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Notion Sign up page</p>
<p>After that, you should see a blank page with a welcome message from Notion. If any other action is required, just follow the page instructions.</p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>How to do it…</h2>
<p>Let’s imagine a scenario where we work at a hospital and need to apply the data discovery process. Here is how we go about it:</p>
<ol>
<li><strong class="bold">Identifying our data sources</strong>: Two main departments need their data to be ingested—the administration and research departments. We know they usually keep their CSV files in a local data center so we can access them via the intranet. Don’t mind the <a id="_idIndexMarker245"/>filenames; generally, in a real application, they are not supported.</li>
</ol>
<p>The following are the research department’s files:</p>
<div><div><img alt="Figure 3.3 – Research files on the evolution of genes in E. coli" height="276" src="img/Figure_3.03_B19453.jpg" width="669"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Research files on the evolution of genes in E. coli</p>
<p>The following are the administration department’s files:</p>
<div><div><img alt="Figure 3.4 – Hospital administration files" height="117" src="img/Figure_3.04_B19453.jpg" width="772"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Hospital administration files</p>
<ol>
<li value="2"><strong class="bold">Categorizing data per department or project</strong>: Here, we create folders and subfolders related to the department and the type of data (on patients or specific diseases).</li>
</ol>
<div><div><img alt="Figure 3.5 – Research Department page" height="526" src="img/Figure_3.05_B19453.jpg" width="1020"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Research Department page</p>
<ol>
<li value="3"><strong class="bold">Identifying the datasets or databases</strong>: When looking at the files, we can find four patterns. There <a id="_idIndexMarker246"/>are the exclusive datasets: <strong class="bold">E.Coli Genomes</strong>, <strong class="bold">Protein Annotations</strong>, <strong class="bold">Escherichia Virus</strong> in general, and <strong class="bold">Patients</strong>.</li>
</ol>
<div><div><img alt="Figure 3.6 – Subsections created by research type and hospital administration topic" height="649" src="img/Figure_3.06_B19453.jpg" width="1053"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Subsections created by research type and hospital administration topic</p>
<ol>
<li value="4"><strong class="bold">Describing our data</strong>: Now, at the dataset level, we need to have helpful information about it, such as the overall description of that dataset table, when it is updated, where other teams can find it, a description of each column of the table, and, last but not least, all metadata.</li>
</ol>
<div><div><img alt="Figure 3.7 – Patient data documentation using Notion" height="1891" src="img/Figure_3.07_B19453.jpg" width="1459"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Patient data documentation using Notion</p>
<p class="callout-heading">Note</p>
<p class="callout">The description of where the file is stored may not be applied in all cases. You can find the reference of the database name instead, such as <code>'admin_database.patients'</code>.</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>How it works…</h2>
<p>When starting data discovery, the <a id="_idIndexMarker247"/>first objective is identifying patterns and categorizing them to create a logical flow. Usually, the first categorizations are by department or project, followed by database and dataset identification, and finally, describing the data inside.</p>
<p>There are some ways to document data discovery manually. People more used to the old-fashioned style of <strong class="bold">BI</strong> (short for <strong class="bold">Business Intelligence</strong>) tend to create more beautiful visualization <a id="_idIndexMarker248"/>models to apply discovery. However, this recipe’s objective is to create a catalog using a simple tool such as Notion:</p>
<ol>
<li><strong class="bold">Categorizing data as per department or project</strong>: The first thing we did was to identify the department responsible for each piece of data. Who is the contact in the case of an ingestion problem or if the dataset is broken? In formal terms, they are also known as data stewards. In some companies, categorization by project can also be applied since some companies can have their particular necessities and data.</li>
<li><strong class="bold">Identifying the datasets or databases</strong>: Here, we have only used datasets. Under the projects and/or departments, we insert the name of each table and other helpful information. If the tables are periodically updated, it is a good practice to also document that.</li>
<li><strong class="bold">Describing our data</strong>: Finally, we document the expected columns with their data types in detail. It helps data engineers plan their scripts when ingesting raw data; if something goes wrong after the automation, they can easily detect the issue.</li>
</ol>
<p>You might notice that <a id="_idIndexMarker249"/>some data behaves strangely. For instance, the <strong class="bold">medical_speciality</strong> column in <em class="italic">Figure 3</em><em class="italic">.7</em> has values described and a number to reference something else. In a real-world project, it would be necessary to create auxiliary data inside our ingestion to make a pattern and later facilitate the report or dashboards.</p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>Configuring OpenMetadata</h1>
<p><strong class="bold">OpenMetadata</strong> is an open <a id="_idIndexMarker250"/>source tool used for metadata management, allowing the process of <strong class="bold">data discovery</strong> and <strong class="bold">governance</strong>. You can find <a id="_idIndexMarker251"/>more about it here: <a href="https://open-metadata.org/">https://open-metadata.org/</a>.</p>
<p>By performing a few steps, it is possible to create a local or production instance using <strong class="bold">Docker</strong> or <strong class="bold">Kubernetes</strong>. OpenMetadata can connect to multiple resources, such as <strong class="bold">MySQL</strong>, <strong class="bold">Redis</strong>, <strong class="bold">Redshift</strong>, <strong class="bold">BigQuery</strong>, and others, to bring the information needed to build a data catalog.</p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Getting ready</h2>
<p>Before starting our configuration, we must install <strong class="bold">OpenMetadata</strong> and ensure the Docker <a id="_idIndexMarker252"/>containers are running correctly. Let us see how it is done:</p>
<p class="callout-heading">Note</p>
<p class="callout">At the time this book was written, the application was in the 0.12 version and with some documentation and installation improvements. This means the best approach to installing it may change over time. Please refer to the official documentation for it here: <a href="https://docs.open-metadata.org/quick-start/local-deployment">https://docs.open-metadata.org/quick-start/local-deployment</a>.</p>
<ol>
<li>Let’s create a folder and <code>virtualenv</code> (optional):<pre class="source-code">
<strong class="bold">$ mkdir openmetadata-docker</strong>
<strong class="bold">$ cd openmetadata-docker</strong></pre></li>
</ol>
<p>Since we are using a Docker environment to deploy the application locally, you can create it with <code>virtualenv</code> or not:</p>
<pre class="source-code">
<strong class="bold">$ python3 -m venv openmetadata</strong>
<strong class="bold">$ source openmetadata /bin/activate</strong></pre>
<ol>
<li value="2">Next, we install OpenMetadata as follows:<pre class="source-code">
<strong class="bold">$ pip3 install --upgrade "openmetadata-ingestion[docker]"</strong></pre></li>
<li>Then we check the installation, as follows:<pre class="source-code">
<strong class="bold">$ metadata</strong>
<strong class="bold">Usage: metadata [OPTIONS] COMMAND [ARGS]...</strong>
<strong class="bold">  Method to set logger information</strong>
<strong class="bold">Options:</strong>
<strong class="bold">  --version                       Show the version and exit.</strong>
<strong class="bold">  --debug / --no-debug</strong>
<strong class="bold">  -l, --log-level [INFO|DEBUG|WARNING|ERROR|CRITICAL]</strong>
<strong class="bold">                                  Log level</strong>
<strong class="bold">  --help                          </strong><strong class="bold">Show this message and exit.</strong>
<strong class="bold">Commands:</strong>
<strong class="bold">  backup                          Run a backup for the metadata DB.</strong>
<strong class="bold">  check</strong>
<strong class="bold">  docker                          Checks Docker Memory Allocation Run...</strong>
<strong class="bold">  ingest                          Main command for ingesting metadata...</strong>
<strong class="bold">  openmetadata-imports-migration  </strong><strong class="bold">Update DAG files generated after...</strong>
<strong class="bold">  profile                         Main command for profiling Table...</strong>
<strong class="bold">  restore                         Run a restore for the metadata DB.</strong>
<strong class="bold">  test                            Main command for running test suites</strong>
<strong class="bold">  webhook                         Simple Webserver to test webhook...</strong></pre></li>
</ol>
<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>How to do it…</h2>
<p>After downloading the <strong class="bold">Python</strong> package and <strong class="bold">Docker</strong>, we will proceed with the configurations as follows:</p>
<ol>
<li><strong class="bold">Running containers</strong>: It may <a id="_idIndexMarker253"/>take some time to finish when you execute it for the first time:<pre class="source-code">
<strong class="bold">$ metadata docker –start</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">It is common for this type of error to appear:</p>
<p class="callout"><strong class="bold">Error response from daemon: driver failed programming external connectivity on endpoint openmetadata_ingestion (3670b9566add98a3e79cd9a252d2d0d377dac627b4be94b669482f6ccce350e0): Bind for 0.0.0.0:8080 failed: port is </strong><strong class="bold">already allocated</strong></p>
<p class="callout">It means other containers or applications are already using port <code>8080</code>. To solve this, specify another port (such as <code>8081</code>) or stop the other applications.</p>
<p>The first time you run this command, the results might take a while due to other containers associated with it.</p>
<p>In the end, you should see the following output:</p>
<div><div><img alt="Figure 3.8 – Command line showing success running OpenMetadata containers" height="256" src="img/Figure_3.08_B19453.jpg" width="1214"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Command line showing success running OpenMetadata containers</p>
<ol>
<li value="2"><code>http://localhost:8585</code> address:</li>
</ol>
<div><div><img alt="Figure 3.9 – OpenMetadata sign-in page in the browser" height="934" src="img/Figure_3.09_B19453.jpg" width="1540"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – OpenMetadata sign-in page in the browser</p>
<ol>
<li value="3"><strong class="bold">Creating a user account and logging in</strong>: To access the UI panel, we need to create a user account as follows:</li>
</ol>
<div><div><img alt="Figure 3.10 – Creating a user account in the OpenMetadata Create Account section" height="911" src="img/Figure_3.10_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Creating a user account in the OpenMetadata Create Account section</p>
<p>After that, we will <a id="_idIndexMarker255"/>be redirected to the main page and be able to access the panel, shown as follows:</p>
<div><div><img alt="Figure 3.11 – Main page of OpenMetadata" height="729" src="img/Figure_3.11_B19453.jpg" width="1406"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Main page of OpenMetadata</p>
<p class="callout-heading">Note</p>
<p class="callout">Is also possible to log in using the default admin user by inserting the <a href="mailto:admin@openmetadata.org">admin@openmetadata.org</a> username and <code>admin</code> as the password.</p>
<p class="callout">For production <a id="_idIndexMarker256"/>matters, please refer to the Enable Security Guide here: <a href="https://docs.open-metadata.org/deployment/docker/security">https://docs.open-metadata.org/deployment/docker/security</a>.</p>
<ol>
<li value="4"><strong class="bold">Creating teams</strong>: In the <strong class="bold">Settings</strong> section, you should see several possible configurations, from <a id="_idIndexMarker257"/>creating users to access the console to integrations with messengers such as <strong class="bold">Slack</strong> or <strong class="bold">MS Teams</strong>.</li>
</ol>
<p>Some ingestion and integration requires the user to be allocated to a team. To create a team, we first need to log in as <code>admin</code>. Then, go to <strong class="bold">Settings</strong> | <strong class="bold">Teams</strong> | <strong class="bold">Create </strong><strong class="bold">new team</strong>:</p>
<div><div><img alt="Figure 3.12 – Creating a team in the OpenMetadata settings" height="1206" src="img/Figure_3.12_B19453.jpg" width="1409"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Creating a team in the OpenMetadata settings</p>
<ol>
<li value="5"><strong class="bold">Adding users to our teams</strong>: Select <a id="_idIndexMarker258"/>the team you just created and go to the <strong class="bold">Users</strong> tab. Then select the user you want to add.</li>
</ol>
<div><div><img alt="Figure 3.13 – Adding users to a team" height="1254" src="img/Figure_3.13_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Adding users to a team</p>
<p>Creating teams is very <a id="_idIndexMarker259"/>convenient to keep track of users’ activity and define a group of roles and policies. In the following case, all users added to this team will be able to navigate through and create their data discovery pipelines.</p>
<div><div><img alt="Figure 3.14 – Team page and the default associated Data Consumer role" height="1040" src="img/Figure_3.14_B19453.jpg" width="1290"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Team page and the default associated Data Consumer role</p>
<p>We must have <a id="_idIndexMarker260"/>a Data Steward or Administrator role for the activities in <a id="_idIndexMarker261"/>this chapter and the following recipe. The Data Steward role has almost the same permissions as the Administrator role since it is a position that is <a id="_idIndexMarker262"/>responsible for defining and implementing data policies, standards, and procedures to govern data usage and ensure consistency.</p>
<p>You can read more about the <strong class="bold">Roles and Policies</strong> of OpenMetadata here: <a href="https://github.com/open-metadata/OpenMetadata/issues/4199">https://github.com/open-metadata/OpenMetadata/issues/4199</a>.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>How it works…</h2>
<p>Now, let’s understand a bit more about how OpenMetadata works.</p>
<p>OpenMetadata is an open source metadata management tool designed to help organizations to manage their <a id="_idIndexMarker263"/>data and metadata across different systems or platforms. Since it centralizes data information in one place, it makes it easier to discover and understand data.</p>
<p>It is also a flexible and extensible tool, allowing integration with tools such as Apache Kafka, Apache Hive, and others since it uses programming languages such as <strong class="bold">Python</strong> (main core code) and Java <a id="_idIndexMarker264"/>behind the scenes.</p>
<p>To orchestrate <a id="_idIndexMarker265"/>and ingest the metadata from sources, OpenMetadata counts the sources using Airflow code. If you look at its core, all Airflow code can be found in <code>openmetadata-ingestion</code>. For more heavy users who want to debug any problems related to the ingestion process in this framework, Airflow can be easily accessed at <code>http://localhost:8080/</code>, when the metadata Docker container is up and running.</p>
<p>It also uses <strong class="bold">MySQL DB</strong> to store user information and relationships and an <strong class="bold">Elasticsearch</strong> container to create efficient indexes. Refer to the following figure (<a href="https://docs.open-metadata.org/developers/architecture">https://docs.open-metadata.org/developers/architecture</a>):</p>
<div><div><img alt="Figure 3.15 – OpenMetadata architecture diagram Font source: OpenMetadata documentation" height="771" src="img/Figure_3.15_B19453.jpg" width="651"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – OpenMetadata architecture diagram Font source: OpenMetadata documentation</p>
<p>For more detailed information <a id="_idIndexMarker266"/>about the design decisions, you can access the <strong class="bold">Main Concepts</strong> page and explore in detail the ideas behind them: <a href="https://docs.open-metadata.org/main-concepts/high-level-design">https://docs.open-metadata.org/main-concepts/high-level-design</a>.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>There’s more…</h2>
<p>We saw how <strong class="bold">OpenMetadata</strong> can be easily configured and installed locally on our machines and a brief overview of its architecture. However, other good options on the market can be used to document data, or even a <strong class="bold">SaaS</strong> solution of <strong class="bold">OpenMetadata</strong> using <strong class="bold">Google Cloud</strong>.</p>
<h3>OpenMetadata SaaS sandbox</h3>
<p>Recently, OpenMetadata implemented a <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>) sandbox (<a href="https://sandbox.open-metadata.org/signin">https://sandbox.open-metadata.org/signin</a>) using Google, making it easier to deploy and start the discovery <a id="_idIndexMarker267"/>and catalog process. However, it may have costs applied, so keep that in mind.</p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor120"/>See also</h2>
<ul>
<li>You can read more about OpenMetadata in their blog: <a href="https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac">https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac</a></li>
<li>Explore OpenMetadata on GitHub: <a href="https://github.com/open-metadata/OpenMetadata">https://github.com/open-metadata/OpenMetadata</a></li>
</ul>
<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Connecting OpenMetadata to our database</h1>
<p>Now that we have <a id="_idIndexMarker268"/>configured our <strong class="bold">Data Discovery</strong> tool, let’s <a id="_idIndexMarker269"/>create a sample connection to our local database instance. Let’s try to use PostgreSQL to do an easy integration and practice another database usage.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor122"/>Getting ready</h2>
<p>First, ensure our application runs appropriately by accessing the <code>http://localhost:8585/my-data address</code>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Inside OpenMetadata, the user must have the <code>admin</code> user using the previous credentials we saw.</p>
<p>You can check the Docker status here:</p>
<div><div><img alt="Figure 3.16 – Active containers are shown in the Docker desktop application" height="450" src="img/Figure_3.16_B19453.jpg" width="610"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Active containers are shown in the Docker desktop application</p>
<p>Use PostgreSQL for testing. Since <a id="_idIndexMarker270"/>we already have a Google <a id="_idIndexMarker271"/>project ready, let us create a SQL instance using the PostgreSQL engine.</p>
<p>As we kept the queries to create the database and tables in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>, we can build it again in Postgres. The queries can also be found in the GitHub repository of this chapter. However, feel free to create your own data.</p>
<div><div><img alt="Figure 3.17 – Google Cloud console header for SQL instances" height="391" src="img/Figure_3.17_B19453.jpg" width="563"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Google Cloud console header for SQL instances</p>
<p>Remember to let this instance allow public access; otherwise, our local OpenMetadata instance won’t be able to access it.</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>How to do it…</h2>
<p>Go to the OpenMetadata <a id="_idIndexMarker272"/>home page by typing <code>http://localhost:8585/my-data</code> in <a id="_idIndexMarker273"/>the browser header:</p>
<ol>
<li><strong class="bold">Adding a new database to OpenMetadata</strong>: Go to <strong class="bold">Settings</strong> | <strong class="bold">Services</strong> | <strong class="bold">Databases</strong> and click on <strong class="bold">Add new Database Service</strong>. Some options will appear. Click on <strong class="bold">Postgres</strong>:</li>
</ol>
<div><div><img alt="Figure 3.18 – OpenMetadata page to add a database as a source" height="1168" src="img/Figure_3.18_B19453.jpg" width="1412"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – OpenMetadata page to add a database as a source</p>
<p>Click on <code>CookBookData</code>.</p>
<ol>
<li value="2"><strong class="bold">Adding our connection settings</strong>: After clicking on <strong class="bold">Next</strong> again, a page with some fields to input the MySQL connection settings will appear:</li>
</ol>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 3.19 – Adding new database connection information" height="1111" src="img/Figure_3.19_B19453.jpg" width="1405"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Adding new database connection information</p>
<ol>
<li value="3"><strong class="bold">Testing our connection</strong>: With <a id="_idIndexMarker276"/>all the credentials <a id="_idIndexMarker277"/>in place, we need to test the connection to the database.</li>
</ol>
<div><div><img alt="Figure 3.20 – Connection test successful message for database connection" height="411" src="img/Figure_3.20_B19453.jpg" width="639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 – Connection test successful message for database connection</p>
<ol>
<li value="4"><strong class="bold">Creating an ingestion pipeline</strong>: You can leave all the fields as they are without <a id="_idIndexMarker278"/>worrying about the <strong class="bold">database tool</strong> (<strong class="bold">DBT</strong>). For <strong class="bold">Schedule Interval</strong>, you <a id="_idIndexMarker279"/>can set what suits you best. I will leave it as <strong class="bold">Daily</strong>.</li>
</ol>
<div><div><img alt="Figure 3.21 – Adding database metadata ingestion" height="599" src="img/Figure_3.21_B19453.jpg" width="1266"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 – Adding database metadata ingestion</p>
<ol>
<li value="5"><strong class="bold">Ingesting the metadata</strong>: Heading <a id="_idIndexMarker280"/>to <strong class="bold">Ingestions</strong>, our <a id="_idIndexMarker281"/>database metadata is successfully ingested.</li>
</ol>
<div><div><img alt="Figure 3.22 – Postgres metadata successfully ingested" height="410" src="img/Figure_3.22_B19453.jpg" width="1322"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 – Postgres metadata successfully ingested</p>
<ol>
<li value="6"><strong class="bold">Exploring our metadata</strong>: To explore the metadata, go to <strong class="bold">Explore</strong> | <strong class="bold">Tables</strong>:</li>
</ol>
<div><div><img alt="Figure 3.23 – Explore page showing the tables metadata ingested" height="581" src="img/Figure_3.23_B19453.jpg" width="1234"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Explore page showing the tables metadata ingested</p>
<p>You can see that the <code>people</code> table is there with other internal tables:</p>
<div><div><img alt="Figure 3.24 – The people table metadata" height="848" src="img/Figure_3.24_B19453.jpg" width="1322"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24 – The people table metadata</p>
<p>Here, you can explore some functionalities of the application, such as defining the level of importance to the organization and the owners, querying the table, and others.</p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>How it works…</h2>
<p>As we saw previously, OpenMetadata uses Python to build and connect to different sources.</p>
<p>In <code>Connection Scheme</code> uses <code>psycopg2</code>, a widely used <a id="_idIndexMarker282"/>library in Python. All other arguments are passed <a id="_idIndexMarker283"/>to the behind-the-scenes Python code to create a connection string.</p>
<p>For each metadata <a id="_idIndexMarker284"/>ingestion, OpenMetadata will create a new Airflow <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) to process it based on a generic one. Having a separate DAG for each metadata ingestion makes debugging more manageable in case of errors.</p>
<div><div><img alt="Figure 3.25 – Airflow DAGs created by OpenMetadata" height="935" src="img/Figure_3.25_B19453.jpg" width="1349"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Airflow DAGs created by OpenMetadata</p>
<p>If you open the Airflow <a id="_idIndexMarker285"/>instance used by OpenMetadata, you can <a id="_idIndexMarker286"/>see it clearly and have other information about the metadata ingestion. It’s a nice place to debug in case an error occurs. Understanding how our solution works and where to look in case of a problem helps identify and solve issues more efficiently.</p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Further reading</h1>
<ul>
<li><a href="https://nira.com/data-discovery/">https://nira.com/data-discovery/</a></li>
<li><a href="https://coresignal.com/blog/data-discovery/">https://coresignal.com/blog/data-discovery/</a></li>
<li><a href="https://www.polymerhq.io/blog/diligence/what-is-data-discovery-guide/">https://www.polymerhq.io/blog/diligence/what-is-data-discovery-guide/</a></li>
<li><a href="https://bi-survey.com/data-discovery">https://bi-survey.com/data-discovery</a></li>
<li><a href="https://www.heavy.ai/technical-glossary/data-discovery">https://www.heavy.ai/technical-glossary/data-discovery</a></li>
<li><a href="https://www.datapine.com/blog/what-are-data-discovery-tools/">https://www.datapine.com/blog/what-are-data-discovery-tools/</a></li>
<li><a href="https://www.knowsolution.com.br/data-discovery-como-relaciona-bi-descubra/">https://www.knowsolution.com.br/data-discovery-como-relaciona-bi-descubra/</a> (in Portuguese)</li>
</ul>
<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>Other tools</h2>
<p>If you are interested in learning more about other data discovery tools available on the market, here are some:</p>
<ul>
<li><strong class="bold">Tableau</strong>: Tableau (<a href="https://www.tableau.com/">https://www.tableau.com/</a>) is more extensively used for data visualizations <a id="_idIndexMarker287"/>and dashboards but comes with some features to discover and catalog data. You can read more about how to use Tableau for data discovery on their resources page here: <a href="https://www.tableau.com/learn/whitepapers/data-driven-organization-7-keys-data-discovery">https://www.tableau.com/learn/whitepapers/data-driven-organization-7-keys-data-discovery</a>.</li>
<li><strong class="bold">OpenDataDiscovery</strong> (free and open source): OpenDataDiscovery has recently arrived on the <a id="_idIndexMarker288"/>market and can provide a very nice starting point. Check it out here: <a href="https://opendatadiscovery.org/">https://opendatadiscovery.org/</a>.</li>
<li><strong class="bold">Atlan</strong>: Atlan (<a href="https://atlan.com/">https://atlan.com/</a>) is a complete solution and also brings a data governance <a id="_idIndexMarker289"/>structure; however, the costs can be high and it requires a call <a id="_idIndexMarker290"/>with their sales team to start an <strong class="bold">MVP</strong> (short for <strong class="bold">Minimum </strong><strong class="bold">Viable Product</strong>).</li>
<li><strong class="bold">Alation</strong>: Alation is an enterprise <a id="_idIndexMarker291"/>tool that provides several data solutions that include all pillars of data governance. Find out more here: <a href="https://www.alation.com/">https://www.alation.com/</a>.</li>
</ul>
</div>
</div></body></html>