["```py\n!pip install tabula-py xlrd lxml\n```", "```py\n    import numpy as np\n    import pandas as pd\n    df1 = pd.read_csv(\"../datasets/CSV_EX_1.csv\")\n    df1\n    ```", "```py\n    df2 = pd.read_csv(\"header=None to avoid this.\n    ```", "```py\n    df2 = pd.read_csv(\"0. This is how the pandas library treats a headerless CSV file when you ask it not to consider the first line (which is a data row in this case) as header:  ![Figure 5.3: A CSV file with numeric column headers\n    ](img/B15780_05_03.jpg)Figure 5.3: A CSV file with numeric column headersThis may be fine for data analysis purposes, but if you want the DataFrame to have meaningful headers, then you will have to add them using the `names` argument.\n    ```", "```py\n    df2 = pd.read_csv(\"../datasets/CSV_EX_2.csv\",\\\n                      header=None, names=['Bedroom','Sq.ft',\\\n                                          'Locality','Price($)'])\n    df2\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    df3 = pd.read_csv(\"; separator was not expected, and the reading is flawed. A simple workaround is to specify the separator/delimiter explicitly in the read function.\n    ```", "```py\n    df3 = pd.read_csv(\"../datasets/CSV_EX_3.csv\",sep=';')\n    df3\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    df4 = pd.read_csv(\"../datasets/CSV_EX_1.csv\",\\\n                      names=['A','B','C','D'])\n    df4\n    ```", "```py\n    df4 = pd.read_csv(\"../datasets/CSV_EX_1.csv\",header=0,\\\n                      names=['A','B','C','D'])\n    df4\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    df5 = pd.read_csv(\"../datasets/CSV_EX_skiprows.csv\")\n    df5\n    ```", "```py\n    df5 = pd.read_csv(\"skipfooter and the engine='python' option to enable this. There are two engines for these CSV reader functions, based on C or Python, of which only the Python engine supports the skipfooter option.\n    ```", "```py\n    df6 = pd.read_csv(\"../datasets/CSV_EX_skipfooter.csv\",\\\n                      skiprows=2,skipfooter=1, engine='python')\n    df6\n    ```", "```py\ndf7 = pd.read_csv(\"../datasets/CSV_EX_1.csv\",nrows=2)\ndf7\n```", "```py\n    list_of_dataframe = []\n    ```", "```py\n    rows_in_a_chunk = 10\n    ```", "```py\n    num_chunks = 5\n    ```", "```py\n    import pandas as pd\n    df_dummy = pd.read_csv(\"../datasets/Boston_housing.csv\",nrows=2)\n    colnames = df_dummy.columns\n    ```", "```py\n    for i in range(0,num_chunks*rows_in_a_chunk,rows_in_a_chunk):\n        df = pd.read_csv(\"Boston_housing.csv\", header=0,\\\n                         skiprows=i, nrows=rows_in_a_chunk,\\\n                         names=colnames)\n        list_of_dataframe.append(df)\n    ```", "```py\ndf9 = pd.read_csv(\"../datasets/CSV_EX_blankline.csv\",\\\n                  skip_blank_lines=False)\ndf9\n```", "```py\ndf10 = pd.read_csv('../datasets/CSV_EX_1.zip')\ndf10\n```", "```py\ndf11_1 = pd.read_excel(\"../datasets/Housing_data.xlsx\",\\\n                       sheet_name='Data_Tab_1')\ndf11_2 = pd.read_excel(\"../datasets/Housing_data.xlsx\",\\\n                       sheet_name='Data_Tab_2')\ndf11_3 = pd.read_excel(\"../datasets/Housing_data.xlsx\",\\\n                       sheet_name='Data_Tab_3')\n```", "```py\ndict_df = pd.read_excel(\"../datasets/Housing_data.xlsx\",\\\n                        sheet_name=None)\ndict_df.keys()\n```", "```py\nodict_keys(['Data_Tab_1', 'Data_Tab_2', 'Data_Tab_3'])\n```", "```py\n    import pandas as pd\n    df13 = pd.read_table(\".txt extension will result in the preceding DataFrame if read without explicitly setting the separator. As you can see, for each value read, there is a comma appended. In this case, we have to set the separator explicitly. \n    ```", "```py\n    df13 = pd.read_table(\"../datasets/Table_EX_1.txt\", sep=',')\n    df13\n    ```", "```py\nurl = 'http://www.fdic.gov/bank/individual/failed/banklist.html'\nlist_of_df = pd.read_html(url)\ndf14 = list_of_df[0]\ndf14.head()\n```", "```py\n    import pandas as pd\n    list_of_df = pd.read_html(\"https://en.wikipedia.org/wiki/\"\\\n                              \"2016_Summer_Olympics_medal_table\",\\\n                              header=0)\n    ```", "```py\n    len(list_of_df)\n    ```", "```py\n    7\n    ```", "```py\n    for t in list_of_df:\n        print(t.shape)\n    ```", "```py\n    (1, 1)\n    (87, 6)\n    (10, 9)\n    (0, 2)\n    (1, 2)\n    (4, 2)\n    (1, 2)\n    ```", "```py\n    df15=list_of_df[1]\n    df15.head()\n    ```", "```py\n    import pandas as pd\n    df16 = pd.read_json(\"../datasets/movies.json\")\n    df16.head()\n    ```", "```py\n    cast_of_avengers = df16[(df16['title']==\"The Avengers\") \\\n                       & (df16['year']==2012)]['cast']\n    print(list(cast_of_avengers))\n    ```", "```py\n     [['Robert Downey, Jr.', 'Chris Evans', 'Mark Ruffalo', \n       'Chris Hemsworth', 'Scarlett Johansson', 'Jeremy Renner', \n       'Tom Hiddleston', 'Clark Gregg', 'Cobie Smulders', \n       'Stellan SkarsgÃyrd', 'Samuel L. Jackson']]\n    ```", "```py\n    from tabula import read_pdf\n    df18_1 = read_pdf('../datasets/Housing_data.pdf',\\\n                      pages=[1], pandas_options={'header':None})\n    df18_1\n    ```", "```py\n    df18_2 = read_pdf('../datasets/Housing_data.pdf',\\\n                      pages=[2], pandas_options={'header':None})\n    df18_2\n    ```", "```py\n    import pandas as pd\n    df1 = pd.DataFrame(df18_1)\n    df2 = pd.DataFrame(df18_2)\n    df18=pd.concat([df1,df2],axis=1)\n    df18.values.tolist()\n    ```", "```py\n    names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS',\\\n             'RAD','TAX','PTRATIO','B','LSTAT','PRICE']\n    df18_1 = read_pdf('../datasets/Housing_data.pdf',pages = [1], \\\n                      pandas_options = {'header':None,\\\n                                        'names':names[:10]})\n    df18_2 = read_pdf('../datasets/Housing_data.pdf',pages = [2],\\\n                      pandas_options = {'header':None,\\\n                                        'names':names[10:]})\n    df1 = pd.DataFrame(df18_1)\n    df2 = pd.DataFrame(df18_2)\n    df18 = pd.concat([df1,df2],axis = 1)\n    df18.values.tolist()\n    ```", "```py\n    from bs4 import BeautifulSoup\n    ```", "```py\n    with open(\"BeautifulSoup object and it will read the contents from the file that the handler is attached to. We will see that the return type is an instance of bs4.BeautifulSoup. This class holds all the methods we need to navigate through the DOM tree that the document represents. \n    ```", "```py\n    print(soup.prettify())\n    ```", "```py\n    with open(\"<p> tag. We saw how to read a tag in the last exercise, but we can easily see the problem with this approach. When we look into our HTML document, we can see that we have more than one `<p>` tag there. How can we access all the `<p>` tags? It turns out that this is easy. \n    ```", "```py\n    with open(\"6, which is exactly the number of <p> tags in the document. We have seen how to access all the tags of the same type. We have also seen how to get the content of the entire HTML document. \n    ```", "```py\n    with open(\". notation to get the contents of that tag. We saw in the previous step that we can access the entire content of a particular tag. However, HTML is represented as a tree, and we are able to traverse the children of a particular node. There are a few ways to do this.\n    ```", "```py\n    with open(\"children generator is that it only takes into account the immediate children of the tag. We have <tbody> under <table>, and our whole table structure is wrapped in it. That's why it was considered a single child of the <table> tag. We have looked into how to browse the immediate children of a tag. We will see how we can browse all the possible children of a tag and not only the immediate one. \n    ```", "```py\n    with open(\"../datasets/test.html\", \"r\") as fd:\n        soup = BeautifulSoup(fd)\n        table = soup.table\n        children = table.children\n        des = table.descendants\n        print(len(list(children)), len(list(des)))\n    ```", "```py\n    9 61\n    ```", "```py\n    import pandas as pd\n    from bs4 import BeautifulSoup\n    fd = open(\"../datasets/test.html\", \"r\")\n    soup = BeautifulSoup(fd)\n    data = soup.findAll('tr')\n    print(\"Data is a {} and {} items long\".format(type(data),\\\n          len(data)))\n    Data is a <class 'bs4.element.ResultSet'> and 4 items long\n    ```", "```py\n    data_without_header = data[1:]\n    headers = data[0]\n    headers\n    ```", "```py\n    <tr>\n    <th>Entry Header 1</th>\n    <th>Entry Header 2</th>\n    <th>Entry Header 3</th>\n    <th>Entry Header 4</th>\n    </tr>\n    ```", "```py\n    col_headers = [th.getText() for th in headers.findAll('th')]\n    col_headers\n    ```", "```py\n    ['Entry Header 1', 'Entry Header 2', 'Entry Header 3', 'Entry Header 4']\n    ```", "```py\n    df_data = [[td.getText() for td in tr.findAll('td')] \\\n               for tr in data_without_header]\n    df_data\n    ```", "```py\n    df = pd.DataFrame(df_data, columns=col_headers)\n    df.head()\n    ```", "```py\n    !pip install openpyxl\n    ```", "```py\n    writer = pd.ExcelWriter('../datasets/test_output.xlsx')\n    df.to_excel(writer, \"Sheet1\")\n    writer.save()\n    writer\n    <pandas.io.excel._XlsxWriter at 0x24feb2939b0>\n    ```", "```py\n    d = open(\"../datasets/test.html\", \"r\")\n    soup = BeautifulSoup(d)\n    lis = soup.find('ul').findAll('li')\n    stack = []\n    for li in lis:\n        a = li.find('a', href=True)\n    ```", "```py\n    stack.append(a['href'])\n    ```", "```py\n    print(stack)\n    ```", "```py\n    ['https://www.imdb.com/chart/top']\n    ```"]