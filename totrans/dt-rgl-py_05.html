<html><head></head><body><div><div><h1 id="_idParaDest-144"><em class="italics"><a id="_idTextAnchor166"/>Chapter 5</em></h1>
		</div>
		<div><h1 id="_idParaDest-145"><a id="_idTextAnchor167"/>Getting Comfortable with Different Kinds of Data Sources</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Read CSV, Excel, and JSON files into pandas DataFrames </li>
				<li class="bullets">Read PDF documents and HTML tables into pandas DataFrames</li>
				<li class="bullets">Perform basic web scraping using powerful yet easy to use libraries such as Beautiful Soup</li>
				<li class="bullets">Extract structured and textual information from portals </li>
			</ul>
			<p>In this chapter, you will be exposed to real-life data wrangling techniques, as applied to web scraping.</p>
		</div>
		<div><h2 id="_idParaDest-146"><a id="_idTextAnchor168"/>Introduction</h2>
			<p>So far in this book, we have focused on learning pandas DataFrame objects as the main data structure for the application of wrangling techniques. Now, we will learn about various techniques by which we can read data into a DataFrame from external sources. Some of those sources could be text-based (CSV, HTML, JSON, and so on), whereas some others could be binary (Excel, PDF, and so on), that is, not in ASCII format. In this chapter, we will learn how to deal with data that is present in web pages or HTML documents. This holds very high importance in the work of a data practitioner.</p>
			<h4>Note</h4>
			<p class="callout">Since we have gone through a detailed example of basic operations with NumPy and pandas, in this chapter, we will often skip trivial code snippets such as viewing a table, selecting a column, and plotting. Instead, we will focus on showing code examples for the new topics we aim to learn about here.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor169"/>Reading Data from Different Text-Based (and Non-Text-Based) Sources</h2>
			<p>One of the most valued and widely used skills of a data wrangling professional is the ability to extract and read data from a diverse array of sources into a structured format. Modern analytics pipelines depend on their ability to scan and absorb a variety of data sources to build and analyze a pattern-rich model. Such a feature-rich, multi-dimensional model will have high predictive and generalization accuracy. It will be valued by stakeholders and end users alike for any data-driven product.</p>
			<p>In the first topic of this chapter, we will go through various data sources and how they can be imported into pandas DataFrames, thus imbuing wrangling professionals with extremely valuable data ingestion knowledge.</p>
			<h3 id="_idParaDest-148"><a id="_idTextAnchor170"/>Data Files Provided with This Chapter</h3>
			<p>Because this topic is about reading from various data sources, we will use small files of various types in the following exercises. All of the data files are provided along with the Jupyter notebook in the code repository.</p>
			<h3 id="_idParaDest-149"><a id="_idTextAnchor171"/>Libraries to Install for This Chapter</h3>
			<p>Because this chapter deals with reading various file formats, we need to have the support of additional libraries and software platforms to accomplish our goals.</p>
			<p>Execute the following codes in your Jupyter notebook cells (don't forget the ! before each line of code) to install the necessary libraries:</p>
			<pre>!apt-get update !apt-get install -y default-jdk
!pip install tabula-py xlrd lxml</pre>
			<h3 id="_idParaDest-150"><a id="_idTextAnchor172"/>Exercise 60: Reading Data from a CSV File Where Headers Are Missing</h3>
			<p>The pandas library provides a simple direct method called <code>read_csv</code> to read data in a tabular format from a comma-separated text file, or CSV. This is particularly useful because a CSV is a lightweight yet extremely handy data exchange format for many applications, including such domains as machine-generated data. It is not a proprietary format and therefore is universally used by a variety of data-generating sources.</p>
			<p>At times, headers may be missing from a CSV file and you may have to add proper headers/column names of your own. Let's have a look at how this can be done:</p>
			<ol>
				<li>Read the example CSV file (with a proper header) using the following code and examine the resulting DataFrame, as follows:<pre>import numpy as np
import pandas as pd
df1 = pd.read_csv("CSV_EX_1.csv")
df1</pre><p>The output is as follows:</p><div><img src="img/C11065_05_01.jpg" alt="Figure 5.1: Output of example CSV file" width="545" height="172"/></div><h6>Figure 5.1: Output of example CSV file</h6></li>
				<li>Read a <code>.csv</code> file with no header using a pandas DataFrame:<pre>df2 = pd.read_csv("CSV_EX_2.csv")
df2</pre><p>The output is as follows:</p><div><img src="img/C11065_05_02.jpg" alt="Figure 5.2: Output of the .csv being read using a DataFrame" width="667" height="160"/></div><h6>Figure 5.2: Output of the .csv being read using a DataFrame</h6><p>Certainly, the top data row has been mistakenly read as the column header. You can specify <code>header=None</code> to avoid this.</p></li>
				<li>Read the <code>.csv</code> file by mentioning the header <code>None</code>, as follows:<pre>df2 = pd.read_csv("CSV_EX_2.csv",header=None)
df2</pre><p>However, without any header information, you will get back the following output. The default headers will be just some default numeric indices starting from 0:</p><div><img src="img/C11065_05_03.jpg" alt="Figure 5.3: CSV file with a numeric column header" width="624" height="184"/></div><h6>Figure 5.3: CSV file with a numeric column header</h6><p>This may be fine for data analysis purposes, but if you want the DataFrame to truly reflect the proper headers, then you will have to add them using the <code>names</code> argument.</p></li>
				<li>Add the <code>names</code> argument to get the correct headers:<pre>df2 = pd.read_csv("CSV_EX_2.csv",header=None, names=['Bedroom','Sq.ft','Locality','Price($)'])
df2</pre><p>Finally, you will get a DataFrame that's as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_04.jpg" alt="Figure 5.4: CSV file with correct column header" width="607" height="191"/>
				</div>
			</div>
			<h6>Figure 5.4: CSV file with correct column header</h6>
			<h3 id="_idParaDest-151">Exer<a id="_idTextAnchor173"/>cise 61: Reading from a CSV File where Delimiters are not Commas</h3>
			<p>Although CSV stands for comma-separated-values, it is fairly common to encounter raw data files where the separator/delimiter is a character other than a comma:</p>
			<ol>
				<li value="1">Read a <code>.csv</code> file using pandas DataFrames:<pre>df3 = pd.read_csv("CSV_EX_3.csv")
df3</pre></li>
				<li>The output will be as follows:<div><img src="img/C11065_05_05.jpg" alt="Figure 5.5: A DataFrame that has a semi-colon as a separator" width="573" height="185"/></div><h6>Figure 5.5: A DataFrame that has a semi-colon as a separator</h6></li>
				<li>Clearly, the <em class="italics">;</em> separator was not expected, and the reading is flawed. A simple work around is to specify the separator/delimiter explicitly in the read function:<pre>df3 = pd.read_csv("CSV_EX_3.csv",sep=';')
df3</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_06.jpg" alt="Figure 5.6: Semicolons removed from the DataFrame" width="514" height="165"/>
				</div>
			</div>
			<h6>Figure 5.6: Semicolons removed from the DataFrame</h6>
			<h3 id="_idParaDest-152"><a id="_idTextAnchor174"/>Exercise 62: Bypassing the Headers of a CSV File</h3>
			<p>If your CSV file already comes with headers but you want to bypass them and put in your own, you have to specifically set <code>header</code> <code>=</code> <code>0</code> to make it happen. If you try to set the names variable to your header list, unexpected things can happen:</p>
			<ol>
				<li value="1">Add names to a .csv file that has headers, as follows:<pre>df4 = pd.read_csv("CSV_EX_1.csv",names=['A','B','C','D'])
df4</pre><p>The output is as follows:</p><div><img src="img/C11065_05_07.jpg" alt="Figure 5.7: CSV file with headers overlapped" width="663" height="209"/></div><h6>Figure 5.7: CSV file with headers overlapped</h6></li>
				<li>To avoid this, set <code>header</code> to zero and provide a names list:<pre>df4 = pd.read_csv("CSV_EX_1.csv",header=0,names=['A','B','C','D'])
df4</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_08.jpg" alt="Figure 5.8: CSV file with defined headers" width="499" height="163"/>
				</div>
			</div>
			<h6>Figure 5.8: CSV file with defined headers</h6>
			<h3 id="_idParaDest-153">Exercise<a id="_idTextAnchor175"/> 63: Skipping Initial Rows and Footers when Reading a CSV File</h3>
			<p>Skipping initial rows is a widely useful method because, most of the time, the first few rows of a CSV data file are metadata about the data source or similar information, which is not read into the table:</p>
			<div><div><img src="img/C11065_05_09.jpg" alt="Figure 5.9: Contents of the CSV file" width="1571" height="427"/>
				</div>
			</div>
			<h6>Figure 5.9: Contents of the CSV file</h6>
			<h4>Note</h4>
			<p class="callout">The first two lines in the CSV file are irrelevant data.</p>
			<ol>
				<li value="1">Read the CSV file and examine the results:<pre>df5 = pd.read_csv("CSV_EX_skiprows.csv")
df5</pre><p>The output is as follows:</p><div><img src="img/C11065_05_10.jpg" alt="Figure 5.10: DataFrame with an unexpected error" width="681" height="253"/></div><h6>Figure 5.10: DataFrame with an unexpected error</h6></li>
				<li>Skip the first two rows and read the file:<pre>df5 = pd.read_csv("CSV_EX_skiprows.csv",skiprows=2)
df5</pre><p>The output is as follows:</p><div><img src="img/C11065_05_11.jpg" alt="Figure 5.11: Expected DataFrame after skipping two rows" width="518" height="160"/></div><h6>Figure 5.11: Expected DataFrame after skipping two rows</h6></li>
				<li>Similar to skipping the initial rows, it may be necessary to skip the footer of a file. For example, we do not want to read the data at the end of the following file:<div><img src="img/C11065_05_12.jpg" alt="Figure 5.12: Contents of the CSV file" width="717" height="218"/></div><h6>Figure 5.12: Contents of the CSV file</h6><p>We have to use <code>skipfooter</code> and the <code>engine='python'</code> option to enable this. There are two engines for these CSV reader functions – based on C or Python, of which only the Python engine supports the <code>skipfooter</code> option.</p></li>
				<li>Use the <code>skipfooter</code> option in Python:<pre>df6 = pd.read_csv("CSV_EX_skipfooter.csv",skiprows=2,
skipfooter=1,engine='python')
df6</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_13.jpg" alt="Figure 5.13: DataFrame without a footer" width="524" height="166"/>
				</div>
			</div>
			<h6>Figure 5.13: DataFrame without a footer</h6>
			<h3 id="_idParaDest-154">Reading Only <a id="_idTextAnchor176"/>the First N Rows (Especially Useful for Large Files)</h3>
			<p>In many situations, we may not want to read a whole data file but only the first few rows. This is particularly useful for extremely large data files, where we may just want to read the first couple of hundred rows to check an initial pattern and then decide to read the whole data later on. Reading the entire file can take a long time and slow down the entire data wrangling pipeline. </p>
			<p>A simple option, called <code>nrows</code>, in the <code>read_csv</code> function enables us to do just that:</p>
			<pre>df7 = pd.read_csv("CSV_EX_1.csv",nrows=2)
df7</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_05_14.jpg" alt="" width="470" height="80"/>
				</div>
			</div>
			<h6>Figure 5.14: DataFrame with the first few rows of the CSV file</h6>
			<h3 id="_idParaDest-155">Exercise 64: C<a id="_idTextAnchor177"/>ombining Skiprows and Nrows to Read Data in Small Chunks</h3>
			<p>Continuing our discussion about reading a very large data file, we can cleverly combine <code>skiprows</code> and <code>nrows</code> to read in such a large file in smaller chunks of pre-determined sizes. The following code demonstrates just that:</p>
			<ol>
				<li value="1">Create a list where DataFrames will be stored:<pre>list_of_dataframe = []</pre></li>
				<li>Store the number of rows to be read into a variable:<pre>rows_in_a_chunk = 10</pre></li>
				<li>Create a variable to store the number of chunks to be read:<pre>num_chunks = 5</pre></li>
				<li>Create a dummy DataFrame to get the column names:<pre>df_dummy = pd.read_csv("Boston_housing.csv",nrows=2)
colnames = df_dummy.columns</pre></li>
				<li>Loop over the CSV file to read only a fixed number of rows at a time:<pre>for i in range(0,num_chunks*rows_in_a_chunk,rows_in_a_chunk):
    df = pd.read_csv("Boston_housing.csv",header=0,skiprows=i,nrows=rows_in_a_chunk,names=colnames)
    list_of_dataframe.append(df)</pre></li>
			</ol>
			<p>Note how the <code>iterator</code> variable is set up inside the <code>range</code> function to break it into chunks. Say the number of chunks is 5 and the rows per chunk is 10. Then, the iterator will have a range of (0,5*10,10), where the final 10 is step-size, that is, it will iterate with indices of (0,9,19,29,39,49).</p>
			<h3 id="_idParaDest-156">Setting the sk<a id="_idTextAnchor178"/>ip_blank_lines Option</h3>
			<p>By default, <code>read_csv</code> ignores blank lines. But sometimes, you may want to read them in as NaN so that you can count how many such blank entries were present in the raw data file. In some situations, this is an indicator of the default data streaming quality and consistency. For this, you have to disable the <code>skip_blank_lines</code> option:</p>
			<pre>df9 = pd.read_csv("CSV_EX_blankline.csv",skip_blank_lines=False)
df9</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_05_15.jpg" alt="Figure 5.15: DataFrame that has blank rows of a .csv file" width="561" height="203"/>
				</div>
			</div>
			<h6>Figure 5.15: DataFrame that has blank rows of a .csv file</h6>
			<h3 id="_idParaDest-157"><a id="_idTextAnchor179"/>Read CSV from a Zip file</h3>
			<p>This is an awesome feature of pandas, in that it allows you to read directly from a compressed file such as <code>.zip</code>, <code>.gz</code>, <code>.bz2</code>, or <code>.xz</code>. The only requirement is that the intended data file (CSV) should be the only file inside the compressed file.</p>
			<p>In this example, we compressed the example CSV file with a 7-Zip program and read from it directly using the <code>read_csv</code> method:</p>
			<pre>df10 = pd.read_csv('CSV_EX_1.zip')
df10</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_05_16.jpg" alt="" width="587" height="159"/>
				</div>
			</div>
			<h6>Figure 5.16: DataFrame of a compressed CSV</h6>
			<h3 id="_idParaDest-158">Reading from an <a id="_idTextAnchor180"/>Excel File Using sheet_name and Handling a Distinct sheet_name</h3>
			<p>Next, we will turn our attention to a Microsoft Excel file. It turns out that most of the options and methods we learned about in the previous exercises with the CSV file apply directly to the reading of Excel files too. Therefore, we will not repeat them here. Instead, we will focus on their differences. An Excel file can consist of multiple worksheets and we can read a specific sheet by passing in a particular argument, that is, <code>sheet_name</code>. </p>
			<p>For example, in the associated data file, <code>Housing_data.xlsx</code>, we have three tabs, and the following code reads them one by one in three separate DataFrames:</p>
			<pre>df11_1 = pd.read_excel("Housing_data.xlsx",sheet_name='Data_Tab_1')
df11_2 = pd.read_excel("Housing_data.xlsx",sheet_name='Data_Tab_2')
df11_3 = pd.read_excel("Housing_data.xlsx",sheet_name='Data_Tab_3')</pre>
			<p>If the Excel file has multiple distinct sheets but the <code>sheet_name</code> argument is set to <code>None</code>, then an ordered dictionary will be returned by the <code>read_excel</code> function. Thereafter, we can simply iterate over that dictionary or its keys to retrieve individual DataFrames.</p>
			<p>Let's consider the following example:</p>
			<pre>dict_df = pd.read_excel("Housing_data.xlsx",sheet_name=None)
dict_df.keys()</pre>
			<p>The output is as follows:</p>
			<pre>odict_keys(['Data_Tab_1', 'Data_Tab_2', 'Data_Tab_3'])</pre>
			<h3 id="_idParaDest-159"><a id="_idTextAnchor181"/>Exercise 65: Reading a General Delimited Text File</h3>
			<p>General text files can be read as easily as we read CSV files. However, you have to pass on the proper separator if it is anything other than a whitespace or a tab:</p>
			<ol>
				<li value="1">A comma-separated file, saved with the <code>.txt</code> extension, will result in the following DataFrame if read without explicitly setting the separator:<pre>df13 = pd.read_table("Table_EX_1.txt")
df13</pre><p>The output is as follows:</p><div><img src="img/C11065_05_17.jpg" alt="" width="624" height="194"/></div><h6>Figure 5.17: DataFrame that has a comma-separated CSV file</h6></li>
				<li>In this case, we have to set the separator explicitly, as follows:<pre>df13 = pd.read_table("Table_EX_1.txt",sep=',')
df13</pre><p>The output is follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_18.jpg" alt="Figure 5.18: DataFrame read using comma seperator" width="570" height="157"/>
				</div>
			</div>
			<h6>Figure 5.18: DataFrame read using a comma separator</h6>
			<h3 id="_idParaDest-160">Reading HTML Table<a id="_idTextAnchor182"/>s Directly from a URL</h3>
			<p>The pandas library allows us to read HTML tables directly from a URL. This means that they already have some kind of built-in HTML parser that processes the HTML content of a given page and tries to extract various tables in the page. </p>
			<h4>Note</h4>
			<p class="callout">The <code>read_html</code> method returns a list of DataFrames (even if the page has a single DataFrame) and you have to extract the relevant tables from the list.</p>
			<p>Consider the following example:</p>
			<pre>url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
list_of_df = pd.read_html(url)
df14 = list_of_df[0]
df14.head()</pre>
			<p>These results are shown in the following DataFrame:</p>
			<div><div><img src="img/C11065_05_19.jpg" alt="Figure 5.19: Results of reading HTML tables" width="1109" height="201"/>
				</div>
			</div>
			<h6>Figure 5.19: Results of reading HTML tables</h6>
			<h3 id="_idParaDest-161">Exercise 66: Furthe<a id="_idTextAnchor183"/>r Wrangling to Get the Desired Data</h3>
			<p>As discussed in the preceding exercise, this HTML-reading function almost always returns more than one table for a given HTML page and we have to further parse through the list to extract the particular table we are interested in:</p>
			<ol>
				<li value="1">For example, if we want to get the table of the 2016 summer Olympics medal tally (by nation), we can easily search to get a page on Wikipedia that we can pass on to pandas. We can do this by using the following command:<pre>list_of_df = pd.read_html("https://en.wikipedia.org/wiki/2016_Summer_Olympics_medal_table",header=0)</pre></li>
				<li>If we check the length of the list returned, we will see it is 6:<pre>len(list_of_df)</pre><p>The output is as follows:</p><pre> 6</pre></li>
				<li>To look for the table, we can run a simple loop:<pre>for t in list_of_df:
    print(t.shape)</pre><p>The output is as follows:</p><div><img src="img/C11065_05_20.jpg" alt="" width="543" height="113"/></div><h6>Figure 5.20: Shape of the tables </h6></li>
				<li>It looks like the second element in this list is the table we are looking for:<pre>df15=list_of_df[1]
df15.head()</pre></li>
				<li>The output is as follows:</li>
			</ol>
			<div><div><img src="img/C11065_05_21.jpg" alt="Figure 5.21: Output of the data in the second table" width="701" height="202"/>
				</div>
			</div>
			<h6>Figure 5.21: Output of the data in the second table</h6>
			<h3 id="_idParaDest-162">Exercise 67: Reading <a id="_idTextAnchor184"/>from a JSON File</h3>
			<p>Over the last 15 years, JSON has become a ubiquitous choice for data exchange on the web. Today, it is the format of choice for almost every publicly available web API, and it is frequently used for private web APIs as well. It is a schema-less, text-based representation of structured data that is based on key-value pairs and ordered lists.</p>
			<p>The pandas library provides excellent support for reading data from a JSON file directly into a DataFrame. To practice with this chapter, we have included a file called <code>movies.json</code>. This file contains the cast, genre, title, and year (of release) information for almost all major movies since 1900:</p>
			<ol>
				<li value="1">Extract the cast list for the 2012 Avengers movie (from Marvel comics):<pre>df16 = pd.read_json("movies.json")
df16.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_05_22.jpg" alt="Figure 5.22: DataFrame displaying Avengers movie cast" width="627" height="211"/></div><h6>Figure 5.22: DataFrame displaying the Avengers movie cast</h6></li>
				<li>To look for the cast where the title is "Avengers", we can use filtering:<pre>cast_of_avengers=df16[(df16['title']=="The Avengers") &amp; (df16['year']==2012)]['cast']
print(list(cast_of_avengers))</pre><p>The output will be as follows:</p><pre> [['Robert Downey, Jr.', 'Chris Evans', 'Mark Ruffalo', 'Chris Hemsworth', 'Scarlett Johansson', 'Jeremy Renner', 'Tom Hiddleston', 'Clark Gregg', 'Cobie Smulders', 'Stellan SkarsgÃyrd', 'Samuel L. Jackson']]</pre></li>
			</ol>
			<h3 id="_idParaDest-163"><a id="_idTextAnchor185"/>Reading a Stata File</h3>
			<p>The pandas library provides a direct reading function for Stata files, too. Stata is a popular statistical modeling platform that's used in many governmental and research organizations, especially by economists and social scientists.</p>
			<p>The simple code to read in a Stata file (<code>.dta</code> format) is as follows:</p>
			<pre>df17 = pd.read_stata("wu-data.dta")</pre>
			<h3 id="_idParaDest-164">Exercise 68: Reading T<a id="_idTextAnchor186"/>abular Data from a PDF File</h3>
			<p>Among the various types of data sources, the PDF format is probably the most difficult to parse in general. While there are some popular packages in Python for working with PDF files for general page formatting, the best library to use for table extraction from PDF files is <code>tabula-py</code>.</p>
			<p>From the GitHub page of this package, <code>tabula-py</code> is a simple Python wrapper of <code>tabula-java</code>, which can read a table from a PDF. You can read tables from PDFs and convert them into pandas DataFrames. The <code>tabula-py</code> library also enables you to convert a PDF file into a CSV/TSV/JSON file.</p>
			<p>You will need the following packages installed on your system before you can run this, but they are free and easy to install: </p>
			<ul>
				<li>urllib3</li>
				<li>pandas</li>
				<li>pytest</li>
				<li>flake8</li>
				<li>distro</li>
				<li>pathlib</li>
			</ul>
			<ol>
				<li value="1">Find the PDF file in the following link: https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter05/Exercise60-68/Housing_data.xlsx. The following code retrieves the tables from two pages and joins them to make one table:<pre>from tabula import read_pdf
df18_1 = read_pdf('Housing_data.pdf',pages=[1],pandas_options={'header':None})
df18_1</pre><p>The output is as follows:</p><div><img src="img/C11065_05_23.jpg" alt="Figure 5.23: DataFrame with a table derived by merging a table flowing over two pages in a PDF" width="680" height="171"/></div><h6>Figure 5.23: DataFrame with a table derived by merging a table flowing over two pages in a PDF</h6></li>
				<li>Retrieve the table from another page of the same PDF by using the following command:<pre>df18_2 = read_pdf('Housing_data.pdf',pages=[2],pandas_options={'header':None})
df18_2</pre><p>The output is as follows:</p><div><img src="img/C11065_05_24.jpg" alt="Figure 5.24: DataFrame displaying a table from another page" width="545" height="168"/></div><h6>Figure 5.24: DataFrame displaying a table from another page</h6></li>
				<li>To concatenate the tables that were derived from the first two steps, execute the following code:<pre>df18=pd.concat([df18_1,df18_2],axis=1)
df18</pre><p>The output is as follows:</p><div><img src="img/C11065_05_25.jpg" alt="Figure 5.25: DataFrame derived by concatenating two tables" width="668" height="178"/></div><h6>Figure 5.25: DataFrame derived by concatenating two tables</h6></li>
				<li>With PDF extraction, most of the time, headers will be difficult to extract automatically. You have to pass on the list of headers with the <code>names</code> argument in the <code>read-pdf</code> function as <code>pandas_option</code>, as follows:<pre>names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','PRICE']
df18_1 = read_pdf('Housing_data.pdf',pages=[1],pandas_options={'header':None,'names':names[:10]})
df18_2 = read_pdf('Housing_data.pdf',pages=[2],pandas_options={'header':None,'names':names[10:]})
df18=pd.concat([df18_1,df18_2],axis=1)
df18</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_26.jpg" alt="Figure 5.26: DataFrame with correct column headers for PDF data" width="821" height="174"/>
				</div>
			</div>
			<h6>Figure 5.26: DataFrame with correct column headers for PDF data</h6>
			<p>We will have a full activity on reading tables from a PDF report and processing them at the end of this chapter.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor187"/>Introduction to Beautiful Soup 4 and Web Page Parsing</h2>
			<p>The ability to read and un<a id="_idTextAnchor188"/>derstand web pages is one of paramount interest for a person collecting and formatting data. For example, consider the task of gathering data about movies and then formatting it for a downstream system. Data for the movies is best obtained by the websites such as IMDB and that data does not come pre-packaged in nice forms(CSV, JSON&lt; and so on), so you need to know how to download and read web page.</p>
			<p>Furthermore, you also need to be equipped with the knowledge of the structure of a web page so that you can design a system that can search for (query) a particular piece of information from a whole web page and get the value of it. This involves understanding the grammar of markup languages and being able to write something that can parse them. Doing this, and keeping all the edge cases in mind, for something like HTML is already incredibly complex, and if you extend the scope of the bespoke markup language to include XML as well, then it becomes full-time work for a team of people.</p>
			<p>Thankfully, we are using Python, and Python has a very mature and stable library to do all of the complicated jobs for us. This library is called <code>BeautifulSoup</code> (it is, at present, in version 4 and thus we will call it <code>bs4</code> in short from now on). <code>bs4</code> is a library for getting data from HTML or XML documents, and it gives you a nice, normalized, idiomatic way of navigating and querying a document. It does not include a parser but it supports different ones.</p>
			<h3 id="_idParaDest-166"><a id="_idTextAnchor189"/>Structure of HTML</h3>
			<p>Before we jump into <code>bs4</code> and start working with it, we need to examine the structure of a HTML document. <strong class="bold">H</strong>yper <strong class="bold">T</strong>ext <strong class="bold">M</strong>arkup <strong class="bold">L</strong>anguage is a structured way of telling web browsers about the organization of a web page, meaning which kind of elements (text, image, video, and so on) come from where, in which place inside the page they should appear, what they look like, what they contain, and how they will behave with user input. HTML5 is the latest version of HTML. An HTML document can be viewed as a tree, as we can see from the following diagram:</p>
			<div><div><img src="img/C11065_05_27.jpg" alt="Figure 5.27: HTML structure" width="1800" height="1013"/>
				</div>
			</div>
			<h6>Figure 5.27: HTML structure</h6>
			<p>Each node of the tree represents one element in the document. An element is anything that starts with <code>&lt;</code> and ends with <code>&gt;</code>. For example, <code>&lt;html&gt;</code>, <code>&lt;head&gt;</code>, <code>&lt;p&gt;</code>, <code>&lt;br&gt;</code>, <code>&lt;img&gt;</code>, and so on are various HTML elements. Some elements have a start and end element, where the end element begins with "&lt;/" and has the same name as the start element, such as <code>&lt;p&gt;</code> and <code>&lt;/p&gt;</code>, and they can contain an arbitrary number of elements of other types in them. Some elements do not have an ending part, such as the <code>&lt;br /&gt;</code> element, and they cannot contain anything within them.</p>
			<p>The only other thing that we need to know about an element at this point is the fact that elements can have attributes, which are there to modify the default behavior of an element. An <code>&lt;a&gt;</code> element requires a <code>href</code> attribute to tell the browser which website it should navigate to when that particular <code>&lt;a&gt;</code> is clicked, like this: <code>&lt;a href="http://cnn.com"&gt;</code>. The CNN news channel, <code>&lt;/a&gt;</code>, will take you to cnn.com when clicked:</p>
			<div><div><img src="img/C11065_05_28.jpg" alt="Figure 5.28: CNN news channel hyperlink" width="1214" height="116"/>
				</div>
			</div>
			<h6>Figure 5.28: CNN news channel hyperlink</h6>
			<p>So, when you are at a particular element of the tree, you can visit all the children of that element to get the contents and attributes of them. </p>
			<p>Equipped with this knowledge, let's see how we can read and query data from a HTML document.</p>
			<p>In this topic, we will cover the reading and parsing of web pages, but we do not request them from a live website. Instead, we read them from disk. A section on reading them from the internet will follow in a future chapter. </p>
			<h3 id="_idParaDest-167">Exercise 69: Reading an HTML <a id="_idTextAnchor190"/><a id="_idTextAnchor191"/>file and Extracting its Contents Using BeautifulSoup</h3>
			<p>In this exercise, we will do the simplest thing possible. We will import the <code>BeautifulSoup</code> library and then use it to read an HTML document. Then, we will examine the different kinds of objects it returns. While doing the exercises for this topic, you should have the example HTML file open in a text editor all the time so that you can check for the different tags and their attributes and contents:</p>
			<ol>
				<li value="1">Import the <code>bs4</code> library:<pre>from bs4 import BeautifulSoup</pre></li>
				<li>Please download the following test HTML file and save it on your disk and the use bs4 to read it from the disk:<pre>with open("test.html", "r") as fd:    soup = BeautifulSoup(fd)    print(type(soup))</pre><p>The output is as follows:</p><pre>&lt;class 'bs4.BeautifulSoup'&gt;</pre><p>You can pass a file handler directly to the constructor of the <code>BeautifulSoup</code> object and it will read the contents from the file that the handler is attached to. We will see that return-type is an instance of <code>bs4.BeautifulSoup</code>. This class holds all the methods we need to navigate through the DOM tree that the document represents. </p></li>
				<li>Print the contents of the file in a nice way by using the <code>prettify</code> method from the class like this:<pre>print(soup.prettify())</pre><p>The output is as follows:</p><div><img src="img/C11065_05_29.jpg" alt="Figure 5.29: Contents of the HTML file" width="585" height="325"/></div><h6>Figure 5.29: Contents of the HTML file</h6><p>The same information can also be obtained by using the <code>soup.contents</code> member variable. The differences are: first, it won't print anything pretty and, second, it is essentially a list. </p><p>If we look carefully at the c<a id="_idTextAnchor192"/>ontents of the HTML file in a separate text editor, we will see that there are many paragraph tags, or <code>&lt;p&gt;</code> tags. Let's read content from one such <code>&lt;p&gt;</code> tag. We can do that using the simple <code>.</code> access modifier as we would have done for a normal member variable of a class. </p></li>
				<li>The magic of <code>bs4</code> is the fact that it gives us this excellent way to dereference tags as member variables of the <code>BeautifulSoup</code> class instance:<pre>with open("test.html", "r") as fd:
    soup = BeautifulSoup(fd)
    print(soup.p)</pre><p>The output is as follows:</p><div><img src="img/C11065_05_30.jpg" alt="Figure 5.30: Text from the &lt;p&gt; tag " width="1267" height="620"/></div><h6>Figure 5.30: Text from the &lt;p&gt; tag </h6><p>As we can see, this is the content of a <code>&lt;p&gt;</code> tag. </p><p>We saw how to read a tag<a id="_idTextAnchor193"/> in the last exercise, but we can easily see the problem with this approach. When we look into our HTML document, we can see that we have more than one <code>&lt;p&gt;</code> tag there. How can we access all the <code>&lt;p&gt;</code> tags? It turns out that this is easy. </p></li>
				<li>Use the <code>findall</code> method to extract the content from the tag:<pre>with open("test.html", "r") as fd:
    soup = BeautifulSoup(fd)
    all_ps = soup.find_all('p')
    print("Total number of &lt;p&gt;  --- {}".format(len(all_ps)))</pre><p>The output is as follows:</p><pre>Total number of &lt;p&gt;  --- 6</pre><p>This will print 6, which is exactly the number of <code>&lt;p&gt;</code>tags in the document. </p><p>We have seen how to access all<a id="_idTextAnchor194"/> the tags of the same type. We have also seen how to get the content of the entire HTML document. </p></li>
				<li>Now, we will see how to get the contents under a particular HTML tag, as follows: <pre>with open("test.html", "r") as fd:
    soup = BeautifulSoup(fd)
    table = soup.table
    print(table.contents)</pre><p>The output is as follows:</p><div><img src="img/C11065_05_31.jpg" alt="Figure 5.31: Content under the &lt;table&gt; tag " width="1250" height="861"/></div><h6>Figure 5.31: Content under the &lt;table&gt; tag </h6><p>Here, we are getting the (first) table from the document and then using the same "<code>.</code>" notation, to get the contents under that tag. </p><p>We saw in the previous exercise  that we can acces<a id="_idTextAnchor195"/>s the entire content under a particular tag. However, HTML is represented as a tree and we are able to traverse the children of a particular node. There are a few ways to do this. </p></li>
				<li>The first way is by using the <code>children</code> generator from any <code>bs4</code> instance, as follows:<pre>with open("test.html", "r") as fd:
    soup = BeautifulSoup(fd)
    table = soup.table
    for child in table.children:
        print(child)
        print("*****")</pre><p>When we execute the code, we will see something like the following:</p><div><img src="img/C11065_05_32.jpg" alt="Figure 5.32: Traversing children of a table node" width="1330" height="916"/></div><h6>Figure 5.32: Traversing the children of a table node</h6><p>It seems that the loop has only been executed twice! Well, the problem with the "<code>children</code>" generator is that it only takes into account the immediate children of the tag. We have <code>&lt;tbody&gt;</code> under the <code>&lt;table&gt;</code> and our whole table structure is wrapped in it. That's why it was considered a single child of the <code>&lt;table&gt;</code> tag. </p><p>We looked into how to browse the<a id="_idTextAnchor196"/> immediate children of a tag. We will see how we can browse all the possible children of a tag and not only the immediate one. </p></li>
				<li>To do that, we use the <code>descendants</code> generator from the <code>bs4</code> instance, as follows: <pre>with open("test.html", "r") as fd:
    soup = BeautifulSoup(fd)
    table = soup.table
    children = table.children
    des = table.descendants
    print(len(list(children)), len(list(des)))</pre><p>The output is as follows:</p><pre>9 61</pre></li>
			</ol>
			<p>The comparison print at the end of the code block will show us the difference between <code>children</code> and <code>descendants</code>. The length of the list we got from <code>children</code> is only 9, whereas the length of the list we got from <code>descendants</code> is 61.</p>
			<h3 id="_idParaDest-168">Exercise 70: DataFrames and Beau<a id="_idTextAnchor197"/><a id="_idTextAnchor198"/>tifulSoup</h3>
			<p>So far, we have seen some basic ways to navigate the tags inside a HTML document using <code>bs4</code>. Now, we are going to go one step further and use the power of <code>bs4</code> combined with the power of pandas to generate a DataFrame out of a plain HTML table. This particular k<a id="_idTextAnchor199"/>nowledge is very useful for us. With the knowledge we will acquire now, it will be fairly easy for us to prepare a pandas DataFrame to perform EDA (exploratory data analysis) or modeling. We are going to show this process on a simple small table from the test HTML file, but the exact same concept applies to any arbitrarily large table as well:</p>
			<ol>
				<li value="1">Import <code>pandas</code> and read the document, as follows:<pre>import pandas as pd
fd = open("test.html", "r")
soup = BeautifulSoup(fd)
data = soup.findAll('tr')
print("Data is a {} and {} items long".format(type(data), len(data)))</pre><p>The output is as follows:</p><pre>Data is a &lt;class 'bs4.element.ResultSet'&gt; and 4 items long</pre></li>
				<li>Check the original table structure in the HTML source. You will see that the first row is the column headings and all of the following rows are the data. We assign two different variables for the two sections, as follows:<pre>data_without_header = data[1:]
headers = data[0]
header</pre><p>The output is as follows:</p><pre>&lt;tr&gt;
&lt;th&gt;Entry Header 1&lt;/th&gt;
&lt;th&gt;Entry Header 2&lt;/th&gt;
&lt;th&gt;Entry Header 3&lt;/th&gt;
&lt;th&gt;Entry Header 4&lt;/th&gt;
&lt;/tr&gt;</pre><h4>Note</h4><p class="callout">Keep in mind that the art of scraping a HTML page goes hand in hand with an understanding of the source HTML structure. So, whenever you want to scrape a page, the first thing you need to do is right-click on it and then use "View Source" from the browser to see the source HTML.</p></li>
				<li>Once we have separated the two sections, we need two list comprehensions to make them ready to go in a DataFrame. For the header, this is easy:<pre>col_headers = [th.getText() for th in headers.findAll('th')]
col_headers</pre><p>The output is as follows:</p><pre>['Entry Header 1', 'Entry Header 2', 'Entry Header 3', 'Entry Header 4']</pre></li>
				<li>Data preparation is a bit tricky for a pandas DataFrame. You need to have a two-dimensional list, which is a list of lists. We accomplish that in the following way:<pre>df_data = [[td.getText() for td in tr.findAll('td')] for tr in data_without_header]
df_data</pre><p>The output is as follows:</p><div><img src="img/C11065_05_33.jpg" alt="" width="521" height="164"/></div><h6>Figure 5.33: Output as a two-dimensional list</h6></li>
				<li>Invoke the <code>pd.DataFrame</code> method and supply the right arguments by using the following code: <pre>df = pd.DataFrame(df_data, columns=col_headers)
df.head()</pre></li>
			</ol>
			<div><div><img src="img/C11065_05_34.jpg" alt="" width="1262" height="251"/>
				</div>
			</div>
			<h6>Figure 5.34: Output in tabular format with column headers</h6>
			<h3 id="_idParaDest-169">Exercise 71: Exporting a DataFrame<a id="_idTextAnchor200"/><a id="_idTextAnchor201"/> as an Excel File</h3>
			<p>In this exercise, we will see how we can save a DataFrame as an Excel file. Pandas can natively do this, but it needs the help of the <code>openpyxl</code> library to achieve this goal:</p>
			<ol>
				<li value="1">Install the <code>openpyxl</code> library by using the following command:<pre>!pip install openpyxl</pre></li>
				<li>To save the DataFrame as an Excel file, use the following command from inside of the Jupyter notebook:<pre>writer = pd.ExcelWriter('test_output.xlsx')df.to_excel(writer, "Sheet1")writer.save()
writer</pre><p>The output is as follows:</p><pre>&lt;pandas.io.excel._XlsxWriter at 0x24feb2939b0&gt;</pre></li>
			</ol>
			<h3 id="_idParaDest-170">Exercise 72: Stacking URLs from a <a id="_idTextAnchor202"/><a id="_idTextAnchor203"/>Document using bs4</h3>
			<p>Previously (while discussing stack), we explained how important it is to have a stack that we can push the URLs from a web page to so that we can pop them at a later time to follow each of them. Here, in this exercise, we will see how that works. </p>
			<p>In the given test, HTML file links or <code>&lt;a&gt;</code> tags are under a <code>&lt;ul&gt;</code> tag, and each of them is contained inside a <code>&lt;/li&gt;</code> tag:</p>
			<ol>
				<li value="1">Find all the <code>&lt;a&gt;</code> tags by using the following command:<pre>d = open("test.html", "r")
soup = BeautifulSoup(fd)
lis = soup.find('ul').findAll('li')
stack = []
for li in lis:    a = li.find('a', href=True)</pre></li>
				<li>Define a stack before you start the loop. Then, inside the loop, use the <code>append</code> method to push the links in the stack:<pre>stack.append(a['href'])</pre></li>
				<li>Print the stack:</li>
			</ol>
			<div><div><img src="img/C11065_05_35.jpg" alt="" width="1656" height="78"/>
				</div>
			</div>
			<h6>Figure 5.35: Output of the stack</h6>
			<h3 id="_idParaDest-171">Activity 7: Reading Tabular Data from a Web Pag<a id="_idTextAnchor204"/>e and Creating DataFrames</h3>
			<p>In this activity, you have been given a Wikipedia page where you have the GDP of all countries listed. You have been asked to create three <code>DataFrames</code> from the three sources mentioned in the page (<a href="https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)">https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)</a>):</p>
			<p>You will have to do the following:</p>
			<ol>
				<li value="1">Open the page in a separate Chrome/Firefox tab and use something like an <strong class="bold">Inspect Element</strong> tool to view the source HTML and understand its structure</li>
				<li>Read the page using bs4</li>
				<li>Find the table structure you will need to deal with (how many tables there are?)</li>
				<li>Find the right table using bs4</li>
				<li>Separate the source names and their corresponding data</li>
				<li>Get the source names from the list of sources you have created</li>
				<li>Separate the header and data from the data that you separated before for the first source only, and then create a DataFrame using that</li>
				<li>Repeat the last task for the other two data sources<h4>Note</h4><p class="callout">The solution for this activity can be found on page 308.</p></li>
			</ol>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor205"/>Summary</h2>
			<p>In this topic, we looked at the structure of an HTML document. HTML documents are the cornerstone of the World Wide Web and, given the amount of data that's contained on it, we can easily infer the importance of HTML as a data source. </p>
			<p>We learned about bs4 (BeautifulSoup4), a Python library that gives us Pythonic ways to read and query HTML documents. We used bs4 to load an HTML document and also explored several different ways to navigate the loaded document. We also got necessary information about the difference between all of these methods. </p>
			<p>We looked at how we can create a pandas DataFrame from an HTML document (which contains a table). Although there are some built-in ways to do this job in pandas, they fail as soon as the target table is encoded inside a complex hierarchy of elements. So, the knowledge we gathered in this topic by transforming an HTML table into a pandas DataFrame in a step-by-step manner is invaluable. </p>
			<p>Finally, we looked at how we can create a stack in our code, where we push all the URLs that we encounter while reading the HTML file and then use them at a later time. In the next chapter, we will discuss list comprehensions, zip, format and outlier detection and cleaning.</p>
		</div>
		<div><div></div>
		</div>
	</div>



  </body></html>