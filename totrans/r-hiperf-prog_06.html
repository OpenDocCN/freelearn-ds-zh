<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Simple Tweaks to Use Less RAM</h1></div></div></div><p>So far, we have learned the techniques to overcome CPU limitations and improve the speed of R programs. As you can recall from <a class="link" href="ch01.html" title="Chapter 1. Understanding R's Performance – Why Are R Programs Sometimes Slow?">Chapter 1</a>, <em>Understanding R's Performance – Why Are R Programs Sometimes Slow?</em> that another key constraint of R is memory. All the data that an R program needs to perform its tasks on must be loaded into the computer's memory or RAM. RAM is also needed for any intermediate computations, so the amount of RAM needed to process a given dataset can be many times the size of the dataset, depending on the type of tasks or algorithms being executed. This can become a problem when a large dataset needs to be processed, or when there is little RAM available to complete the tasks.</p><p>In this chapter and the next, we will learn how to optimize the RAM utilization of R programs so that memory-intensive tasks can be executed successfully.</p><p>This chapter covers:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reusing objects without taking up more memory</li><li class="listitem" style="list-style-type: disc">Removing intermediate data when it is no longer needed</li><li class="listitem" style="list-style-type: disc">Calculating values on the fly instead of storing them persistently</li><li class="listitem" style="list-style-type: disc">Swapping active and nonactive data</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Reusing objects without taking up more memory</h1></div></div></div><p>The first tweak <a id="id174" class="indexterm"/>takes advantage of how R manages the memory of objects using a <a id="id175" class="indexterm"/>
<strong>copy-on-modification</strong> model. In this model, when a copy of an object <code class="literal">x</code> is made, for example <a id="id176" class="indexterm"/>with <code class="literal">y &lt;- x</code>, it is not actually copied in the memory. Rather, the new variable <code class="literal">y</code> simply points to the same block of memory that contains <code class="literal">x</code>. The first time when <code class="literal">y</code> is modified, R copies the data into a new block of memory so that <code class="literal">x</code> and <code class="literal">y</code> have their own copies of the data. That is why this model of memory management is called copy-on-modification. What this means is that new objects can sometimes be created from existing objects without taking up additional memory. To identify potential memory bottlenecks and manage the memory utilization of R programs, it is helpful to understand when R copies data and when it does not.</p><p>Take for example the following code, which generates a numeric vector <code class="literal">x</code> with 1 million elements and creates a list <code class="literal">y</code> that contains two copies of <code class="literal">x</code>. We can examine the size of the objects using the <code class="literal">object.size()</code> function:</p><div><pre class="programlisting">x &lt;- runif(1e6)
print(object.size(x), units = "auto")
## 7.6 Mb
y &lt;- list(x, x)
print(object.size(y), units = "auto")
## 15.3 Mb</pre></div><p>At first glance, it looks like there are two objects: <code class="literal">x</code>, which takes up 7.6 MB of the memory, and <code class="literal">y</code>, which takes up 15.3 MB. However, the memory utilization can be measured in a different way, and with surprising results:</p><div><pre class="programlisting">library(pryr)
object_size(x)
## 8 MB
object_size(y)
## 8 MB</pre></div><p>The <code class="literal">object_size()</code> function from the CRAN package <code class="literal">pryr</code> measures the sizes of <code class="literal">x</code> and <code class="literal">y</code> slightly differently and more accurately than <code class="literal">object.size()</code> from the base R. It reports that <code class="literal">y</code>, which contains two numeric vectors takes up only 8 MB—the same as <code class="literal">x</code>, which is a single numeric vector of the same length. How can that be? The <code class="literal">address()</code> function from the <code class="literal">pryr</code> package reveals the actual blocks of memory that each object points to:</p><div><pre class="programlisting">address(x)
## [1] "0x10f992000"
address(y)
## [1] "0x7ff18b30e478"
address(y[[1]])
## [1] "0x10f992000"
address(y[[2]])
## [1] "0x10f992000"</pre></div><p>As expected, <code class="literal">y</code>, a list, points to a different memory location than <code class="literal">x</code>, a numeric vector, indicating<a id="id177" class="indexterm"/> that it is a <a id="id178" class="indexterm"/>different object. But the two elements of <code class="literal">y</code> point to the original object <code class="literal">x</code> in the memory. R is smart about not copying objects unnecessarily. In this case, it simply created two pointers in <code class="literal">y</code> that point to <code class="literal">x</code>. This is so efficient that in fact, <code class="literal">x</code> and <code class="literal">y</code> combined together take up only 8 MB, which is the size of <code class="literal">x</code>!</p><div><pre class="programlisting">object_size(x, y)
## 8 MB</pre></div><div><div><h3 class="title"><a id="note07"/>Note</h3><p>Actually, a tiny bit of extra memory is needed to store <code class="literal">y</code> and its pointers to <code class="literal">x</code>, but that is negligible and does not show up in this measurement.</p></div></div><p>When one of the vectors in <code class="literal">y</code> is modified, R creates a new copy, since this vector is now different from <code class="literal">x</code>:</p><div><pre class="programlisting">y[[1]][1] &lt;- 0
address(x)
## [1] "0x10f992000"
address(y[[1]])
## [1] "0x110134000"
address(y[[2]])
## [1] "0x10f992000"
object_size(y)
## 16 MB
object_size(x, y)
## 16 MB</pre></div><p>The <code class="literal">y[[1]]</code> vector now points to a different vector in the memory than <code class="literal">x</code> and <code class="literal">y[[2]]</code>. As a result, <code class="literal">y</code> takes up 16 MB of RAM while <code class="literal">x</code> and <code class="literal">y</code> combined still take up only 16 MB (since <code class="literal">y[[2]]</code> still points to <code class="literal">x</code>). Another way to track this is when an object is copied to use <code class="literal">tracemem()</code>, which gives an alert whenever the object being tracked is copied. See what happens to <code class="literal">y[[2]]</code> when it is modified:</p><div><pre class="programlisting">tracemem(y[[2]])
## [1] "&lt;0x10f992000&gt;"
y[[2]][1] &lt;- 0
## tracemem[0x10f992000 -&gt; 0x1108d6000]: 
untracemem(y[[2]])</pre></div><p>The <code class="literal">tracemem[0x10f992000 -&gt; 0x1108d6000]</code> line indicates that a copy of vector <code class="literal">y[[2]]</code> was<a id="id179" class="indexterm"/> made when it <a id="id180" class="indexterm"/>was modified and gives the memory address of the new copy. Now, <code class="literal">x</code>, <code class="literal">y[[1]]</code>, and <code class="literal">y[[2]]</code> are different objects in memory, hence the total memory used by <code class="literal">x</code> and <code class="literal">y</code> is 24 MB:</p><div><pre class="programlisting">address(x)
## [1] "0x10f992000"
address(y[[1]])
## [1] "0x110134000"
address(y[[2]])
## [1] "0x1108d6000"
object_size(y)
## 16 MB
object_size(x, y)
## 24 MB</pre></div><p>When an element of <code class="literal">y</code> is modified, a copy of <code class="literal">x</code> needs to be made so that the original object <code class="literal">x</code> is unmodified. Otherwise, modifying one object will cause unintended modifications to the other, resulting in errors in the program that might be difficult to find.</p><p>The way in which R determines whether an object should be copied is by tracking whether other objects refer to it. When <code class="literal">y</code> was created, R knew that <code class="literal">x</code> is being used elsewhere and a copy needs to be made when it is modified.</p><div><div><h3 class="title"><a id="note08"/>Note</h3><p>R counts only up to two references, which is sufficient for it to determine whether to copy an object or not. As long as two or more variables refer to the same object, R will make a copy of it when it is modified.</p></div></div><p>Now, when we modify <code class="literal">x</code> for the first time, R makes a copy of it because <code class="literal">x</code> had been referred to by <code class="literal">y</code> before. Even though <code class="literal">y</code> now has its own copies of the data, R errs on the side of caution and makes a copy of <code class="literal">x</code> to avoid potential conflicts. Subsequent modifications to <code class="literal">x</code>, however, do not lead to unnecessary copying, as the new copy of <code class="literal">x</code> is not being used anywhere else, as this example shows:</p><div><pre class="programlisting">tracemem(x)
## [1] "&lt;0x10f992000&gt;"
x[1]&lt;- 1
## tracemem[0x10f992000 -&gt; 0x111078000]: 
x[1]&lt;- 1
x[1]&lt;- 0.5
x[2] &lt;- 0.3
untracemem(x)</pre></div><p>In general, as long <a id="id181" class="indexterm"/>as a vector has not been referred to by any other object, R allows it to be <a id="id182" class="indexterm"/>modified in place, avoiding the CPU and RAM overheads to make copies of the vector.</p><div><div><h3 class="title"><a id="note09"/>Note</h3><p>This example does not work in RStudio:</p><div><pre class="programlisting">tracemem(x)
## [1] "&lt;0x10e73c000&gt;"
x[1] &lt;- 0
## tracemem[0x10e73c000 -&gt; 0x110d66000]: 
x[2] &lt;- 1
## tracemem[0x110d66000 -&gt; 0x115478000]: 
x[3] &lt;- 0.5
## tracemem[0x115478000 -&gt; 0x115c79000]: 
untracemem(x)</pre></div><p>This is because RStudio keeps a reference of every object in its own environment, so R thinks that <code class="literal">x</code> is being referred to elsewhere. It creates a copy of <code class="literal">x</code> every time it is modified, to be safe.</p></div></div><p>Now that we understand when R copies data, we can optimize R programs in order to avoid copying the data unnecessarily. For example, say we have two vectors containing the ages and genders of 1 million customers:</p><div><pre class="programlisting">customer.age &lt;- sample(18:100, 1e6, replace=TRUE)
customer.gender &lt;- sample(c("Male", "Female"), 1e6, TRUE)</pre></div><p>The retailer uses "<code class="literal">cust #"</code> for customer IDs. We want to label each vector with the customer IDs so that we can easily look up the information by the customer ID, using expressions like <code class="literal">customer.age["cust 1"]</code>. One way to do this is to separately construct the names for each vector. The two vectors combined will then take up 84 MB of memory:</p><div><pre class="programlisting">names(customer.age) &lt;- paste("cust", 1:1e6)
names(customer.gender) &lt;- paste("cust", 1:1e6)
object_size(customer.age, customer.gender)
## 84 MB</pre></div><p>Alternatively, the names can be stored in a separate vector that the age and gender vectors then can refer to:</p><div><pre class="programlisting">customer.names &lt;- paste("cust", 1:1e6)
names(customer.age) &lt;- customer.names
names(customer.gender) &lt;- customer.names
object_size(customer.age, customer.gender, customer.names)
## 76 MB</pre></div><p>This simple <a id="id183" class="indexterm"/>change resulted in a saving of 8 MB of memory. On larger, more complex data structures, these <a id="id184" class="indexterm"/>savings from not copying data unnecessarily can be significant.</p><p>The same copy-on-modification behavior applies to function arguments. When an object is passed to a function, it is not copied; R simply provides a pointer to the object. If, however, the object is modified within the function, R creates a copy of that object in the function's environment so that the original object is not modified in any way outside the function. In programming language <a id="id185" class="indexterm"/>parlance, this is called <strong>pass by value</strong> because functions are given the value of their arguments. This is part of R's design as a <em>functional programming language</em>. Contrast <a id="id186" class="indexterm"/>this with <strong>pass by reference</strong>, which is sometimes used in other programming languages, such as Java and C/C++, where functions can be given references or pointers to the memory's addresses. In this case, functions can modify their arguments without creating additional copies in memory, and the modifications persist even after the functions exit.</p><p>A consequence of R's pass by value model for functions means that many functions need to make a copy of the data they are given. For example, calling <code class="literal">sort(x)</code> returns a new vector with the sorted values of <code class="literal">x</code>, rather than sorting the values in place (which is often the practice in Java and C/C++). Calling functions like <code class="literal">sort()</code> often requires additional free memory that is at least as large as the original data and sometimes more.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec35"/>Removing intermediate data when it is no longer needed</h1></div></div></div><p>In large R programs, objects are created in many places. Often, an object that is created in <a id="id187" class="indexterm"/>an earlier <a id="id188" class="indexterm"/>part of the program is not needed in later parts of the program. When faced with memory limits, it is useful to free up memory taken up by objects when they are no longer needed, so that subsequent parts of the program can run successfully.</p><p>The main tool for this is the <code class="literal">rm()</code> function that removes a given list of objects from the current R environment.</p><p>In the following <a id="id189" class="indexterm"/>example, we have a data frame containing 500,000 transactions from a retail store and the items within each transaction. Each row of the data frame represents a <a id="id190" class="indexterm"/>unique transaction-item pair that occurred in a sales database. Although, we have to generate the data for this example in a real business context, this data could be extracted from a retailer's sales database:</p><div><pre class="programlisting">trans.lengths &lt;- rpois(5e5, 3) + 1L
trans &lt;- rep.int(1:5e5, trans.lengths)
items &lt;- unlist(lapply(trans.lengths, sample.int, n = 1000))
sales.data &lt;- data.frame(trans = trans, item = items)</pre></div><p>The data looks like this where, for example, the first nine rows indicate that transaction 1 includes items 680, 846, 196, and so on (the data that you generate might look different):</p><div><pre class="programlisting">head(sales.data, 15)
##    trans item
## 1      1  680
## 2      1  846
## 3      1  196
## 4      1  191
## 5      1   20
## 6      1  852
## 7      1  623
## 8      1  206
## 9      1  775
## 10     2  624
## 11     2   31
## 12     2  718
## 13     2  190
## 14     3  482
## 15     3  946</pre></div><p>Our task is to find common baskets of items, that is, items that appear frequently together in the same transactions, or frequent itemsets. The <code class="literal">apriori()</code> function in the <code class="literal">arules</code> CRAN package can be used to find these frequent itemsets. But it does not accept data in the form of transaction-item pairs that we can extract from a sales database. Instead, <code class="literal">arules</code> defines the <code class="literal">transactions</code> class that it accepts as input. We need to split the items column of the data frame into the different transactions and then coerce the resulting list into a <code class="literal">transactions</code> object:</p><div><pre class="programlisting">library(arules)
trans.list &lt;- split(sales.data$item, sales.data$trans)
trans.arules &lt;- as(trans.list, "transactions")</pre></div><p>We can now call the <code class="literal">apriori()</code> function to find the frequent itemsets. In this example, we want itemsets that have a support of at least 0.3, that is, sets of items that appear in at least 30 percent of the transactions.</p><div><pre class="programlisting">freq.itemsets &lt;- apriori(trans.arules, list(support = 0.3))</pre></div><p>When we <a id="id191" class="indexterm"/>started with the data frame of transaction-item pairs, we had to convert it into a few different formats before the data could be used by <code class="literal">apriori()</code>. Each of these intermediate <a id="id192" class="indexterm"/>data structures takes up valuable memory:</p><div><pre class="programlisting">object_size(sales.data)
## 16 MB
object_size(trans.list)
## 62.1 MB
object_size(trans.arules)
## 44 MB</pre></div><p>When the dataset is large or when memory is scarce, <code class="literal">apriori()</code> might fail to execute as it runs out of memory. In such situations, <code class="literal">rm()</code> can be used to free up memory by deleting unneeded objects before calling <code class="literal">apriori()</code> or even between each data transformation step. The following code illustrates this:</p><div><pre class="programlisting">trans.list &lt;- split(sales.data$item, sales.data$trans)
rm(sales.data)
trans.arules &lt;- as(trans.list, "transactions")
rm(trans.list)
freq.itemsets &lt;- apriori(trans.arules, list(support = 0.3))</pre></div><p>Another technique to automatically remove temporary variables is to encapsulate code in functions. This way, any variables created in the function will automatically be deleted when the function exists. For example, say we only need to remove temporary variables before calling <code class="literal">apriori()</code>, because that is when the code tends to run into memory limits. We can encapsulate all the previous lines of codes in a function:</p><div><pre class="programlisting"># Automatically remove temporary variables by encapsulating code
# in a function
prepare_data &lt;- function(sales.data) {
    trans.list &lt;- split(sales.data$item, sales.data$trans)
    trans.arules &lt;- as(trans.list, "transactions")
    return(trans.arules)
}
trans.arules &lt;- prepare_data(sales.data)
freq.itemsets &lt;- apriori(trans.arules, list(support = 0.3))</pre></div><p>After calling <code class="literal">prepare_data()</code>, any temporary variables created within it are deleted without having<a id="id193" class="indexterm"/> to explicitly call <code class="literal">rm()</code>. In this case, only one temporary variable, <code class="literal">trans.list</code> is deleted. But the <a id="id194" class="indexterm"/>same technique can be used when more temporary variables are declared in the function. Not only is this means of removing temporary variables convenient, it also makes the code more readable and easier to maintain.</p><p>In large R programs, periodic removal of large data structures can help to minimize overall memory usage. When <code class="literal">rm()</code> is called, the memory might not be freed and returned<a id="id195" class="indexterm"/> to the operating system immediately. Rather, R's <strong>garbage collector</strong> automatically frees the memory when it is needed, or when the amount of memory from removed objects exceeds a threshold.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Calculating values on the fly instead of storing them persistently</h1></div></div></div><p>While executing an R program, it is sometimes convenient to cache all the data needed by the program, including the results of intermediate computations into a RAM prior <a id="id196" class="indexterm"/>to execution. During the execution, as and when the program needs to access any part of the data, it can be done very rapidly as all the data has been loaded into the R workspace. Caching intermediate results in RAM can save computational time significantly, especially when they are accessed frequently, as unnecessary recalculation of the data is avoided.</p><p>This is not a problem when the cached data can fit into RAM. However, it becomes a problem when there is not enough memory space to contain the data. The good news is, in many cases, the program does not need all parts of the data at the same time. One solution is to swap in and out portions of the data between RAM and the hard disk. Because disk I/O is slow, as we have established in <a class="link" href="ch01.html" title="Chapter 1. Understanding R's Performance – Why Are R Programs Sometimes Slow?">Chapter 1</a>, <em>Understanding R's Performance – Why Are R Programs Sometimes Slow?</em>, this approach might result in a slow execution. A better solution is to calculate and recalculate portions of the data that are needed at the moment. Yes, calculation costs computational time, but it is often less costly than disk I/O.</p><p>Let's look at an example from a common task in data science: hierarchical clustering. In some commonly used variants of hierarchical clustering, such as the single, complete, and average linkage, one important step in the algorithm is to calculate the distance matrix <a id="id197" class="indexterm"/>between every pair of observations in the dataset, and then decide which pair of observations is the closest to each other. This step can be accomplished by the following code, in which we have artificially created a random dataset <code class="literal">A</code>, with 10,000 observations (rows) and 10 features (columns). The code first calculates the distance matrix of <code class="literal">A</code>, sets the diagonal element of the distance matrix to <code class="literal">NA</code> because an observation is always closest to itself and finally finds the closest pair using the <code class="literal">which()</code> function. In this instance, observations 6778 and 6737 are found to be the closest pair. To execute this program, about 801 MB of RAM is required, as can be seen from the outputs of <code class="literal">object_size()</code> in the following code. This is because even though the dataset only occupies 800 KB, its distance matrix needs about a quadratic amount of the original space because it is storing all the pairwise distances:</p><div><pre class="programlisting">A &lt;- matrix(rnorm(1E5), 1E4, 10)
dist_mat &lt;- as.matrix(dist(A))
diag(dist_mat) &lt;- NA
res1 &lt;- which(dist_mat == min(dist_mat, na.rm=T), arr.ind = T)[1,]
res1
##  row  col 
## 6778 6737 
object_size(A)
## 800 kB
object_size(dist_mat)
## 801 MB</pre></div><p>On close inspection, we do not actually need the whole distance matrix at once to find the closest pair. It is possible to calculate the set of pairwise distances between the first observation and the rest of the observations to get the minimum pair for this set; repeat the process for the second observation and then compare the minimum of the two sets and so on. Doing this incurs additional steps and hence longer computational time, but it demands only a small fraction of RAM compared to that of the preceding code (because only a chunk of the distance matrix is maintained at a given time). The code to do this is shown below. It first calculates the distances between all the observations in <code class="literal">A</code> with the observation 1 in <code class="literal">A</code> (using the <code class="literal">pdist</code> package) and then finds and saves the closest pair only for this chunk to a temporary list <code class="literal">output</code>. This process is repeated for the observations 2, 3, …, 10,000 in <code class="literal">A</code> using <code class="literal">lapply</code>. The set of closest pairs from every chunk is stored in the <code class="literal">temp_res</code> list. The final step is to find the minimum pair among this set and store it in the variable <code class="literal">res2</code>. Evaluating the output of <code class="literal">res2</code> reveals the same result as the one found using the preceding code. However, this time we need only 2.7 MB of memory for the <code class="literal">temp_res</code> list.</p><div><pre class="programlisting">library(pdist)
temp_res &lt;- lapply(1:nrow(A), function(x) {
  temp &lt;- as.matrix(pdist(X = A, Y = A[x,]));
  temp[x] &lt;- NA;
  output_val &lt;- min(temp, na.rm=T);
  output_ind &lt;- c(x, which(temp == output_val));
  output &lt;- list(val = output_val, ind = output_ind);
})
val_vec &lt;- sapply(temp_res, FUN=function(x) x$val)
ind_vec &lt;- sapply(temp_res, FUN=function(x) x$ind)
res2 &lt;- ind_vec[, which.min(val_vec)]
res2
## [1] 6778 6737
object_size(temp_res)
## 2.72 MB
object_size(val_vec)
## 80 kB
object_size(ind_vec)
## 80.2 kB</pre></div><p>Indeed the time <a id="id198" class="indexterm"/>needed by the second approach is longer. We can speed this up by parallelizing the code, for example by substituting <code class="literal">lapply()</code> with <code class="literal">parLApply()</code> of the <code class="literal">parallel</code> package (see <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <em>Multiplying Performance with Parallel Computing</em>). In practice, for a specific case of finding the closest pair of observations efficiently without storing a full distance matrix, we can leverage optimized k-nearest neighbor functions, like the <code class="literal">knn()</code> function in the <code class="literal">FNN</code> package. Where such an alternative optimized package is unavailable, the approach of calculating values on the fly as illustrated in the preceding code is useful to reduce memory use.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec37"/>Swapping active and nonactive data</h1></div></div></div><p>In some situations, large objects that are removed to free up memory are needed later in the <a id="id199" class="indexterm"/>program. R provides tools to save data to the disk and reload them later when enough <a id="id200" class="indexterm"/>memory is available. Returning to the retail sales data example, suppose that we need the <code class="literal">sales.data</code> data frame for further processing after mining for frequent itemsets. We can save it to the disk using <code class="literal">saveRDS()</code> and reload it later using <code class="literal">readRDS()</code>:</p><div><pre class="programlisting">trans.list &lt;- split(sales.data$item, sales.data$trans)
saveRDS(sales.data, "sales.data.rds")
rm(sales.data)
trans.arules &lt;- as(trans.list, "transactions")
rm(trans.list)
freq.itemsets &lt;- apriori(trans.arules, list(support = 0.3))
sales.data &lt;- readRDS("sales.data.rds")
# Perform further processing with sales.data</pre></div><p>The <code class="literal">saveRDS()</code> and <code class="literal">readRDS()</code> functions save one object at a time without the name of the <a id="id201" class="indexterm"/>object. For example, the name <code class="literal">sales.data</code> is not saved. However, the column <a id="id202" class="indexterm"/>names <code class="literal">trans</code> and <code class="literal">items</code> are saved. As an alternative, the <code class="literal">save()</code> and <code class="literal">load()</code> functions can be used to handle multiple objects or even all objects in an environment, along with their variable names.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec38"/>Summary</h1></div></div></div><p>In this chapter, we learned about the copy-on-modification semantics of R's memory management. A good understanding of how this works enables us to find opportunities to reduce the memory consumption of R programs.</p><p>We also saw how temporary variables and intermediate computations can be removed from the environment when they are no longer needed, to free up memory for subsequent computations. Besides removing temporary variables explicitly, we learned two other ways to manage temporary variables automatically. First, on the fly computations produce intermediate data without creating variables that persist in the memory. Second, functions are a useful way to group related operations and automatically remove temporary variables when exiting the functions.</p><p>Finally, we saw how to save data to the disk to free up memory and reload them later when needed.</p><p>In the next chapter, we will explore more advanced techniques in order to optimize memory consumption and allow R programs to work with larger datasets, even data that is too big to fit in the memory.</p></div></body></html>