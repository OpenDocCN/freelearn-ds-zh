- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingesting Data from Structured and Unstructured Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, we can store and retrieve data from multiple sources, and the optimal
    storage method depends on the type of information being processed. For example,
    most APIs make data available in an unstructured format as this allows the sharing
    of data of multiple formats (for example, audio, video, and image) and has low
    storage costs via the use of data lakes. However, if we want to make quantitative
    data available for use with several tools to support analysis, then the most reliable
    option might be structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, whether you are a data analyst, scientist, or engineer, it is essential
    to understand how to manage both structured and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a JDBC connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data from a JDBC database using SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to a NoSQL database (MongoDB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating our NoSQL table in MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data from MongoDB using PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the **Jupyter Notebook** is not mandatory but allows us to explore the
    code interactively. Since we will execute both Python and PySpark code, Jupyter
    can help us to understand the scripts better. Once you have Jupyter installed,
    you can execute it using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended to create a separate folder to store the Python files or notebooks
    we will cover in this chapter; however, feel free to organize it in the most appropriate
    way for you.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a JDBC connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with different systems brings the challenge of finding an efficient
    way to connect the systems. An adaptor, or a driver, is the solution to this communication
    problem, creating a bridge to translate information from one system to another.
  prefs: []
  type: TYPE_NORMAL
- en: '**JDBC**, or **Java Database Connectivity**, is used to facilitate communication
    between Java-based systems and databases. This recipe covers configuring JDBC
    in SparkSession to connect to a PostgreSQL database, using best practices as always.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before configuring SparkSession, we need to download the `.jars` file (Java
    Archive). You can do this at [https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)
    on the PostgreSQL official site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Download**, and you will be redirected to another page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – PostgreSQL JDBC home page](img/Figure_5.1_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – PostgreSQL JDBC home page
  prefs: []
  type: TYPE_NORMAL
- en: Then, select the **Java 8** **Download** button.
  prefs: []
  type: TYPE_NORMAL
- en: Keep this `.jar` file somewhere safe, as you will need it later. I suggest keeping
    it inside the folder where your code is.
  prefs: []
  type: TYPE_NORMAL
- en: For the PostgreSQL database, you can use a Docker image or the instance we created
    on Google Cloud in *Chapter 4*. If you opt for the Docker image, ensure it is
    up and running.
  prefs: []
  type: TYPE_NORMAL
- en: The final preparatory step for this recipe is to import a dataset to be used.
    We will use the `word_population.csv` file (which you can find in the GitHub repository
    of this book, at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets)).
    Import it using DBeaver or any other SQL IDE of your choice. We will use this
    dataset with SQL in the *Ingesting data from a JDBC database using SQL* recipe
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To import data into DBeaver, create a table with the name of your choice under
    the Postgres database. I chose to give my table the exact name of the CSV file.
    You don’t need to insert any columns for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, right-click on the table and select **Import Data**, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Importing data on a table using DBeaver](img/Figure_5.2_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Importing data on a table using DBeaver
  prefs: []
  type: TYPE_NORMAL
- en: 'A new window will open, showing the options to use a CSV file or a database
    table. Select **CSV** and then **Next**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Importing CSV files into a table using DBeaver](img/Figure_5.3_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Importing CSV files into a table using DBeaver
  prefs: []
  type: TYPE_NORMAL
- en: 'A new window will open where you can select the file. Choose the `world_population.csv`
    file and select the **Next** button, leaving the default settings shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – CSV file successfully imported into the world_population table](img/Figure_5.4_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – CSV file successfully imported into the world_population table
  prefs: []
  type: TYPE_NORMAL
- en: 'If all succeeds, you should be able to see the `world_population` table populated
    with the columns and data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – The world_population table populated with data from the CSV](img/Figure_5.5_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The world_population table populated with data from the CSV
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I will use a Jupyter notebook to insert and execute the code to make this exercise
    more dynamic. Here is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession`, we will need an additional class called `SparkConf` to set
    our new configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`SparkConf(`), which we instantiated, we can set the path to the `.jar` with
    `spark.jars`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see a `SparkConf` object created, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – SparkConf object](img/Figure_5.6_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – SparkConf object
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` and create it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If a warning message appears as in the following screenshot, you can ignore
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – SparkSession initialization warning messages](img/Figure_5.7_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – SparkSession initialization warning messages
  prefs: []
  type: TYPE_NORMAL
- en: '**Connecting to our database**: Finally, we can connect to the PostgreSQL database
    by passing the required credentials including host, database name, username, and
    password as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the credentials are correct, we should expect no output here.
  prefs: []
  type: TYPE_NORMAL
- en: '`.printSchema()`, it is possible now to see the table columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Executing the code will show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – DataFrame of the world_population schema](img/Figure_5.8_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – DataFrame of the world_population schema
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can observe that PySpark (and Spark) require additional configuration to
    create a connection with a database. In this recipe, using the PostgreSQL `.jars`
    file is essential to make it work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand what kind of configuration Spark requires by looking at our
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We started by instantiating the `SparkConf()` method, responsible for defining
    configurations used in SparkSession. After instantiating the class, we used the
    `set()` method to pass a key-value pair parameter: `spark.jars`. If more than
    one `.jars` file was used, the paths could be passed on the value parameter separated
    by commas. It is also possible to define more than one `conf.set()`method; they
    just need to be included one after the other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is on the second line of SparkSession where the set of configurations is
    passed, as you can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, with our SparkSession instantiated, we can use it to read our database,
    as you can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Since we are handling a third-party application, we must set the format for
    reading the output using the `.format()` method. The `.options()` method will
    carry the authentication values and the driver.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With time you will observe that there are a few diverse ways to declare the
    `.options()` key-value pairs. For example, another frequently used format is .`options("driver",
    "org.postgresql.Driver)`. Both ways are correct depending on the *taste* of the
    developer.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe covered how to use a JDBC driver, and the same logic applies to
    **Open Database Connectivity** (**ODBC**). However, determining the criteria for
    using JDBC or ODBC requires understanding which data source we are ingesting data
    from.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ODBC connection in Spark is usually associated with Spark Thrift Server,
    a Spark SQL extension from Apache HiveServer2 that allows users to execute SQL
    queries in **Business Intelligence** (**BI**) tools such as MS PowerBI or Tableau.
    See the following diagram for an outline of this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation
    (https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml)](img/Figure_5.9_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation ([https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml))
  prefs: []
  type: TYPE_NORMAL
- en: By contrast to JDBC, ODBC is used in real-life projects that are smaller and
    more specific to certain system integrations. It also requires the use of another
    Python library called `pyodbc`. You can read more about it at [https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark](https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging connection errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PySpark errors can be very confusing and lead to misinterpretations. It happens
    because the errors are often related to a problem on the JVM, and Py4J (a Python
    interpreter that communicates dynamically with the JVM) consolidates the message
    with other Python errors that may have occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some error messages are prevalent and can easily be identified when managing
    database connections. Let’s take a look at an error that occurred when using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the error message that resulted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Py4JJavaError message](img/Figure_5.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Py4JJavaError message
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first line, we see `Py4JJavaError` informing us of an error when calling
    the load function. Continuing to the second line, we can see the message: `java.sql.SQLException:
    No suitable driver`. It informs us that even though the `.jars` file is configured
    and set, PySpark doesn’t know which drive to use to load data from PostgreSQL.
    This can be easily fixed by adding the `driver` parameter under `.options()`.
    Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Find more about Spark Thrift Server at [https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data from a JDBC database using SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the connection tested and SparkSession configured, the next step is to
    ingest the data from PostgreSQL, filter it, and save it in an analytical format
    called a Parquet file. Don’t worry about how Parquet files work for now; we will
    cover it in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe aims to use the connection we created with our JDBC database and
    ingest the data from the `world_population` table.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will use the same dataset and code as the *Configuring a JDBC connection*
    recipe to connect to the PostgreSQL database. Ensure your Docker container is
    running or your PostgreSQL server is up.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe continues from the content presented in *Configuring a JDBC connection*.
    We will now learn how to ingest the data inside the Postgres database.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following on from our previous code, let’s read the data in our database as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`world_population` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Creating a TempView**: Using the exact name of our table (for organization
    purposes), we create a temporary view in the Spark default database from the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no output expected here.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Depending on the size of your monitor, the output may look confusing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – world_population view using Spark SQL](img/Figure_5.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – world_population view using Spark SQL
  prefs: []
  type: TYPE_NORMAL
- en: '**Filtering data**: Using a SQL statement, let’s filter only the South American
    countries in our DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we attribute the results to a variable, there is no output.
  prefs: []
  type: TYPE_NORMAL
- en: '`.toPandas()` function to bring a more user-friendly view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is how the result appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – south_america countries with toPandas() visualization](img/Figure_5.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – south_america countries with toPandas() visualization
  prefs: []
  type: TYPE_NORMAL
- en: '**Saving our work**: Now, we can save our filtered data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at your script’s folder, you should see a folder named `south_america_population`.
    Inside, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – south_america data in the Parquet file](img/Figure_5.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – south_america data in the Parquet file
  prefs: []
  type: TYPE_NORMAL
- en: This is our filtered and ingested DataFrame in an analytical format.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A significant advantage of working with Spark is the possibility of using SQL
    statements to filter and query data from a DataFrame. It allows data analytics
    and BI teams to help the data engineers by handling queries. This helps to build
    analytical data and insert it into data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, there are some considerations we need to take to execute a SQL
    statement properly. One of them is using `.createOrReplaceTempView()`, as seen
    in this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, this temporary view will work as a SQL table and organize
    the data from the DataFrame without needing physical files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we used the instantiated `SparkSession` variable to execute the SQL statements.
    Note that the name of the table is the same as the temporary view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing the SQL queries we required, we proceeded to save our files using
    the .`write()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The parameter inside the `parquet()` method defines the file’s path and name.
    Several other configurations are available when writing Parquet files, which we
    will cover later, in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although we used a temporary view to make our SQL statements, it is also possible
    to use the filtering and aggregation functions from the DataFrame. Let’s use the
    example from this recipe by filtering only the South American countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – South American countries filtered using DataFrame operations](img/Figure_5.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – South American countries filtered using DataFrame operations
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to understand that not all SQL functions can be used as DataFrame
    operations. You can see more practical examples of filtering and aggregation functions
    using DataFrame operations at [https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml](https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*TowardsDataScience* has a fantastic blog post about SQL functions using PySpark,
    at [https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53).'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to a NoSQL database (MongoDB)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB is an open source, unstructured, document-oriented database made in
    C++. It is well known in the data world for its scalability, flexibility, and
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: As someone who will work with data (or maybe already does), it is essential
    to know how to explore a MongoDB (or any other unstructured) database. MongoDB
    has some peculiarities, which we will explore practically here.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to create a connection to access MongoDB
    documents via Studio 3T Free, a MongoDB GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start our work with this robust database, first, we need to install and
    create a MongoDB server on our local machine. We already configured a MongoDB
    Docker container in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022), so let’s
    get it up and running. You can do this using Docker Desktop or via the command
    line using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget to change the variables using the username and password of your
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Docker Desktop, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – MongoDB Docker container running](img/Figure_5.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – MongoDB Docker container running
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to download and configure Studio 3T Free, free software the
    development community uses to connect to MongoDB servers. You can download this
    software from [https://studio3t.com/download-studio3t-free](https://studio3t.com/download-studio3t-free)
    and follow the installer’s steps for your given OS.
  prefs: []
  type: TYPE_NORMAL
- en: During the installation, a message may appear like that shown in the following
    figure. If so, you can leave the fields blank. We don’t need password encryption
    for local or testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Studio 3T Free password encryption message](img/Figure_5.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Studio 3T Free password encryption message
  prefs: []
  type: TYPE_NORMAL
- en: 'When the installation process is finished, you will see the following window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Studio 3T Free connection window](img/Figure_5.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Studio 3T Free connection window
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to connect our MongoDB instance to the IDE.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have Studio 3T installed, let’s connect to our local MongoDB instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating the connection**: Right after you open Studio 3T, a window will
    appear and ask you to insert the connection string or manually configure it. Select
    the second option and click on **Next**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will have something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Studio 3T New Connection initial options](img/Figure_5.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Studio 3T New Connection initial options
  prefs: []
  type: TYPE_NORMAL
- en: '`localhost` and `27017`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your screen should look as follows for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – New Connection server information](img/Figure_5.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – New Connection server information
  prefs: []
  type: TYPE_NORMAL
- en: Now select the **Authentication** tab under the **Connection group** field,
    and from the **Authentication Mode** drop-down menu, choose **Basic**.
  prefs: []
  type: TYPE_NORMAL
- en: Three fields will appear—`admin` in the **Authentication** **DB** field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – New Connection Authentication information](img/Figure_5.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – New Connection Authentication information
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing our connection**: With this configuration, we should be able to test
    our database connection. In the lower-left corner, select the **Test** **Connection**
    button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the credentials you provided are correct, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Test connection successful](img/Figure_5.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Test connection successful
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Save** button, and the window will close.
  prefs: []
  type: TYPE_NORMAL
- en: '**Connecting to our database**: After we save our configuration, a window with
    the available connections will appear, including our newly created one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Connection manager with the connection created](img/Figure_5.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – Connection manager with the connection created
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the **Connect** button, and three default databases will appear: **admin**,
    **config**, and **local**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – The main page of the local MongoDB with the default databases
    on the server](img/Figure_5.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – The main page of the local MongoDB with the default databases
    on the server
  prefs: []
  type: TYPE_NORMAL
- en: We have now finished our MongoDB configuration and are ready for the following
    recipes in this chapter and others, including *Chapters 6*, *11*, and *12*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like available databases, creating and running MongoDB through a Docker container
    is straightforward. Check the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we saw in the *Getting ready* section, the most crucial information to be
    passed is the username and password (using the `-e` parameter), the ports over
    which to connect (using the `-p` parameter), and the container image version,
    which is the latest available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the MongoDB container connected to Studio 3T Free is even
    more straightforward. Once the connection port is available, we can easily access
    the database. You can see the architectural representation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free](img/Figure_5.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free
  prefs: []
  type: TYPE_NORMAL
- en: 'As described at the beginning of this recipe, MongoDB is a document-oriented
    database. Its structure is similar to a JSON file, except each line is interpreted
    as a document and has its own `ObjectId`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – MongoDB document format](img/Figure_5.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – MongoDB document format
  prefs: []
  type: TYPE_NORMAL
- en: 'The group of documents is referred to as a *collection*, which is better understood
    as a table representation in a structured database. You can see how it is hierarchically
    organized in the schema shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – MongoDB data structure](img/Figure_5.26_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – MongoDB data structure
  prefs: []
  type: TYPE_NORMAL
- en: 'As we observed when logging in to MongoDB using Studio 3T Free, there are three
    default databases: `admin`, `config`, and `local`. For now, let’s disregard the
    last two since they pertain to operational working of the data. The `admin` database
    is the main one created by the `root` user. That’s why we provided this database
    for the **Authentication DB** option in *step 3*.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a user to ingest data and access specific databases or collections
    is generally recommended. However, we will keep using root access here and in
    the following recipes in this book for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The connection string will vary depending on how your MongoDB server is configured.
    For instance, when *replicas* or *sharded clusters* are in place, we need to specify
    which instances we want to connect to.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Sharded clusters are a complex and interesting topic. You can read more and
    go deeper on the topic in MongoDB’s official documentation at [https://www.mongodb.com/docs/manual/core/sharded-cluster-components/](https://www.mongodb.com/docs/manual/core/sharded-cluster-components/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of a standalone server string connection using basic authentication
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it is similar to other database connections. If we wanted to
    connect to a local server, we would change the host to `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for a replica or sharded cluster, the string connection looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `authSource=admin` parameter in this URI is essential to inform MongoDB
    that we want to authenticate using the administration user of the database. Without
    it, an error or authentication will be raised, like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Another way to avoid this error is to create a specific user to access the database
    and collection.
  prefs: []
  type: TYPE_NORMAL
- en: SRV URI connection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MongoDB introduced the **Domain Name System** (**DNS**) seed list connection,
    constructed by a **DNS Service Record** (**SRV**) specification of data in the
    DNS, to try to solve this verbose string. We saw the possibility of using an SRV
    URI to configure the MongoDB connection in the first step of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It is similar to the standard connection string format we saw earlier. However,
    we need to indicate the use of SRV at the beginning and then provide the DNS entry.
  prefs: []
  type: TYPE_NORMAL
- en: This type of connection is advantageous when handling replicas or nodes since
    the SRV creates a single identity for the cluster. You can find a more detailed
    explanation of this, along with an outline of how to configure it, in the MongoDB
    official documentation at [https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format](https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are interested, other MongoDB GUI tools are available on the market:
    [https://www.guru99.com/top-20-mongodb-tools.xhtml](https://www.guru99.com/top-20-mongodb-tools.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating our NoSQL table in MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After successfully connecting and understanding how Studio 3T works, we will
    now import some MongoDB collections. We have seen in the *Connecting to a NoSQL
    database (MongoDB)* recipe how to get started with MongoDB, and in this recipe,
    we will import a MongoDB database and come to understand its structure. Although
    MongoDB has a specific format to organize data internally, understanding how a
    NoSQL database behaves is crucial when working with data ingestion.
  prefs: []
  type: TYPE_NORMAL
- en: We will practice by ingesting the imported collections in the following recipes
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will use a sample dataset of Airbnb reviews called `listingsAndReviews.json`.
    You can find this dataset in the GitHub repository of this book at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb).
    After downloading it, put the file into our `mongo-local` directory, created in
    [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  prefs: []
  type: TYPE_NORMAL
- en: 'I kept mine inside the `sample_airbnb` folder just for organization purposes,
    as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Command line with listingsAndReviews.json](img/Figure_5.27_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Command line with listingsAndReviews.json
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the dataset, we need to install `pymongo`, a Python library
    to connect to and manage MongoDB operations. To install it, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to create `virtualenv` for this installation.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to start inserting data into MongoDB. Don’t forget to check
    that your Docker image is up and running before we begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pymongo`, we can easily establish a connection with the MongoDB database.
    Refer to the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Defining our database and collection**: We will create a database and collection
    instance using the client connection we instantiated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the `json_collection` variable, insert the path where you put the Airbnb
    sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`bulk_write` function, we will insert all the documents inside the JSON file
    into the sales collection we created and close the connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No output is expected from this operation, but we can check the database to
    see if it is successful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Checking the MongoDB database results**: Let’s check our database to see
    if the data was inserted correctly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open Studio 3T Free and refresh the connection (right-click on the connection
    name and select **Refresh All**). You should see a new database named **db_airbnb**
    has been created, containing a **reviews** collection, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Database and collection successfully created on MongoDB](img/Figure_5.28_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Database and collection successfully created on MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: With the collection now created and containing some data, let’s go deeper into
    how the code works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, the code we implemented is straightforward, using just a few
    lines to create and insert data in our database. However, there are important
    points to pay attention to due to the particularities of MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the code line by line now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This line defines the connection to our MongoDB database, and from this instance,
    we can create a new database and its collections.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the URI connection contains hardcoded values for the username and
    password. This must be avoided in real applications, and even development servers.
    It is recommended to store those values as environment variables or use a secret
    manager vault.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the database and collection names; you may have noticed we didn’t
    create them previously in our database. At the time of execution of the code,
    MongoDB checks whether the database exists; if not, MongoDB will create it. The
    same rule applies to the **reviews** collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the collection derives from the `db_cookbook` instance, which makes
    it clear that it is linked to the `db_airbnb` database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the code, the next step is to open the JSON file and parse every
    line. Here we start to see some tricky peculiarities of MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is common to wonder why we actually need to parse the lines of JSON, since
    MongoDB accepts this format. Let’s check our `listingsAndReviews.json` file, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – JSON file with MongoDB document lines](img/Figure_5.29_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – JSON file with MongoDB document lines
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use any tool to verify this as valid JSON, it will certainly say it’s
    not a valid format. This happens because each line of this file represents one
    document of the MongoDB collection. Trying to open that file using only the conventional
    `open()` and `json.loads()` methods will produce an error like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To make it acceptable to the Python interpreter, we need to open and read each
    line individually and append it to the `requesting_collection` list. Also, the
    `InsertOne()` method will ensure that each line is inserted separately. A problem
    that occurs while inserting a specific row will be much easier to identify.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `bulk_write()` will take the list of documents and insert them
    into the MongoDB database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This operation will finish without returning any output or error messages if
    everything is OK.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen how simple it is to create a Python script to insert data into
    our MongoDB server. Nevertheless, MongoDB has database tools to provide the same
    result and can be executed via the command line. The `mongoimport` command is
    used to insert data into our database, as you can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If you are interested to learn more about the other database tools and commands
    available, check the official MongoDB documentation at [https://www.mongodb.com/docs/database-tools/installation/installation/](https://www.mongodb.com/docs/database-tools/installation/installation/).
  prefs: []
  type: TYPE_NORMAL
- en: Restrictions on field names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When loading data into MongoDB, one big problem is the restrictions on characters
    used in the field names. Due to MongoDB server versions or programming language
    specificities, sometimes the key names of fields come with a `$` prefix, and,
    by default, MongoDB is not compatible with it, creating an error like the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, a JSON file dump was exported from a MongoDB server, and the
    reference of `ObjectID` came with the `$` prefix. Even though the more recent
    versions of MongoDB have started to accept these characters (see the thread here:
    [https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D](https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D)),
    it is a good practice to avoid using them where possible. In this case, we have
    two main options: remove all the restricted characters using a script, or encode
    the JSON file into a **Binary JavaScript Object Notation** (**BSON**) file. You
    can find out more about encoding the file into BSON format at [https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545](https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can read more about the MongoDB restrictions on field names at [https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names](https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names).
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data from MongoDB using PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though it seems impractical to create and ingest the data ourselves, this
    exercise can be applied to real-life projects. People who work with data are often
    involved in the architectural process of defining the type of database, helping
    other engineers to insert data from applications into a database server, and later
    ingesting only the relevant information for dashboards or other analytical tools.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have created and evaluated our server and then created collections
    inside our MongoDB instance. With all this preparation, we can now ingest our
    data using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires the execution of the *Creating our NoSQL table in MongoDB*
    recipe due to data insertion. However, you can create and insert other documents
    into the MongoDB database and use them here. If you do this, ensure you set the
    suitable configurations to make it run properly.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as in the *Creating our NoSQL table in MongoDB* recipe, check that the
    Docker container is up and running since this is our MongoDB instance’s primary
    data source. Let’s proceed to the ingesting!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Docker container for MongoDB is running](img/Figure_5.30._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – Docker container for MongoDB is running
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You need to perform the following steps to try this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession`, but this time passing specific configurations for reading our
    MongoDB database, `db_airbnb`, such as the URI and the `.jars`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should expect a significant output here since Spark downloads the package
    and sets the rest of the configuration we passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – SparkSession being initialized with MongoDB configurations](img/Figure_5.31._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – SparkSession being initialized with MongoDB configurations
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` we instantiated. Here, no output is expected because the `SparkSession`
    is set only to send logs at the `WARN` level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Getting our DataFrame schema**: We can see the collection’s schema using
    the print operation on the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should observe the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – Reviews DataFrame collection schema printed](img/Figure_5.32._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 – Reviews DataFrame collection schema printed
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, the structure is similar to a JSON file with nested objects.
    Unstructured data is usually presented in this form and can hold a large amount
    of information to create a Python script to insert data into our data. Now, let’s
    go deeper and understand our code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MongoDB required a few additional configurations in `SparkSession` to execute
    the `.read` function. It is essential to understand why we used the configurations
    instead of just using code from the documentation. Let’s explore the code for
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of `spark.mongodb.input.uri`, which tells our `SparkSession` that
    a *read* operation needs to be performed using a MongoDB URI. If, for instance,
    we wanted to do a *write* operation (or both read and write), we would just need
    to add the `spark.mongodb.output.uri` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we pass the URI containing the user and password information, the name
    of the database, and the authentication source. Since we use the root user to
    retrieve the data, this last parameter is set to `admin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the name of our collection to be used in the read operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Even though it might seem odd to define these parameters in the SparkSession,
    and it is possible to set the database and collection, this is a good practice
    that has been adopted by the community when manipulating MongoDB connections.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Another new configuration here is `spark.jars.packages`. When using this key
    with the `.config()` method, Spark will search its available online packages,
    download them, and place them in the `.jar` folders to be used. Although this
    is an advantageous way to set the `.jar` connectors, this is not available for
    all databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the connection is established, the reading process is remarkably similar
    to the JDBC: we pass the `.format()` of the database (here, `mongo`), and since
    the database and collection name are already set, we don’t need to configure `.option()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing `.load()`, Spark will verify whether the connection is valid
    and throw an error if not. In the following screenshot, you can see an example
    of the error message when the credentials are not correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection](img/Figure_5.33._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection'
  prefs: []
  type: TYPE_NORMAL
- en: Even though we are handling an unstructured data format, as soon as PySpark
    transforms our collection into a DataFrame, all the filtering, cleaning, and manipulating
    of data is pretty much the same as PySpark data.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw previously, PySpark error messages can be confusing and cause discomfort
    at first glance. Let’s explore other common errors when ingesting data from a
    MongoDB database without the proper configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, let’s not set `spark.jars.packages` in the `SparkSession`
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try to execute the preceding code (passing the rest of the memory settings),
    you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package
    is not set in the configuration](img/Figure_5.34._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package
    is not set in the configuration
  prefs: []
  type: TYPE_NORMAL
- en: Looking carefully at the second line, which begins with `java.lang.ClassNotFoundException`,
    the JVM highlights a missing package or class that needs to be searched for in
    a third-party repository. The package contains the connector code to our JVM and
    establishes communication with the database server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another widespread error message is `IllegalArgumentException`. This type of
    error indicates to the developer that an argument was wrongly passed to a method
    or class. Usually, when related to database connections, it refers to an invalid
    string connection, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.35 – IllegalArgumentException error when the URI is invalid](img/Figure_5.35._B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 – IllegalArgumentException error when the URI is invalid
  prefs: []
  type: TYPE_NORMAL
- en: Although it seems unclear, there is a typo in the URI, where `db_aibnb/?` contains
    an extra forward slash. Removing it and running `SparkSession` again will make
    this error disappear.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to shut down and restart the kernel processes when re-defining
    the SparkSession configurations because SparkSession tends to append to the processes
    rather than replacing them.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MongoDB Spark connector documentation: [https://www.mongodb.com/docs/spark-connector/current/configuration/](https://www.mongodb.com/docs/spark-connector/current/configuration/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can check the MongoDB documentation for a full explanation of how the MongoDB
    connector behaves with PySpark: [https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/](https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also some interesting use cases of MongoDB here: [https://www.mongodb.com/use-cases](https://www.mongodb.com/use-cases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.talend.com/resources/structured-vs-unstructured-data/](https://www.talend.com/resources/structured-vs-unstructured-data/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/](https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml](https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options](https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/](https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
