["```py\n>>> import os, os.path\n>>> path = os.path.expanduser('~/nltk_data')\n>>> if not os.path.exists(path):\n...    os.mkdir(path)\n>>> os.path.exists(path)\nTrue\n>>> import nltk.data\n>>> path in nltk.data.path\nTrue\n```", "```py\n>>> import nltk.data\n>>> nltk.data.load('corpora/cookbook/mywords.txt', format='raw')\n'nltk\\n'\n```", "```py\n>>> import nltk.data\n>>> nltk.data.load('corpora/cookbook/synonyms.yaml')\n{'bday': 'birthday'}\n```", "```py\nnltk\ncorpus\ncorpora\nwordnet\n```", "```py\n>>> from nltk.corpus.reader import WordListCorpusReader\n>>> reader = WordListCorpusReader('.', ['wordlist'])\n>>> reader.words()\n['nltk', 'corpus', 'corpora', 'wordnet']\n>>> reader.fileids()\n['wordlist']\n```", "```py\n>>> reader.raw()\n'nltk\\ncorpus\\ncorpora\\nwordnet\\n'\n>>> from nltk.tokenize import line_tokenize\n>>> line_tokenize(reader.raw())\n['nltk', 'corpus', 'corpora', 'wordnet']\n```", "```py\n>>> from nltk.corpus import names\n>>> names.fileids()\n['female.txt', 'male.txt']\n>>> len(names.words('female.txt'))\n5001\n>>> len(names.words('male.txt'))\n2943\n```", "```py\n>>> from nltk.corpus import words\n>>> words.fileids()\n['en', 'en-basic']\n>>> len(words.words('en-basic'))\n850\n>>> len(words.words('en'))\n234936\n```", "```py\nThe/at-tl expense/nn and/cc time/nn involved/vbn are/ber astronomical/jj ./.\n```", "```py\n>>> from nltk.corpus.reader import TaggedCorpusReader\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos')\n>>> reader.words()\n['The', 'expense', 'and', 'time', 'involved', 'are', ...]\n>>> reader.tagged_words()\n[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), …]\n>>> reader.sents()\n[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]\n>>> reader.tagged_sents()\n[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', 'JJ'), ('.', '.')]]\n>>> reader.paras()\n[[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]]\n>>> reader.tagged_paras()\n[[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', 'JJ'), ('.', '.')]]]\n```", "```py\n>>> from nltk.tokenize import SpaceTokenizer\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos', word_tokenizer=SpaceTokenizer())\n>>> reader.words()\n['The', 'expense', 'and', 'time', 'involved', 'are', ...]\n```", "```py\n>>> from nltk.tokenize import LineTokenizer\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos', sent_tokenizer=LineTokenizer())\n>>> reader.sents()\n[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]\n```", "```py\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos', tag_mapping_function=lambda t: t.lower())\n>>> reader.tagged_words(simplify_tags=True)\n[('The', 'at-tl'), ('expense', 'nn'), ('and', 'cc'), …]\n```", "```py\n>>> from nltk.tag import simplify\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos', tag_mapping_function=simplify.simplify_brown_tag)\n>>> reader.tagged_words(simplify_tags=True)\n[('The', 'DET'), ('expense', 'N'), ('and', 'CNJ'), ...]\n>>> reader = TaggedCorpusReader('.', r'.*\\.pos', tag_mapping_function=simplify.simplify_tag)\n>>> reader.tagged_words(simplify_tags=True)\n[('The', 'A'), ('expense', 'N'), ('and', 'C'), ...]\n```", "```py\n[Earlier/JJR staff-reduction/NN moves/NNS] have/VBP trimmed/VBN about/IN [300/CD jobs/NNS] ,/, [the/DT spokesman/NN] said/VBD ./.\n```", "```py\n>>> from nltk.corpus.reader import ChunkedCorpusReader\n>>> reader = ChunkedCorpusReader('.', r'.*\\.chunk')\n>>> reader.chunked_words()\n[Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ...]\n>>> reader.chunked_sents()\n[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]\n>>> reader.chunked_paras()\n[[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]]\n```", "```py\nMr. NNP B-NP\nMeador NNP I-NP\nhad VBD B-VP\nbeen VBN I-VP\nexecutive JJ B-NP\nvice NN I-NP\npresident NN I-NP\nof IN B-PP\nBalcor NNP B-NP\n. . O\n```", "```py\n>>> from nltk.corpus.reader import ConllChunkCorpusReader\n>>> conllreader = ConllChunkCorpusReader('.', r'.*\\.iob', ('NP', 'VP', 'PP'))\n>>> conllreader.chunked_words()\n[Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]\n>>> conllreader.chunked_sents()\n[Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), Tree('NP', [('executive', 'JJ'), ('vice', 'NN'), ('president', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('Balcor', 'NNP')]), ('.', '.')])]\n>>> conllreader.iob_words()\n[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ...]\n>>> conllreader.iob_sents()\n[[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ('had', 'VBD', 'B-VP'), ('been', 'VBN', 'I-VP'), ('executive', 'JJ', 'B-NP'), ('vice', 'NN', 'I-NP'), ('president', 'NN', 'I-NP'), ('of', 'IN', 'B-PP'), ('Balcor', 'NNP', 'B-NP'), ('.', '.', 'O')]]\n\n```", "```py\n>>> reader.chunked_words()[0].leaves()\n[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]\n>>> reader.chunked_sents()[0].leaves()\n[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n>>> reader.chunked_paras()[0][0].leaves()\n[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n\n```", "```py\n>\n>> from nltk.corpus import brown\n>>> brown.categories()\n['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n\n```", "```py\nthe thin red line is flawed but it provokes .\n```", "```py\na big-budget and glossy production can not make up for a lack of spontaneity that permeates their tv show .\n```", "```py\n>>> from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n>>> reader = CategorizedPlaintextCorpusReader('.', r'movie_.*\\.txt', cat_pattern=r'movie_(\\w+)\\.txt')\n>>> reader.categories()\n['neg', 'pos']\n>>> reader.fileids(categories=['neg'])\n['movie_neg.txt']\n>>> reader.fileids(categories=['pos'])\n['movie_pos.txt']\n\n```", "```py\n>>> reader = CategorizedPlaintextCorpusReader('.', r'movie_.*\\.txt', cat_map={'movie_pos.txt': ['pos'], 'movie_neg.txt': ['neg']})\n>>> reader.categories()\n['neg', 'pos']\n\n```", "```py\nca44 news\ncb01 editorial\n\n```", "```py\ntest/14840 rubber coffee lumber palm-oil veg-oil\ntest/14841 wheat grain\n\n```", "```py\nfrom nltk.corpus.reader import CategorizedCorpusReader, ChunkedCorpusReader\n\nclass CategorizedChunkedCorpusReader(CategorizedCorpusReader, ChunkedCorpusReader):\n  def __init__(self, *args, **kwargs):\n    CategorizedCorpusReader.__init__(self, kwargs)\n    ChunkedCorpusReader.__init__(self, *args, **kwargs)\n\n  def _resolve(self, fileids, categories):\n    if fileids is not None and categories is not None:\n      raise ValueError('Specify fileids or categories, not both')\n    if categories is not None:\n      return self.fileids(categories)\n    else:\n      return fileids\n\n```", "```py\n  def raw(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.raw(self, self._resolve(fileids, categories))\n  def words(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.words(self, self._resolve(fileids, categories))\n\n  def sents(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.sents(self, self._resolve(fileids, categories))\n\n  def paras(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.paras(self, self._resolve(fileids, categories))\n\n```", "```py\n  def tagged_words(self, fileids=None, categories=None, simplify_tags=False):\n    return ChunkedCorpusReader.tagged_words(\n      self, self._resolve(fileids, categories), simplify_tags)\n\n  def tagged_sents(self, fileids=None, categories=None, simplify_tags=False):\n    return ChunkedCorpusReader.tagged_sents(\n      self, self._resolve(fileids, categories), simplify_tags)\n\n  def tagged_paras(self, fileids=None, categories=None, simplify_tags=False):\n    return ChunkedCorpusReader.tagged_paras(\n      self, self._resolve(fileids, categories), simplify_tags)\n\n```", "```py\n  def chunked_words(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.chunked_words(\n      self, self._resolve(fileids, categories))\n\n  def chunked_sents(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.chunked_sents(\n      self, self._resolve(fileids, categories))\n\n  def chunked_paras(self, fileids=None, categories=None):\n    return ChunkedCorpusReader.chunked_paras(\n      self, self._resolve(fileids, categories))\n\n```", "```py\n>>> import nltk.data\n>>> from catchunked import CategorizedChunkedCorpusReader\n>>> path = nltk.data.find('corpora/treebank/tagged')\n>>> reader = CategorizedChunkedCorpusReader(path, r'wsj_.*\\.pos', cat_pattern=r'wsj_(.*)\\.pos')\n>>> len(reader.categories()) == len(reader.fileids())\nTrue\n>>> len(reader.chunked_sents(categories=['0001']))\n16\n\n```", "```py\nfrom nltk.corpus.reader import CategorizedCorpusReader, ConllCorpusReader, ConllChunkCorpusReader\n\nclass CategorizedConllChunkCorpusReader(CategorizedCorpusReader, ConllChunkCorpusReader):\n  def __init__(self, *args, **kwargs):\n    CategorizedCorpusReader.__init__(self, kwargs)\n    ConllChunkCorpusReader.__init__(self, *args, **kwargs)\n\n  def _resolve(self, fileids, categories):\n    if fileids is not None and categories is not None:\n      raise ValueError('Specify fileids or categories, not both')\n    if categories is not None:\n      return self.fileids(categories)\n    else:\n      return fileids\n\n```", "```py\n  def raw(self, fileids=None, categories=None):\n    return ConllCorpusReader.raw(self, self._resolve(fileids, categories))\n\n  def words(self, fileids=None, categories=None):\n    return ConllCorpusReader.words(self, self._resolve(fileids, categories))\n\n  def sents(self, fileids=None, categories=None):\n    return ConllCorpusReader.sents(self, self._resolve(fileids, categories))\n\n```", "```py\n  def tagged_words(self, fileids=None, categories=None):\n    return ConllCorpusReader.tagged_words(self, self._resolve(fileids, categories))\n\n  def tagged_sents(self, fileids=None, categories=None):\n    return ConllCorpusReader.tagged_sents(self, self._resolve(fileids, categories))\n\n  def chunked_words(self, fileids=None, categories=None, chunk_types=None):\n    return ConllCorpusReader.chunked_words(\n      self, self._resolve(fileids, categories), chunk_types)\n\n  def chunked_sents(self, fileids=None, categories=None, chunk_types=None):\n    return ConllCorpusReader.chunked_sents(\n      self, self._resolve(fileids, categories), chunk_types)\n\n```", "```py\n  def parsed_sents(self, fileids=None, categories=None, pos_in_tree=None):\n    return ConllCorpusReader.parsed_sents(\n      self, self._resolve(fileids, categories), pos_in_tree)\n\n  def srl_spans(self, fileids=None, categories=None):\n    return ConllCorpusReader.srl_spans(self, self._resolve(fileids, categories))\n\n  def srl_instances(self, fileids=None, categories=None, pos_in_tree=None, flatten=True):\n    return ConllCorpusReader.srl_instances(\n      self, self._resolve(fileids, categories), pos_in_tree, flatten)\n\n  def iob_words(self, fileids=None, categories=None):\n    return ConllCorpusReader.iob_words(self, self._resolve(fileids, categories))\n\n  def iob_sents(self, fileids=None, categories=None):\n    return ConllCorpusReader.iob_sents(self, self._resolve(fileids, categories))\n\n```", "```py\n>>> import nltk.data\n>>> from catchunked import CategorizedConllChunkCorpusReader\n>>> path = nltk.data.find('corpora/conll2000')\n>>> reader = CategorizedConllChunkCorpusReader(path, r'.*\\.txt', ('NP','VP','PP'), cat_pattern=r'(.*)\\.txt')\n>>> reader.categories()\n['test', 'train']\n>>> reader.fileids()\n['test.txt', 'train.txt']\n>>> len(reader.chunked_sents(categories=['test']))\n2012\n\n```", "```py\n>>> from nltk.corpus.util import LazyCorpusLoader\n>>> from nltk.corpus.reader import WordListCorpusReader\n>>> reader = LazyCorpusLoader('cookbook', WordListCorpusReader, ['wordlist'])\n>>> isinstance(reader, LazyCorpusLoader)\nTrue\n>>> reader.fileids()\n['wordlist']\n>>> isinstance(reader, LazyCorpusLoader)\nFalse\n>>> isinstance(reader, WordListCorpusReader)\nTrue\n\n```", "```py\ntreebank = LazyCorpusLoader(\n\n    'treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg',\n\n    tag_mapping_function=simplify_wsj_tag)\n\ntreebank_chunk = LazyCorpusLoader(\n\n    'treebank/tagged', ChunkedCorpusReader, r'wsj_.*\\.pos',\n\n    sent_tokenizer=RegexpTokenizer(r'(?<=/\\.)\\s*(?![^\\[]*\\])', gaps=True),\n\n    para_block_reader=tagged_treebank_para_block_reader)\n\ntreebank_raw = LazyCorpusLoader(\n\n    'treebank/raw', PlaintextCorpusReader, r'wsj_.*')\n\n```", "```py\nA simple heading\n\nHere is the actual text for the corpus.\n\nParagraphs are split by blanklines.\n\nThis is the 3rd paragraph.\n\n```", "```py\nfrom nltk.corpus.reader import PlaintextCorpusReader\nfrom nltk.corpus.reader.util import StreamBackedCorpusView\n\nclass IgnoreHeadingCorpusView(StreamBackedCorpusView):\n  def __init__(self, *args, **kwargs):\n    StreamBackedCorpusView.__init__(self, *args, **kwargs)\n    # open self._stream\n    self._open()\n    # skip the heading block\n    self.read_block(self._stream)\n    # reset the start position to the current position in the stream\n    self._filepos = [self._stream.tell()]\n\nclass IgnoreHeadingCorpusReader(PlaintextCorpusReader):\n  CorpusView = IgnoreHeadingCorpusView\n\n```", "```py\n>>> from nltk.corpus.reader import PlaintextCorpusReader\n>>> plain = PlaintextCorpusReader('.', ['heading_text.txt'])\n>>> len(plain.paras())\n4\n>>> from corpus import IgnoreHeadingCorpusReader\n>>> reader = IgnoreHeadingCorpusReader('.', ['heading_text.txt'])\n>>> len(reader.paras())\n3\n\n```", "```py\nimport pymongo\nfrom nltk.data import LazyLoader\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.util import AbstractLazySequence, LazyMap, LazyConcatenation\n\nclass MongoDBLazySequence(AbstractLazySequence):\n  def __init__(self, host='localhost', port=27017, db='test', collection='corpus', field='text'):\n    self.conn = pymongo.Connection(host, port)\n    self.collection = self.conn[db][collection]\n    self.field = field\n\n  def __len__(self):\n    return self.collection.count()\n\n  def iterate_from(self, start):\n    f = lambda d: d.get(self.field, '')\n    return iter(LazyMap(f, self.collection.find(fields=[self.field], skip=start)))\n\nclass MongoDBCorpusReader(object):\n  def __init__(self, word_tokenizer=TreebankWordTokenizer(),\n         sent_tokenizer=LazyLoader('tokenizers/punkt/english.pickle'),\n         **kwargs):\n    self._seq = MongoDBLazySequence(**kwargs)\n    self._word_tokenize = word_tokenizer.tokenize\n    self._sent_tokenize = sent_tokenizer.tokenize\n\n  def text(self):\n    return self._seq\n\n  def words(self):\n    return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))\n\n  def sents(self):\n    return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))\n\n```", "```py\n>>> reader = MongoDBCorpusReader(db='website', collection='comments', field='comment')\n\n```", "```py\nimport lockfile, tempfile, shutil\n\ndef append_line(fname, line):\n  with lockfile.FileLock(fname):\n    fp = open(fname, 'a+')\n    fp.write(line)\n    fp.write('\\n')\n    fp.close()\n\ndef remove_line(fname, line):\n\n  with lockfile.FileLock(fname):\n    tmp = tempfile.TemporaryFile()\n    fp = open(fname, 'r+')\n    # write all lines from orig file, except if matches given line\n    for l in fp:\n      if l.strip() != line:\n        tmp.write(l)\n\n    # reset file pointers so entire files are copied\n    fp.seek(0)\n    tmp.seek(0)\n    # copy tmp into fp, then truncate to remove trailing line(s)\n    shutil.copyfileobj(tmp, fp)\n    fp.truncate()\n    fp.close()\n    tmp.close()\n\n```", "```py\n>>> from corpus import append_line, remove_line\n>>> append_line('test.txt', 'foo')\n>>> remove_line('test.txt', 'foo')\n\n```"]