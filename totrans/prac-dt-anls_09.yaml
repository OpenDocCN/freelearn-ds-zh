- en: Exploring, Cleaning, Refining, and Blending Datasets
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 探索、清洗、精炼和融合数据集
- en: In the previous chapter, we learned about the power of data visualizations,
    and the importance of having good-quality, consistent data defined with dimensions
    and measures.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了数据可视化的力量，以及拥有高质量、一致数据的重要性，这些数据通过维度和度量来定义。
- en: Now that we understand *why* that's important, we are going to focus on the
    *how* throughout this chapter by working hands-on with data. Most of the examples
    provided so far included data that was already *prepped* (prepared) ahead of time
    for easier consumption. We are now switching gears by learning the skills that
    are necessary to be comfortable working with data to increase your data literacy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了*为什么*这很重要，我们将通过实际操作数据，在本章中专注于*如何*。到目前为止，提供的多数示例都包括了已经*准备*（准备）好的数据，以便更容易消费。我们现在正在学习必要的技能，以便在处理数据时感到舒适，从而提高你的数据素养。
- en: A key concept of this chapter is cleaning, filtering, and refining data. In
    many cases, the reason why you need to perform these actions is the source data
    does not provide high-quality analytics *as is*. Throughout my career, high-quality
    data is not the norm and data gaps are common. As good data analysts, we need
    to work with what we have available. We will cover some techniques to enrich the
    quality of the data so you can provide quality insights and answer questions from
    the data even when the source does not include all of the information required.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一个关键概念是清洗、过滤和精炼数据。在许多情况下，你需要执行这些操作的原因是源数据本身并不提供高质量的分析。在我的整个职业生涯中，高质量的数据不是常态，数据缺口很常见。作为优秀的数据分析师，我们需要利用我们所能拥有的。我们将介绍一些技术来提高数据质量，这样你就可以在源数据不包含所需的所有信息的情况下，提供高质量的见解并回答数据中的问题。
- en: In my experience, highlighting the poor quality of the source data is the insight
    because not enough transparency exists and key stakeholders are unaware of the
    challenges of using the data. The bottom line is poor quality should not stop
    you from proceeding with working with data. My goal is to demonstrate a repeatable
    technique and workflow to improve data quality for analysis.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，强调源数据质量差是一个洞察力，因为透明度不足，关键利益相关者对使用数据的挑战并不了解。底线是，数据质量差不应该阻止你继续与数据工作。我的目标是展示一个可重复的技术和工作流程，以提高分析数据的质量。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Retrieving, viewing, and storing tabular data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索、查看和存储表格数据
- en: Learning how to restrict, sort, and sift through data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何限制、排序和筛选数据
- en: Cleaning, refining, and purifying data using Python
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python清洗、精炼和净化数据
- en: Combining and binning data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并和分箱数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的GitHub仓库链接：[https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07)。
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从以下链接下载和安装所需的软件：[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)。
- en: Retrieving, viewing, and storing tabular data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索、查看和存储表格数据
- en: The ability to retrieve and view tabular data has been covered multiple times
    in prior chapters; however, those examples were focused on the perspective of
    the consumer. We learned the skills necessary to understand what structured data
    is in, the many different forms it can take, and how to answer some questions
    from data. Our data literacy has increased during this time but we have relied
    on the producers of data sources to make it easier to read using a few Python
    commands or SQL commands. In this chapter, we are switching gears from being exclusively
    a **consumer** to now a **producer** of data by learning skills to manipulate
    data for analysis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中已经多次介绍了检索和查看表格数据的能力；然而，那些例子都是侧重于消费者的视角。我们学习了理解结构化数据是什么，它可以采取的许多不同形式，以及如何从数据中回答一些问题的技能。在这段时间里，我们的数据素养有所提高，但我们一直依赖数据源的生产者，通过使用一些Python命令或SQL命令使其更容易阅读。在本章中，我们将从仅作为消费者转变为现在成为数据的生产者，通过学习操纵数据以进行分析的技能。
- en: As a good data analyst, you will need both sides of the consumer and producer
    spectrum of skills to solve more complicated questions with data. For example,
    a common measure requested by businesses with web or mobile users is called **usage
    analytics**. This means counting the number of users over snapshots of time, such
    as by day, week, month, and year. More importantly, you want to better understand
    whether those users are new, returning, or lost.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名优秀的数据分析师，您需要消费者和供应商技能谱系的两侧来解决更复杂的数据问题。例如，企业要求的一个常见度量标准，针对网站或移动用户，被称为**使用分析**。这意味着在时间快照中计算用户数量，例如按日、周、月和年。更重要的是，您想更好地了解这些用户是新的、回头的还是流失的。
- en: 'Common questions related to usage analytics are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用分析相关的一些常见问题如下：
- en: How many new users have hit the website this day, week, or month?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本日、本周或这个月有多少新用户访问了网站？
- en: How many returning users have accessed the website this day, week, or month?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本日、本周或这个月有多少回头用户访问了网站？
- en: How many users have we lost (inactive for more than 60 days) this week, month,
    or year?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本周、这个月或这一年我们失去了多少用户（60天以上未活跃）？
- en: To answer these types of questions, your data source must have, at a minimum, `timestamp`
    and unique `user_id` fields available. In many cases, this data will have high
    volume and velocity, so analyzing this information will require the right combination
    of people, processes, and technology, which I have had the pleasure of working
    with. Data engineering teams build out ingestion pipelines to make this data accessible
    for reporting and analytics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这些问题，您的数据源至少必须提供 `timestamp` 和唯一的 `user_id` 字段。在许多情况下，这些数据将具有高容量和速度，因此分析这些信息将需要正确的人员、流程和技术组合，我有幸与这些合作。数据工程团队构建数据摄取管道，以便使这些数据可用于报告和分析。
- en: You may need to work with the data engineering team to apply the business rules
    and summary levels (also known as aggregates) to the data that include additional
    fields required to answer the user analytics questions. For our examples, I have
    provided a much smaller sample of data and we are going to derive new data fields
    from the existing source data file provided.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要与数据工程团队合作，将业务规则和汇总级别（也称为聚合）应用于包含回答用户分析问题所需额外字段的数据。在我们的示例中，我已经提供了一个更小的数据样本，我们将从现有的源数据文件中推导出新的数据字段。
- en: I find the best way to learn is to walk through the steps together, so let's
    create a new Jupyter notebook named `user_churn_prep`. We will begin with retrieving
    data from SQL against a database and loading it into a DataFrame, similar to the
    steps outlined in [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering
    and Loading Data in Python*. To keep it simple, we are using another SQLite database
    to retrieve the source data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为最好的学习方法是一起逐步操作，所以让我们创建一个新的 Jupyter 笔记本，命名为 `user_churn_prep`。我们将从数据库中检索数据并将其加载到
    DataFrame 中开始，类似于第 5 章*在 Python 中收集和加载数据*中概述的步骤。为了简化，我们正在使用另一个 SQLite 数据库来检索源数据。
- en: If you would like more details about connecting to SQL data sources, please
    refer to [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering and
    Loading Data* *in Python*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于连接到 SQL 数据源的信息，请参阅第 5 章，*在 Python 中收集和加载数据*。
- en: Retrieving
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: 'To create a connection and use SQLite, we have to import a new library using
    the code. For this example, I have provided the database file named `user_hits.db`,
    so be sure to download it from my GitHub repository beforehand:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建连接并使用 SQLite，我们必须使用代码导入一个新的库。对于这个例子，我已经提供了一个名为 `user_hits.db` 的数据库文件，所以请确保您事先从我的
    GitHub 仓库下载它：
- en: 'To load a SQLite database connection, you just need to add the following command
    in your Jupyter notebook and run the cell. I have placed a copy on GitHub for
    reference:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要加载 SQLite 数据库连接，您只需在您的 Jupyter 笔记本中添加以下命令并运行该单元格。我已经在 GitHub 上放置了一个副本供参考：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将一个连接分配给名为 `conn` 的变量，并指向数据库文件的位置，该文件名为 `user_hits.db`：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Be sure that you have copied the `user_hits.db` file to the correct Jupyter
    folder directory to avoid errors with the connection.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已将 `user_hits.db` 文件复制到正确的 Jupyter 文件夹目录中，以避免连接错误。
- en: 'Import the `pandas` library so you can create a DataFrame:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库，以便您可以创建一个 DataFrame：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run a SQL statement and assign the results to a DataFrame:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一个 SQL 语句并将结果分配给一个 DataFrame：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    Your code should look similar to the following screenshot:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将结果存储在 DataFrame 中，我们可以使用所有可用的 `pandas` 库命令来处理这些数据，而无需返回到数据库。您的代码应类似于以下截图：
- en: '![](img/9989271e-f748-4d28-915e-049d558dff00.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9989271e-f748-4d28-915e-049d558dff00.png)'
- en: Viewing
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看
- en: 'Perform the following steps to view the results of the retrieved data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以查看检索到的数据的结果：
- en: 'To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看结果，我们可以运行 `head()` 命令对此 DataFrame 进行操作，使用以下代码：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will look like the following table, where the `tbl_user_hits` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下表格，其中 `tbl_user_hits` 表已被加载到 DataFrame 中，带有标签的标题行，左侧的索引列从 `0` 开始：
- en: '![](img/8eff19fc-3499-4283-841d-ec0b75b7c443.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8eff19fc-3499-4283-841d-ec0b75b7c443.png)'
- en: Before we move on to the next step, let's verify the data we loaded with a few
    metadata commands.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行下一步之前，让我们使用一些元数据命令来验证我们加载的数据。
- en: 'Type in `df_user_churn.info()` in the next `In[]:` cell and run the cell:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个 `In[]:` 单元格中输入 `df_user_churn.info()` 并运行该单元格：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows,
    including data types for all columns, similar to the following screenshot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 确认输出单元格显示 `Out []`。将显示多行，包括所有列的数据类型，类似于以下截图：
- en: '![](img/b8de6e38-5c79-4911-bd87-b3cb59aaf02e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8de6e38-5c79-4911-bd87-b3cb59aaf02e.png)'
- en: Storing
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Now that we have the data available to work with as a DataFrame in Jupyter,
    let's run a few commands to store it as a file for reference. Storing data as
    a snapshot for analysis is a useful technique to learn, and while our example
    is simplistic, the concept will help in future data analysis projects.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据作为 DataFrame 可用于 Jupyter 中的工作，让我们运行一些命令将其存储为文件以供参考。将数据作为分析快照存储是一种有用的技术，虽然我们的示例很简单，但这个概念将有助于未来的数据分析项目。
- en: 'To store your DataFrame into a CSV file, you just have to run the following
    command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要将您的 DataFrame 存储到 CSV 文件中，您只需运行以下命令：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results will look similar to the following screenshot, where a new CSV
    file is created in the same project folder as your current Jupyter notebook. Based
    on the OS you are using on your workstation, the results will vary:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中在您的当前 Jupyter 笔记本相同的项目文件夹中创建了一个新的 CSV 文件。根据您在工作站上使用的操作系统，结果可能会有所不同：
- en: '![](img/32997d53-84a6-4c1a-9881-ec3357bdf233.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/32997d53-84a6-4c1a-9881-ec3357bdf233.png)'
- en: There are other formats you can export your DataFrame to, including Excel. You
    should also note the file path from which you are exporting the data file. Check
    out the *Further reading* section for more information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 DataFrame 导出为其他格式，包括 Excel。您还应该注意您导出数据文件的文件路径。查看 *进一步阅读* 部分以获取更多信息。
- en: Learning how to restrict, sort, and sift through data
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何限制、排序和筛选数据
- en: Now that we have the data available in a DataFrame, we can walk through how
    to restrict, sort, and sift through data with a few Python commands. The concepts
    we are going to walk through using pandas are also common using SQL, so I will
    also include the equivalent SQL commands for reference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据存储在 DataFrame 中，我们可以通过几个 Python 命令来了解如何限制、排序和筛选数据。我们将使用 pandas 来讲解的概念在
    SQL 中也很常见，因此我也会包括相应的 SQL 命令以供参考。
- en: Restricting
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The concept of restricting data, which is also known as filtering data, is all
    about isolating one or more records based on conditions. Simple examples are when
    you are only retrieving results based on matching a specific field and value.
    For example, you only want to see results for one user or a specific point in
    time. Other requirements for restricting data can be more complicated, including
    explicit conditions that require elaborate logic, business rules, and multiple
    steps. I will not be covering complex examples that require complex logic but
    will add some references in the *Further reading* section. However, the concepts
    covered will teach you essential skills to satisfy many common use cases.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 限制数据的概念，也称为过滤数据，主要是基于条件隔离一个或多个记录。简单的例子是当你只根据匹配特定字段和值检索结果时。例如，你可能只想看到某个用户或特定时间点的结果。限制数据的其他要求可能更复杂，包括需要复杂逻辑、业务规则和多个步骤的明确条件。我不会涵盖需要复杂逻辑的复杂示例，但将在
    *进一步阅读* 部分添加一些参考。然而，涵盖的概念将教会你满足许多常见用例的基本技能。
- en: 'For our first example, let''s isolate one specific user from our DataFrame.
    Using `pandas` commands, that is pretty easy, so let''s start up a new Jupyter
    notebook named `user_churn_restricting`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个例子中，让我们从 DataFrame 中隔离一个特定的用户。使用 `pandas` 命令，这相当简单，所以让我们启动一个新的 Jupyter
    notebook，命名为 `user_churn_restricting`：
- en: 'Import the `pandas` library so you can create a DataFrame:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库，以便你可以创建一个 DataFrame：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a new DataFrame by loading the data from the CSV file we created in
    the prior example:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从先前示例中创建的 CSV 文件加载数据创建一个新的 DataFrame：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The file path and filename must be the same as those you used in the prior example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 文件路径和文件名必须与先前示例中使用的相同。
- en: Now that we have all user data loaded into a single DataFrame, we can easily
    reference the source dataset to restrict results. It is a best practice to keep
    this source DataFrame intact so you can reference it for other purposes and analysis.
    It is also common during analysis to need to make adjustments based on changing
    requirements, or that you will only gain insights by making adjustments.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有用户数据加载到一个单独的 DataFrame 中，我们可以轻松地引用源数据集来限制结果。保留这个源 DataFrame 完整是一个最佳实践，这样你可以为其他目的和分析引用它。在分析过程中，根据不断变化的需求进行调整，或者你只能通过调整来获得洞察力，这也是常见的。
- en: In my career, I follow a common practice of *you don't know what you don't know* while
    working with data, so having the flexibility to easily reference the source data
    without undoing your changes is important. This is commonly known as snapshotting
    your analysis and having the ability to roll back changes as needed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的职业生涯中，我遵循在处理数据时的一种常见做法，即 *你不知道你不知道的*，所以能够轻松地引用源数据而不撤销你的更改是很重要的。这通常被称为快照分析，并具有根据需要回滚更改的能力。
- en: When working with big data sources where the sources are larger than a billion
    rows, snapshots will require a large number of resources where RAM and CPU will
    be impacted. You may be required to snapshot incrementally for a specific date
    or create a rolling window of time to limit the amount of data you can work with
    at one time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大于十亿行的数据源时，快照将需要大量的资源，其中 RAM 和 CPU 将受到影响。你可能需要按日期增量快照，或者创建一个滚动的时间窗口来限制一次可以处理的数据量。
- en: To restrict our data to a specific user, we will be creating a new DataFrame
    from the source DataFrame. That way, if we need to make adjustments to the filters
    used to create the new DataFrame, we don't have to rerun all of the steps from
    the beginning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的数据限制到特定用户，我们将从源 DataFrame 创建一个新的 DataFrame。这样，如果我们需要调整创建新 DataFrame 所使用的过滤器，我们不需要从头开始重新运行所有步骤。
- en: 'Create a new DataFrame by loading the data from the source DataFrame. The syntax
    is nested so you are actually calling the same `df_user_churn` DataFrame within
    itself and filtering results only for the explicit value where `userid` is equal
    to `1`:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从源 DataFrame 加载数据创建一个新的 DataFrame。语法是嵌套的，所以你实际上是在同一个 `df_user_churn` DataFrame
    内部调用它，并仅过滤出 `userid` 等于 `1` 的显式值：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To view and verify the results, you can run a simple `head()` command:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看和验证结果，你可以运行一个简单的 `head()` 命令：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results will look similar to the following screenshot, where the only two
    rows in the new `df_user_restricted` DataFrame have a value where `userid` is
    `1`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中新的 `df_user_restricted` DataFrame 中只有两行在 `userid` 为 `1` 时有值：
- en: '![](img/fdc9fa3a-878d-45af-adfd-7c3b6b77e979.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/fdc9fa3a-878d-45af-adfd-7c3b6b77e979.png)'
- en: Restricting data helps to isolate records for specific types of analysis to
    help to answer additional questions. In the next step, we can start answering
    questions related to usage patterns.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 限制数据有助于隔离特定类型的分析记录，从而帮助回答更多的问题。在下一步中，我们可以开始回答与使用模式相关的问题。
- en: Sorting
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序
- en: 'Now that we have isolated a specific user by creating a new DataFrame, which
    is now available for reference, we can enhance our analysis by asking questions
    such as the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过创建一个新的DataFrame来隔离了特定的用户，该DataFrame现在可供参考，我们可以通过提出以下问题来增强我们的分析：
- en: When did a specific user start using our website?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定用户是什么时候开始使用我们的网站的？
- en: How frequently does this user access our website?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这位用户多久访问一次我们的网站？
- en: When was the last time this user accessed our website?
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这位用户最后一次访问我们的网站是什么时候？
- en: All of these questions can be answered with a few simple Python commands focused
    on sorting commands. Sorting data is a skill that computer programmers of any
    programming language are familiar with. It's easily done with SQL by adding an `order
    by` command. Many third-party software, such as Microsoft Excel, Google Sheets,
    or Qlik Sense, has a sorting feature built in. The concept of sorting data is
    well known, so I will not go into a detailed definition; rather, I will focus
    on important features and best practices when performing data analysis.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都可以通过一些简单的Python命令来回答，这些命令专注于排序命令。排序数据是任何编程语言的计算机程序员都熟悉的一项技能。通过添加`order
    by`命令，很容易用SQL完成排序。排序数据的概念是众所周知的，因此我将不会深入定义；相反，我将专注于在执行数据分析时的重要功能和最佳实践。
- en: With structured data, sorting is commonly understood to be row-level by specific
    columns, which will be defined by ordering the sequence of the values from either
    low to high or high to low. The default is low to high unless you explicitly change
    it. If the values have a data type that is numeric, such as integer or float,
    the sort order sequence will be easy to identify. For textual data, the values
    are sorted alphabetically, and, depending on the technology used, mixed case text
    will be handled differently. In Python and pandas, we have specific functions
    available along with parameters to handle many different use cases and needs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据中，排序通常被理解为按特定列的行级排序，这将通过从低到高或从高到低排序值的顺序来定义。默认情况下是低到高，除非你明确更改它。如果值的数据类型是数值型，例如整数或浮点数，排序顺序将很容易识别。对于文本数据，值将按字母顺序排序，并且根据所使用的技术的不同，混合大小写的文本将被不同地处理。在Python和pandas中，我们有特定的函数和参数，可以处理许多不同的用例和需求。
- en: 'Let''s start answering some of the questions we outlined previously using the
    `sort()` function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用`sort()`函数回答我们之前概述的一些问题：
- en: 'To answer the question *When did a specific user start using our website?*,
    we just need to run the following command:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要回答问题“*特定用户是什么时候开始使用我们的网站的？*”，我们只需要运行以下命令：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The results will look similar to the following screenshot, where the results
    are sorted in ascending order by the `date` field:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中结果按`date`字段升序排序：
- en: '![](img/68281f91-b36c-4ed4-baae-eb3b505f4694.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68281f91-b36c-4ed4-baae-eb3b505f4694.png)'
- en: 'To answer the question "*When is the last time this user accessed our website?*", we
    just need to run the following command:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要回答问题“*这位用户最后一次访问我们的网站是什么时候？*”，我们只需要运行以下命令：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The results will look similar to the following screenshot, where the same records
    are displayed as the previous one; however, the values are sorted in descending
    order by last date available for this specific `userid`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中显示的记录与之前的相同；然而，值是按此特定`userid`的最后可用日期降序排序的：
- en: '![](img/e34fe94c-7b2f-4e90-9b5b-e6be0a53018a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e34fe94c-7b2f-4e90-9b5b-e6be0a53018a.png)'
- en: Sifting
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 筛选
- en: The concept of sifting through data means we are isolating specific columns
    and/or rows from a dataset based on one or more conditions. There are nuanced
    differences between sifting versus restricting, so I would distinguish sifting
    as the need to include additional business rules or conditions applied to a population
    of data to isolate a subset of that data. Sifting data usually requires creating
    new derived columns from the source data to answer more complex questions. For
    our next example, a good question about usage would be: *Do the same users who
    hit our website on Monday also return during the same week?*
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据筛选的概念意味着我们根据一个或多个条件从数据集中隔离特定的列和/或行。在筛选与限制之间有一些细微的差别，所以我将筛选定义为需要对数据集的群体应用额外的业务规则或条件，以隔离数据的一个子集。筛选数据通常需要从源数据中创建新的派生列，以回答更复杂的问题。对于我们的下一个例子，关于使用情况的一个好问题是：*在周一访问我们网站的同一位用户是否也在同一周内返回？*
- en: 'To answer this question, we need to isolate the usage patterns for a specific
    day of the week. This process requires a few steps, which we will outline together
    from the original DataFrame we created previously:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们需要隔离特定星期的使用模式。这个过程需要几个步骤，我们将从之前创建的原始 DataFrame 中一起概述：
- en: 'Create a new DataFrame by loading the data from the source file:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从源文件加载数据创建一个新的 DataFrame：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we need to extend the DataFrame by adding new derived columns to help
    to make the analysis easier. Since we have a `Timestamp` field available, the
    pandas library has some very useful functions available to help the process. Standard
    SQL has built-in features as well and will vary depending on which RDMS is used,
    so you will have to reference the date/time functions available. For example,
    a Postgres database uses the syntax of `select to_char(current_date,'Day');` to
    convert a date field into the current day of the week.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过添加新的派生列来扩展 DataFrame，以帮助简化分析。由于我们有可用的 `Timestamp` 字段，pandas 库有一些非常有用的函数可以帮助这个过程。标准
    SQL 也有内置的功能，并且会根据使用的 RDMS 而有所不同，因此您需要参考可用的日期/时间函数。例如，Postgres 数据库使用 `select to_char(current_date,'Day');`
    语法将日期字段转换为当前星期几。
- en: 'Import a new `datetime` library for easy reference to date and time functions:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入一个新的 `datetime` 库，以便轻松引用日期和时间函数：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Assign a variable to the current `datetime` for easier calculation of the `age`
    from today:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为当前的 `datetime` 分配一个变量，以便更容易地计算从今天起的 `age`：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Add a new derived column called `age` that is calculated from the current date
    minus the date value per user:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个名为 `age` 的新派生列，该列是从当前日期减去每个用户的日期值计算得出的：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you receive `datetime` errors in your notebook, you may need to upgrade your
    `pandas` library.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在笔记本中收到 `datetime` 错误，您可能需要升级您的 `pandas` 库。
- en: Cleaning, refining, and purifying data using Python
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 清理、精炼和净化数据
- en: 'Data quality is highly important for any data analysis and analytics. In many
    cases, you will not understand how good or bad the data quality is until you start
    working with it. I would define good-quality data as information that is well
    structured, defined, and consistent, where almost all of the values in each field
    are defined as expected. In my experience, data warehouses will have high-quality
    data because it has been reported on across the organization. In my experience, bad
    data quality occurs where a lack of transparency exists against the data source.
    Bad data quality examples are a lack of conformity and inconsistency in the expected
    data type or any consistent pattern of values in delimited datasets. To help to
    solve these data quality issues, you can begin to understand your data with the
    concepts and questions we covered in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),* Fundamentals
    of Data Analysis*, with **Know Your Data (KYD)**. Since the quality of data will
    vary by source, some specific questions you can ask to understand data quality
    are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量对于任何数据分析和分析都至关重要。在许多情况下，您直到开始处理数据之前都不会了解数据质量是好是坏。我会将高质量数据定义为结构良好、定义明确且一致的信息，其中每个字段中的几乎所有值都按照预期定义。根据我的经验，数据仓库将拥有高质量的数据，因为整个组织都有报告。根据我的经验，不良的数据质量出现在数据源缺乏透明度的地方。不良的数据质量示例包括预期数据类型的不一致性和分隔数据集中值的一致模式。为了帮助解决这些数据质量问题，您可以从我们在[第
    1 章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，“数据分析基础”，中涵盖的概念和问题开始理解您的数据，即**了解您的数据
    (KYD)**。由于数据质量会因来源而异，您可以提出以下一些具体问题来了解数据质量：
- en: Is the data structured or unstructured?
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是有结构还是无结构的？
- en: Does the data lineage trace back to a system or application?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的来源是否可以追溯到某个系统或应用程序？
- en: Does the data get transformed and stored in a warehouse?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否被转换并存储在仓库中？
- en: Does the data have a schema with each field having a defined data type?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否有具有定义的数据类型的模式？
- en: Do you have a data dictionary available with business rules documented?
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否有包含业务规则文档的数据字典可用？
- en: Receiving answers to these questions ahead of time would be a luxury; uncovering
    them as you go is more common for a data analyst. During this process, you will
    still find the need to clean, refine, and purify your data for analysis purposes.
    How much time you need to spend will vary on many different factors, and the true
    cost of quality will be the time and effort required to improve data quality.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在事先收到这些问题的答案将是一种奢侈；在过程中发现它们对于数据分析师来说更为常见。在这个过程中，你仍然需要清洗、精炼和纯化你的数据以供分析。你需要花费多少时间将取决于许多不同的因素，而真正质量成本将是提高数据质量所需的时间和精力。
- en: Cleaning data can take on many different forms and has been a common practice
    for decades for data engineers and analytic practitioners. There are many different
    technologies and skillsets required for enterprise and big data cleansing. Data
    cleaning is an industry within **Information Technology** (**IT**) because good-quality
    data is worth the price of outsourcing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗可以采取多种不同的形式，并且几十年来一直是数据工程师和分析实践者的常见做法。企业级和大数据清洗需要许多不同的技术和技能集。数据清洗是信息技术（**IT**）行业的一部分，因为高质量数据值得外包的价格。
- en: 'A common definition of data cleansing is the process of removing or resolving
    poor-quality data records from the source, which can vary based on the technology
    used to persist the data, such as a database table or encoded file. Poor-quality
    data can be identified as any data that does not match the producers'' intended
    and defined requirements. This can include the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的常见定义是从源数据中移除或解决低质量数据记录的过程，这取决于用于持久化数据的技术的不同，例如数据库表或编码文件。低质量数据可以识别为任何不符合生产者预期和定义要求的数据。这可以包括以下内容：
- en: Missing or null (`NaN`) values from the fields of one or more rows
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一行或多行字段中的缺失或空（`NaN`）值
- en: Orphan records where the primary or foreign keys cannot be found in any referenced
    source tables
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孤立记录，其中主键或外键在任何引用的源表中找不到
- en: Corrupted records where one or more records cannot be read by any reporting
    or analysis technology
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 腐坏记录，其中一条或多条记录无法被任何报告或分析技术读取
- en: 'For our example, let''s look at our usage data again and see whether we can
    find any issues by profiling it to see whether we can find any anomalies:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的示例为例，让我们再次查看我们的使用数据，并看看我们是否可以通过分析它来发现任何问题，以查看是否存在任何异常：
- en: 'Import the CSV file and run the `info()` command to confirm the data types
    and row counts and profile the DataFrame for more information:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入CSV文件并运行`info()`命令以确认数据类型和行数，并获取更多关于DataFrame的信息的概要：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The results will look similar to the following screenshot, where metadata of
    the DataFrame is presented:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中展示了DataFrame的元数据：
- en: '![](img/a045eaa8-bd04-4e25-9fc9-503c47d0d599.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a045eaa8-bd04-4e25-9fc9-503c47d0d599.png)'
- en: One anomaly that is uncovered is that the number of values is different between
    the two fields. For `userid`, there are 9 non-null values and for the `date` field,
    there are 12 non-null values. For this dataset, we expect each row to have one
    value for both fields, but this command is telling us there are missing values.
    Let's run another command to identify which index/row has the missing data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 发现的一个异常是两个字段之间的值数量不同。对于`userid`字段，有9个非空值，而对于`date`字段，有12个非空值。对于这个数据集，我们期望每一行两个字段都有一个值，但这个命令告诉我们存在缺失值。让我们运行另一个命令来识别哪个索引/行有缺失数据。
- en: 'Run the `isnull()` command to confirm the data types and row counts and profile
    the DataFrame for more information:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`isnull()`命令以确认数据类型和行数，并获取更多关于DataFrame的信息的概要：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The results will look similar to the following table, where a list of `True`
    and `False` values is displayed by row and column:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下表格，其中按行和列显示了一列`True`和`False`值：
- en: '![](img/e00c8355-7f06-4f3a-9934-d2868db06239.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e00c8355-7f06-4f3a-9934-d2868db06239.png)'
- en: The record count looks okay but notice that there are null values (NaN) that
    exist in the `userid` field. A unique identifier for each row to help us to identify
    each user is critical for accurate analysis of this data. The reason why `userid` is
    blank would have to be explained by the producer of this data and may require
    additional engineering resources to help to investigate and troubleshoot the root
    cause of the issue. In some cases, it may be a simple technical hiccup during
    data source creation that requires a minor code change and reprocessing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 记录计数看起来不错，但请注意`userid`字段中存在空值（NaN）。对于每行数据的唯一标识符对于准确分析这些数据至关重要。`userid`为空的原因需要由数据的生产者解释，可能需要额外的工程资源来帮助调查和排除问题的根本原因。在某些情况下，这可能是数据源创建过程中简单的技术故障，需要微小的代码更改和重新处理。
- en: I always recommend cleaning data as close to the source as possible, which saves
    time by avoiding reworking by other data analysts or reporting systems.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是建议尽可能接近数据源进行数据清理，这样可以节省时间，避免其他数据分析师或报告系统重新工作。
- en: Having nulls included in our analysis will impact our summary statistics and
    metrics. For example, the count of the average daily users would be lower on the
    dates where the null values exist. For the user churn analysis, the measure of
    the frequency of reporting users would be skewed because the NaN values could
    be one of the returning `user_ids` or a new user.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中包含空值将影响我们的汇总统计和指标。例如，平均每日用户的计数在存在空值的日期会较低。对于用户流失分析，报告用户的频率度量将受到扭曲，因为NaN值可能是返回的`user_ids`之一或新用户。
- en: With any high volume transaction-based system, there could be a margin of error
    that you may need to account for. As a good data analyst, ask the question, *what
    is the cost of quality and of being one hundred percent accurate?* If the price
    is too high due to the time and resources required to change it, a good alternative
    is to exclude and isolate the missing data so it can be investigated later.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何高容量基于事务的系统，可能存在需要考虑的误差范围。作为一名优秀的数据分析师，要问的问题是：“质量成本和百分之一百准确性的成本是多少？”如果由于更改所需的时间和资源而价格过高，一个好的替代方案是排除和隔离缺失数据，以便稍后进行调查。
- en: Note that if you end up adding isolated data back into your analysis, you will
    have to restate results and inform any consumers of the change to your metrics.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您最终将隔离的数据添加回分析中，您将不得不重新陈述结果，并通知任何消费者您的指标发生了变化。
- en: 'Let''s walk through an example of how to isolate and exclude any missing data
    by identifying the NaN records and creating a new DataFrame that has them removed:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来了解如何通过识别NaN记录并创建一个新的DataFrame来隔离和排除任何缺失数据：
- en: 'Create a new DataFrame by loading the data from the source DataFrame, except
    we will exclude the null values by adding the `dropna()` command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从源DataFrame加载数据创建一个新的DataFrame，但我们将通过添加`dropna()`命令排除空值：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To view and verify the results, you can run a simple `head()` command and confirm
    the NaN/null values have been removed:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看和验证结果，您可以运行一个简单的`head()`命令，并确认NaN/空值已被删除：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The results will look similar to the following table, where the new DataFrame
    has complete records with no missing values in either `userid` or `date`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下表格，其中新的DataFrame包含没有在`userid`或`date`中缺失值的完整记录：
- en: '![](img/0ecc9d12-931f-41d3-b27f-20d94839ffc2.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ecc9d12-931f-41d3-b27f-20d94839ffc2.png)'
- en: Combining and binning data
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据合并和分箱
- en: 'Combining multiple data sources is sometimes necessary for multiple reasons,
    which include the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多种原因，有时需要合并多个数据源，以下是一些原因：
- en: The source data is broken up into many different files with the same defined
    schema (tables and field names), but the number of rows will vary slightly. A
    common reason is for storage purposes, where it is easier to maintain multiple
    smaller file sizes versus one large file.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据被拆分为许多具有相同定义模式（表和字段名称）的不同文件，但行数会有所不同。一个常见的原因是为了存储目的，与一个大型文件相比，维护多个较小的文件大小更容易。
- en: The data is partitioned where one field is used to break apart the data for
    faster response time reading or writing to the source data. For example, HIVE/HDFS
    recommends storing data by a single date value so you can easily identify when
    it was processed and quickly extract data for a specific day.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被分区，其中一个字段用于将数据拆分以加快对源数据的读取或写入响应时间。例如，HIVE/HDFS建议按单个日期值存储数据，这样您可以轻松地识别数据何时被处理，并快速提取特定一天的数据。
- en: Historical data is stored in a different technology than more current data.
    For example, the engineering team changed the technology being used to manage
    the source data and it was decided not to import historical data beyond a specific
    date.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史数据存储在不同于当前数据的技术中。例如，工程团队更改了用于管理源数据的技术，并决定不导入特定日期之后的历史数据。
- en: For any of the reasons defined here, combining data is a common practice in
    data analysis. I would define the process of combining data as when you are layering
    two or more data sources into one where the same fields/columns from all sources
    align. In SQL, this would be known as `UNION ALL` and in `pandas`, we use the
    `concat()` function to bring all of the data together.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里定义的任何原因，数据合并是数据分析中的常见做法。我将数据合并的过程定义为当你将两个或多个数据源分层到一个地方，其中所有来源的相同字段/列对齐时。在
    SQL 中，这被称为 `UNION ALL`，而在 `pandas` 中，我们使用 `concat()` 函数将所有数据汇集在一起。
- en: 'A good visual example of how data is combined is in the following screenshot,
    where multiple source files are named `user_data_YYYY.cvs` and each year is defined
    as YYYY. These three files, which all have the same field names of `userid`, `date`, and
    `year`, are imported into one SQL table named `tbl_user_data_stage`, which is
    shown in the following screenshot. The target table that stores this information
    also includes a new field named `filesource` so the data lineage is more transparent
    to both the producer and the consumer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合并的一个良好视觉示例如下截图所示，其中多个源文件命名为 `user_data_YYYY.cvs`，并且每年都定义为 YYYY。这三个文件，它们都具有相同的字段名
    `userid`、`date` 和 `year`，被导入到一个名为 `tbl_user_data_stage` 的 SQL 表中，如下截图所示。存储此信息的目标表还包括一个名为
    `filesource` 的新字段，以便数据来源对生产者和消费者都更加透明：
- en: '![](img/0868ad5f-464e-4e91-b680-9dc5e1de379e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/0868ad5f-464e-4e91-b680-9dc5e1de379e.png)'
- en: 'Once the data has been processed and persisted into a table named `tbl_user_data_stage`,
    all of the records from the three files are preserved as displayed in the following
    table. In this example, any duplicates would be preserved between what existed
    in the source files and the target table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被处理并持久化到名为 `tbl_user_data_stage` 的表中，所有三个文件中的记录都将按以下表格所示保留。在此示例中，任何重复的记录都将保留在源文件和目标表之间：
- en: '![](img/525acd5e-4dbe-469d-9ab7-5ac7efe74497.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/525acd5e-4dbe-469d-9ab7-5ac7efe74497.png)'
- en: One of the reasons data engineering teams create `stage` tables is to help to
    build data ingestion pipelines and create business rules where duplicate records
    are removed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程团队创建 `阶段` 表的一个原因是为了帮助构建数据摄取管道并创建业务规则，其中重复的记录被删除。
- en: 'To recreate the example in Jupyter, let''s create a new notebook and name it
    `ch_07_combining_data`. There are more efficient ways to import multiple files
    but, in our example, we will import each one in separate DataFrames and then combine
    them into one:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Jupyter 中重现示例，让我们创建一个新的笔记本并将其命名为 `ch_07_combining_data`。导入多个文件有更高效的方法，但在我们的示例中，我们将分别将每个文件导入到单独的
    DataFrame 中，然后合并它们：
- en: 'Import the `pandas` library:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You will also need to copy the three CSV files to your local folder.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要将三个 CSV 文件复制到您的本地文件夹中。
- en: 'Import the first CSV file named `user_data_2017.csv`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入名为 `user_data_2017.csv` 的第一个 CSV 文件：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the `head()` command to verify the results:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `head()` 命令以验证结果：
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中行以标题行和从值 `0` 开始的索引显示：
- en: '![](img/352f93fc-909f-4097-8bb8-514cfcbf05ab.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/352f93fc-909f-4097-8bb8-514cfcbf05ab.png)'
- en: 'Repeat the process for the next CSV file, which is named `user_data_2018.csv`:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对下一个 CSV 文件重复此过程，该文件名为 `user_data_2018.csv`：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Run the `head()` command to verify the results:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `head()` 命令以验证结果：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中行以标题行和从值 `0` 开始的索引显示：
- en: '![](img/f521d692-48b2-4660-83ac-690e2f755de4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/f521d692-48b2-4660-83ac-690e2f755de4.png)'
- en: 'Repeat the process for the next CSV file, which is named `user_data_2019.csv`:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对下一个 CSV 文件重复此过程，该文件名为 `user_data_2019.csv`：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Run the `head()` command to verify the results:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `head()` 命令以验证结果：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中行以标题行和从值 `0` 开始的索引显示：
- en: '![](img/05142418-c3a4-4661-9b16-5d7bdbc3d62b.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/05142418-c3a4-4661-9b16-5d7bdbc3d62b.png)'
- en: 'The next step is to merge the DataFrames using the `concat()` function. We
    include the `ignore_index=True` parameter to create a new index value for all
    of the results:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用`concat()`函数合并DataFrame。我们包含`ignore_index=True`参数以为所有结果创建一个新的索引值：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the `head()` command to verify the results:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`head()`命令以验证结果：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下截图，其中行以标题行和从值`0`开始的索引显示：
- en: '![](img/b8b88974-fba2-4880-91f5-463fad4056ea.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8b88974-fba2-4880-91f5-463fad4056ea.png)'
- en: Binning
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分箱
- en: 'Binning is a very common analysis technique that allows you to group numeric
    data values based on one or more criteria. These groups become named categories;
    they are ordinal in nature and can have equal widths between the ranges or customized
    requirements. A good example is age ranges, which you commonly see on surveys
    such as that seen in the following screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱是一种非常常见的分析技术，允许您根据一个或多个标准对数值数据进行分组。这些组成为命名类别；它们在本质上是有序的，并且可以在范围之间具有相等的宽度或自定义要求。一个很好的例子是年龄范围，您通常在以下截图中所见的调查中看到：
- en: '![](img/9726f6e6-d29c-4d90-9c0c-b365e195734f.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9726f6e6-d29c-4d90-9c0c-b365e195734f.png)'
- en: In this example, a person's age range is the input, but what if we actually
    had the birthdate of each person available in the data? Then, we could calculate
    the age as of today and assign an age band based on the criteria in the preceding
    screenshot. This is the process of binning your data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，一个人的年龄范围是输入，但如果我们实际上在数据中有了每个人的出生日期怎么办？那么，我们可以计算今天的年龄并根据前面的截图中的标准分配年龄段。这就是分箱数据的过程。
- en: Another common example is weather data, where the assigned categories of *hot*,
    *warm*, or *cold* are assigned to ranges of Fahrenheit or Celsius temperature.
    Each bin value is defined by a condition that is arbitrarily decided by the data
    analyst.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的例子是天气数据，其中*热*、*温暖*或*冷*等分配给华氏或摄氏温度的范围。每个区间值由数据分析师任意决定的条件定义。
- en: 'For our user data, let''s assign age bins based on when the user first appeared
    in our dataset. We will define three bins based on the requirements, which allows
    us the flexibility to adjust the assigned ranges. For our example, we define the
    bins as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用户数据，让我们根据用户首次出现在我们的数据集中时的时间来分配年龄区间。我们将根据要求定义三个区间，这使我们能够调整分配的范围。在我们的例子中，我们定义区间如下：
- en: Less than 1 year
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不到1年
- en: 1 to 2 years
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1至2年
- en: Greater than 3 years
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过3年
- en: The specific conditions on how to create the bins will be evident once we walk
    through the code.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建区间的具体条件，一旦我们通过代码演示，就会变得明显。
- en: Another cool feature of this type of analysis is the fact that our calculated
    age is based on the usage data and a point in time that's calculated each time
    we run the code. For example, if the date of the first time a user hits the website
    is *1/1/2017* and we did this analysis on December 3, 2018, the age in days of
    the user would be 360, which would be assigned to the *Less than 1 year* bin.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型分析的另一项酷特性是，我们的计算年龄是基于使用数据和每次我们运行代码时计算的一个时间点。例如，如果用户第一次访问网站的时间是*2017年1月1日*，而我们在这个分析中是在2018年12月3日进行的，那么用户的年龄（以天为单位）将是360天，这将分配给*不到1年*的区间。
- en: If we rerun this analysis at a later date such as November 18, 2019, the calculated
    age would change, so the new assigned bin would be *1 to 2 years*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在稍后的日期，例如2019年11月18日，重新运行此分析，计算出的年龄将改变，因此新的分配区间将是*1至2年*。
- en: The decision on where to add the logic for each bin will vary. The most flexible
    to make changes to the assigned bins is to add the logic where you deliver the
    analytics. In our examples, that would be directly in the Jupyter notebook. However,
    in enterprise environments where many different technologies could be used to
    deliver the same analysis, it makes sense to move the binning logic closer to
    the source. In some cases of very large datasets stored in databases, using SQL
    or even having the schema changed in the table is a better option.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 每个区间添加逻辑的位置的决定会有所不同。最灵活地更改分配的区间是在您提供分析的地方添加逻辑。在我们的例子中，那将是直接在Jupyter笔记本中。然而，在企业环境中，可能使用许多不同的技术来提供相同分析，将分箱逻辑移得更靠近源是有意义的。在某些情况下，存储在数据库中的非常大的数据集，使用SQL甚至更改表的模式是一个更好的选择。
- en: If you have the luxury of a skilled data engineering team and experience of
    working with big data like I had, the decision to move the binning logic closer
    to the data tables is easy. In SQL, you could use `CASE Statement` or if/then
    logic. Qlik has a function called `class()`, which will bin values based on a
    linear scale. In Microsoft Excel, a nested formula can be used to assign bins
    based on a mix of functions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样有幸拥有一个熟练的数据工程团队和与大数据工作的经验，将分箱逻辑移至数据表附近的决定就很容易了。在SQL中，你可以使用`CASE语句`或if/then逻辑。Qlik有一个名为`class()`的函数，可以根据线性尺度对值进行分箱。在Microsoft
    Excel中，可以使用嵌套公式根据函数混合来分配分箱。
- en: So, the concept of binning can be applied across different technologies and
    as a good data analyst, you now have a foundation of understanding how it can
    be done.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分箱的概念可以应用于不同的技术，作为一名优秀的数据分析师，你现在已经具备了理解如何实现它的基础。
- en: Let's reinforce the knowledge by walking through an example using our usage
    data and Jupyter Notebook.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用我们的使用数据和Jupyter Notebook的示例来巩固知识。
- en: Remember to copy any dependency CSV files into the working folder before walking
    through the following steps.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行以下步骤之前，请记住将任何依赖的CSV文件复制到工作文件夹中。
- en: 'To recreate the example in Jupyter, let''s create a new notebook and name it
    `ch_07_sifting_and_binning_data`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Jupyter中重现示例，让我们创建一个新的笔记本，并将其命名为`ch_07_sifting_and_binning_data`：
- en: 'Import the `pandas` library:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`库：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Read in the CSV file provided that includes additional data for this example
    and create a new DataFrame named `df_user_churn_cleaned`. We are also converting
    the `date` field found in the source CSV file into a data type of `datetime64`
    while importing using the `parse_dates` parameter. This will make it easier to
    manipulate in the next few steps:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取提供的CSV文件，该文件包含本例的附加数据，并创建一个新的DataFrame，命名为`df_user_churn_cleaned`。我们还在导入时使用`parse_dates`参数将源CSV文件中找到的`date`字段转换为`datetime64`数据类型，这将使在接下来的几个步骤中更容易操作：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Verify the DataFrame is valid using the `head()` function:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`head()`函数验证DataFrame是否有效：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output of the function will look similar to the following table, where
    the DataFrame is loaded with two fields with the correct data types and is available
    for analysis:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出将类似于以下表格，其中DataFrame已加载具有正确数据类型的两个字段，并可用于分析：
- en: '![](img/26491120-81b8-4bd5-bd08-2a03283b3564.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26491120-81b8-4bd5-bd08-2a03283b3564.png)'
- en: 'Import the `datetime` and `numpy` libraries for reference later to calculate
    the `age` value of the `userid` field:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`datetime`和`numpy`库以供稍后参考，用于计算`userid`字段的`age`值：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a new derived column named `age` by calculating the difference between
    the current date and time using the `now` function and the `date` field. To format
    the `age` field in days, we include the `dt.days` function, which will convert
    the values into a clean `"%d"` format:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用`now`函数和`date`字段计算当前日期和时间之间的差异来创建一个新的派生列`age`。为了以天为单位格式化`age`字段，我们包括`dt.days`函数，它将值转换为干净的`"%d"`格式：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To match the screenshots, I explicitly defined the date value to 2020-02-28
    with a date format of YYYY-MM-DD. You can uncomment the preceding line to calculate
    the current timestamp. Since the timestamp changes every time you run the function,
    the results will not match exactly to any image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与截图匹配，我明确地将日期值定义为2020-02-28，日期格式为YYYY-MM-DD。你可以取消注释前面的行来计算当前的时间戳。由于时间戳每次运行函数都会变化，结果将不会与任何图像完全匹配。
- en: 'Verify that the new `age` column has been included in your DataFrame:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证新的`age`列是否已包含在你的DataFrame中：
- en: '[PRE35]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output of the function will look similar to the following table, where
    the DataFrame has been modified from its original import and includes a new field
    called `age`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出将类似于以下表格，其中DataFrame已经从原始导入中修改，并包含一个名为`age`的新字段：
- en: '![](img/807c6815-1eab-40a6-8929-4d8ae8a49dcf.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/807c6815-1eab-40a6-8929-4d8ae8a49dcf.png)'
- en: 'Create a new DataFrame called `df_ages` that groups the dimensions from the
    existing DataFrame and calculates the max `age` value by `userid`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的DataFrame，命名为`df_ages`，它从现有的DataFrame中分组维度并按`userid`计算最大`age`值：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will look similar to the following screenshot, where the number
    of rows has decreased from the source DateFrame. Only a distinct list of `userid` values
    will be displayed along with the maximum `age` when the first record was created
    by `userid`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中行数已从源DataFrame中减少。仅显示具有最大`age`值的唯一`userid`值，当第一条记录由`userid`创建时：
- en: '![](img/a2cc716b-cae4-43f8-a6cc-049d9eef0faf.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2cc716b-cae4-43f8-a6cc-049d9eef0faf.png)'
- en: 'Create a new `age_bin` column by using the `pandas` library''s `cut()` function.
    This will thread each value from the `age` field between one of the assigned `bins`
    range we have assigned. We use the `labels` parameter to make the analysis easier
    to consume for any audience. Note that the value of `9999` was chosen to create
    a maximum boundary for the `age` value:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 `pandas` 库的 `cut()` 函数创建一个新的 `age_bin` 列。这将把 `age` 字段中的每个值放置在我们分配的 `bins`
    范围之一之间。我们使用 `labels` 参数使分析对任何受众都更容易理解。注意，我们选择了 `9999` 的值来为 `age` 值创建一个最大边界：
- en: '[PRE37]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the DataFrame and validate the bin values displayed:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 DataFrame 并验证显示的 bin 值：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the function will look similar to the following screenshot, where
    the DataFrame has been modified and we now see the values in the `age_bin` field:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出将类似于以下截图，其中 DataFrame 已被修改，我们现在可以看到 `age_bin` 字段中的值：
- en: '![](img/54d672fc-1106-43d2-b44a-99fbe93b5a92.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54d672fc-1106-43d2-b44a-99fbe93b5a92.png)'
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations, you have now increased your data literacy skills by working with
    data as both a consumer and producer of analytics. We covered some important topics,
    including essential skills to manipulate data by creating views of data, sorting,
    and querying tabular data from a SQL source. You now have a repeatable workflow
    for combining multiple data sources into one refined dataset.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，通过作为数据分析的消费者和生产者来处理数据，你已经提高了你的数据素养技能。我们涵盖了一些重要主题，包括通过创建数据视图、排序和从 SQL 源查询表格数据来操纵数据的必要技能。你现在有一个可重复的工作流程，可以将多个数据源合并成一个精炼的数据集。
- en: We explored additional features of working with `pandas` DataFrames, showing
    how to restrict and sift data. We walked through real-world practical examples
    using the concept of *u*<q>ser churn</q> to answer key business questions about
    usage patterns by isolating specific users and dealing with missing values from
    the source data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了使用 `pandas` DataFrame 的其他功能，展示了如何限制和筛选数据。我们通过使用 *u*<q>用户流失</q> 的概念来介绍现实世界的实际示例，以回答关于使用模式的关键业务问题，通过隔离特定用户和处理源数据中的缺失值。
- en: Our next chapter is [Chapter 8](9bdac090-8534-480e-8154-a854115c0b7a.xhtml), *Understanding
    Joins, Relationships, and Data Aggregates*. Along with creating a summary analysis
    using a concept called aggregation, we will also go into detail on how to join
    data with defined relationships.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一章是[第 8 章](9bdac090-8534-480e-8154-a854115c0b7a.xhtml)，*理解连接、关系和数据聚合*。除了使用聚合的概念创建总结分析外，我们还将详细介绍如何通过定义的关系连接数据。
- en: Further reading
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following links for more information on the topics of
    this chapter:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接以获取有关本章主题的更多信息：
- en: A nice walk-through of filtering and grouping using DataFrames: [https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb](https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataFrame 进行过滤和分组的良好教程：[https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb](https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb)
- en: Comparison of SQL features and their equivalent pandas functions: [https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 特性与它们等效的 pandas 函数的比较：[https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)
- en: Additional information on exporting data to Excel: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于将数据导出到 Excel 的更多信息：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel)
- en: Examples of SQL date and time functions: [https://www.postgresql.org/docs/8.1/functions-datetime.html](https://www.postgresql.org/docs/8.1/functions-datetime.html)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 日期和时间函数的示例：[https://www.postgresql.org/docs/8.1/functions-datetime.html](https://www.postgresql.org/docs/8.1/functions-datetime.html)
