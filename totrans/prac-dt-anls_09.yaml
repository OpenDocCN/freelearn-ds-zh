- en: Exploring, Cleaning, Refining, and Blending Datasets
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the power of data visualizations,
    and the importance of having good-quality, consistent data defined with dimensions
    and measures.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand *why* that's important, we are going to focus on the
    *how* throughout this chapter by working hands-on with data. Most of the examples
    provided so far included data that was already *prepped* (prepared) ahead of time
    for easier consumption. We are now switching gears by learning the skills that
    are necessary to be comfortable working with data to increase your data literacy.
  prefs: []
  type: TYPE_NORMAL
- en: A key concept of this chapter is cleaning, filtering, and refining data. In
    many cases, the reason why you need to perform these actions is the source data
    does not provide high-quality analytics *as is*. Throughout my career, high-quality
    data is not the norm and data gaps are common. As good data analysts, we need
    to work with what we have available. We will cover some techniques to enrich the
    quality of the data so you can provide quality insights and answer questions from
    the data even when the source does not include all of the information required.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, highlighting the poor quality of the source data is the insight
    because not enough transparency exists and key stakeholders are unaware of the
    challenges of using the data. The bottom line is poor quality should not stop
    you from proceeding with working with data. My goal is to demonstrate a repeatable
    technique and workflow to improve data quality for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving, viewing, and storing tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to restrict, sort, and sift through data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning, refining, and purifying data using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining and binning data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving, viewing, and storing tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to retrieve and view tabular data has been covered multiple times
    in prior chapters; however, those examples were focused on the perspective of
    the consumer. We learned the skills necessary to understand what structured data
    is in, the many different forms it can take, and how to answer some questions
    from data. Our data literacy has increased during this time but we have relied
    on the producers of data sources to make it easier to read using a few Python
    commands or SQL commands. In this chapter, we are switching gears from being exclusively
    a **consumer** to now a **producer** of data by learning skills to manipulate
    data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: As a good data analyst, you will need both sides of the consumer and producer
    spectrum of skills to solve more complicated questions with data. For example,
    a common measure requested by businesses with web or mobile users is called **usage
    analytics**. This means counting the number of users over snapshots of time, such
    as by day, week, month, and year. More importantly, you want to better understand
    whether those users are new, returning, or lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common questions related to usage analytics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: How many new users have hit the website this day, week, or month?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many returning users have accessed the website this day, week, or month?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many users have we lost (inactive for more than 60 days) this week, month,
    or year?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To answer these types of questions, your data source must have, at a minimum, `timestamp`
    and unique `user_id` fields available. In many cases, this data will have high
    volume and velocity, so analyzing this information will require the right combination
    of people, processes, and technology, which I have had the pleasure of working
    with. Data engineering teams build out ingestion pipelines to make this data accessible
    for reporting and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: You may need to work with the data engineering team to apply the business rules
    and summary levels (also known as aggregates) to the data that include additional
    fields required to answer the user analytics questions. For our examples, I have
    provided a much smaller sample of data and we are going to derive new data fields
    from the existing source data file provided.
  prefs: []
  type: TYPE_NORMAL
- en: I find the best way to learn is to walk through the steps together, so let's
    create a new Jupyter notebook named `user_churn_prep`. We will begin with retrieving
    data from SQL against a database and loading it into a DataFrame, similar to the
    steps outlined in [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering
    and Loading Data in Python*. To keep it simple, we are using another SQLite database
    to retrieve the source data.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like more details about connecting to SQL data sources, please
    refer to [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering and
    Loading Data* *in Python*.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a connection and use SQLite, we have to import a new library using
    the code. For this example, I have provided the database file named `user_hits.db`,
    so be sure to download it from my GitHub repository beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a SQLite database connection, you just need to add the following command
    in your Jupyter notebook and run the cell. I have placed a copy on GitHub for
    reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Be sure that you have copied the `user_hits.db` file to the correct Jupyter
    folder directory to avoid errors with the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library so you can create a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a SQL statement and assign the results to a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    Your code should look similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9989271e-f748-4d28-915e-049d558dff00.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to view the results of the retrieved data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following table, where the `tbl_user_hits` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8eff19fc-3499-4283-841d-ec0b75b7c443.png)'
  prefs: []
  type: TYPE_IMG
- en: Before we move on to the next step, let's verify the data we loaded with a few
    metadata commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type in `df_user_churn.info()` in the next `In[]:` cell and run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows,
    including data types for all columns, similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8de6e38-5c79-4911-bd87-b3cb59aaf02e.png)'
  prefs: []
  type: TYPE_IMG
- en: Storing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the data available to work with as a DataFrame in Jupyter,
    let's run a few commands to store it as a file for reference. Storing data as
    a snapshot for analysis is a useful technique to learn, and while our example
    is simplistic, the concept will help in future data analysis projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To store your DataFrame into a CSV file, you just have to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where a new CSV
    file is created in the same project folder as your current Jupyter notebook. Based
    on the OS you are using on your workstation, the results will vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32997d53-84a6-4c1a-9881-ec3357bdf233.png)'
  prefs: []
  type: TYPE_IMG
- en: There are other formats you can export your DataFrame to, including Excel. You
    should also note the file path from which you are exporting the data file. Check
    out the *Further reading* section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to restrict, sort, and sift through data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the data available in a DataFrame, we can walk through how
    to restrict, sort, and sift through data with a few Python commands. The concepts
    we are going to walk through using pandas are also common using SQL, so I will
    also include the equivalent SQL commands for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Restricting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of restricting data, which is also known as filtering data, is all
    about isolating one or more records based on conditions. Simple examples are when
    you are only retrieving results based on matching a specific field and value.
    For example, you only want to see results for one user or a specific point in
    time. Other requirements for restricting data can be more complicated, including
    explicit conditions that require elaborate logic, business rules, and multiple
    steps. I will not be covering complex examples that require complex logic but
    will add some references in the *Further reading* section. However, the concepts
    covered will teach you essential skills to satisfy many common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our first example, let''s isolate one specific user from our DataFrame.
    Using `pandas` commands, that is pretty easy, so let''s start up a new Jupyter
    notebook named `user_churn_restricting`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library so you can create a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new DataFrame by loading the data from the CSV file we created in
    the prior example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The file path and filename must be the same as those you used in the prior example.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have all user data loaded into a single DataFrame, we can easily
    reference the source dataset to restrict results. It is a best practice to keep
    this source DataFrame intact so you can reference it for other purposes and analysis.
    It is also common during analysis to need to make adjustments based on changing
    requirements, or that you will only gain insights by making adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: In my career, I follow a common practice of *you don't know what you don't know* while
    working with data, so having the flexibility to easily reference the source data
    without undoing your changes is important. This is commonly known as snapshotting
    your analysis and having the ability to roll back changes as needed.
  prefs: []
  type: TYPE_NORMAL
- en: When working with big data sources where the sources are larger than a billion
    rows, snapshots will require a large number of resources where RAM and CPU will
    be impacted. You may be required to snapshot incrementally for a specific date
    or create a rolling window of time to limit the amount of data you can work with
    at one time.
  prefs: []
  type: TYPE_NORMAL
- en: To restrict our data to a specific user, we will be creating a new DataFrame
    from the source DataFrame. That way, if we need to make adjustments to the filters
    used to create the new DataFrame, we don't have to rerun all of the steps from
    the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame by loading the data from the source DataFrame. The syntax
    is nested so you are actually calling the same `df_user_churn` DataFrame within
    itself and filtering results only for the explicit value where `userid` is equal
    to `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To view and verify the results, you can run a simple `head()` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the only two
    rows in the new `df_user_restricted` DataFrame have a value where `userid` is
    `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdc9fa3a-878d-45af-adfd-7c3b6b77e979.png)'
  prefs: []
  type: TYPE_IMG
- en: Restricting data helps to isolate records for specific types of analysis to
    help to answer additional questions. In the next step, we can start answering
    questions related to usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have isolated a specific user by creating a new DataFrame, which
    is now available for reference, we can enhance our analysis by asking questions
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: When did a specific user start using our website?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How frequently does this user access our website?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When was the last time this user accessed our website?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these questions can be answered with a few simple Python commands focused
    on sorting commands. Sorting data is a skill that computer programmers of any
    programming language are familiar with. It's easily done with SQL by adding an `order
    by` command. Many third-party software, such as Microsoft Excel, Google Sheets,
    or Qlik Sense, has a sorting feature built in. The concept of sorting data is
    well known, so I will not go into a detailed definition; rather, I will focus
    on important features and best practices when performing data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: With structured data, sorting is commonly understood to be row-level by specific
    columns, which will be defined by ordering the sequence of the values from either
    low to high or high to low. The default is low to high unless you explicitly change
    it. If the values have a data type that is numeric, such as integer or float,
    the sort order sequence will be easy to identify. For textual data, the values
    are sorted alphabetically, and, depending on the technology used, mixed case text
    will be handled differently. In Python and pandas, we have specific functions
    available along with parameters to handle many different use cases and needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start answering some of the questions we outlined previously using the
    `sort()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer the question *When did a specific user start using our website?*,
    we just need to run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the results
    are sorted in ascending order by the `date` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68281f91-b36c-4ed4-baae-eb3b505f4694.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To answer the question "*When is the last time this user accessed our website?*", we
    just need to run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the same records
    are displayed as the previous one; however, the values are sorted in descending
    order by last date available for this specific `userid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e34fe94c-7b2f-4e90-9b5b-e6be0a53018a.png)'
  prefs: []
  type: TYPE_IMG
- en: Sifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of sifting through data means we are isolating specific columns
    and/or rows from a dataset based on one or more conditions. There are nuanced
    differences between sifting versus restricting, so I would distinguish sifting
    as the need to include additional business rules or conditions applied to a population
    of data to isolate a subset of that data. Sifting data usually requires creating
    new derived columns from the source data to answer more complex questions. For
    our next example, a good question about usage would be: *Do the same users who
    hit our website on Monday also return during the same week?*
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, we need to isolate the usage patterns for a specific
    day of the week. This process requires a few steps, which we will outline together
    from the original DataFrame we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame by loading the data from the source file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to extend the DataFrame by adding new derived columns to help
    to make the analysis easier. Since we have a `Timestamp` field available, the
    pandas library has some very useful functions available to help the process. Standard
    SQL has built-in features as well and will vary depending on which RDMS is used,
    so you will have to reference the date/time functions available. For example,
    a Postgres database uses the syntax of `select to_char(current_date,'Day');` to
    convert a date field into the current day of the week.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import a new `datetime` library for easy reference to date and time functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign a variable to the current `datetime` for easier calculation of the `age`
    from today:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new derived column called `age` that is calculated from the current date
    minus the date value per user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you receive `datetime` errors in your notebook, you may need to upgrade your
    `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning, refining, and purifying data using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data quality is highly important for any data analysis and analytics. In many
    cases, you will not understand how good or bad the data quality is until you start
    working with it. I would define good-quality data as information that is well
    structured, defined, and consistent, where almost all of the values in each field
    are defined as expected. In my experience, data warehouses will have high-quality
    data because it has been reported on across the organization. In my experience, bad
    data quality occurs where a lack of transparency exists against the data source.
    Bad data quality examples are a lack of conformity and inconsistency in the expected
    data type or any consistent pattern of values in delimited datasets. To help to
    solve these data quality issues, you can begin to understand your data with the
    concepts and questions we covered in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),* Fundamentals
    of Data Analysis*, with **Know Your Data (KYD)**. Since the quality of data will
    vary by source, some specific questions you can ask to understand data quality
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the data structured or unstructured?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data lineage trace back to a system or application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data get transformed and stored in a warehouse?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data have a schema with each field having a defined data type?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have a data dictionary available with business rules documented?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiving answers to these questions ahead of time would be a luxury; uncovering
    them as you go is more common for a data analyst. During this process, you will
    still find the need to clean, refine, and purify your data for analysis purposes.
    How much time you need to spend will vary on many different factors, and the true
    cost of quality will be the time and effort required to improve data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data can take on many different forms and has been a common practice
    for decades for data engineers and analytic practitioners. There are many different
    technologies and skillsets required for enterprise and big data cleansing. Data
    cleaning is an industry within **Information Technology** (**IT**) because good-quality
    data is worth the price of outsourcing.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common definition of data cleansing is the process of removing or resolving
    poor-quality data records from the source, which can vary based on the technology
    used to persist the data, such as a database table or encoded file. Poor-quality
    data can be identified as any data that does not match the producers'' intended
    and defined requirements. This can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing or null (`NaN`) values from the fields of one or more rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orphan records where the primary or foreign keys cannot be found in any referenced
    source tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corrupted records where one or more records cannot be read by any reporting
    or analysis technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our example, let''s look at our usage data again and see whether we can
    find any issues by profiling it to see whether we can find any anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the CSV file and run the `info()` command to confirm the data types
    and row counts and profile the DataFrame for more information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where metadata of
    the DataFrame is presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a045eaa8-bd04-4e25-9fc9-503c47d0d599.png)'
  prefs: []
  type: TYPE_IMG
- en: One anomaly that is uncovered is that the number of values is different between
    the two fields. For `userid`, there are 9 non-null values and for the `date` field,
    there are 12 non-null values. For this dataset, we expect each row to have one
    value for both fields, but this command is telling us there are missing values.
    Let's run another command to identify which index/row has the missing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `isnull()` command to confirm the data types and row counts and profile
    the DataFrame for more information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following table, where a list of `True`
    and `False` values is displayed by row and column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e00c8355-7f06-4f3a-9934-d2868db06239.png)'
  prefs: []
  type: TYPE_IMG
- en: The record count looks okay but notice that there are null values (NaN) that
    exist in the `userid` field. A unique identifier for each row to help us to identify
    each user is critical for accurate analysis of this data. The reason why `userid` is
    blank would have to be explained by the producer of this data and may require
    additional engineering resources to help to investigate and troubleshoot the root
    cause of the issue. In some cases, it may be a simple technical hiccup during
    data source creation that requires a minor code change and reprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: I always recommend cleaning data as close to the source as possible, which saves
    time by avoiding reworking by other data analysts or reporting systems.
  prefs: []
  type: TYPE_NORMAL
- en: Having nulls included in our analysis will impact our summary statistics and
    metrics. For example, the count of the average daily users would be lower on the
    dates where the null values exist. For the user churn analysis, the measure of
    the frequency of reporting users would be skewed because the NaN values could
    be one of the returning `user_ids` or a new user.
  prefs: []
  type: TYPE_NORMAL
- en: With any high volume transaction-based system, there could be a margin of error
    that you may need to account for. As a good data analyst, ask the question, *what
    is the cost of quality and of being one hundred percent accurate?* If the price
    is too high due to the time and resources required to change it, a good alternative
    is to exclude and isolate the missing data so it can be investigated later.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you end up adding isolated data back into your analysis, you will
    have to restate results and inform any consumers of the change to your metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through an example of how to isolate and exclude any missing data
    by identifying the NaN records and creating a new DataFrame that has them removed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame by loading the data from the source DataFrame, except
    we will exclude the null values by adding the `dropna()` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To view and verify the results, you can run a simple `head()` command and confirm
    the NaN/null values have been removed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following table, where the new DataFrame
    has complete records with no missing values in either `userid` or `date`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ecc9d12-931f-41d3-b27f-20d94839ffc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Combining and binning data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Combining multiple data sources is sometimes necessary for multiple reasons,
    which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The source data is broken up into many different files with the same defined
    schema (tables and field names), but the number of rows will vary slightly. A
    common reason is for storage purposes, where it is easier to maintain multiple
    smaller file sizes versus one large file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is partitioned where one field is used to break apart the data for
    faster response time reading or writing to the source data. For example, HIVE/HDFS
    recommends storing data by a single date value so you can easily identify when
    it was processed and quickly extract data for a specific day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historical data is stored in a different technology than more current data.
    For example, the engineering team changed the technology being used to manage
    the source data and it was decided not to import historical data beyond a specific
    date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any of the reasons defined here, combining data is a common practice in
    data analysis. I would define the process of combining data as when you are layering
    two or more data sources into one where the same fields/columns from all sources
    align. In SQL, this would be known as `UNION ALL` and in `pandas`, we use the
    `concat()` function to bring all of the data together.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good visual example of how data is combined is in the following screenshot,
    where multiple source files are named `user_data_YYYY.cvs` and each year is defined
    as YYYY. These three files, which all have the same field names of `userid`, `date`, and
    `year`, are imported into one SQL table named `tbl_user_data_stage`, which is
    shown in the following screenshot. The target table that stores this information
    also includes a new field named `filesource` so the data lineage is more transparent
    to both the producer and the consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0868ad5f-464e-4e91-b680-9dc5e1de379e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the data has been processed and persisted into a table named `tbl_user_data_stage`,
    all of the records from the three files are preserved as displayed in the following
    table. In this example, any duplicates would be preserved between what existed
    in the source files and the target table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/525acd5e-4dbe-469d-9ab7-5ac7efe74497.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the reasons data engineering teams create `stage` tables is to help to
    build data ingestion pipelines and create business rules where duplicate records
    are removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recreate the example in Jupyter, let''s create a new notebook and name it
    `ch_07_combining_data`. There are more efficient ways to import multiple files
    but, in our example, we will import each one in separate DataFrames and then combine
    them into one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You will also need to copy the three CSV files to your local folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the first CSV file named `user_data_2017.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `head()` command to verify the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/352f93fc-909f-4097-8bb8-514cfcbf05ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat the process for the next CSV file, which is named `user_data_2018.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `head()` command to verify the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f521d692-48b2-4660-83ac-690e2f755de4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat the process for the next CSV file, which is named `user_data_2019.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `head()` command to verify the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05142418-c3a4-4661-9b16-5d7bdbc3d62b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to merge the DataFrames using the `concat()` function. We
    include the `ignore_index=True` parameter to create a new index value for all
    of the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `head()` command to verify the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will look similar to the following screenshot, where the rows are
    displayed with a header row and index added starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8b88974-fba2-4880-91f5-463fad4056ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Binning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Binning is a very common analysis technique that allows you to group numeric
    data values based on one or more criteria. These groups become named categories;
    they are ordinal in nature and can have equal widths between the ranges or customized
    requirements. A good example is age ranges, which you commonly see on surveys
    such as that seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9726f6e6-d29c-4d90-9c0c-b365e195734f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, a person's age range is the input, but what if we actually
    had the birthdate of each person available in the data? Then, we could calculate
    the age as of today and assign an age band based on the criteria in the preceding
    screenshot. This is the process of binning your data.
  prefs: []
  type: TYPE_NORMAL
- en: Another common example is weather data, where the assigned categories of *hot*,
    *warm*, or *cold* are assigned to ranges of Fahrenheit or Celsius temperature.
    Each bin value is defined by a condition that is arbitrarily decided by the data
    analyst.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our user data, let''s assign age bins based on when the user first appeared
    in our dataset. We will define three bins based on the requirements, which allows
    us the flexibility to adjust the assigned ranges. For our example, we define the
    bins as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Less than 1 year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 to 2 years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater than 3 years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific conditions on how to create the bins will be evident once we walk
    through the code.
  prefs: []
  type: TYPE_NORMAL
- en: Another cool feature of this type of analysis is the fact that our calculated
    age is based on the usage data and a point in time that's calculated each time
    we run the code. For example, if the date of the first time a user hits the website
    is *1/1/2017* and we did this analysis on December 3, 2018, the age in days of
    the user would be 360, which would be assigned to the *Less than 1 year* bin.
  prefs: []
  type: TYPE_NORMAL
- en: If we rerun this analysis at a later date such as November 18, 2019, the calculated
    age would change, so the new assigned bin would be *1 to 2 years*.
  prefs: []
  type: TYPE_NORMAL
- en: The decision on where to add the logic for each bin will vary. The most flexible
    to make changes to the assigned bins is to add the logic where you deliver the
    analytics. In our examples, that would be directly in the Jupyter notebook. However,
    in enterprise environments where many different technologies could be used to
    deliver the same analysis, it makes sense to move the binning logic closer to
    the source. In some cases of very large datasets stored in databases, using SQL
    or even having the schema changed in the table is a better option.
  prefs: []
  type: TYPE_NORMAL
- en: If you have the luxury of a skilled data engineering team and experience of
    working with big data like I had, the decision to move the binning logic closer
    to the data tables is easy. In SQL, you could use `CASE Statement` or if/then
    logic. Qlik has a function called `class()`, which will bin values based on a
    linear scale. In Microsoft Excel, a nested formula can be used to assign bins
    based on a mix of functions.
  prefs: []
  type: TYPE_NORMAL
- en: So, the concept of binning can be applied across different technologies and
    as a good data analyst, you now have a foundation of understanding how it can
    be done.
  prefs: []
  type: TYPE_NORMAL
- en: Let's reinforce the knowledge by walking through an example using our usage
    data and Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to copy any dependency CSV files into the working folder before walking
    through the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recreate the example in Jupyter, let''s create a new notebook and name it
    `ch_07_sifting_and_binning_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the CSV file provided that includes additional data for this example
    and create a new DataFrame named `df_user_churn_cleaned`. We are also converting
    the `date` field found in the source CSV file into a data type of `datetime64`
    while importing using the `parse_dates` parameter. This will make it easier to
    manipulate in the next few steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the DataFrame is valid using the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the function will look similar to the following table, where
    the DataFrame is loaded with two fields with the correct data types and is available
    for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26491120-81b8-4bd5-bd08-2a03283b3564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Import the `datetime` and `numpy` libraries for reference later to calculate
    the `age` value of the `userid` field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new derived column named `age` by calculating the difference between
    the current date and time using the `now` function and the `date` field. To format
    the `age` field in days, we include the `dt.days` function, which will convert
    the values into a clean `"%d"` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To match the screenshots, I explicitly defined the date value to 2020-02-28
    with a date format of YYYY-MM-DD. You can uncomment the preceding line to calculate
    the current timestamp. Since the timestamp changes every time you run the function,
    the results will not match exactly to any image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the new `age` column has been included in your DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the function will look similar to the following table, where
    the DataFrame has been modified from its original import and includes a new field
    called `age`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/807c6815-1eab-40a6-8929-4d8ae8a49dcf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new DataFrame called `df_ages` that groups the dimensions from the
    existing DataFrame and calculates the max `age` value by `userid`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look similar to the following screenshot, where the number
    of rows has decreased from the source DateFrame. Only a distinct list of `userid` values
    will be displayed along with the maximum `age` when the first record was created
    by `userid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2cc716b-cae4-43f8-a6cc-049d9eef0faf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new `age_bin` column by using the `pandas` library''s `cut()` function.
    This will thread each value from the `age` field between one of the assigned `bins`
    range we have assigned. We use the `labels` parameter to make the analysis easier
    to consume for any audience. Note that the value of `9999` was chosen to create
    a maximum boundary for the `age` value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the DataFrame and validate the bin values displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the function will look similar to the following screenshot, where
    the DataFrame has been modified and we now see the values in the `age_bin` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54d672fc-1106-43d2-b44a-99fbe93b5a92.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, you have now increased your data literacy skills by working with
    data as both a consumer and producer of analytics. We covered some important topics,
    including essential skills to manipulate data by creating views of data, sorting,
    and querying tabular data from a SQL source. You now have a repeatable workflow
    for combining multiple data sources into one refined dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We explored additional features of working with `pandas` DataFrames, showing
    how to restrict and sift data. We walked through real-world practical examples
    using the concept of *u*<q>ser churn</q> to answer key business questions about
    usage patterns by isolating specific users and dealing with missing values from
    the source data.
  prefs: []
  type: TYPE_NORMAL
- en: Our next chapter is [Chapter 8](9bdac090-8534-480e-8154-a854115c0b7a.xhtml), *Understanding
    Joins, Relationships, and Data Aggregates*. Along with creating a summary analysis
    using a concept called aggregation, we will also go into detail on how to join
    data with defined relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for more information on the topics of
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A nice walk-through of filtering and grouping using DataFrames: [https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb](https://github.com/bhavaniravi/pandas_tutorial/blob/master/Pandas_Basics_To_Beyond.ipynb)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of SQL features and their equivalent pandas functions: [https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional information on exporting data to Excel: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of SQL date and time functions: [https://www.postgresql.org/docs/8.1/functions-datetime.html](https://www.postgresql.org/docs/8.1/functions-datetime.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
