- en: Chapter 1. Setting Up a Spark Virtual Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. 设置Spark虚拟环境
- en: 'In this chapter, we will build an isolated virtual environment for development
    purposes. The environment will be powered by Spark and the PyData libraries provided
    by the Python Anaconda distribution. These libraries include Pandas, Scikit-Learn,
    Blaze, Matplotlib, Seaborn, and Bokeh. We will perform the following activities:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为开发目的构建一个隔离的虚拟环境。该环境将由Spark和Python Anaconda发行版提供的PyData库供电。这些库包括Pandas、Scikit-Learn、Blaze、Matplotlib、Seaborn和Bokeh。我们将执行以下活动：
- en: Setting up the development environment using the Anaconda Python distribution.
    This will include enabling the IPython Notebook environment powered by PySpark
    for our data exploration tasks.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Anaconda Python发行版设置开发环境。这包括启用由PySpark支持的IPython Notebook环境，用于我们的数据探索任务。
- en: Installing and enabling Spark, and the PyData libraries such as Pandas, Scikit-
    Learn, Blaze, Matplotlib, and Bokeh.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并启用Spark，以及Pandas、Scikit-Learn、Blaze、Matplotlib和Bokeh等PyData库。
- en: Building a `word count` example app to ensure that everything is working fine.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个`word count`示例应用程序以确保一切正常工作。
- en: The last decade has seen the rise and dominance of data-driven behemoths such
    as Amazon, Google, Twitter, LinkedIn, and Facebook. These corporations, by seeding,
    sharing, or disclosing their infrastructure concepts, software practices, and
    data processing frameworks, have fostered a vibrant open source software community.
    This has transformed the enterprise technology, systems, and software architecture.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年见证了数据驱动巨头如亚马逊、谷歌、推特、领英和Facebook的崛起和主导地位。这些公司通过播种、共享或披露其基础设施概念、软件实践和数据处理框架，培育了一个充满活力的开源软件社区。这已经改变了企业技术、系统和软件架构。
- en: This includes new infrastructure and DevOps (short for development and operations),
    concepts leveraging virtualization, cloud technology, and software-defined networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括新的基础设施和DevOps（代表开发和运营），这些概念利用了虚拟化、云计算技术和软件定义网络。
- en: To process petabytes of data, Hadoop was developed and open sourced, taking
    its inspiration from the **Google File System** (**GFS**) and the adjoining distributed
    computing framework, MapReduce. Overcoming the complexities of scaling while keeping
    costs under control has also led to a proliferation of new data stores. Examples
    of recent database technology include Cassandra, a columnar database; MongoDB,
    a document database; and Neo4J, a graph database.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理PB级的数据，Hadoop被开发并开源，其灵感来自**谷歌文件系统**（**GFS**）和相邻的分布式计算框架MapReduce。在控制成本的同时克服扩展的复杂性，也导致了新的数据存储的激增。最近数据库技术的例子包括Cassandra，一个列式数据库；MongoDB，一个文档数据库；以及Neo4J，一个图数据库。
- en: Hadoop, thanks to its ability to process huge datasets, has fostered a vast
    ecosystem to query data more iteratively and interactively with Pig, Hive, Impala,
    and Tez. Hadoop is cumbersome as it operates only in batch mode using MapReduce.
    Spark is creating a revolution in the analytics and data processing realm by targeting
    the shortcomings of disk input-output and bandwidth-intensive MapReduce jobs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop凭借其处理大量数据集的能力，催生了一个庞大的生态系统，通过Pig、Hive、Impala和Tez等工具以更迭代和交互的方式查询数据。由于Hadoop仅以批处理模式使用MapReduce进行操作，因此它显得有些繁琐。Spark通过针对磁盘输入/输出和带宽密集型MapReduce作业的不足，正在创造分析和数据处理领域的革命。
- en: Spark is written in Scala, and therefore integrates natively with the **Java
    Virtual Machine** (**JVM**) powered ecosystem. Spark had early on provided Python
    API and bindings by enabling PySpark. The Spark architecture and ecosystem is
    inherently polyglot, with an obvious strong presence of Java-led systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala编写的，因此与由Java虚拟机（**JVM**）驱动的生态系统实现了原生集成。Spark很早就通过启用PySpark提供了Python
    API和绑定。Spark的架构和生态系统本质上是多语言的，Java主导的系统具有明显的强大存在。
- en: This book will focus on PySpark and the PyData ecosystem. Python is one of the
    preferred languages in the academic and scientific community for data-intensive
    processing. Python has developed a rich ecosystem of libraries and tools in data
    manipulation with Pandas and Blaze, in Machine Learning with Scikit-Learn, and
    in data visualization with Matplotlib, Seaborn, and Bokeh. Hence, the aim of this
    book is to build an end-to-end architecture for data-intensive applications powered
    by Spark and Python. In order to put these concepts in to practice, we will analyze
    social networks such as Twitter, GitHub, and Meetup. We will focus on the activities
    and social interactions of Spark and the Open Source Software community by tapping
    into GitHub, Twitter, and Meetup.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Building data-intensive applications requires highly scalable infrastructure,
    polyglot storage, seamless data integration, multiparadigm analytics processing,
    and efficient visualization. The following paragraph describes the data-intensive
    app architecture blueprint that we will adopt throughout the book. It is the backbone
    of the book. We will discover Spark in the context of the broader PyData ecosystem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of data-intensive applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the architecture of data-intensive applications, the
    following conceptual framework is used. The is architecture is designed on the
    following five layers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure layer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence layer
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration layer
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytics layer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engagement layer
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the five layers of the **Data Intensive App
    Framework**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the architecture of data-intensive applications](img/B03968_01_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: From the bottom up, let's go through the layers and their main purpose.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure layer
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The infrastructure layer is primarily concerned with virtualization, scalability,
    and continuous integration. In practical terms, and in terms of virtualization,
    we will go through building our own development environment in a VirtualBox and
    virtual machine powered by Spark and the Anaconda distribution of Python. If we
    wish to scale from there, we can create a similar environment in the cloud. The
    practice of creating a segregated development environment and moving into test
    and production deployment can be automated and can be part of a continuous integration
    cycle powered by DevOps tools such as **Vagrant**, **Chef**, **Puppet**, and **Docker**.
    Docker is a very popular open source project that eases the installation and deployment
    of new environments. The book will be limited to building the virtual machine
    using VirtualBox. From a data-intensive app architecture point of view, we are
    describing the essential steps of the infrastructure layer by mentioning scalability
    and continuous integration beyond just virtualization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Persistence layer
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The persistence layer manages the various repositories in accordance with data
    needs and shapes. It ensures the set up and management of the polyglot data stores.
    It includes relational database management systems such as **MySQL** and **PostgreSQL**;
    key-value data stores such as **Hadoop**, **Riak**, and **Redis**; columnar databases
    such as **HBase** and **Cassandra**; document databases such as **MongoDB** and
    **Couchbase**; and graph databases such as **Neo4j**. The persistence layer manages
    various filesystems such as Hadoop's HDFS. It interacts with various storage systems
    from native hard drives to Amazon S3\. It manages various file storage formats
    such as `csv`, `json`, and `parquet`, which is a column-oriented format.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Integration layer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The integration layer focuses on data acquisition, transformation, quality,
    persistence, consumption, and governance. It is essentially driven by the following
    five Cs: *connect*, *collect*, *correct*, *compose*, and *consume*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'The five steps describe the lifecycle of data. They are focused on how to acquire
    the dataset of interest, explore it, iteratively refine and enrich the collected
    information, and get it ready for consumption. So, the steps perform the following
    operations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Connect**: Targets the best way to acquire data from the various data sources,
    APIs offered by these sources, the input format, input schemas if they exist,
    the rate of data collection, and limitations from providers'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correct**: Focuses on transforming data for further processing and also ensures
    that the quality and consistency of the data received are maintained'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collect**: Looks at which data to store where and in what format, to ease
    data composition and consumption at later stages'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compose**: Concentrates its attention on how to mash up the various data
    sets collected, and enrich the information in order to build a compelling data-driven
    product'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consume**: Takes care of data provisioning and rendering and how the right
    data reaches the right individual at the right time'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control**: This sixth *additional* step will sooner or later be required
    as the data, the organization, and the participants grow and it is about ensuring
    data governance'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the iterative process of data acquisition and
    refinement for consumption:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Integration layer](img/B03968_01_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Analytics layer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analytics layer is where Spark processes data with the various models, algorithms,
    and machine learning pipelines in order to derive insights. For our purpose, in
    this book, the analytics layer is powered by Spark. We will delve deeper in subsequent
    chapters into the merits of Spark. In a nutshell, what makes it so powerful is
    that it allows multiple paradigms of analytics processing in a single unified
    platform. It allows batch, streaming, and interactive analytics. Batch processing
    on large datasets with longer latency periods allows us to extract patterns and
    insights that can feed into real-time events in streaming mode. Interactive and
    iterative analytics are more suited for data exploration. Spark offers bindings
    and APIs in Python and R. With its **SparkSQL** module and the Spark Dataframe,
    it offers a very familiar analytics interface.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Engagement layer
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The engagement layer interacts with the end user and provides dashboards, interactive
    visualizations, and alerts. We will focus here on the tools provided by the PyData
    ecosystem such as Matplotlib, Seaborn, and Bokeh.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop scales horizontally as the data grows. Hadoop runs on commodity hardware,
    so it is cost-effective. Intensive data applications are enabled by scalable,
    distributed processing frameworks that allow organizations to analyze petabytes
    of data on large commodity clusters. Hadoop is the first open source implementation
    of map-reduce. Hadoop relies on a distributed framework for storage called **HDFS**
    (**Hadoop Distributed File System**). Hadoop runs map-reduce tasks in batch jobs.
    Hadoop requires persisting the data to disk at each map, shuffle, and reduce process
    step. The overhead and the latency of such batch jobs adversely impact the performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a fast, distributed general analytics computing engine for large-scale
    data processing. The major breakthrough from Hadoop is that Spark allows data
    sharing between processing steps through in-memory processing of data pipelines.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark is unique in that it allows four different styles of data analysis and
    processing. Spark can be used in:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch**: This mode is used for manipulating large datasets, typically performing
    large map-reduce jobs'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming**: This mode is used to process incoming information in near real
    time'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative**: This mode is for machine learning algorithms such as a gradient
    descent where the data is accessed repetitively in order to reach convergence'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive**: This mode is used for data exploration as large chunks of
    data are in memory and due to the very quick response time of Spark'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure highlights the preceding four processing styles:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spark](img/B03968_01_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Spark operates in three modes: one single mode, standalone on a single machine
    and two distributed modes on a cluster of machines—on Yarn, the Hadoop distributed
    resource manager, or on Mesos, the open source cluster manager developed at Berkeley
    concurrently with Spark:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spark](img/B03968_01_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Spark offers a polyglot interface in Scala, Java, Python, and R.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Spark libraries
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes with batteries included, with some powerful libraries:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**SparkSQL**: This provides the SQL-like ability to interrogate structured
    data and interactively explore large datasets'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SparkMLLIB**: This provides major algorithms and a pipeline framework for
    machine learning'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming**: This is for near real-time analysis of data using micro
    batches and sliding widows on incoming streams of data'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark GraphX**: This is for graph processing and computation on complex connected
    entities and relationships'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySpark in action
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark is written in Scala. The whole Spark ecosystem naturally leverages the
    JVM environment and capitalizes on HDFS natively. Hadoop HDFS is one of the many
    data stores supported by Spark. Spark is agnostic and from the beginning interacted
    with multiple data sources, types, and formats.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: PySpark is not a transcribed version of Spark on a Java-enabled dialect of Python
    such as Jython. PySpark provides integrated API bindings around Spark and enables
    full usage of the Python ecosystem within all the nodes of the cluster with the
    pickle Python serialization and, more importantly, supplies access to the rich
    ecosystem of Python's machine learning libraries such as Scikit-Learn or data
    processing such as Pandas.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: When we initialize a Spark program, the first thing a Spark program must do
    is to create a `SparkContext` object. It tells Spark how to access the cluster.
    The Python program creates a `PySparkContext`. Py4J is the gateway that binds
    the Python program to the Spark JVM `SparkContext`. The JVM `SparkContextserializes`
    the application codes and the closures and sends them to the cluster for execution.
    The cluster manager allocates resources and schedules, and ships the closures
    to the Spark workers in the cluster who activate Python virtual machines as required.
    In each machine, the Spark Worker is managed by an executor that controls computation,
    storage, and cache.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of how the Spark driver manages both the PySpark context
    and the Spark context with its local filesystems and its interactions with the
    Spark worker through the cluster manager:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark in action](img/B03968_01_05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: The Resilient Distributed Dataset
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark applications consist of a driver program that runs the user's main function,
    creates distributed datasets on the cluster, and executes various parallel operations
    (transformations and actions) on those datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Spark applications are run as an independent set of processes, coordinated by
    a `SparkContext` in a driver program.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` will be allocated system resources (machines, memory, CPU)
    from the **Cluster manager**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` manages executors who manage workers in the cluster. The
    driver program has Spark jobs that need to run. The jobs are split into tasks
    submitted to the executor for completion. The executor takes care of computation,
    storage, and caching in each machine.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The key building block in Spark is the **RDD** (**Resilient Distributed Dataset**).
    A dataset is a collection of elements. Distributed means the dataset can be on
    any node in the cluster. Resilient means that the dataset could get lost or partially
    lost without major harm to the computation in progress as Spark will re-compute
    from the data lineage in memory, also known as the **DAG** (short for **Directed
    Acyclic Graph**) of operations. Basically, Spark will snapshot in memory a state
    of the RDD in the cache. If one of the computing machines crashes during operation,
    Spark rebuilds the RDDs from the cached RDD and the DAG of operations. RDDs recover
    from node failure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of operation on RDDs:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations**: A transformation takes an existing RDD and leads to a
    pointer of a new transformed RDD. An RDD is immutable. Once created, it cannot
    be changed. Each transformation creates a new RDD. Transformations are lazily
    evaluated. Transformations are executed only when an action occurs. In the case
    of failure, the data lineage of transformations rebuilds the RDD.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: An action on an RDD triggers a Spark job and yields a value. An
    action operation causes Spark to execute the (lazy) transformation operations
    that are required to compute the RDD returned by the action. The action results
    in a DAG of operations. The DAG is compiled into stages where each stage is executed
    as a series of tasks. A task is a fundamental unit of work.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s some useful information on RDDs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'RDDs are created from a data source such as an HDFS file or a DB query. There
    are three ways to create an RDD:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading from a datastore
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming an existing RDD
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an in-memory collection
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RDDs are transformed with functions such as `map` or `filter`, which yield new
    RDDs.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An action such as first, take, collect, or count on an RDD will deliver the
    results into the Spark driver. The Spark driver is the client through which the
    user interacts with the Spark cluster.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the RDD transformation and action:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![The Resilient Distributed Dataset](img/B03968_01_06.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Understanding Anaconda
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Anaconda is a widely used free Python distribution maintained by **Continuum**
    ([https://www.continuum.io/](https://www.continuum.io/)). We will use the prevailing
    software stack provided by Anaconda to generate our apps. In this book, we will
    use PySpark and the PyData ecosystem. The PyData ecosystem is promoted, supported,
    and maintained by **Continuum** and powered by the **Anaconda** Python distribution.
    The Anaconda Python distribution essentially saves time and aggravation in the
    installation of the Python environment; we will use it in conjunction with Spark.
    Anaconda has its own package management that supplements the traditional `pip`
    `install` and `easy-install`. Anaconda comes with batteries included, namely some
    of the most important packages such as Pandas, Scikit-Learn, Blaze, Matplotlib,
    and Bokeh. An upgrade to any of the installed library is a simple command at the
    console:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A list of installed libraries in our environment can be obtained with command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The key components of the stack are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaconda**: This is a free Python distribution with almost 200 Python packages
    for science, math, engineering, and data analysis.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conda**: This is a package manager that takes care of all the dependencies
    of installing a complex software stack. This is not restricted to Python and manages
    the install process for R and other languages.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba**: This provides the power to speed up code in Python with high-performance
    functions and just-in-time compilation.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blaze**: This enables large scale data analytics by offering a uniform and
    adaptable interface to access a variety of data providers, which include streaming
    Python, Pandas, SQLAlchemy, and Spark.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh**: This provides interactive data visualizations for large and streaming
    datasets.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wakari**: This allows us to share and deploy IPython Notebooks and other
    apps on a hosted environment.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the components of the Anaconda stack:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Anaconda](img/B03968_01_07.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Setting up the Spark powered environment
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn to set up Spark:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Create a segregated development environment in a virtual machine running on
    Ubuntu 14.04, so it does not interfere with any existing system.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Spark 1.3.0 with its dependencies, namely.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the Anaconda Python 2.7 environment with all the required libraries
    such as Pandas, Scikit-Learn, Blaze, and Bokeh, and enable PySpark, so it can
    be accessed through IPython Notebooks.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up the backend or data stores of our environment. We will use MySQL as the
    relational database, MongoDB as the document store, and Cassandra as the columnar
    database.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each storage backend serves a specific purpose depending on the nature of the
    data to be handled. The MySQL RDBMs is used for standard tabular processed information
    that can be easily queried using SQL. As we will be processing a lot of JSON-type
    data from various APIs, the easiest way to store them is in a document. For real-time
    and time-series-related information, Cassandra is best suited as a columnar database.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives a view of the environment we will build and use
    throughout the book:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up the Spark powered environment](img/B03968_01_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Setting up an Oracle VirtualBox with Ubuntu
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up a clean new VirtualBox environment on Ubuntu 14.04 is the safest
    way to create a development environment that does not conflict with existing libraries
    and can be later replicated in the cloud using a similar list of commands.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: In order to set up an environment with Anaconda and Spark, we will create a
    VirtualBox virtual machine running Ubuntu 14.04.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the steps of using VirtualBox with Ubuntu:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Oracle VirtualBox VM is free and can be downloaded from [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
    The installation is pretty straightforward.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After installing VirtualBox, let's open the Oracle VM VirtualBox Manager and
    click the **New** button.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll give the new VM a name, and select Type **Linux** and Version **Ubuntu
    (64 bit)**.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You need to download the ISO from the Ubuntu website and allocate sufficient
    RAM (4 GB recommended) and disk space (20 GB recommended). We will use the Ubuntu
    14.04.1 LTS release, which is found here: [http://www.ubuntu.com/download/desktop](http://www.ubuntu.com/download/desktop).'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation completed, it is advisable to install the VirtualBox Guest
    Additions by going to (from the VirtualBox menu, with the new VM running) **Devices**
    | **Insert Guest Additions CD image**. Failing to provide the guest additions
    in a Windows host gives a very limited user interface with reduced window sizes.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the additional installation completes, reboot the VM, and it will be ready
    to use. It is helpful to enable the shared clipboard by selecting the VM and clicking
    **Settings**, then go to **General** | **Advanced** | **Shared Clipboard** and
    click on **Bidirectional**.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Anaconda with Python 2.7
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PySpark currently runs only on Python 2.7\. (There are requests from the community
    to upgrade to Python 3.3.) To install Anaconda, follow these steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Download the Anaconda Installer for Linux 64-bit Python 2.7 from [http://continuum.io/downloads#all](http://continuum.io/downloads#all).
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After downloading the Anaconda installer, open a terminal and navigate to the
    directory or folder where the installer has been saved. From here, run the following
    command, replacing the `2.x.x` in the command with the version number of the downloaded
    installer file:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After accepting the license terms, you will be asked to specify the install
    location (which `defaults to ~/anaconda`).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the self-extraction is finished, you should add the anaconda binary directory
    to your PATH environment variable:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Installing Java 8
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark runs on the JVM and requires the Java **SDK** (short for **Software Development
    Kit**) and not the **JRE** (short for **Java Runtime Environment**), as we will
    build apps with Spark. The recommended version is Java Version 7 or higher. Java
    8 is the most suitable, as it includes many of the functional programming techniques
    available with Scala and Python.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Java 8, follow these steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Oracle Java 8 using the following commands:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Set the `JAVA_HOME` environment variable and ensure that the Java program is
    on your PATH.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check that `JAVA_HOME` is properly installed:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Installing Spark
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Head over to the Spark download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The Spark download page offers the possibility to download earlier versions
    of Spark and different package and download types. We will select the latest release,
    pre-built for Hadoop 2.6 and later. The easiest way to install Spark is to use
    a Spark package prebuilt for Hadoop 2.6 and later, rather than build it from source.
    Move the file to the directory `~/spark` under the root directory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the latest release of Spark—Spark 1.5.2, released on November 9, 2015:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Select Spark release **1.5.2 (Nov 09 2015),**
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the package type **Prebuilt for Hadoop 2.6 and later**,
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the download type **Direct Download**,
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download Spark: **spark-1.5.2-bin-hadoop2.6.tgz**,'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify this release using the 1.3.0 signatures and checksums,
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can also be accomplished by running:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we''ll extract the files and clean up:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can run the Spark Python interpreter with:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see something like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The interpreter will have already provided us with a Spark context object,
    `sc`, which we can see by running:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Enabling IPython Notebook
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with IPython Notebook for a friendlier user experience than the
    console.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'You can launch IPython Notebook by using the following command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Launch PySpark with `IPYNB` in the directory `examples/AN_Spark` where Jupyter
    or IPython Notebooks are stored:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Building our first app with PySpark
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are ready to check now that everything is working fine. The obligatory word
    count will be put to the test in processing a word count on the first chapter
    of this book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we will be running is listed here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this program, we are first reading the file from the directory `/home/an/Documents/A00_Documents/Spark4Py
    20150315` into `file_in`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: We are then introspecting the file by counting the number of lines and the number
    of characters per line.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We are splitting the input file in to words and getting them in lower case.
    For our word count purpose, we are choosing words longer than three characters
    in order to avoid shorter and much more frequent words such as *the*, *and*, *for*
    to skew the count in their favor. Generally, they are considered stop words and
    should be filtered out in any language processing task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we are getting ready for the MapReduce steps. To each word, we
    map a value of `1` and reduce it by summing all the unique words.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Here are illustrations of the code in the IPython Notebook. The first 10 cells
    are preprocessing the word count on the dataset, which is retrieved from the local
    file directory.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_09.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Swap the word count tuples in the format `(count, word)` in order to sort by
    `count`, which is now the primary key of the tuple:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to display our result, we are creating the tuple `(count, word)` and
    displaying the top 20 most frequently used words in descending order:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_10.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create a histogram function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we visualize the most frequent words by plotting them in a bar chart.
    We have to first swap the tuple from the original `(count, word)` to `(word, count)`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_11.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'So here you have it: the most frequent words used in the first chapter are
    **Spark**, followed by **Data** and **Anaconda**.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Virtualizing the environment with Vagrant
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built with a `vagrantfile`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'We will point to the **Massive Open Online Courses** (**MOOCs**) delivered
    by *Berkeley University and Databricks*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Big Data with Apache Spark, Professor Anthony D. Joseph* can
    be found at [https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalable Machine Learning, Professor* *Ameet Talwalkar* can be found at [https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The course labs were executed on IPython Notebooks powered by PySpark. They
    can be found in the following GitHub repository: [https://github.com/spark-mooc/mooc-setup/](https://github.com/spark-mooc/mooc-setup/).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have set up Vagrant on your machine, follow these instructions to
    get started: [https://docs.vagrantup.com/v2/getting-started/index.html](https://docs.vagrantup.com/v2/getting-started/index.html).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `spark-mooc/mooc-setup/ github` repository in your work directory
    and launch the command `$ vagrant up`, within the cloned directory:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the version of Spark may be outdated as the `vagrantfile` may
    not be up-to-date.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see an output similar to this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will launch the IPython Notebooks powered by PySpark on `localhost:8001`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Virtualizing the environment with Vagrant](img/B03968_01_12.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Moving to the cloud
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are dealing with distributed systems, an environment on a virtual machine
    running on a single laptop is limited for exploration and learning. We can move
    to the cloud in order to experience the power and scalability of the Spark distributed
    framework.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Deploying apps in Amazon Web Services
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we are ready to scale our apps, we can migrate our development environment
    to **Amazon** **Web Services** (**AWS**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'How to run Spark on EC2 is clearly described in the following page: [https://spark.apache.org/docs/latest/ec2-scripts.html](https://spark.apache.org/docs/latest/ec2-scripts.html).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'We emphasize five key steps in setting up the AWS Spark environment:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Create an AWS EC2 key pair via the AWS console [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Export your key pair to your environment:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Launch your cluster:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'SSH into a cluster to run Spark jobs:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Destroy your cluster after usage:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Virtualizing the environment with Docker
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built in Docker containers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'We wish capitalize on Docker''s two main functions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Creating isolated containers that can be easily deployed on different operating
    systems or in the cloud.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing easy sharing of the development environment image with all its dependencies
    using The DockerHub. The DockerHub is similar to GitHub. It allows easy cloning
    and version control. The snapshot image of the configured environment can be the
    baseline for further enhancements.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram illustrates a Docker-enabled environment with Spark, Anaconda,
    and the database server and their respective data volumes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Virtualizing the environment with Docker](img/B03968_01_13.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Docker offers the ability to clone and deploy an environment from the Dockerfile.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find an example Dockerfile with a PySpark and Anaconda setup at the
    following address: [https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/](https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Docker as per the instructions provided at the following links:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/mac/started/](http://docs.docker.com/mac/started/)
    if you are on Mac OS X'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/linux/started/](http://docs.docker.com/linux/started/)
    if you are on Linux'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/windows/started/](http://docs.docker.com/windows/started/)
    if you are on Windows'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install the docker container with the Dockerfile provided earlier with the
    following command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Other great sources of information on how to *dockerize* your environment can
    be seen at Lab41\. The GitHub repository contains the necessary code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Lab41/ipython-spark-docker](https://github.com/Lab41/ipython-spark-docker)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The supporting blog post is rich in information on thought processes involved
    in building the docker environment: [http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/](http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的博客文章中包含了构建 Docker 环境过程中涉及的思想过程的信息：[http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/](http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/).
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We set the context of building data-intensive apps by describing the overall
    architecture structured around the infrastructure, persistence, integration, analytics,
    and engagement layers. We also discussed Spark and Anaconda with their respective
    building blocks. We set up an environment in a VirtualBox with Anaconda and Spark
    and demonstrated a word count app using the text content of the first chapter
    as input.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过描述围绕基础设施、持久化、集成、分析和参与层构建的整体架构来设定构建数据密集型应用程序的背景。我们还讨论了 Spark 和 Anaconda 及其相应的构建模块。我们在
    VirtualBox 中设置了 Anaconda 和 Spark 的环境，并使用第一章的文本内容作为输入演示了一个词频统计应用程序。
- en: In the next chapter, we will delve more deeply into the architecture blueprint
    for data-intensive apps and tap into the Twitter, GitHub, and Meetup APIs to get
    a feel of the data we will be mining with Spark.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨数据密集型应用程序的架构蓝图，并利用 Twitter、GitHub 和 Meetup API 来感受我们将使用 Spark
    进行挖掘的数据。
