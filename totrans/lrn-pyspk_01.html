<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 1. Understanding Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Understanding Spark</h1></div></div></div><p>Apache Spark is a powerful open source processing engine originally developed by Matei Zaharia as a part of his PhD thesis while at UC Berkeley. The first version of Spark was released in 2012. Since then, in 2013, Zaharia co-founded and has become the CTO at Databricks; he also holds a professor position at Stanford, coming from MIT. At the same time, the Spark codebase was donated to the Apache Software Foundation and has become its flagship project.</p><p>Apache Spark is fast, easy to use framework, that allows you to solve a wide variety of complex data problems whether semi-structured, structured, streaming, and/or machine learning / data sciences. It also has become one of the largest open source communities in big data with more than 1,000 contributors from 250+ organizations and with 300,000+ Spark Meetup community members in more than 570+ locations worldwide.</p><p>In this chapter, we will provide a primer to understanding Apache Spark. We will explain the concepts behind Spark Jobs and APIs, introduce the Spark 2.0 architecture, and explore the features of Spark 2.0.</p><p>The topics covered are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is Apache Spark?</li><li class="listitem" style="list-style-type: disc">Spark Jobs and APIs</li><li class="listitem" style="list-style-type: disc">Review of Resilient Distributed Datasets (RDDs), DataFrames, and Datasets</li><li class="listitem" style="list-style-type: disc">Review of Catalyst Optimizer and Project Tungsten</li><li class="listitem" style="list-style-type: disc">Review of the Spark 2.0 architecture</li></ul></div><div class="section" title="What is Apache Spark?"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec07"/>What is Apache Spark?</h1></div></div></div><p>Apache Spark is<a id="id0" class="indexterm"/> an open-source powerful distributed querying and processing engine. It provides flexibility and extensibility of MapReduce but at significantly higher speeds: Up to 100 times faster than Apache Hadoop when data is stored in memory and up to 10 times when accessing disk.</p><p>Apache Spark allows the user to read, transform, and aggregate data, as well as train and deploy sophisticated<a id="id1" class="indexterm"/> statistical models with ease. The Spark APIs are accessible in Java, Scala, Python, R and SQL. Apache Spark can be used to build applications or package them up as libraries to be deployed on a cluster or perform <span class="emphasis"><em>quick </em></span>analytics interactively through notebooks (like, for instance, Jupyter, Spark-Notebook, Databricks notebooks, and Apache Zeppelin).</p><p>Apache Spark exposes a host of libraries familiar to data analysts, data scientists or researchers who have worked with Python's <code class="literal">pandas</code> or R's <code class="literal">data.frames</code> or <code class="literal">data.tables</code>. It is important to note that while Spark DataFrames will be <span class="emphasis"><em>familiar</em></span> to <code class="literal">pandas</code> or <code class="literal">data.frames</code> / <code class="literal">data.tables</code> users, there are some differences so please temper your expectations. Users with more of a SQL background can use the language to shape their data as well. Also, delivered with Apache Spark are several already implemented and tuned algorithms, statistical models, and frameworks: MLlib and ML for machine learning, GraphX and GraphFrames for graph processing, and Spark Streaming (DStreams and Structured). Spark allows the user to combine these libraries seamlessly in the same application.</p><p>Apache Spark can easily run locally on a laptop, yet can also easily be deployed in standalone mode, over YARN, or Apache Mesos - either on your local cluster or in the cloud. It can read and write from a diverse data sources including (but not limited to) HDFS, Apache Cassandra, Apache HBase, and S3:</p><div class="mediaobject"><img src="images/B05793_01_01.jpg" alt="What is Apache Spark?"/><div class="caption"><p>Source: Apache Spark is the smartphone of Big Data <a class="ulink" href="http://bit.ly/1QsgaNj">http://bit.ly/1QsgaNj</a>
</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>For more information, please refer to: Apache Spark<a id="id2" class="indexterm"/> is the Smartphone of Big Data at <a class="ulink" href="http://bit.ly/1QsgaNj">http://bit.ly/1QsgaNj</a>
</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Spark Jobs and APIs"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Spark Jobs and APIs</h1></div></div></div><p>In this section, we will<a id="id3" class="indexterm"/> provide a short overview of the Apache Spark<a id="id4" class="indexterm"/> Jobs and APIs. This provides the necessary foundation for the subsequent section on Spark 2.0 architecture.</p><div class="section" title="Execution process"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec06"/>Execution process</h2></div></div></div><p>Any Spark<a id="id5" class="indexterm"/> application spins off a single driver process (that can contain multiple jobs) on the <span class="emphasis"><em>master</em></span> node that then directs executor processes (that contain multiple tasks) distributed to a number of <span class="emphasis"><em>worker</em></span> nodes as noted in the following diagram:</p><div class="mediaobject"><img src="images/B05793_01_02.jpg" alt="Execution process"/></div><p>The driver process determines the number and the composition of the task processes directed to the<a id="id6" class="indexterm"/> executor nodes based on the graph generated for the given job. Note, that any worker node can execute tasks from a number of different jobs.</p><p>A Spark job is associated with a chain of object dependencies organized in a direct acyclic graph (DAG) such as the following example generated from the Spark UI. Given this, Spark can optimize the scheduling (for example, determine the number of tasks and workers required) and execution of these tasks:</p><div class="mediaobject"><img src="images/B05793_01_03.jpg" alt="Execution process"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>For more<a id="id7" class="indexterm"/> information on the DAG scheduler, please refer to <a class="ulink" href="http://bit.ly/29WTiK8">http://bit.ly/29WTiK8</a>.</p></div></div></div><div class="section" title="Resilient Distributed Dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Resilient Distributed Dataset</h2></div></div></div><p>Apache Spark is<a id="id8" class="indexterm"/> built around a distributed<a id="id9" class="indexterm"/> collection of immutable Java Virtual Machine (JVM) objects called Resilient Distributed Datasets (RDDs for short). As we are working with Python, it is important to note that the Python data is stored within these JVM objects. More of this will be discussed in the subsequent chapters on RDDs and DataFrames. These objects allow any job to perform calculations very quickly. RDDs are calculated against, cached, and stored in-memory: a scheme that results in orders of magnitude faster computations compared to other traditional distributed frameworks like Apache Hadoop.</p><p>At the same time, RDDs expose some coarse-grained transformations (such as <code class="literal">map(...)</code>, <code class="literal">reduce(...)</code>, and <code class="literal">filter(...)</code> which we will cover in greater detail in Chapter 2, <span class="emphasis"><em>Resilient Distributed Datasets</em></span>), keeping the flexibility and extensibility of the Hadoop platform to perform a wide variety of calculations. RDDs apply and log transformations to the data in parallel, resulting in both increased speed and fault-tolerance. By registering the transformations, RDDs provide data lineage - a form of an ancestry tree for each intermediate step in the form of a graph. This, in effect, guards the RDDs against data loss - if a partition of an RDD is lost it still has enough information to recreate that partition instead of simply depending on replication.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>If you want<a id="id10" class="indexterm"/> to learn more about data lineage check this link <a class="ulink" href="http://ibm.co/2ao9B1t">http://ibm.co/2ao9B1t</a>
.</p></div></div><p>RDDs have two sets of parallel operations: <span class="emphasis"><em>transformations</em></span> (which return pointers to new RDDs) and <span class="emphasis"><em>actions</em></span> (which return values to the driver after running a computation); we will cover these in greater detail in later chapters.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>For the<a id="id11" class="indexterm"/> latest list of transformations and actions, please refer<a id="id12" class="indexterm"/> to the Spark Programming Guide at <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations</a>.</p></div></div><p>RDD transformation operations are <span class="emphasis"><em>lazy</em></span> in a sense that they do not compute their results immediately. The transformations are only computed when an action is executed and the results need to be returned to the driver. This delayed execution results in more fine-tuned queries: Queries that are optimized for performance. This optimization starts with Apache Spark's DAGScheduler – the stage oriented scheduler that transforms using <span class="emphasis"><em>stages</em></span> as seen in the preceding screenshot. By having separate RDD <span class="emphasis"><em>transformations</em></span> and <span class="emphasis"><em>actions</em></span>, the DAGScheduler can perform optimizations in the query including being able to avoid <span class="emphasis"><em>shuffling</em></span>, the data (the most resource<a id="id13" class="indexterm"/> intensive task).</p><p>For more<a id="id14" class="indexterm"/> information on the DAGScheduler and optimizations (specifically around narrow or wide dependencies), a great reference is the <span class="emphasis"><em>Narrow vs. Wide Transformations</em></span> section in <span class="emphasis"><em>High Performance Spark</em></span> in <span class="emphasis"><em><a class="link" href="ch05.html" title="Chapter 5. Introducing MLlib">Chapter 5</a>, Effective Transformations</em></span> (<a class="ulink" href="https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203">https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203</a>).</p></div><div class="section" title="DataFrames"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>DataFrames</h2></div></div></div><p>DataFrames, like RDDs, are immutable<a id="id15" class="indexterm"/> collections of data<a id="id16" class="indexterm"/> distributed among the nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into named columns.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>If you are familiar with Python's <code class="literal">pandas</code> or R <code class="literal">data.frames</code>, this is a similar concept.</p></div></div><p>DataFrames were designed to make large data sets processing even easier. They allow developers to formalize the structure of the data, allowing higher-level abstraction; in that sense DataFrames resemble tables from the relational database world. DataFrames provide a domain specific language API to manipulate the distributed data and make Spark accessible to a wider audience, beyond specialized data engineers.</p><p>One of the major benefits of DataFrames is that the Spark engine initially builds a logical execution plan and executes generated code based on a physical plan determined by a cost optimizer. Unlike RDDs that can be significantly slower on Python compared with Java or Scala, the introduction of DataFrames has brought performance parity across all the languages.</p></div><div class="section" title="Datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Datasets</h2></div></div></div><p>Introduced in Spark 1.6, the goal of Spark Datasets is to provide an API that allows users to easily express<a id="id17" class="indexterm"/> transformations on domain objects, while also<a id="id18" class="indexterm"/> providing the performance and benefits of the robust Spark SQL execution engine. Unfortunately, at the time of writing this book Datasets are only available in Scala or Java. When they are available in PySpark we will cover them in future editions.</p></div><div class="section" title="Catalyst Optimizer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Catalyst Optimizer</h2></div></div></div><p>Spark SQL is one of the most technically involved components of Apache Spark as it powers both SQL<a id="id19" class="indexterm"/> queries and the DataFrame API. At the core of Spark SQL is the Catalyst Optimizer. The optimizer is based on functional<a id="id20" class="indexterm"/> programming constructs and was designed with two purposes in mind: To ease the addition of new optimization techniques and features to Spark SQL and to allow external developers to extend the optimizer (for example, adding data source specific rules, support for new data types, and so on):</p><div class="mediaobject"><img src="images/B05793_01_04.jpg" alt="Catalyst Optimizer"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>For more<a id="id21" class="indexterm"/> information, check out <span class="emphasis"><em>Deep Dive into Spark SQL's Catalyst Optimizer</em></span> (<a class="ulink" href="http://bit.ly/271I7Dk">http://bit.ly/271I7Dk</a>) and <span class="emphasis"><em>Apache Spark DataFrames: Simple and Fast Analysis of Structured Data</em></span> (<a class="ulink" href="http://bit.ly/29QbcOV">http://bit.ly/29QbcOV</a>)</p></div></div></div><div class="section" title="Project Tungsten"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Project Tungsten</h2></div></div></div><p>Tungsten is the<a id="id22" class="indexterm"/> codename for an umbrella project of Apache<a id="id23" class="indexterm"/> Spark's execution engine. The project focuses on improving the Spark algorithms so they use memory and CPU more efficiently, pushing the performance of modern hardware closer to its limits.</p><p>The efforts of this project focus, among others, on:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Managing memory explicitly so the overhead of JVM's object model and garbage collection are eliminated</li><li class="listitem" style="list-style-type: disc">Designing algorithms and data structures that exploit the memory hierarchy</li><li class="listitem" style="list-style-type: disc">Generating code in runtime so the applications can exploit modern compliers and optimize for CPUs</li><li class="listitem" style="list-style-type: disc">Eliminating virtual function dispatches so that multiple CPU calls are reduced</li><li class="listitem" style="list-style-type: disc">Utilizing low-level programming (for example, loading immediate data to CPU registers)  speed up the memory access and optimizing Spark's engine to efficiently compile and execute simple loops</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>For more<a id="id24" class="indexterm"/> information, please refer to</p><p>
<span class="emphasis"><em>Project Tungsten: Bringing Apache Spark Closer to Bare Metal</em></span> (<a class="ulink" href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a>)</p><p>
<span class="emphasis"><em>Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal</em></span> [SSE 2015 Video and Slides] (<a class="ulink" href="https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/">https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/</a>) and</p><p>
<span class="emphasis"><em>Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</em></span> (<a class="ulink" href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html">https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html</a>)</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Spark 2.0 architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Spark 2.0 architecture</h1></div></div></div><p>The introduction<a id="id25" class="indexterm"/> of Apache Spark 2.0 is the recent major release of the Apache Spark project based on the key learnings from the last two years of development of the platform:</p><div class="mediaobject"><img src="images/B05793_01_05.jpg" alt="Spark 2.0 architecture"/><div class="caption"><p>Source: Apache Spark 2.0: Faster, Easier, and Smarter <a class="ulink" href="http://bit.ly/2ap7qd5">http://bit.ly/2ap7qd5</a>
</p></div></div><p>The three overriding themes of the Apache Spark 2.0 release surround performance enhancements (via Tungsten Phase 2), the introduction of structured streaming, and unifying Datasets and DataFrames. We will describe the Datasets as they are part of Spark 2.0 even though they are currently only available in Scala and Java.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Refer to the<a id="id26" class="indexterm"/> following presentations by key Spark committers for more information about Apache Spark 2.0:</p><p>
<span class="emphasis"><em>Reynold Xin's Apache Spark 2.0: Faster, Easier, and Smarter</em></span> webinar <a class="ulink" href="http://bit.ly/2ap7qd5">http://bit.ly/2ap7qd5</a>
</p><p>
<span class="emphasis"><em>Michael Armbrust's Structuring Spark: DataFrames, Datasets, and Streaming</em></span> <a class="ulink" href="http://bit.ly/2ap7qd5">http://bit.ly/2ap7qd5</a>
</p><p>
<span class="emphasis"><em>Tathagata Das' A Deep Dive into Spark Streaming</em></span> <a class="ulink" href="http://bit.ly/2aHt1w0">http://bit.ly/2aHt1w0</a>
</p><p>
<span class="emphasis"><em>Joseph Bradley's Apache Spark MLlib 2.0 Preview: Data Science and Production</em></span> <a class="ulink" href="http://bit.ly/2aHrOVN">http://bit.ly/2aHrOVN</a>
</p></div></div><div class="section" title="Unifying Datasets and DataFrames"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Unifying Datasets and DataFrames</h2></div></div></div><p>In the previous section, we stated out that Datasets (at the time of writing this book) are only available in Scala or Java. However, we are providing the following context to better understand the direction of Spark 2.0.</p><p>Datasets were introduced in 2015 as part of the Apache Spark 1.6 release. The goal for datasets was<a id="id27" class="indexterm"/> to provide a type-safe, programming interface. This allowed developers to work with semi-structured data (like JSON or key-value pairs) with compile time type safety (that is, production applications can be checked for errors before they run). Part of the reason<a id="id28" class="indexterm"/> why Python does not implement a Dataset API is because Python is not a type-safe language.</p><p>Just as important, the Datasets API contain high-level domain specific language operations such as <code class="literal">sum()</code>, <code class="literal">avg()</code>, <code class="literal">join()</code>, and <code class="literal">group()</code>. This latter trait means that you have the flexibility of traditional Spark RDDs but the code is also easier to express, read, and write. Similar to DataFrames, Datasets can take advantage of Spark's catalyst optimizer by exposing expressions and data fields to a query planner and making use of Tungsten's fast in-memory encoding.</p><p>The history of the Spark APIs is denoted in the following diagram noting the progression from RDD to DataFrame to Dataset:</p><div class="mediaobject"><img src="images/B05793_01_06.jpg" alt="Unifying Datasets and DataFrames"/><div class="caption"><p>Source: From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD? <a class="ulink" href="http://bit.ly/29JPJSA">http://bit.ly/29JPJSA</a>
</p></div></div><p>The unification of the DataFrame and Dataset APIs has the potential of creating breaking changes to<a id="id29" class="indexterm"/> backwards compatibility. This was one of the main reasons Apache Spark 2.0 was a major release (as opposed to a 1.x minor release which would have minimized any breaking changes). As you can see from the following diagram, DataFrame and Dataset both belong to the new Dataset API introduced as part of Apache Spark 2.0:</p><div class="mediaobject"><img src="images/B05793_01_07.jpg" alt="Unifying Datasets and DataFrames"/><div class="caption"><p>Source: A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets <a class="ulink" href="http://bit.ly/2accSNA">http://bit.ly/2accSNA</a>
</p></div></div><p>As noted previously, the Dataset API provides a type-safe, object-oriented programming interface. Datasets can take advantage of the Catalyst optimizer by exposing expressions and data fields<a id="id30" class="indexterm"/> to the query planner and Project Tungsten's Fast In-memory encoding. But with DataFrame and Dataset now unified as<a id="id31" class="indexterm"/> part of Apache Spark 2.0, DataFrame is now an alias for the Dataset Untyped API. More specifically:</p><div class="informalexample"><pre class="programlisting">DataFrame = Dataset[Row]</pre></div></div><div class="section" title="Introducing SparkSession"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Introducing SparkSession</h2></div></div></div><p>In the past, you would<a id="id32" class="indexterm"/> potentially work with <code class="literal">SparkConf</code>, <code class="literal">SparkContext</code>, <code class="literal">SQLContext</code>, and <code class="literal">HiveContext</code> to execute your various Spark<a id="id33" class="indexterm"/> queries for configuration, Spark context, SQL context, and Hive context respectively. The <code class="literal">SparkSession</code> is essentially the combination of these contexts including <code class="literal">StreamingContext</code>.</p><p>For example, instead of writing:</p><div class="informalexample"><pre class="programlisting">df = sqlContext.read \
    .format('json').load('py/test/sql/people.json')</pre></div><p>now you can write:</p><div class="informalexample"><pre class="programlisting">df = spark.read.format('json').load('py/test/sql/people.json')</pre></div><p>or:</p><div class="informalexample"><pre class="programlisting">df = spark.read.json('py/test/sql/people.json')</pre></div><p>The <code class="literal">SparkSession</code> is now the entry point for reading data, working with metadata, configuring the session, and managing the cluster resources.</p></div><div class="section" title="Tungsten phase 2"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Tungsten phase 2</h2></div></div></div><p>The fundamental<a id="id34" class="indexterm"/> observation of the computer hardware landscape when the project started was that, while there were improvements in <span class="emphasis"><em>price per performance</em></span> in RAM memory, disk, and (to an extent) network interfaces, the <span class="emphasis"><em>price per performance</em></span> advancements for CPUs were not the same. Though hardware<a id="id35" class="indexterm"/> manufacturers could put more cores in each socket (i.e. improve performance through parallelization), there were no significant improvements in the actual core speed.</p><p>Project Tungsten was introduced in 2015 to make significant changes to the Spark engine with the focus<a id="id36" class="indexterm"/> on improving performance. The first phase of these improvements focused on the following facets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Memory Management and Binary Processing</strong></span>: Leveraging application semantics to manage memory explicitly and eliminate the overhead of the JVM object model and garbage collection</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cache-aware computation</strong></span>: Algorithms and data structures to exploit memory hierarchy</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Code generation</strong></span>: Using code generation to exploit modern compilers and CPUs</li></ul></div><p>The following<a id="id37" class="indexterm"/> diagram is the updated Catalyst engine to denote the inclusion<a id="id38" class="indexterm"/> of Datasets. As you see at the right of the diagram (right of the Cost Model), <span class="strong"><strong>Code Generation</strong></span> is used against the selected physical plans to generate the underlying RDDs:</p><div class="mediaobject"><img src="images/B05793_01_08.jpg" alt="Tungsten phase 2"/><div class="caption"><p>Source: Structuring Spark: DataFrames, Datasets, and Streaming <a class="ulink" href="http://bit.ly/2cJ508x">http://bit.ly/2cJ508x</a>
</p></div></div><p>As part of Tungsten Phase 2, there is the push into <span class="emphasis"><em>whole-stage</em></span> code generation. That is, the Spark engine will now generate the byte code at compile time for the entire Spark stage instead of just<a id="id39" class="indexterm"/> for specific jobs or tasks. The primary<a id="id40" class="indexterm"/> facets surrounding these improvements include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>No virtual function dispatches</strong></span>: This reduces multiple CPU calls that can have a profound impact on performance when dispatching billions of times</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Intermediate data in memory vs CPU registers</strong></span>: Tungsten Phase 2 places intermediate data into CPU registers. This is an order of magnitude reduction in the number of cycles to obtain data from the CPU registers instead of from memory</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Loop unrolling and SIMD</strong></span>: Optimize Apache Spark's execution engine to take advantage of modern compilers and CPUs' ability to efficiently compile and execute simple <code class="literal">for</code> loops (as opposed to complex function call graphs)</li></ul></div><p>For a more in-depth<a id="id41" class="indexterm"/> review of Project Tungsten, please refer to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Apache Spark Key Terms, Explained</em></span> <a class="ulink" href="https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html">https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</em></span> <a class="ulink" href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html">https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Project Tungsten: Bringing Apache Spark Closer to Bare Metal</em></span> <a class="ulink" href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a></li></ul></div></div><div class="section" title="Structured Streaming"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Structured Streaming</h2></div></div></div><p>As quoted by Reynold Xin during Spark Summit East 2016:</p><div class="blockquote"><blockquote class="blockquote"><p>"The simplest way to perform streaming analytics is not having to <span class="emphasis"><em>reason</em></span> about streaming."</p></blockquote></div><p>This is the underlying foundation for building Structured Streaming. While streaming is powerful, one of<a id="id42" class="indexterm"/> the key issues is that streaming<a id="id43" class="indexterm"/> can be difficult to build and maintain. While companies such as Uber, Netflix, and Pinterest have Spark Streaming applications running in production, they also have dedicated teams to ensure the systems are highly available.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>For a high-level<a id="id44" class="indexterm"/> overview of Spark Streaming, please review Spark Streaming: What Is It and Who's Using It? <a class="ulink" href="http://bit.ly/1Qb10f6">http://bit.ly/1Qb10f6</a>
</p></div></div><p>As implied previously, there are many things that can go wrong when operating Spark Streaming (and any streaming system for that matter) including (but not limited to) late events, partial outputs to the final data source, state recovery on failure, and/or distributed reads/writes:</p><div class="mediaobject"><img src="images/B05793_01_09.jpg" alt="Structured Streaming"/><div class="caption"><p>Source: A Deep Dive into Structured Streaming <a class="ulink" href="http://bit.ly/2aHt1w0">http://bit.ly/2aHt1w0</a>
</p></div></div><p>Therefore, to simplify Spark Streaming, there is now a single API that addresses both batch and streaming<a id="id45" class="indexterm"/> within the Apache Spark 2.0 release. More succinctly, the high-level streaming API is now built on top of the<a id="id46" class="indexterm"/> Apache Spark SQL Engine. It runs the same queries as you would with Datasets/DataFrames providing you with all the performance and optimization benefits as well as benefits such as event time, windowing, sessions, sources, and sinks.</p></div><div class="section" title="Continuous applications"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Continuous applications</h2></div></div></div><p>Altogether, Apache Spark 2.0 not only unified DataFrames and Datasets but also unified streaming, interactive, and batch queries. This opens a whole new set of use cases including the ability<a id="id47" class="indexterm"/> to aggregate data<a id="id48" class="indexterm"/> into a stream and then serving it using traditional JDBC/ODBC, to change queries at run time, and/or to build and apply ML models in for many scenario in a variety of latency use cases:</p><div class="mediaobject"><img src="images/B05793_01_10.jpg" alt="Continuous applications"/><div class="caption"><p>Source: Apache Spark Key Terms, Explained <a class="ulink" href="https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html">https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html</a>.
</p></div></div><p>Together, you can now build end-to-end <span class="strong"><strong>continuous applications</strong></span>, in which you can issue the same<a id="id49" class="indexterm"/> queries to batch processing<a id="id50" class="indexterm"/> as to real-time data, perform ETL, generate reports, update or track specific data in the stream.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>For more information on continuous applications, please refer to Matei Zaharia's blog post <span class="emphasis"><em>Continuous Applications: Evolving Streaming in Apache Spark 2.0 - A foundation for end-to-end real-time applications</em></span> <a class="ulink" href="http://bit.ly/2aJaSOr">http://bit.ly/2aJaSOr</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Summary</h1></div></div></div><p>In this chapter, we reviewed what is Apache Spark and provided a primer on Spark Jobs and APIs. We also provided a primer on Resilient Distributed Datasets (RDDs), DataFrames, and Datasets; we will dive further into RDDs and DataFrames in subsequent chapters. We also discussed how DataFrames can provide faster query performance in Apache Spark due to the Spark SQL Engine's Catalyst Optimizer and Project Tungsten. Finally, we also provided a high-level overview of the Spark 2.0 architecture including the Tungsten Phase 2, Structured Streaming, and Unifying DataFrames and Datasets.</p><p>In the next chapter, we will cover one of the fundamental data structures in Spark: The Resilient Distributed Datasets, or RDDs. We will show you how to create and modify these schema-less data structures using transformers and actions so your journey with PySpark can begin.</p><p>Before we do that, however, please, check the link <a class="ulink" href="http://www.tomdrabas.com/site/book">http://www.tomdrabas.com/site/book</a> for the Bonus Chapter 1 where we outline instructions on how to install Spark locally on your machine (unless you already have it installed). Here's a direct link to the manual: <a class="ulink" href="https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf">https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf</a>.</p></div></div>
</body></html>