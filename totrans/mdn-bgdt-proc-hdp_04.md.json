["```py\n$sqoop import -connect jdbc:mysql://localhost/dbname -table <table_name>   --username <username> --password >password> -m 4\n```", "```py\n$ mysql>  create database  sales;\n$ mysql>  use sales;\n$  mysql>   create table customer \n (cust_num int not null,cust_fname  varchar(30),cust_lname varchar     (30),cust_address  varchar (30),cust_city varchar (20),cust_state      varchar (3), cust_zip  varchar (6),primary key (cust_num));\n$ ctrl-C   -- to exit from MySQL\n```", "```py\n$  sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --fields-terminated-by \",\"  --driver com.mysql.jdbc.Driver --target-dir /user/data/customer\n```", "```py\n$ hadoop fs -ls /user/data/customerFound 5 items-rw-r--r--   1 root hdfs          0 2017-04-28 23:35 /user/data/customer/_SUCCESS-rw-r--r--   1 root hdfs        154 2017-04-28 23:35 /user/data/customer/part-m-00000-rw-r--r--   1 root hdfs         95 2017-04-28 23:35 /user/data/customer/part-m-00001-rw-r--r--   1 root hdfs         96 2017-04-28 23:35 /user/data/customer/part-m-00002-rw-r--r--   1 root hdfs        161 2017-04-28 23:35 /user/data/customer/part-m-00003\n```", "```py\n$ hive$hive > CREATE EXTERNAL TABLE customer_H \n(cust_num int,cust_fname  string,cust_lname  string,cust_address string,  cust_city  string,cust_state  string,cust_zip   string) \nROW FORMAT DELIMITED FIELDS TERMINATED BY ','LINES TERMINATED BY 'n'LOCATION '/user/data/customer';\n$hive> select * from customer_H; \n```", "```py\n$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --driver com.mysql.jdbc.Driver --m 1 --hive-import  --hive-table customor_H\n```", "```py\n$hive$use default;\n$ show tables;\n```", "```py\ninsert into customer values (11,'Abel','Maclead','25 E 75th St #69','Los Angeles','CA','90034');\n```", "```py\nsqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --driver com.mysql.jdbc.Driver --incremental append --check-column cust_num \n      --last-value 10 \n    --m 1 --split-by cust_state --target-dir /user/data/customer\n```", "```py\n$hive> select * from  customer_H;\n```", "```py\n$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table sales.customer  --driver com.mysql.jdbc.Driver --m 1 --where \"city = 'OH' --hive-import  --hive-table customer_H_1$ hive> select * from customer_H_1;\n```", "```py\n$sqoop import -connect jdbc:mysql://localhost/dbname -table <table_name>  --username <username> --password >password>  --hive-import -m 4                                    --hbase-create-table --hbase-table <table_name>--column-family <col family name>\n```", "```py\n$ mysql>  use sales;$  mysql>   create table customer_export (      cust_num int not null,      cust_fname  varchar(30),      cust_lname varchar (30),      cust_address  varchar (30),      cust_city varchar (20),      cust_state  varchar (3),      cust_zip  varchar (6),      primary key (cust_num));\n\n$  sqoop export --connect jdbc:mysql://127.0.0.1:3306/sales --driver com.mysql.jdbc.Driver --username root --password hadoop --table customer_exported  --export-dir /user/data/customer\n```", "```py\n$ mysql>  use sales;$ mysql>  select * from customer_exported;\n```", "```py\n## Sample Flume Agent Configuration  \n## This conf file should deploy on each webserver \n##   \n\na1.sources = apache \na1.sources.apache.type = exec \na1.sources.apache.command = gtail -F /var/log/httpd/access_log \na1.sources.apache.batchSize = 1 \na1.sources.apache.channels = memoryChannel \n\na1.channels = memoryChannel \na1.channels.memoryChannel.type = memory \na1.channels.memoryChannel.capacity = 100 \n\n## Collector Details \n\na1.sinks = AvroSink \na1.sinks.AvroSink.type = avro \na1.sinks.AvroSink.channel = memoryChannel \na1.sinks.AvroSink.hostname = 10.0.0.10 \na1.sinks.AvroSink.port = 6565 \n```", "```py\n\n## Collector get data from all agents \n\ncollector.sources = AvroIn \ncollector.sources.AvroIn.type = avro \ncollector.sources.AvroIn.bind = 0.0.0.0 \ncollector.sources.AvroIn.port = 4545 \ncollector.sources.AvroIn.channels = mc1 mc2 \n\ncollector.channels = mc1 mc2 \ncollector.channels.mc1.type = memory \ncollector.channels.mc1.capacity = 100 \n\ncollector.channels.mc2.type = memory \ncollector.channels.mc2.capacity = 100 \n\n## Write copy to Local Filesystem (Debugging) \n# http://flume.apache.org/FlumeUserGuide.html#file-roll-sink \ncollector.sinks.LocalOut.type = file_roll \ncollector.sinks.LocalOut.sink.directory = /var/log/flume \ncollector.sinks.LocalOut.sink.rollInterval = 0 \ncollector.sinks.LocalOut.channel = mc1 \n\n## Write to HDFS \ncollector.sinks.HadoopOut.type = hdfs \ncollector.sinks.HadoopOut.channel = mc2 \ncollector.sinks.HadoopOut.hdfs.path = /flume/events/%{log_type}/%{host}/%y-%m-%d \ncollector.sinks.HadoopOut.hdfs.fileType = DataStream \ncollector.sinks.HadoopOut.hdfs.writeFormat = Text \ncollector.sinks.HadoopOut.hdfs.rollSize = 0 \ncollector.sinks.HadoopOut.hdfs.rollCount = 10000 \ncollector.sinks.HadoopOut.hdfs.rollInterval = 600 \n```", "```py\n$ /bin/kafka-server-start.sh config/server.properties \n```", "```py\n$ .bin/kafka-topics --create --topic demo-1-standalone --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181 \n```", "```py\nname=source-file-stream-standalone \nconnector.class=org.apache.kafka.connect.file.FileStreamSourceConnector \ntasks.max=1 \nfile=demo-source.txt \ntopic=file-source-topic \n```", "```py\nname=sinkfile-stream-standalone \nconnector.class=org.apache.kafka.file.FileStreamSourceConnector \ntasks.max=1 \nfile=demo-sink.txt \ntopics=file-source-topic \n```", "```py\nbootstrap.servers=127.0.0.1:9092 \nkey.converter=org.apache.kafka.connect.json.JsonConverter \nkey.converter.schemas.enable=false \nvalue.converter=org.apache.kafka.connect.json.JsonConverter \nvalue.converter.schemas.enable=false \n# we always leave the internal key to JsonConverter \ninternal.key.converter=org.apache.kafka.connect.json.JsonConverter \ninternal.key.converter.schemas.enable=false \ninternal.value.converter=org.apache.kafka.connect.json.JsonConverter \ninternal.value.converter.schemas.enable=false \nrest.port=8086 \nrest.host.name=127.0.0.1 \n# this config is only for standalone workers \noffset.storage.file.filename=standalone.offsets \noffset.flush.interval.ms=10000 \n```", "```py\n$ .bin/connect-standalone config/file-worker.properties config/source-file-stream-standalone.properties config/ sink-file-stream-standalone.properties \n```", "```py\n$ touch demo-source.txt \n\n$ echo \"Test Line 1 \" >>  demo-source.txt \n\n$ echo \"Test Line 2 \" >>  demo-source.txt \n\n$ echo \"Test Line 2 \" >>  demo-source.txt \n```", "```py\n$ cat demo-sink.file \n```", "```py\nkey.converter=org.apache.kafka.connect.json.JsonConverter \nkey.converter.schemas.enable=true \nvalue.converter=org.apache.kafka.connect.json.JsonConverter \nvalue.converter.schemas.enable=true \n```"]