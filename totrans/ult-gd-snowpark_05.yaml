- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying Data Processing Using Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to set up a development environment
    for Snowpark, as well as various Snowpark components, such as DataFrames, UDFs,
    and stored procedures. We also covered how to operate those objects and run them
    in Snowflake. In this chapter, we will cover data processing with Snowpark and
    learn how to load, prepare, analyze, and transform data using Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration and transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data grouping and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you require an active Snowflake account and Python installed
    with Anaconda configured locally. You can refer to the following documentation
    for installation instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: You can sign up for a Snowflake Trial account at [https://signup.snowflake.com/](https://signup.snowflake.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To configure Anaconda, follow the guide at https://conda.io/projects/conda/en/latest/user-guide/getting-started.html
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To install and set up Python for VS Code, follow the guide at [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/python/python-tutorial)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn how to operate Jupyter Notebook in VS Code, go to [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supporting materials for this chapter are available in this book’s GitHub
    repository at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of the data engineering process is data ingestion – it is crucial
    to get all the different data into a usable format in Snowflake for analytics.
    In the previous chapter, we learned how Snowpark can access data through a DataFrame.
    This DataFrame can access data from Snowflake tables, views, and objects, such
    as streams, if we run a query against it. Snowpark supports structured data in
    various formats, such as Excel and CSV, as well as semi-structured data, such
    as JSON, XML, Parquet, Avro, and ORC; specialized formats, such as HL7 and DICOM,
    and unstructured data, such as images and media, can be ingested and handled in
    Snowpark. Snowpark enables secure and programmatic access to files in Snowflake
    stages.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of Snowpark Python allows you to adapt to changing data requirements
    effortlessly. Suppose you start with a CSV file as your data source; you can switch
    to a JSON or packet format at a later stage. With Snowpark, you don’t need to
    rewrite your entire code base. Instead, you can make minor adjustments or configuration
    changes to accommodate the new structure while keeping the core logic intact.
    This flexibility saves you valuable time and effort, enabling you to switch between
    different data formats as your needs evolve quickly.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging Snowpark’s capabilities, you can focus more on analyzing and utilizing
    data rather than worrying about the intricacies of data format handling. This
    streamlined approach empowers you to experiment with different data sources, adapt
    to evolving data requirements, and efficiently load data into Snowflake tables,
    all with minimal code changes and maximum flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s delve into the power of Snowpark Python and its ability to effortlessly
    handle different data formats, allowing you to work with diverse sources without
    cumbersome code modifications. You will experience the freedom to explore, analyze,
    and extract insights from your data while enjoying a seamless and flexible integration
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data ingestion scripts are provided in this book’s GitHub repository: [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
    These scripts will simplify the process of uploading any new dataset that will
    be used for analysis, ensuring a smooth and efficient workflow. Following a similar
    approach to what was outlined in the preceding chapters, you can effortlessly
    upload new datasets and explore Snowflake’s data engineering and machine learning
    functionalities. The provided data ingestion scripts will act as your guide, making
    the process seamless and hassle-free.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note on datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset we’ll be using in this chapter provides unique insights into customer
    behavior, campaign responses, and complaints, enabling data-driven decision-making
    and customer satisfaction improvement. The original dataset is from the Kaggle
    platform ([https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign)).
    However, the datasets that will be discussed in this section are not directly
    accessible via a Kaggle link. Instead, we started with a base dataset and generated
    new data formats to illustrate loading various dataset formats using Snowpark.
    These datasets can be found in this book’s GitHub repository under the `datasets`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets include purchase history in CSV format, campaign information in
    JSON format, and complaint information in Parquet format. These datasets provide
    valuable information about customer behavior, campaign responses, and complaints:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Purchase history (CSV)**: This file contains customer information, such as
    ID, education, marital status, and purchase metrics. The dataset offers insights
    into customer buying habits and can be further analyzed for data-driven decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Campaign information (JSON)**: The JSON dataset includes data on campaign
    acceptance and customer responses. Analyzing this dataset will help you refine
    marketing strategies and understand campaign effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complaint information (Parquet)**: This file contains details about customer
    complaints, including contact and revenue metrics. This dataset aids in tracking
    and addressing customer complaints for improved satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will be utilizing our local development environment to execute
    all Snowpark code, rather than relying on Snowflake worksheets. This approach
    offers greater flexibility and control over the development and testing of Snowpark
    scripts. When worksheets are used for specific tasks, we will explicitly call
    out their usage for clarity and context.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting a CSV file into Snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Snowflake supports ingesting data easily using CSV files. We will load the
    purchase history data into the `PURCHASE_HISTORY` table as a CSV file. We’ll upload
    `purchase_history.csv` into an internal stage by using a Snowpark session, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, the file has been uploaded to the internal stage. We will reference
    this directly in Snowpark. The data schema for the marketing table can also be
    directly defined as a Snowpark type. The following code provides the necessary
    columns and data types to create the table in Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we take our first step toward understanding the structure
    of our data by defining a schema for our purchase history dataset. Using the Snowflake
    Snowpark library, we establish the fields and corresponding data types, setting
    the foundation for our data analysis journey. This code serves as a starting point,
    guiding us in defining and working with structured data. This is not the only
    way we can load the dataset using Snowpark. We will continue to explore different
    methodologies to load other tabular datasets as we progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code imports the necessary types from the Snowflake Snowpark library.
    It creates a variable called `purchase_history_schema` and assigns it a `StructType`
    object, representing a structured schema for the dataset. The `StructType` object
    contains multiple `StructField` objects, each representing a field in the dataset.
    Each `StructField` object specifies the name of the area and its corresponding
    data type using the types provided by Snowflake Snowpark. The following code reads
    the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV file is read with file format options such as `FIELD_DELIMITER`, `SKIP_HEADER`,
    and others, all of which are specified alongside the schema defined in the preceding
    definition. The `PURCHASE_HISTORY` table was created with the data from the CSV
    file, which is now ready for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the output of the `PURCHASE_HISTORY` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The PURCHASE_HISTORY table](img/B19923_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The PURCHASE_HISTORY table
  prefs: []
  type: TYPE_NORMAL
- en: The CSV is easy to load as it uses the file format options available in Snowflake.
    Now, let’s see how we can load JSON files into Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting JSON into Snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Snowflake allows JSON structures to be ingested and processed via the `Variant`
    data type. We can ingest JSON similar to how we would ingest a CSV file – by uploading
    it into the internal stage. The `campaign_info.json` file contains data about
    marketing campaigns. We can load this into the `CAMPAIGN_INFO` table by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, the file has been uploaded to the internal stage; we will reference
    it in Snowpark. Snowpark can access the file to load it into a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the JSON file are read into the DataFrame as JSON objects.
    This DataFrame can be written into a table as a variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CAMPAIGN_INFO_TEMP` table contains the JSON data. We can query the table
    to view the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command displays the JSON data from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The Campaign Info table](img/B19923_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The Campaign Info table
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet utilizes the Snowpark library in Snowflake to manipulate
    a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code selects specific columns from an existing DataFrame and renames
    them using the `col` function. The transformed DataFrame is then saved as a new
    table in Snowflake. The code performs data **extraction, transformation, and loading**
    (**ETL**) operations by selecting and renaming columns within the DataFrame and
    saving the result as a new table in Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CAMPAIGN_INFO` table now contains the flattened data, with the data in
    separate columns so that it’s easier to process. Let’s have a look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the output of the `CAMPAIGN_INFO` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The CAMPAIGN_INFO table](img/B19923_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The CAMPAIGN_INFO table
  prefs: []
  type: TYPE_NORMAL
- en: Loading and processing JSON files in Snowpark becomes easier when using the
    `Variant` column. Next, we will cover how to load a Parquet file into Snowflake
    using Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting Parquet files into Snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parquet is a popular open source format for storing data licensed under Apache.
    The column-oriented format is lighter to store and faster to process. Parquet
    also supports complex data types since the data and the column information are
    stored in Parquet format. The `COMPLAINT_INFO` table consists of customer complaint
    information. Let’s load this into Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The file will be uploaded into the internal stage. Snowpark can access it to
    process and load it into a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Parquet file is read into the DataFrame and then copied into the `COMPLAINT_INFO`
    table. Since the Parquet file already contains the table metadata information,
    it defines the table structure. We can query the table to view the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following `COMPLAINT_INFO` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The COMPLAINT_INFO table](img/B19923_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The COMPLAINT_INFO table
  prefs: []
  type: TYPE_NORMAL
- en: Parquet is one of the preferred formats for Snowflake since it’s the format
    that’s used by Apache Iceberg. Parquet stands out in data engineering and data
    science for its columnar storage, which optimizes compression and query performance.
    Its support for schema evolution and partitioning ensures flexibility and efficiency
    in handling evolving data structures. With broad compatibility across various
    data processing frameworks, Parquet enables seamless integration into existing
    workflows, making it a cornerstone format in modern data pipelines. In the next
    section, we will cover how easy it is to load unstructured data, such as an image,
    into Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We’ve chosen to maintain separate stages for handling images and text, although
    it’s not mandatory to do so. The **MY_TEXT** and **MY_IMAGES** stages can be prepared
    using the same methods we outlined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting images into Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowflake supports versatile data, such as images, that can be uploaded into
    a stage and executed directly in Snowpark without the need to manage dependencies
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Platforms such as Amazon S3, Google Cloud Storage, and Azure Blob Storage are
    commonly preferred for managing and storing image data due to their scalability
    and reliability. However, it’s worth noting that Snowpark also offers flexibility
    in loading image files, making it a versatile option for handling image data in
    data engineering and data science workflows. We will be loading a bunch of sample
    images that can be used for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code loads the images from the local folder to the internal stage.
    The path can support wildcard entries to upload all the images in a particular
    folder. The folder in the stage can be queried to get the list of images that
    were uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows a list of all the images that are present in the stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – List of images](img/B19923_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – List of images
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the image has been uploaded, it can be directly accessed via Snowpark.
    Snowpark supports the `get_stream` function to stream the file’s contents as bytes
    from the stage. We can use a library such as Pillow to read the file from the
    bytes stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Rendering images](img/B19923_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Rendering images
  prefs: []
  type: TYPE_NORMAL
- en: The image is displayed directly in the notebook. Snowpark’s native support for
    images supports capabilities for use cases such as image classification, image
    processing, and image recognition. Snowpark also supports rendering images dynamically.
    We will cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Reading files dynamically with Snowpark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Snowpark contains the `files` module and the `SnowflakeFile` class, both of
    which provide access to files dynamically and stream them for processing. These
    dynamic files are also helpful for reading multiple files as we can iterate over
    them. `open()` extends the `IOBase` file objects and provides the functionality
    to open a file. The `SnowflakeFile` object also supports other `IOBase` methods
    for processing the file. The following code shows an example of reading multiple
    files using a relative path from the internal stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code iterates over the `@MY_TEXTS` stage location and calculates
    the length of each file using the `SnowflakeFile` method. The path is passed as
    the input to the UDF. We can execute the function to get the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Dynamic files within Snowpark](img/B19923_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Dynamic files within Snowpark
  prefs: []
  type: TYPE_NORMAL
- en: The files in the stage are displayed as output. In this section, we covered
    ingesting different types of files into Snowflake using Snowpark. In the next
    section, we will learn how to perform data preparation and transformations using
    Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the data has been loaded, the next step is to prepare the data so that
    it can be transformed. In this section, we will cover how to perform data exploration
    so that we understand how the modify the data as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data exploration** is a critical step in data analysis as it sets the stage
    for successful insights and informed decision-making. By delving into the data,
    analysts can deeply understand its characteristics, uncover underlying patterns,
    and identify potential issues or outliers. Exploring the data provides valuable
    insights into its structure, distribution, and relationships, enabling analysts
    to choose the appropriate data transformation techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data’s characteristics and patterns helps analysts determine
    the appropriate transformations and manipulations needed to clean, reshape, or
    derive new variables from the data. Additionally, data exploration aids in identifying
    subsets of data that are relevant to the analysis, facilitating the filtering
    and sub-setting operations required for specific analytical objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before embarking on data transformation, we must understand the data we have
    in place. By comprehensively understanding the data, we can effectively identify
    its structure, quality, and patterns. This understanding is a solid foundation
    for informed decision-making during the data transformation process, enabling
    us to extract meaningful insights and derive maximum value from the data. Take
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we loaded the necessary tables into a session. These tables are now available
    in the Snowpark session for further data preparation. We will start by preparing
    the `PURCHASE_HISTORY` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `show()` method returns the data from the DataFrame. The preceding code
    produces the top 5 rows from the `PURCHASE_HISTORY` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – PURCHASE_HISTORY – top 5 rows](img/B19923_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – PURCHASE_HISTORY – top 5 rows
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `collect()` method to display the data in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The records from the `PURCHASE_HISTORY` table are shown in the JSON array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – PURCHASE_HISTORY – full table](img/B19923_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – PURCHASE_HISTORY – full table
  prefs: []
  type: TYPE_NORMAL
- en: The difference between collect() and show()
  prefs: []
  type: TYPE_NORMAL
- en: 'In Snowpark Python, there are two essential functions: **collect()** and **show()**.
    These functions serve different purposes in processing and displaying data. The
    **collect()** function in Snowpark Python is used to gather or retrieve data from
    a specified source, such as a table, file, or API. It allows you to perform queries,
    apply filters, and extract the desired information from the data source. The collected
    data is stored in a variable or structure, such as a DataFrame, for further analysis
    or manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the **show()** function in Snowpark Python is primarily used
    to display the contents of a DataFrame or any other data structure in a tabular
    format. It provides a convenient way to visualize and inspect the data at different
    stages of the data processing pipeline. The **show()** function presents the data
    in a human-readable manner, showing the rows and columns in a structured table-like
    format. It can be helpful for debugging, understanding the data’s structure, or
    performing exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the **collect()** function focuses on gathering and retrieving data
    from a source, while the **show()** function displays the data in a readable format.
    Both functions play essential roles in Snowpark Python when it comes to working
    with data, but they serve distinct purposes in the data processing workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use the `count()` method to get the total count of the rows in
    the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From the resulting output, we can see that the `PURCHASE_HISTORY` table contains
    around 2,000 rows of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now check the columns of the table to understand more about this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the column information, which helps us understand the data better.
    The column information contains the data related to customer purchase history:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – PURCHASE_HISTORY columns](img/B19923_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – PURCHASE_HISTORY columns
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now filter the data to slice and dice it. We can use the following code
    to filter specific rows or a single row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the column. where `id` is set to `1`. We can pass multiple values
    in the column filter to perform additional row-level operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – PURCHASE_HISTORY ID filter](img/B19923_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – PURCHASE_HISTORY ID filter
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need to add multiple filter values, we can use the `&` operation to pass
    multiple column filter values to the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides data about those with `MARITAL_STATUS` set to `Married`
    and who have kids at home (`KIDHOME`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – PURCHASE_HISTORY filters](img/B19923_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – PURCHASE_HISTORY filters
  prefs: []
  type: TYPE_NORMAL
- en: 'This helps us understand the purchase history pattern of married customers
    with kids. We can also filter it to the year of birth by passing the year of birth
    range between 1964 and 1980:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This displays the purchase data for customers born between 1964 and 1980:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – PURCHASE_HISTORY filters](img/B19923_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – PURCHASE_HISTORY filters
  prefs: []
  type: TYPE_NORMAL
- en: 'This data helps us understand their purchases. We can also use the `select()`
    method to select only the columns that are required for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding returns only the customer’s ID, year, and education status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – PURCHASE_HISTORY columns](img/B19923_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – PURCHASE_HISTORY columns
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, we will delve deeper into data exploration, uncovering
    more techniques to gain insights from our data.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon these basic exploration steps, we will dive into the realm of
    data transformation operations. By combining our understanding of the data and
    the power of transformation techniques, we will unlock the full potential of our
    data and extract valuable insights for informed decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how to perform data transformation using
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data transformation** is a fundamental process that involves modifying and
    reshaping data to make it more suitable for analysis or other downstream tasks,
    such as the machine learning model building process. It entails applying a series
    of operations to the data, such as cleaning, filtering, aggregating, and reformatting,
    to ensure its quality, consistency, and usability. Data transformation allows
    us to convert raw data into a structured and organized format that can be easily
    interpreted and analyzed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data requires minimal transformation, and we will cover it extensively
    in the coming chapters. Our goal for this section is to combine data from different
    sources, creating a unified table for further processing that we will use in the
    next chapter. We will leverage Snowpark’s robust join and union capabilities to
    accomplish this. By utilizing joins, we can merge data based on standard columns
    or conditions. Unions, on the other hand, allow us to append data from multiple
    sources vertically. These techniques will enable us to integrate and consolidate
    our data efficiently, setting the stage for comprehensive analysis and insights.
    Let’s explore how Snowpark’s join and union capabilities can help us achieve this
    data combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are joining the purchase history to campaign information to establish
    the relationship between purchases and campaigns. The standard ID column is used
    to select the join and defaults to an inner join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We are dropping the extra ID column from the joined result. The DataFrame now
    contains just a single ID column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This displays the data of the purchase campaign combined with the purchase
    history and the campaign information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Purchase campaign data](img/B19923_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Purchase campaign data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s combine this with the complaint information to get the complete data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are combining the result of the purchase campaign along with the complaint
    information by using the standard ID column. The resultant DataFrame contains
    the complete data required for data analysis. We are dropping the extra ID column
    from the joined result. The DataFrame now has just a single ID column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This displays the final data combined from all three tables. We can now write
    this data into the table for further analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, the data is written into the `MARKETING_DATA` table, at which point it
    will be available inside Snowflake. We need to append this data with the additional
    marketing data that must be loaded into this table.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between joins and unions
  prefs: []
  type: TYPE_NORMAL
- en: Joins combine data from two or more tables based on a shared column or condition.
    In Snowflake Snowpark, you can perform different types of joins, such as inner
    join, left join, right join, and full outer join. Joins allow you to merge data
    horizontally by aligning rows based on matching values in the specified columns.
    This enables you to combine related data from multiple tables, resulting in a
    combined dataset that includes information from all the joined tables.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, unions are used to append data from multiple tables vertically,
    or result sets into a single dataset. Unlike joins, unions do not require any
    specific conditions or matching columns. Instead, they stack rows on top of each
    other, concatenating the data vertically. This is useful when you have similar
    datasets with the same structure and want to consolidate them into a single dataset.
    Unions can be performed in Snowflake Snowpark to create a new dataset that contains
    all the rows from the input tables or result sets.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, joins in Snowflake Snowpark are used to combine data horizontally
    by matching columns, while unions are used to stack data vertically without any
    specific conditions. Joins merge related data from multiple tables, while unions
    append similar datasets into a single dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Appending data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Snowflake Snowpark `UNION` function is vital in combining and integrating
    new data into a Snowflake database. The importance of the `UNION` function lies
    in its ability to append rows from different data sources vertically, or result
    sets into a single consolidated dataset. When new data is added to the database,
    it is often necessary to merge or combine it with existing data for comprehensive
    analysis. The `UNION` function allows us to seamlessly integrate the newly added
    data with the existing dataset, creating a unified view encompassing all relevant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: This capability of the `UNION` function is precious in scenarios where data
    is received or updated periodically. For example, suppose we receive daily sales
    data or log files. In that case, the `UNION` function enables us to effortlessly
    append the new records to the existing dataset, ensuring that our analysis reflects
    the most up-to-date information. Additionally, it ensures data consistency and
    allows for seamless continuity in data analysis, enabling us to derive accurate
    insights and make informed decisions based on the complete and unified dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional marketing data is available in the `MARKETING_ADDITIONAL` table.
    Let’s see how we can leverage Snowpark’s `UNION` function to include this additional
    data for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the data from the `MARKETING_ADDITIONAL` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – The MARKETING_ADDITIONAL table](img/B19923_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – The MARKETING_ADDITIONAL table
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, the table has been loaded into the DataFrame. Let’s look at the
    number of rows in our original and appended tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This code displays the total number of rows in the `MARKETING_ADDITIONAL` and
    `PURCHASE_HISTORY` tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Data row count](img/B19923_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Data row count
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MARKETING_ ADDITIONAL` table contains 240 rows of new data that must be
    appended with the `PURCHASE_HISTORY` table, which contains 2,000 rows of data.
    Since the column names are identical, the data can be appended by using `union_by_name`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the DataFrame contains the appended data. Let’s look at the number of
    rows in this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the final data that’s in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – The MARKETING_FINAL table](img/B19923_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – The MARKETING_FINAL table
  prefs: []
  type: TYPE_NORMAL
- en: 'The total count of the rows is 2,240\. With that, the new data has been appended.
    Now, we will write this data into the `MARKETING_FINAL` table in Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `MARKETING_DATA` table is now available in Snowflake and can be consumed.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between union and union_by_name
  prefs: []
  type: TYPE_NORMAL
- en: 'Two methods are available for combining data: **union_by_name** and **union**.
    Both methods allow multiple datasets to be merged, but they differ in their approach
    and functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: The **union_by_name** method in Snowpark Python is specifically designed to
    combine datasets by matching and merging columns based on their names. This method
    ensures that the columns with the same name from different datasets are merged,
    creating a unified dataset. It is beneficial when you have datasets with similar
    column structures and want to consolidate them while preserving the column names.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the **union** method in Snowpark Python combines datasets
    by simply appending them vertically, regardless of column names or structures.
    This method concatenates the rows from one dataset with the rows from another,
    resulting in a single dataset with all the rows from both sources. The **union**
    method is suitable for stacking datasets vertically without considering column
    names or matching structures. However, note that in certain cases, the column
    type matters, such as when casting a string column to a numeric value.
  prefs: []
  type: TYPE_NORMAL
- en: Data grouping and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the data is ready and has been transformed, the next step is to see
    how we can group data to understand important patterns and analyze it. In this
    section, we will aggregate this data and analyze it.
  prefs: []
  type: TYPE_NORMAL
- en: Data grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data analysis, understanding patterns within datasets is crucial for gaining
    insights and making informed decisions. One powerful tool that aids in this process
    is the `group_by` function in Snowpark Python. This function allows us to group
    data based on specific criteria, enabling us to dissect and analyze the dataset
    in a structured manner.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing the `group_by` function, we can uncover valuable insights into
    how data is distributed and correlated across different categories or attributes.
    For example, we can group sales data by product category to analyze sales trends,
    or group customer data by demographics to understand buying behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the `group_by` function can be combined with other data manipulation
    and visualization techniques to gain deeper insights. For instance, we can create
    visualizations such as bar charts or heatmaps to visually represent the aggregated
    data, making it easier to spot patterns and trends.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate grouping and conducting deeper analysis, we’ll utilize the `MARKETING_FINAL`
    table we established earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are loading the data from the `MARKETING_FINAL` table into the DataFrame.
    We will use this DataFrame to perform aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the average income by `EDUCATION`. People with PhDs have the highest
    average income, and people with primary education have the lowest average income:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Average income by education](img/B19923_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Average income by education
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create an alias for the column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The average income is displayed as an alias – `AVG_INCOME`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – The AVG_INCOME alias](img/B19923_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – The AVG_INCOME alias
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also achieve similar results by using the `function()` method to pass
    the respective operation from Snowpark functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Sum of revenue by marital status](img/B19923_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Sum of revenue by marital status
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that married customers generate the highest revenue. We can
    also use `agg()` to perform this particular aggregation. Let’s calculate the maximum
    income by marital status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Income by marital status](img/B19923_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Income by marital status
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that customers who are together and married as a family have
    the maximum income to spend, and hence they generate the maximum revenue. Next,
    we will find the count of different types of graduates and their maximum income:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Count of category](img/B19923_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Count of category
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that `PhD` has a maximum income of `162397`, and that people
    with `Basic` income have the lowest maximum income – that is, `34445`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also perform complex multi-level aggregations in Snowpark. Let’s find
    out how people with different educations and marital statuses spend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Multi-level aggregation](img/B19923_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Multi-level aggregation
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s determine the relationship between `EDUCATION`, `MARITAL_STATUS`, and
    `SUM_PURCHASE`. People who are graduates and married spend the most compared to
    single people. We can also sort the results by using the `sort()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Sorted result](img/B19923_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Sorted result
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are sorting the results in ascending order by purchase amount after
    the aggregation is completed. The following section will cover some standard data
    analysis that can be performed on this data.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we delved into data exploration, transformation, and
    aggregation, where we learned about various techniques we can use to find out
    what our data is all about and how we can combine different datasets. Armed with
    a solid foundation of general dataset exploration, we are ready to dive deeper
    into data analysis using Snowpark Python.
  prefs: []
  type: TYPE_NORMAL
- en: This section focuses on leveraging the power of statistical functions, sampling
    techniques, pivoting operations, and converting data into a pandas DataFrame for
    advanced analysis. We will explore applying statistical functions to extract meaningful
    information from our data. Then, we will learn about different sampling techniques
    to work efficiently with large datasets. Additionally, we will discover how to
    reshape our data using pivoting operations to facilitate in-depth analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we will explore the seamless integration of Snowpark Python with pandas,
    a widely used data manipulation library. We will understand how to convert our
    Snowpark data into a pandas DataFrame, enabling us to leverage pandas’ extensive
    analytical and visualization capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The following section provides a glimpse into the capabilities of Snowpark Python
    for data analysis; we will delve deeper into each topic in the subsequent chapter.
    Here, we aim to provide a foundational understanding of the key concepts and techniques
    of analyzing data using Snowpark Python. In the next chapter, we will explore
    these topics in greater detail, unraveling the full potential of Snowpark Python
    for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in our analysis is understanding how our data is distributed.
    The `describe()` function in pandas is a valuable tool that helps us gain insights
    into the statistical properties of our numerical data. When we apply `describe()`
    to a DataFrame, it computes various descriptive statistics, including the count,
    mean, standard deviation, minimum, quartiles, and maximum values for each numerical
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'This summary comprehensively overviews our data’s distribution and central
    tendencies. By examining these statistics, we can quickly identify key characteristics,
    such as the range of values, the spread of the data, and any potential outliers.
    This initial exploration sets the stage for more advanced analysis techniques
    and allows us to make informed decisions based on a solid understanding of our
    dataset’s distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the data from the `MARKETING_FINAL` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – MARKETING_FINAL DataFrame](img/B19923_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – MARKETING_FINAL DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: The result shows the different columns and the data in the `MARKETING_FINAL`
    table.
  prefs: []
  type: TYPE_NORMAL
- en: Finding distinct data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Snowpark DataFrames, the `distinct()` function is crucial in identifying
    unique values within a column or set of columns. When applied to a Snowpark DataFrame,
    `distinct()` eliminates duplicate records, resulting in a new DataFrame that contains
    only distinct values. This function is particularly useful for dealing with large
    datasets or extracting unique records for analysis or data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the total count of the `MARKETING_FINAL` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – MARKETING_FINAL count](img/B19923_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – MARKETING_FINAL count
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the entire dataset is returned since we do not have any duplicate
    rows. `distinct()` preserves the original rows of the DataFrame and only filters
    out repeated values within the specified columns.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping duplicates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`drop_duplicates()` removes duplicate rows from a Snowpark DataFrame. It analyzes
    the entire row and compares it with other rows in the DataFrame. If a row is found
    to be an exact duplicate of another row, `drop_duplicates()` will remove it, keeping
    only the first occurrence. By default, this function considers all columns in
    the DataFrame for duplicate detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Marketing duplicates removed](img/B19923_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – Marketing duplicates removed
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can specify specific columns using the `subset` parameter to check
    for duplicates based on those columns alone. `drop_duplicates()` modifies the
    original DataFrame by removing duplicate rows.
  prefs: []
  type: TYPE_NORMAL
- en: Crosstab analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have identified the unique combinations of the `EDUCATION` and `MARITAL_STATUS`
    columns in our dataset, we might still be curious about how frequently each combination
    occurs. We can utilize the `crosstab` function to determine the occurrence of
    these unique combinations. By applying the `crosstab` function to our dataset,
    we can generate a cross-tabulation or contingency table that displays the frequency
    distribution of the unique combinations of `EDUCATION` and `MARITAL_STATUS`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the crosstab data in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Crosstab data](img/B19923_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – Crosstab data
  prefs: []
  type: TYPE_NORMAL
- en: This table provides a comprehensive overview of how often each unique combination
    occurs in the dataset, allowing us to gain valuable insights into the relationships
    between these variables. The `crosstab` function aids us in understanding the
    distribution and occurrence patterns of the unique combinations, further enhancing
    our data analysis capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Pivot analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Upon using the `crosstab` function to examine the unique combinations of the
    `EDUCATION` and `MARITAL_STATUS` columns in our dataset, we might encounter certain
    combinations with zero occurrences. We can construct a pivot table to gain a more
    comprehensive understanding of the data and further investigate the relationships
    between these variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Constructing a pivot table allows us to summarize and analyze the data more
    dynamically and flexibly. Unlike the `crosstab` function, which only provides
    the frequency distribution of unique combinations, a pivot table allows us to
    explore additional aggregate functions, such as sum, average, or maximum values.
    This enables us to delve deeper into the dataset and obtain meaningful insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the data in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Pivot table](img/B19923_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 – Pivot table
  prefs: []
  type: TYPE_NORMAL
- en: By constructing a pivot table for the `EDUCATION` and `MARITAL_STATUS` columns,
    we can uncover the occurrence counts and various statistical measures or calculations
    associated with each combination. This expanded analysis provides a more comprehensive
    view of the data and allows for a more nuanced and detailed exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When the **crosstab** function displays zero occurrences for certain combinations
    of variables, it is essential to note that those combinations will be represented
    as **NULL** values instead of zeros when constructing a pivot table.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike **crosstab**, which explicitly highlights zero counts for combinations
    absent in the dataset, a pivot table considers all possible combinations of the
    variables. Consequently, if a variety does not exist in the dataset, the corresponding
    cell in the pivot table will be represented as a **NULL** value rather than a
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of **NULL** values in the pivot table highlights the absence of
    data for those particular combinations. Interpreting and handling these **NULL**
    values appropriately during subsequent data analysis processes, such as data cleaning,
    imputation, or further statistical calculations, is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping missing values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `dropna()` function in pandas is a powerful tool for handling missing values
    in a DataFrame. In this case, we will be utilizing the `dropna()` functionality
    of Snowpark, which allows us to remove rows or columns that contain missing or
    `NULL` values, helping to ensure the integrity and accuracy of our data. The `dropna()`
    function offers several parameters that provide flexibility in controlling the
    operation’s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the data with the applied filter from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Pivot table – dropna()](img/B19923_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 – Pivot table – dropna()
  prefs: []
  type: TYPE_NORMAL
- en: 'The `how` parameter determines the criteria that are used to drop rows or columns.
    It accepts the input as `any` and `all`: `any` drops the row or column if it contains
    any missing value, and `all` drops the row or column only if all its values are
    missing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `thresh` parameter specifies the minimum number of non-null values required
    to keep a row or column. The row or column is dropped if the *non-null values
    exceed* the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the data with the applied filter from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – Pivot threshold](img/B19923_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 – Pivot threshold
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subset` parameter allows us to specify a subset of columns or rows for
    missing value removal. It accepts a list of column or row labels. By default,
    `dropna()` checks all columns or rows for missing values. However, with a subset,
    we can focus on specific columns or rows for the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code drops any rows from the `market_pivot` DataFrame where the
    `Graduation` column has missing values and then displays the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Pivot subset](img/B19923_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – Pivot subset
  prefs: []
  type: TYPE_NORMAL
- en: This shows the data with the applied filter from the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When working with pivot tables, it is crucial to handle **NULL** values appropriately
    because they can impact the accuracy and reliability of subsequent analyses. This
    allows us to ensure that we have complete data for further analysis and calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Having **NULL** values in the pivot result can lead to incorrect interpretations
    or calculations since **NULL** values can propagate through the analysis and affect
    subsequent aggregations, statistics, or visualizations. By replacing **NULL**
    values with a specific value, such as 0, we can provide a meaningful representation
    of the data in the pivot table, allowing us to perform reliable analysis and make
    informed decisions based on complete information.
  prefs: []
  type: TYPE_NORMAL
- en: Filling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `fillna()` function allows us to replace null values with specific values
    or apply various techniques for imputation. It also allows us to fill in the missing
    values in a DataFrame, ensuring that we maintain the integrity of the data structure.
    We can specify the values for filling nulls, such as a constant value, or values
    derived from statistical calculations such as mean, median, or mode. The `fillna()`
    function is useful when we’re treating null values while considering the data’s
    nature and the desired analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fills any null values in the `market_pivot` DataFrame with
    a value of `0` and then displays the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – Missing values](img/B19923_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – Missing values
  prefs: []
  type: TYPE_NORMAL
- en: This is a handy function that fills in missing values that need to be used for
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Variable interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `corr()` function calculates the correlation coefficient, which measures
    the strength and direction of the linear relationship between two variables. It
    returns a value between -1 and 1, where -1 represents a perfect negative correlation,
    1 illustrates a perfect positive correlation, and 0 indicates no linear correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing this code, we obtain the correlation coefficient between the `INCOME`
    and `NUMSTOREPURCHASES` columns, providing insights into the potential relationship
    between income levels and the number of store purchases in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.35 – Correlation value](img/B19923_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.35 – Correlation value
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cov()` function, on the other hand, calculates the covariance, which measures
    the degree of association between two variables without normalizing for scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.36 – Covariance value](img/B19923_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.36 – Covariance value
  prefs: []
  type: TYPE_NORMAL
- en: The covariance between the `INCOME` and `NUMSTOREPURCHASES` columns helps us
    understand how changes in income levels correspond to changes in the number of
    store purchases in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While both **corr()** and **cov()** help analyze relationships between variables,
    it is essential to note that in Snowpark Python, these functions only support
    the analysis of two variables at a time. This limitation means we can only calculate
    the correlation or covariance between two columns in a DataFrame, and not simultaneously
    across multiple variables. Additional techniques or functions may be required
    to overcome this limitation and perform correlation or covariance analysis for
    various variables.
  prefs: []
  type: TYPE_NORMAL
- en: Operating with pandas DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converting a Snowpark DataFrame into a pandas DataFrame is a valuable step that
    opens up a wide range of analysis capabilities. Snowpark provides seamless integration
    with pandas, allowing us to leverage pandas’ extensive data manipulation, analysis,
    and visualization functionalities. By converting a Snowpark DataFrame into a pandas
    DataFrame, we gain access to a vast ecosystem of tools and libraries that are
    designed explicitly for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This transition enables us to leverage pandas’ rich functions and methods, such
    as statistical calculations, advanced filtering, grouping operations, and time
    series analysis. pandas also provide many visualization options, such as generating
    insightful plots, charts, and graphs that are more accessible, to visualize the
    data. With pandas, we can create meaningful visual representations of our data,
    facilitating the exploration of patterns, trends, and relationships. Additionally,
    working with pandas allows us to utilize its extensive community support and resources.
    The pandas library has a vast user community, making finding documentation, tutorials,
    and helpful discussions on specific data analysis tasks more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converting a Snowpark DataFrame into a pandas DataFrame can have its limitations,
    mainly when dealing with large datasets. The primary constraint is memory consumption
    as converting the entire dataset simultaneously may exceed available memory resources.
    This can hinder the analysis process and potentially lead to system crashes or
    performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: However, these limitations can be mitigated by breaking the DataFrame into batches
    and sampling the data. We’ll discuss this shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis using pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Converting a Snowpark DataFrame into a pandas DataFrame empowers us to seamlessly
    transition from Snowpark’s powerful data processing capabilities to pandas’ feature-rich
    environment. This interoperability expands our analytical possibilities and enables
    us to perform advanced analysis and gain deeper insights from our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code converts the `marketing_final` Snowpark DataFrame into a
    pandas DataFrame, allowing us to work with the data using pandas’ extensive data
    analysis and manipulation functionalities. It will print out the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37 – The resulting pandas DataFrame](img/B19923_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.37 – The resulting pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: This shows the data that has been converted into the pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation in pandas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In pandas, calculating correlations among multiple columns is straightforward:
    it involves selecting the desired columns and applying the `corr()` function.
    It generates a correlation matrix, allowing us to examine the relationships between
    each pair of columns simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code calculates the correlation matrix among the `INCOME`, `KIDHOME`,
    and `RECENCY` columns in the `pandas_df` pandas DataFrame. It computes the pairwise
    correlation coefficients between these columns, providing insights into their
    relationships. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.38 – Pandas correlation](img/B19923_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.38 – Pandas correlation
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at frequency distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Calculating the frequency of values in a single column is simpler in pandas
    than in Snowpark Python. We can quickly obtain the frequency distribution in pandas
    by using the `value_counts()` function on a specific column. It returns a Series
    with unique values as indices and their corresponding counts as values. This concise
    method allows us to quickly understand the distribution and prevalence of each
    unique value in the column. On the other hand, in Snowpark Python, obtaining the
    frequency of values in a single column requires more steps and additional coding.
    We typically need to group the DataFrame by the desired column and then perform
    aggregation operations to count the occurrences of each unique value. Although
    this can be achieved in Snowpark Python, it involves more complex syntax and multiple
    transformations, making the process more cumbersome compared to pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`frequency = pandas_df.EDUCATION.value_counts()` calculates the frequency distribution
    of unique values in the `EDUCATION` column of the `pandas_df` pandas DataFrame
    and assigns the result to the `frequency` variable. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.39 – Pandas data frequency](img/B19923_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.39 – Pandas data frequency
  prefs: []
  type: TYPE_NORMAL
- en: This shows the data frequency values in the pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization in pandas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating visualizations is made easy with pandas due to its seamless integration
    with popular visualization libraries such as Matplotlib and Seaborn. pandas provides
    a simple and intuitive interface to generate various visualizations, including
    line plots, bar charts, histograms, scatter plots, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'By leveraging pandas’ built-in plotting functions, we can effortlessly transform
    our data into insightful visual representations, enabling us to explore patterns,
    trends, and relationships within our dataset. With just a few lines of code, pandas
    *empowers* us to produce visually appealing and informative plots, facilitating
    the communication and interpretation of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates a horizontal bar plot from the frequency distribution
    data stored in the `frequency` variable, where each unique value is represented
    by a bar with a length proportional to its count, and the plot has a customized
    size of 8 inches in width and 3 inches in height:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.40 – Frequency plot](img/B19923_03_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.40 – Frequency plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can generate a Hexbin plot by changing `kind` to `hexbin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates a Hexbin plot that visualizes the relationship between
    the `INCOME` and `MNTGOLDPRODS` columns in the `pandas_df` pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.41 – Hexbin plot](img/B19923_03_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.41 – Hexbin plot
  prefs: []
  type: TYPE_NORMAL
- en: Here, the *X*-axis represents income values and the *Y*-axis represents the
    number of gold products. The plot is limited to X-axis limits of 0 to 100,000
    and Y-axis limits of 0 to 100, with a customized size of 8 inches in width and
    3 inches in height.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking a DataFrame into batches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `to_pandas_batches()` function converts a Snowpark DataFrame into multiple
    smaller pandas DataFrames to be processed in batches. This approach reduces memory
    usage by converting the data into manageable portions, enabling efficient analysis
    of large datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.42 – DataFrame batches](img/B19923_03_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.42 – DataFrame batches
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code demonstrates how to analyze a large dataset in batches using
    the `to_pandas_batches()` function in Snowpark Python. By iterating over the `to_pandas_batches()`
    function, the code processes the dataset in manageable batches rather than loading
    the entire dataset into memory at once. In each iteration, a batch of the dataset
    is converted into a pandas DataFrame and stored in the `batch` variable. The `print(batch.shape)`
    statement provides the shape of each batch, indicating the number of rows and
    columns in that specific batch.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the dataset in batches allows for more efficient memory utilization,
    enabling us to process large datasets that might otherwise exceed available memory
    resources. This approach facilitates the analysis of large datasets by breaking
    them into smaller, more manageable portions, allowing for faster computations
    and reducing the risk of memory-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling a DataFrame
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `sample()` function in Snowpark Python allows us to retrieve a random subset
    of data from the Snowpark DataFrame. By specifying the desired fraction or number
    of rows, we can efficiently extract a representative sample for analysis. This
    technique reduces the memory footprint required for conversion and subsequent
    analysis while providing meaningful insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.43 – Sampling data](img/B19923_03_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.43 – Sampling data
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code selects a random sample of 50% of the rows from the `marketing_final`
    DataFrame and assigns it to the `sample_df` DataFrame. The final count step produces
    slightly different output each time you run the code segment as it involves sampling
    the original table. The subsequent `sample_df.count()` function calculates the
    count of non-null values in each column of the `sample_df` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing the methods we covered here in Snowpark Python, we can overcome
    the limitations of converting large Snowpark DataFrames into pandas DataFrames,
    allowing for practical analysis while efficiently managing memory resources. These
    functions provide flexibility and control, enabling us to work with sizable datasets
    in a manageable and optimized manner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Snowpark provides different data processing capabilities and supports various
    techniques. It provides us with an easy and versatile way to ingest different
    structured and unstructured file formats, and Snowpark’s DataFrames support various
    data transformation and analysis operations. We covered various Snowpark session
    variables and different data operations that can be performed using Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how to build data engineering pipelines with
    Snowpark.
  prefs: []
  type: TYPE_NORMAL
