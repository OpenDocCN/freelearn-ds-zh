- en: Chapter 4. Connecting the Dots with Models – Regression Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. 通过模型连接点 – 回归方法
- en: The trend line is a common feature of many business analyses. How much do purchases
    increase when ads are shown more often on a homepage? What is the average rating
    of videos on social media based on user age? What is the likelihood that a customer
    will buy a second product from your website if they bought their first more than
    6 months ago? These sorts of questions can be answered by drawing a line representing
    the average change in our response (for example, purchases or ratings) as we vary
    the input (for example, user age or amount of past purchases) based on historical
    data, and using it to extrapolate the response for future data (where we only
    know the input, but not output yet). Calculating this line is termed *regression*,
    based on the hypothesis that our observations are scattered around the true relationship
    between the two variables, and on average future observations will regress (approach)
    the trend line between input and output.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 趋势线是许多商业分析中的常见特征。当在主页上更频繁地展示广告时，购买量会增加多少？根据用户年龄，社交媒体上视频的平均评分是多少？如果客户在6个月前购买了第一个产品，他们从你的网站上购买第二个产品的可能性有多大？这些问题可以通过绘制一条线来回答，这条线表示随着输入（例如，用户年龄或过去购买量）的变化，我们的响应（例如，购买或评分）的平均变化，并基于历史数据使用它来外推未来数据的响应（在这种情况下，我们只知道输入，但不知道输出）。计算这条线被称为**回归**，基于假设我们的观察值围绕着两个变量之间真实关系的周围散布，并且平均来说，未来的观察值将回归（接近）输入和输出之间的趋势线。
- en: Several complexities complicate this analysis in practice. First, the relationships
    we fit usually involve not one, but several inputs. We can no longer draw a two
    dimensional line to represent this multi-variate relationship, and so must increasingly
    rely on more advanced computational methods to calculate this trend in a high-dimensional
    space. Secondly, the trend we are trying to calculate may not even be a straight
    line – it could be a curve, a wave, or even more complex patterns. We may also
    have more variables than we need, and need to decide which, if any, are relevant
    for the problem at hand. Finally, we need to determine not just the trend that
    best fits the data we have, but also generalizes best to new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，几个复杂性使得这种分析变得复杂。首先，我们拟合的关系通常涉及不止一个输入，而不仅仅是单一输入。我们不能再画一个二维线来表示这种多变量关系，因此必须越来越多地依赖更高级的计算方法来计算这个在多维空间中的趋势。其次，我们试图计算的趋势甚至可能不是一条直线——它可能是一条曲线、一个波或更复杂的模式。我们可能也有比我们需要的更多变量，需要决定哪些，如果有的话，与当前问题相关。最后，我们需要确定不仅是最适合我们已有数据的趋势，而且也是对新数据推广得最好的趋势。
- en: 'In this chapter we will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习：
- en: How to prepare data for a regression problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何为回归问题准备数据
- en: How to choose between linear and nonlinear methods for a given problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在给定问题中选择线性或非线性方法
- en: How to perform variable selection and assess over-fitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行变量选择和评估过拟合
- en: Linear regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: '**Ordinary Least Squares** (**OLS**).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通最小二乘法**（**OLS**）。'
- en: 'We will start with the simplest model of linear regression, where we will simply
    try to fit the best straight line through the data points we have available. Recall
    that the formula for linear regression is:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最简单的线性回归模型开始，尝试通过我们拥有的数据点拟合最佳直线。回忆一下，线性回归的公式是：
- en: '![Linear regression](img/B04881_04_06.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04881_04_06.jpg)'
- en: Where y is a vector of n responses we are trying to predict, X is a vector of
    our input variable also of length n, and β is the slope response (how much the
    response y increases for each 1-unit increase in the value of X). However, we
    rarely have only a single input; rather, X will represent a set of input variables,
    and the response y is a linear combination of these inputs. In this case, known
    as multiple linear regression, X is a matrix of n rows (observations) and m columns
    (features), and β is a vector set of slopes or coefficients which, when multiplied
    by the features, gives the output. In essence, it is just the trend line incorporating
    many inputs, but will also allow us to compare the magnitude effect of different
    inputs on the outcome. When we are trying to fit a model using multiple linear
    regression, we also assume that the response incorporates a white noise error
    term ε, which is a normal distribution with mean 0 and a constant variance for
    all data points.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 y 是我们试图预测的 n 个响应的向量，X 是长度为 n 的输入变量的向量，β 是斜率响应（响应 y 在 X 的值增加 1 个单位时增加多少）。然而，我们很少只有一个输入；相反，X
    将代表一组输入变量，响应 y 是这些输入的线性组合。在这种情况下，称为多元线性回归，X 是 n 行（观测值）和 m 列（特征）的矩阵，β 是斜率或系数的向量集，当乘以特征时给出输出。本质上，它只是包含许多输入的趋势线，但也将允许我们比较不同输入对结果的影响程度。当我们试图使用多元线性回归拟合模型时，我们还假设响应包含一个白噪声误差项
    ε，它是一个均值为 0 且对所有数据点具有恒定方差的正态分布。
- en: 'To solve for the coefficients β in this model, we can perform the following
    calculations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了求解此模型中的系数 β，我们可以进行以下计算：
- en: '![Linear regression](img/B04881_04_03.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04881_04_03.jpg)'
- en: 'The value of β is known the ordinary least squares estimate of the coefficients.
    The result will be a vector of coefficients β for the input variables. We make
    the following assumptions about the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: β 的值是系数的普通最小二乘估计。结果将是一个系数 β 的向量，用于输入变量。我们对数据做出以下假设：
- en: We assume the input variables (X) are accurately measured (there is no error
    in the values we are given). If they were not, and incorporated error, then they
    represent random variables, and we would need to incorporate this error in our
    estimate of the response in order to be accurate.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们假设输入变量（X）被准确测量（我们给出的值中没有误差）。如果不是这样，并且包含了误差，那么它们代表随机变量，我们需要在我们的响应估计中包含这个误差，以便准确。
- en: The response is a linear combination of the input variables – in other words,
    we need to be able to fit a straight line through the response. As we will see
    later in this chapter, we can frequently perform transformations to change nonlinear
    data into a linear form to satisfy this assumption.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应是输入变量的线性组合——换句话说，我们需要能够在响应中拟合一条直线。正如我们将在本章后面看到的那样，我们可以经常进行变换，将非线性数据转换为线性形式以满足这个假设。
- en: The residual (the difference between the fitted and actual response) of the
    response y is assumed to have constant variance over the range of its values.
    If this is not the case (for example, if smaller values of y have smaller errors
    than larger values), then it suggests we are not appropriately incorporating a
    source of error in our model, because the only variation left after we account
    for the predictors X should be the error term ε. As previously mentioned, this
    error term ε should have constant variance, meaning the fit should have constant
    residual variance.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应 y 的残差（拟合值与实际响应之间的差异）假设在其值域内具有恒定的方差。如果情况并非如此（例如，如果 y 的较小值比较大值具有较小的误差），那么这表明我们没有适当地在我们的模型中包含一个误差来源，因为在我们考虑了预测变量
    X 之后，剩下的唯一变化应该是误差项 ε。如前所述，这个误差项 ε 应该具有恒定的方差，这意味着拟合应该具有恒定的残差方差。
- en: The residuals are assumed to be un-correlated based on the value of the predictors
    X. This is important because we assume we are trying to fit a line that goes through
    the average of the response data points at each predictor value, which would be
    accurate if we assume that the residual error is randomly distributed about 0\.
    If the residuals are correlated with the value of a predictor, then the line that
    accurately fits the data may not go through the average, but rather be determined
    by the underlying correlation in the data. For example, if we are looking at time-series
    data, days of the week may have more correlated error at a 7-day pattern, meaning
    that our model should fit this periodicity rather than trying to simply draw a
    line through the data points for all days together.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设残差与预测值X的值不相关。这一点很重要，因为我们假设我们试图拟合一条通过每个预测值处的响应数据点平均值的线，如果我们假设残差误差在0周围随机分布，这将是非常准确的。如果残差与预测值的值相关，那么准确拟合数据的线可能不会通过平均值，而是由数据中的潜在相关性决定。例如，如果我们正在查看时间序列数据，一周中的某一天可能有一个7天的模式，这意味着我们的模型应该拟合这个周期性，而不是试图简单地通过所有天的数据点来画一条线。
- en: The predictors are assumed not to be collinear (correlated with one another).
    If two predictors are identical, then they cancel each other out when we make
    a linear combination of the input matrix X. As we can see in the derivation of
    β above, to calculate the coefficients we need to take an inverse. If columns
    in the matrix exactly cancel each other out, then this matrix (XTX)-1 is rank
    deficient and has no inverse. Recall that if a matrix is full rank, its columns
    (rows) cannot be represented by a linear combination of the other columns (rows).
    A rank-deficient does not have an inverse because if we attempt to solve the linear
    system represented by:![Linear regression](img/B04881_04_04.jpg)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设预测变量之间不存在共线性（彼此相关）。如果两个预测变量相同，那么当我们对输入矩阵X进行线性组合时，它们会相互抵消。正如我们上面在β的推导中看到的，为了计算系数，我们需要取逆。如果矩阵中的列完全相互抵消，那么这个矩阵(XTX)^-1是秩亏的，没有逆。回想一下，如果一个矩阵是满秩的，它的列（行）不能由其他列（行）的线性组合来表示。一个秩亏的矩阵没有逆，因为如果我们试图解决由以下线性系统表示的问题：![线性回归](img/B04881_04_04.jpg)
- en: Where A is the inverse we are trying to solve and I is the identity matrix,
    we will end up with columns in the solution the exactly cancel each-other, meaning
    any set of coefficients will solve the equation and we cannot have a unique solution.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当A是我们试图解决的逆矩阵，I是单位矩阵时，我们将在解的列中遇到完全相互抵消的情况，这意味着任何一组系数都可以解决这个方程，我们无法得到一个唯一解。
- en: 'Why does the OLS formula for β represent the best estimate of the coefficients?
    The reason is that this value minimizes the squared error:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么OLS公式对于β的估计代表系数的最佳估计？原因是这个值最小化了平方误差：
- en: '![Linear regression](img/B04881_04_05.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04881_04_05.jpg)'
- en: 'While a derivation of this fact is outside the scope of this text, this result
    is known as the Gauss Markov Theorem, and states that the OLS estimate is the
    Best Linear Unbiased Estimator (BLUE) of the coefficients β. Recall that when
    we are estimating these coefficients, we are doing so under the assumption that
    our calculations have some error, and deviate from the real (unseen) values. The
    BLUE is then the set of coefficients β that have the smallest mean error from
    these real values. For more details, we refer the reader to more comprehensive
    texts (Greene, William H. Econometric analysis. Pearson Education India, 2003;
    Plackett, Ronald L. "Some theorems in least squares." Biometrika 37.1/2 (1950):
    149-157).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管这个事实的推导超出了本文的范围，但这个结果被称为高斯-马尔可夫定理，它表明OLS估计量是系数β的最佳线性无偏估计量（BLUE）。回想一下，当我们估计这些系数时，我们是在假设我们的计算存在一些误差，并且与真实（未见的）值有所偏差。因此，BLUE是具有从这些真实值中最小平均误差的系数β的集合。更多细节，我们建议读者参考更全面的文本（Greene,
    William H. 经济计量分析. Pearson Education India, 2003; Plackett, Ronald L. "一些最小二乘定理."
    生物统计学 37.1/2 (1950): 149-157）。'
- en: Depending upon the problem and dataset, we can relax many of the assumptions
    described above using alternative methods that are extensions of the basic linear
    model. Before we explore these alternatives, however, let us start with a practical
    example. The data we will be using for this exercise is a set of news articles
    from the website [http://mashable.com/](http://mashable.com/). (Fernandes, Kelwin,
    Pedro Vinagre, and Paulo Cortez. "A Proactive Intelligent Decision Support System
    for Predicting the Popularity of Online News." Progress in Artificial Intelligence.
    Springer International Publishing, 2015\. 535-546.). Each article has been annotated
    using a number of features such as its number of words and what day it was posted
    - a complete list appears in the data file associated with this exercise. The
    task is to predict the popularity (the share column in the dataset) using these
    other features. In the process of fitting this first model, we will examine some
    of the common feature preparation tasks that arise in such analyses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题和数据集，我们可以使用基本线性模型的扩展方法放松上述许多假设。在我们探索这些替代方案之前，让我们从一个实际例子开始。我们将为此练习使用的数据是从网站[http://mashable.com/](http://mashable.com/)获取的一系列新闻文章。（Fernandes,
    Kelwin, Pedro Vinagre, and Paulo Cortez. "A Proactive Intelligent Decision Support
    System for Predicting the Popularity of Online News." Progress in Artificial Intelligence.
    Springer International Publishing, 2015. 535-546.）。每篇文章都使用诸如单词数量和发布日期等特征进行了注释——完整的列表出现在与此练习相关的数据文件中。任务是使用这些其他特征预测流行度（数据集中的共享列）。在拟合第一个模型的过程中，我们将检查在这种分析中出现的常见特征准备任务。
- en: Data preparation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Let us start by taking a look at the data by typing the following commands:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过输入以下命令来查看数据：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Which gives the output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you look carefully, you will realize that all the column names have a leading
    whitespace; you probably would have found out anyway the first time you try to
    extract one of the columns by using the name as an index. The first step of our
    data preparation is to fix this formatting using the following code to strip whitespace
    from every column name:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你会意识到所有列名都有前导空格；你可能在第一次尝试使用名称作为索引提取某一列时就已经发现了这一点。我们数据准备的第一步是使用以下代码从每个列名中去除空格来修复这种格式：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have correctly formatted the column headers, let us examine the
    distribution of the data using the `describe()` command as we have seen in previous
    chapters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经正确格式化了列标题，让我们使用`describe()`命令检查数据的分布，就像我们在前面的章节中看到的那样：
- en: '![Data preparation](img/B04881_04_28.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_28.jpg)'
- en: 'As you scroll from left to right along the columns, you will notice that the
    range of the values in each column is very different. Some columns have maximum
    values in the hundreds or thousands, while others are strictly between 0 and 1\.
    In particular, the value that we are trying to predict, shares, has a very wide
    distribution, as we can see if we plot the distribution using the following command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从左到右滚动列时，你会注意到每列中值的范围差异很大。有些列的最大值在几百或几千，而其他列则严格在0到1之间。特别是，我们试图预测的值，即共享值，分布非常广泛，如果我们使用以下命令绘制分布图，就可以看到：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Data preparation](img/B04881_04_29.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_29.jpg)'
- en: 'Why is this distribution a problem? Recall that conceptually, when we fit a
    line through a dataset, we are finding the solution to the equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种分布是个问题？回想一下，从概念上讲，当我们通过数据集拟合一条线时，我们是在寻找以下方程的解：
- en: '![Data preparation](img/B04881_04_06.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_06.jpg)'
- en: 'Where y is a response variable (such as shares), and β is the vector slopes
    by which we increase/decrease the value of the response for a 1-unit change in
    a column of X. If our response is logarithmically distributed, then the coefficients
    will be biased to accommodate extremely large points in order to minimize the
    total error of the fit given by:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中y是响应变量（如份额），β是通过X列的1单位变化增加/减少响应值的向量斜率。如果我们的响应是对数分布的，那么系数将偏向于适应极端大的点，以最小化给定以下拟合的总误差：
- en: '![Data preparation](img/B04881_04_07.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_07.jpg)'
- en: 'To reduce this effect, we can logarithmically transform the response variable,
    which as you can see through the following code makes a distribution that looks
    much more like a normal curve:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少这种影响，我们可以对响应变量进行对数变换，正如以下代码所示，这使得分布看起来更像正态曲线：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Data preparation](img/B04881_04_30.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_30.jpg)'
- en: This same rule of thumb holds true for our predictor variables, X. If some predictors
    are much larger than others, the solution of our equation will mainly emphasize
    those with the largest range, as they will contribute most to the overall error.
    In this example, we can systemically scale all of our variables using a logarithmic
    transformation. First, we remove all uninformative columns, such as the URL, which
    simply gives website location for the article.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经验法则同样适用于我们的预测变量X。如果某些预测变量比其他变量大得多，我们方程的解将主要强调那些范围最大的变量，因为它们将对总体误差贡献最大。在这个例子中，我们可以系统地使用对数转换来缩放所有变量。首先，我们移除所有无信息列，例如URL，它只是为文章提供网站位置。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with
    Unstructured Data"), *Words and Pixels – Working with Unstructured Data* we will
    explore potential ways to utilize information in textual data such as the url,
    but for now we will simply discard it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在[第6章](ch06.html "第6章。文字与像素 - 处理非结构化数据")*文字与像素 - 处理非结构化数据*中，我们将探讨利用文本数据中信息（如url）的潜在方法，但到目前为止，我们只是简单地丢弃它。
- en: Then, we identify the variables we wish to transform (here an easy rule of thumb
    is that their max, given by the 8th row (index 7) of the describe() data frame
    is > 1, indicating that they are not in the range 0 to 1) and use the following
    code to apply the logarithmic transform. Note that we add the number 1 to each
    logarithmically transformed variable so that we avoid errors for taking the logarithm
    of 0.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们确定要转换的变量（这里一个简单的经验法则是，它们的最大值，由describe()数据框的第8行（索引7）给出，大于1，表明它们不在0到1的范围内）并使用以下代码应用对数转换。请注意，我们给每个对数转换的变量加1，以避免对0取对数时出现错误。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the describe() command again confirms that the columns now have comparable
    distributions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用describe()命令确认，现在列具有可比的分布：
- en: '![Data preparation](img/B04881_04_31.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备](img/B04881_04_31.jpg)'
- en: 'We also need to remove infinite or nonexistent values from the dataset. We
    first convert infinite values to the placeholder ''not a number'', or NaN, using
    the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从数据集中移除无穷大或不存在的数据。我们首先使用以下方法将无穷大值转换为占位符“不是一个数字”，或NaN：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then use the `fill` function to substitute the *NaN* placeholder with the
    proceeding value in the column (we could also have specified a fixed value, or
    used the preceding value in the column) using the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`fill`函数用列中的前一个值替换*NaN*占位符（我们也可以指定一个固定值，或使用列中的前一个值）如下：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can split the data into the response variable (`''shares''`) and the
    features (all columns from `''timedelta''` to `''abs_title_sentiment_polarity''`),
    which we will use as inputs in the regression models described later using the
    commands:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将数据分为响应变量（`'shares'`）和特征（从`'timedelta'`到`'abs_title_sentiment_polarity'`的所有列），这些我们将作为后面描述的回归模型的输入使用以下命令：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let us now also take another look at variables that we did not transform logarithmically.
    If you try to fit a linear model using the dataset at this point, you will find
    that the slopes for many of these are extremely large or small. This can be explained
    by looking at what the remaining variables represent. For example, one set of
    columns which we did not logarithmically transform encodes a `0/1` value for whether
    a news article was published on a given day of the week. Another (annotated LDA)
    gives a `0/1` indicator for whether an article was tagged with a particular algorithmically-defined
    topic (we will cover this algorithm, known as Latent Dirichlet Allocation, in
    more detail in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with
    Unstructured Data"), *Words and Pixels – Working with Unstructured Data*). In
    both cases, any row in the dataset must have the value 1 in one of the columns
    of these features (for example, the day of week has to take one of the seven potential
    values). Why is this a problem?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再看看我们没有进行对数转换的变量。如果你现在尝试使用数据集拟合线性模型，你会发现其中许多斜率非常大或非常小。这可以通过查看剩余变量代表的内容来解释。例如，一组我们没有进行对数转换的列编码了一个新闻文章是否在特定一周的某一天发布的`0/1`值。另一个（标注LDA）提供了一个`0/1`指示符，表示文章是否被标记为特定算法定义的主题（我们将在[第6章](ch06.html
    "第6章。文字与像素 - 处理非结构化数据")*文字与像素 - 处理非结构化数据*中更详细地介绍这个算法，称为潜在狄利克雷分配）。在这两种情况下，数据集中的任何行都必须在这些特征的一列中具有值1（例如，星期几必须取七个潜在值之一）。这为什么会成为问题？
- en: 'Recall that in most linear fits, we have both a slope and an intercept, which
    is the offset of the line vertically from the origin (*0, 0*) of the *x-y* plane.
    In a linear model with many variables, we represent this multi-dimensional intercept
    by a column of all 1 in the feature matrix X, which will be added by default in
    many model-fitting libraries. This means that a set of columns (for example, the
    days of the week), since they are independent, could form a linear combination
    that exactly equals the intercept column, making it impossible to find a unique
    solution for the slopes β. This is the same issue as the last assumption of linear
    regression we discussed previously, in which the matrix (XTX) is not invertible,
    and thus we cannot obtain a numerically stable solution for the coefficients.
    This instability results in the unreasonably large coefficient values you will
    observe if you were to fit a regression model on this dataset. It is for this
    reason that we either need to leave out the intercept column (an option you usually
    need to specify in a modeling library), or leave out one of the columns for these
    binary variables. Here we will do the second, dropping one column from each set
    of binary features using the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在大多数线性拟合中，我们都有一个斜率和一个截距，这是线在 *x-y* 平面原点 (*0, 0*) 的垂直偏移。在具有许多变量的线性模型中，我们通过特征矩阵
    X 中的一个全 1 列来表示这个多维截距，这在许多模型拟合库中默认添加。这意味着一组列（例如，一周中的某一天），由于它们是独立的，可以形成一个线性组合，正好等于截距列，这使得无法找到斜率
    β 的唯一解。这与我们之前讨论的线性回归的最后一个假设相同，即矩阵 (XTX) 不可逆，因此我们无法获得系数的数值稳定解。这种不稳定性导致如果你在这个数据集上拟合回归模型，你会观察到系数值不合理地大。正因为如此，我们可能需要省略截距列（通常需要在建模库中指定此选项），或者省略这些二元变量的某一列。在这里，我们将执行第二种方法，使用以下代码从每个二元特征集省略一列：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have taken care of these feature engineering concerns, we are ready
    to fit a regression model to our data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了这些特征工程问题，我们准备将回归模型拟合到我们的数据上。
- en: Model fitting and evaluation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型拟合和评估
- en: Now that we are ready to fit a regression model to our data, it is important
    to clarify the goal of our analysis. As we discussed briefly in [Chapter 1](ch01.html
    "Chapter 1. From Data to Decisions – Getting Started with Analytic Applications"),
    *From Data to Decisions – Getting Started with Analytic Applications*, the goals
    of modeling can be either a) to predict a future response given historical data,
    or `b`) infer the statistical significance and effect of a given variable on an
    outcome.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备将回归模型拟合到我们的数据上，明确分析目标是重要的。正如我们在[第1章](ch01.html "第1章。从数据到决策 – 开始使用分析应用")中简要讨论的，“从数据到决策
    – 开始使用分析应用”，建模的目标可以是 a) 根据历史数据预测未来的响应，或 `b`) 推断给定变量对结果的影响的统计意义和效应。
- en: In the first scenario, we will choose a subset of data to train our model, and
    then evaluate the goodness of fit of the linear model on an independent data set
    not used to derive the model parameters. In this case, we want to validate that
    the trends represented by the model generalize beyond a particular set of data
    points. While the coefficient outputs of the linear model are interpretable, we
    are still more concerned in this scenario about whether we can accurately predict
    future responses rather than the meaning of the coefficients.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们将选择数据的一个子集来训练我们的模型，然后在一个独立的数据集上评估线性模型的拟合优度，这个数据集没有用于推导模型参数。在这种情况下，我们希望验证模型所表示的趋势是否可以推广到特定数据点之外。虽然线性模型的系数输出是可解释的，但在这个场景中，我们更关心的是我们能否准确预测未来的响应，而不是系数的意义。
- en: In the second scenario, we may not use a test data set at all for validation,
    and instead fit a linear model using all of our data. In this case, we are more
    interested in the coefficients of the model and whether they are statistically
    significant. In this scenario, we are also frequently interested in comparing
    models with more or fewer coefficients to determine the most important parameters
    that predict an outcome.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，我们可能根本不使用测试数据集进行验证，而是使用所有数据来拟合线性模型。在这种情况下，我们更感兴趣的是模型的系数以及它们是否具有统计学意义。在这个场景中，我们通常还感兴趣的是比较具有更多或更少系数的模型，以确定预测结果的最重要的参数。
- en: 'We will return to this second case, but for now let us continue under the assumption
    that we are trying to predict future data. To obtain test and validation data,
    we use the following commands to split the response and predictor data into 60%
    training and 40% test splits:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到这个第二个案例，但就目前而言，让我们继续假设我们正在尝试预测未来的数据。为了获得测试和验证数据，我们使用以下命令将响应和预测数据分割成60%的训练和40%的测试分割：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We use the ''random state'' argument to set a fixed outcome for this randomization,
    so that we can reproduce the same train/test split if we want to rerun the analysis
    at later date. With these training and test sets we can then fit the model and
    compare the predicted and observed values visually using the following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用'随机状态'参数来设置随机化的固定结果，这样我们就可以在稍后日期重新运行分析时重现相同的训练/测试分割。有了这些训练和测试集，我们就可以拟合模型，并使用以下代码通过可视化比较预测值和观察值：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Which gives the following plot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图：
- en: '![Model fitting and evaluation](img/B04881_04_32.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![模型拟合和评估](img/B04881_04_32.jpg)'
- en: 'Similarly, we can look at the performance of the model on the test data set
    using the commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用以下命令查看模型在测试数据集上的性能：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Which gives the plot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图：
- en: '![Model fitting and evaluation](img/B04881_04_33.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![模型拟合和评估](img/B04881_04_33.jpg)'
- en: 'The visual similarities are confirmed by looking at the coefficient of variation,
    or ''R-squared'' value. This is a metric often used in regression problems, which
    defines how much of the variation in the response is explained by the variation
    in the predictors according to the model. It is computed as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察变异系数，或'R-squared'值，可以确认视觉上的相似性。这是一个在回归问题中常用的指标，它定义了响应中的多少变化可以由模型中预测变量的变化来解释。它被计算为：
- en: '![Model fitting and evaluation](img/B04881_04_08.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![模型拟合和评估](img/B04881_04_08.jpg)'
- en: 'Where `Cov` and `Var` are the **Covariance** and **Variance** (respectively)
    of the two variables (the observed response y, and the predicted response given
    by yβ). A perfect score is `1` (a straight line), while `0` represents no correlation
    between a predicted and observed value (an example would be a spherical cloud
    of points). Using scikit learn, we can obtain the *R²* value using the `score()`
    method of the linear model, with arguments the features and response variable.
    Running the following for our data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`Cov`和`Var`分别是两个变量（观察到的响应y和由yβ给出的预测响应）的**协方差**和**方差**（分别）。完美分数是`1`（一条直线），而`0`表示预测值和观察值之间没有相关性（一个例子是一个球形点云）。使用scikit
    learn，我们可以使用线性模型的`score()`方法获得*R²*值，其中特征和响应变量作为参数。运行以下代码来处理我们的数据：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We get a value of `0.129` for the training data and `0.109` for the test set.
    Thus, we see that while there is some relationship between the predicted and observed
    response captured in the news article data, though we have room for improvement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据得到`0.129`的值，测试集得到`0.109`的值。因此，我们看到，尽管新闻文章数据中捕获了预测值和观察值之间的一些关系，但我们仍有改进的空间。
- en: 'In addition to looking for overall performance, we may also be interested in
    which variables from our inputs are most important in the model. We can sort the
    coefficients of the model by their absolute magnitude to analyse this using the
    following code to obtain the sorted positions of the coefficients, and reorder
    the column names using this new index:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了寻找整体性能外，我们还可能对模型中哪些输入变量最重要感兴趣。我们可以通过绝对值对模型的系数进行排序，使用以下代码来分析排序后的系数位置，并使用这个新索引重新排序列名：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This gives the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You will notice that there is no information on the variance of the parameter
    values. In other words, we do not know the confidence interval for a given coefficient
    value, nor whether it is statistically significant. In fact, the scikit-learn
    regression method does not calculate statistical significance measurements, and
    for this sort of inference analysis—the second kind of regression analysis discussed
    previously and in [Chapter 1](ch01.html "Chapter 1. From Data to Decisions – Getting
    Started with Analytic Applications"), *From Data to Decisions – Getting Started
    with Analytic Applications*—we will turn to a second Python library, `statsmodels`
    ([http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到参数值的方差信息没有提供。换句话说，我们不知道给定系数值的置信区间，也不知道它是否具有统计显著性。实际上，scikit-learn回归方法不计算统计显著性测量值，对于这种推断分析——之前在[第1章](ch01.html
    "第1章. 从数据到决策 – 开始使用分析应用")和本章讨论的第二种回归分析——“从数据到决策 – 开始使用分析应用”——我们将转向第二个Python库，`statsmodels`
    ([http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/))。
- en: Statistical significance of regression outputs
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归输出的统计显著性
- en: 'After installing the `statsmodels` library, we can perform the same linear
    model analysis as previously, using all of the data rather than a train/test split.
    With `statsmodels`, we can use two different methods to fit the linear model,
    `api` and `formula.api`, which we import using the following commands:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了`statsmodels`库之后，我们可以像之前一样执行相同的线性模型分析，使用所有数据而不是训练/测试分割。使用`statsmodels`，我们可以使用两种不同的方法来拟合线性模型，`api`和`formula.api`，我们使用以下命令导入：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `api` methods first resembles the scikit-learn function call, except we
    get a lot more detailed output about the statistical significance of the model
    after running the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`api`方法首先类似于scikit-learn函数调用，除了在运行以下命令后，我们得到了关于模型统计显著性的更多详细输出：'
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Which gives the following output:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Statistical significance of regression outputs](img/B04881_04_34.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_34.jpg)'
- en: 'What do all these parameters mean? The number of observations and number of
    dependent variables are probably obvious, but the others we have not seen before.
    Briefly, their names and interpretation are:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些参数意味着什么？观测数和因变量数量可能很明显，但其他我们之前没有见过。简要来说，它们的名称和解释如下：
- en: '**Df model**: This is the number of independent elements in the model parameters.
    We have 57 columns; once we know the value of 56 of them, the last is fixed by
    the need to minimize the remaining error, so there are only 56 degrees of freedom
    overall.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Df model**：这是模型参数中的独立元素数量。我们有57列；一旦我们知道其中56个的值，最后一个就由最小化剩余误差的需要来确定，所以总共有56个自由度。'
- en: '**Df residuals**: This is the number of independent pieces of information in
    the error estimates of the model. Recall that we obtain the errors by `y-X`.We
    only have up to `m` independent columns in `X`, where `m` is the number of predictors.
    So our estimate of the error has `n-1` independent elements from the data itself,
    from which we subtract another `m` which is determined by the inputs, giving us
    `n-m-1`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Df residuals**：这是模型误差估计中独立信息块的数量。回想一下，我们通过`y-X`获得误差。在`X`中，我们只有最多`m`个独立的列，其中`m`是预测器的数量。因此，我们的误差估计从数据本身中有`n-1`个独立元素，从中我们减去另一个由输入决定的`m`，这给我们留下`n-m-1`。'
- en: '**Covariance type**: This is the kind of covariance used in the model; here
    we just use white noise (a mean `0`, normally distributed error), but we could
    just as easily have specified a particular structure that would accommodate, for
    example, a case where the error is correlated with the magnitude of the response.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Covariance type**：这是模型中使用的协方差类型；在这里，我们只是使用白噪声（均值为`0`，正态分布的误差），但我们也可以指定一个特定的结构，以适应例如误差与响应幅度相关的情形。'
- en: '**Adj. R-squared**: If we include more variables in a model, we can start to
    increase the R2 by simply having more degrees of freedom with which to fit the
    data. If we wish to fairly compare the R2 for models with different numbers of
    parameters, then we can adjust the R2 calculation with the following formula:![Statistical
    significance of regression outputs](img/B04881_04_09.jpg)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adj. R-squared**：如果我们在一个模型中包含更多的变量，我们可以通过简单地增加更多的自由度来拟合数据，从而开始增加R2。如果我们希望公平地比较具有不同参数数量的模型的R2，那么我们可以使用以下公式调整R2的计算：![回归输出的统计显著性](img/B04881_04_09.jpg)'
- en: Using this formula, for models with larger numbers of parameters, we penalize
    the R2 by the amount of error in the fit.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这个公式，对于具有更多参数的模型，我们通过拟合误差的量来惩罚R2。
- en: '**F-statistics**: This measure is used to compare (through a Chi-squared distribution)
    that any of the regression coefficients are statistically different than `0`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量**: 这个度量用于通过卡方分布比较（任何回归系数是否在统计上与`0`不同）。'
- en: '**Prob (F-statistic)**: This is the p-value (from the F-statistic) that the
    null hypothesis (that the coefficients are `0` and the fit is no better than the
    intercept-only model) is true.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量的概率**: 这是来自F统计量的p值（假设系数为`0`且拟合不优于仅截距模型），表明零假设（系数为`0`且拟合不优于仅截距模型）是真实的。'
- en: '**Log-likelihood**: Recall that we assume the error of the residuals in the
    linear model is normally distributed. Therefore, to determine how well our result
    fits this assumption, we can compute the likelihood function:![Statistical significance
    of regression outputs](img/B04881_04_10.jpg)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数似然**: 回想一下，我们假设线性模型中残差的误差是正态分布的。因此，为了确定我们的结果是否符合这个假设，我们可以计算似然函数：![回归输出的统计显著性](img/B04881_04_10.jpg)'
- en: 'Where σ is the standard deviation of the residuals and μ is the mean of the
    residuals (which we expect be very near 0 based on the linear regression assumptions
    above). Because the log of a product is a sum, which is easier to work with numerically,
    we usually take the logarithm of this value, expressed as:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 σ 是残差的标准差，μ 是残差的均值（根据上述线性回归假设，我们期望这个值非常接近0）。因为乘积的对数是和，这在数值上更容易处理，所以我们通常取这个值的对数，表示为：
- en: '![Statistical significance of regression outputs](img/B04881_04_11.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_11.jpg)'
- en: While this value is not very useful on its own, it can help us compare two models
    (for example with different numbers of coefficients). Better goodness of fit is
    represented by a larger log likelihood, or a lower negative log likelihood.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这个值本身并不很有用，但它可以帮助我们比较两个模型（例如具有不同系数数量的模型）。更好的拟合优度由较大的对数似然或较低的对数负似然表示。
- en: Note
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, we usually minimize the negative log likelihood instead of maximizing
    the log likelihood, as most optimization algorithms that we might use to obtain
    the optimal parameters assume minimization as the default objective.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实践中，我们通常最小化负对数似然，而不是最大化对数似然，因为我们可能使用的多数优化算法默认目标是最小化。
- en: '**AIC/BIC**: AIC and BIC are abbreviations for the Akaike Information Criterion
    and Bayes Information Criterion. These are two statistics that help to compare
    models with different numbers of coefficients, thus giving a sense of the benefit
    of greater model complexity from adding more features. AIC is given by:![Statistical
    significance of regression outputs](img/B04881_04_12.jpg)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AIC/BIC**: AIC和BIC是Akaike信息准则和Bayes信息准则的缩写。这两个统计量有助于比较具有不同系数数量的模型，从而给出增加更多特征后模型复杂度增加的好处。AIC的计算公式如下：![回归输出的统计显著性](img/B04881_04_12.jpg)'
- en: 'Where *m* is the number of coefficients in the model and *L* is the likelihood,
    as described previously. Better goodness of fit is represented by lower AIC. Thus,
    increasing the number of parameters penalizes the model, while improving the likelihood
    that it decreases the AIC. BIC is similar, but uses the formula:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *m* 是模型中的系数数量，*L* 是似然，如前所述。更好的拟合优度由较低的AIC表示。因此，增加参数数量会惩罚模型，同时提高其降低AIC的可能性。BIC类似，但使用以下公式：
- en: '![Statistical significance of regression outputs](img/B04881_04_13.jpg)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_13.jpg)'
- en: 'Where *n* is the number of data points in the model. For a fuller comparison
    of AIC and BIC, please see (Burnham, Kenneth P., and David R. Anderson. *Model
    selection and multimodel inference: a practical information-theoretic approach*.
    Springer Science & Business Media, 2003).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是模型中的数据点数量。对于AIC和BIC的更全面比较，请参阅（Burnham, Kenneth P. 和 David R. Anderson.
    *模型选择和多模型推断：一种实用的信息论方法*. Springer Science & Business Media, 2003）。
- en: 'Along with these, we also receive an output of the statistical significance
    of each coefficient, as judged by a *t-test* of its standard error:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，我们还会收到每个系数的统计显著性输出，这是通过对其标准误差的*t-检验*来判断的：
- en: '![Statistical significance of regression outputs](img/B04881_04_35.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_35.jpg)'
- en: 'We also receive a final block of statistics:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会收到一个最终的统计块：
- en: '![Statistical significance of regression outputs](img/B04881_04_36.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_36.jpg)'
- en: 'Most of these are outside the scope of this volume, but the Durbin-Watson (DW)
    statistic will be important later, when we discuss dealing with time series data.
    The DW statistic is given by:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中大部分内容超出了本卷的范围，但Durbin-Watson (DW) 统计量将在我们讨论处理时间序列数据时变得重要。DW统计量由以下公式给出：
- en: '![Statistical significance of regression outputs](img/B04881_04_14.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![回归输出的统计显著性](img/B04881_04_14.jpg)'
- en: Where *e* are the residuals (here *y-Xβ* for the linear model). In essence,
    this statistic asks whether the residuals are positively or negatively correlated.
    If its value is `>2`, this suggests a positive correlation. Values between `1`
    and `2` indicate little to correlation, with 2 indicating no correlation. Values
    less than `1` represent negative correlation between successive residuals. For
    more detail please see (Chatterjee, Samprit, and Jeffrey S. Simonoff. *Handbook
    of regression analysis*. Vol. 5\. John Wiley & Sons, 2013).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *e* 是残差（在这里是线性模型的 *y-Xβ*）。本质上，这个统计量询问残差是正相关还是负相关。如果其值大于`2`，这表明存在正相关。介于`1`和`2`之间的值表示几乎没有相关性，其中`2`表示没有相关性。小于`1`的值表示连续残差之间的负相关。更多细节请参阅（Chatterjee,
    Samprit, and Jeffrey S. Simonoff. *回归分析手册*. 第5卷. 约翰·威利父子出版社，2013年）。
- en: 'We could also have fit the model using the `formula.api` commands, by constructing
    a string from the input data representing the formula for the linear model. We
    generate the formula using the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`formula.api`命令来拟合模型，通过从输入数据构造一个表示线性模型公式的字符串。我们使用以下代码生成公式：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can print this formula to the console to verify it gives the correct output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此公式打印到控制台以验证它是否给出了正确的输出：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can then use this formula to fit the full pandas dataframe containing both
    the response and the input variables, by concatenating the response and feature
    variables along their columns (axis 1) and calling the `ols` method of the formula
    API we imported previously:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个公式来拟合包含响应变量和输入变量的完整pandas数据框，通过沿着它们的列（轴1）连接响应变量和特征变量，并调用我们之前导入的公式API的`ols`方法：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this example, it seems reasonable to assume that the residuals in the model
    we fit for popularity as a function of new article features are independent. For
    other cases, we might make multiple observations on the same set of inputs (such
    as when a given customer appears more than once in a dataset), and this data may
    be correlated with time (as when records of a single customer are more likely
    to be correlated when they appear closer together in time). Both scenarios violate
    our assumptions of independence among the residuals in a model. In the following
    sections we will introduce three methods to deal with these cases.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，假设我们拟合的模型中，以新文章特征为函数的流行度残差是独立的，这似乎是合理的。在其他情况下，我们可能在同一组输入上做出多次观察（例如，当某个客户在数据集中出现多次时），这些数据可能与时间相关（例如，当单个客户的记录在时间上更接近时，它们更有可能相关）。这两种情况都违反了我们对模型残差之间独立性的假设。在接下来的几节中，我们将介绍三种处理这些情况的方法。
- en: Generalize estimating equations
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义估计方程
- en: 'In our next set of exercises we will use an example of student grades in a
    math course in several schools recorded over three terms, expressed by the symbols
    (G1-3) (Cortez, Paulo, and Alice Maria Gonçalves Silva. "Using data mining to
    predict secondary school student performance." (2008)). It might be expected that
    there is a correlation between the school in which the student is enrolled and
    their math grades each term, and we do see some evidence of this when we plot
    the data using the following commands:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的练习中，我们将使用记录在几个学校中数学课程学生成绩的例子，这些成绩是在三个学期内记录的，用符号（G1-3）表示（Cortez, Paulo,
    and Alice Maria Gonçalves Silva. "Using data mining to predict secondary school
    student performance." (2008)）。我们可能预计学生所在的学校和他们每个学期的数学成绩之间存在相关性，当我们使用以下命令绘制数据时，我们确实看到了一些证据：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Generalize estimating equations](img/B04881_04_37.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![广义估计方程](img/B04881_04_37.jpg)'
- en: 'We can see that there is some correlation between a decline in math grades
    in terms 2 and 3, and the school. If we want to estimate the effect of other variables
    on student grades, then we want to account for this correlation. How we do so
    depends upon what our objective is. If we want to simply have an accurate estimate
    of the coefficients β of the model at the population level, without being able
    to predict individual students'' responses with our model, we can use the **Generalize
    Estimating Equations** (**GEE**) (Liang, Kung-Yee, and Scott L. Zeger. "Longitudinal
    data analysis using generalized linear models." *Biometrika* 73.1 (1986): 13-22).
    The motivating idea of the GEE is that we treat this correlation between school
    and grade as an additional parameter (which we estimate by performing a linear
    regression on the data and calculating the residuals) in the model. By doing so,
    we account for the effect of this correlation on the coefficient estimate, and
    thus obtain a better estimate of their value. However, we still usually assume
    that the responses within a group are exchangeable (in other words, the order
    does not matter), which is not the case with clustered data that might have a
    time-dependent component.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到，在 2 和 3 学期数学成绩下降与学校之间存在某种相关性。如果我们想估计其他变量对学生成绩的影响，那么我们想要考虑这种相关性。我们如何做到这一点取决于我们的目标。如果我们只想在总体水平上对模型的系数
    β 有一个准确的估计，而无法使用我们的模型预测个别学生的响应，那么我们可以使用**广义估计方程**（**GEE**）（Liang, Kung-Yee, 和
    Scott L. Zeger. "使用广义线性模型进行纵向数据分析." *Biometrika* 73.1 (1986): 13-22）。广义估计方程的激励思想是，我们将学校与成绩之间的这种相关性视为模型中的附加参数（我们通过在数据上执行线性回归并计算残差来估计它）。通过这样做，我们考虑了这种相关性对系数估计的影响，从而获得了更好的估计值。然而，我们通常仍然假设组内的响应是可交换的（换句话说，顺序不重要），这与可能具有时间依赖成分的聚类数据的情况不符。'
- en: 'Unlike the linear model, GEE parameter estimates are obtained through nonlinear
    optimization of the objective function `U(`**β**`)`, using the following formula:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性模型不同，广义估计方程的参数估计是通过目标函数 `U(β)` 的非线性优化获得的，使用以下公式：
- en: '![Generalize estimating equations](img/B04881_04_15.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![广义估计方程](img/B04881_04_15.jpg)'
- en: Where `μ[k]` is the mean response of a group *k* (such as a school, in our example),
    `V[k]` is the variance matrix giving the correlation between residuals for members
    of the group k and `Y[k]- μ[k]` is the vector of residuals within this group.
    This is usually solved using the Newton-Raphson equation, which we will look at
    in more detail in [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place
    – Classification Methods and Analysis"), *Putting Data in its Place – Classification
    Methods and Analysis*. Conceptually, we can estimate the variance matrix V using
    the residuals from a regression, and optimize the formula above until convergence.
    Thus, by optimizing both the correlation structure between grouped data samples
    given by V along with the coefficients β, we have effectively obtained an estimate
    β independent of V.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `μ[k]` 是组 *k*（例如，在我们的例子中是学校）的平均响应，`V[k]` 是方差矩阵，它给出了组 k 成员残差之间的相关性，而 `Y[k]-
    μ[k]` 是该组内的残差向量。这通常使用牛顿-拉夫森方程来解决，我们将在第 5 章（[Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis")，*Putting Data in its
    Place – Classification Methods and Analysis*）中更详细地探讨。从概念上讲，我们可以使用回归的残差来估计方差矩阵
    V，并优化上述公式直到收敛。因此，通过优化由 V 给出的分组数据样本之间的相关性结构以及系数 β，我们有效地获得了与 V 无关的估计 β。
- en: 'To apply this method to our data, we can again create the model string using
    the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此方法应用于我们的数据，我们再次可以使用以下命令创建模型字符串：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can then run the GEE using the school as the grouping variable:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用学校作为分组变量来运行广义估计方程（GEE）：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: However, in some cases we would instead like to obtain an estimate of individual,
    not population-level, responses, even with the kind of group correlations we discussed
    previously. In this scenario, we could instead use mixed effects models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，我们可能更希望获得个体而不是总体水平的响应估计，即使在我们之前讨论的组相关性的情况下。在这种情况下，我们可以改用混合效应模型。
- en: Mixed effects models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合效应模型
- en: 'Recall that in the linear models we have fitted in this chapter, we assume
    the response is modeled as:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在本章中我们拟合的线性模型中，我们假设响应是按以下方式建模的：
- en: '![Mixed effects models](img/B04881_04_16.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![混合效应模型](img/B04881_04_16.jpg)'
- en: 'Where *ε* is an error term. However, when we have correlation between data
    points belonging to a group, we can also use a model of the form:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ε* 是误差项。然而，当我们有属于同一组的数据点之间的相关性时，我们也可以使用以下形式的模型：
- en: '![Mixed effects models](img/B04881_04_17.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![混合效应模型](img/B04881_04_17.jpg)'
- en: 'Where `Z` and `u` are group-level variables and coefficients, respectively.
    The coefficient `u` has a mean `0` and a variance structure that needs to be specified.
    It could be uncorrelated between groups, for example, or have a more complex covariance
    relationship where certain groups are correlated with one another more strongly
    than others. Unlike the GEE model, we are not attempting to simply estimate the
    group level effect of the coefficients (after factoring out the effect of group
    membership), but within-group coefficients that control for the effect of belonging
    to a particular group. The name of mixed effects models comes from the fact that
    the variables `β` are fixed effects whose value is exactly known, while `u` are
    random effects, where the value `u` represents an observation of a group level
    coefficient which is a random variable. The coefficients `u` could either be a
    set of group-level intercepts (random intercepts model), or combined with group-level
    slopes (random slopes model). Groups may even be nested within one another (hierarchical
    mixed effects models), such as if town-level groups capture one kind of correlated
    variation, while state-level groups capture another. A full discussion of the
    many variations of mixed effects models is outside the scope of this book, but
    we refer the interested reader to references such as (West, Brady T., Kathleen
    B. Welch, and Andrzej T. Galecki. *Linear mixed models: a practical guide using
    statistical software*. CRC Press, 2014; Stroup, Walter W. *Generalized linear
    mixed models: modern concepts, methods and applications*. CRC press, 2012). As
    with GEE, we can fit this model similar to the linear model by including a group-level
    variable using the following commands:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `Z` 和 `u` 分别是组别变量和系数。系数 `u` 的均值为 `0`，其方差结构需要指定。例如，它可以在组别之间不相关，或者具有更复杂的协方差关系，其中某些组别之间的相关性比其他组别更强。与GEE模型不同，我们并不是试图简单地估计系数的组别水平效应（在考虑了组别成员效应之后），而是控制特定组别归属效应的组内系数。混合效应模型的名字来源于变量
    `β` 是固定效应，其值是确切已知的，而 `u` 是随机效应，其中 `u` 的值代表一个组别水平系数的观察值，这是一个随机变量。系数 `u` 可以是一组组别水平的截距（随机截距模型），或者与组别水平的斜率相结合（随机斜率模型）。组别甚至可以嵌套在彼此之中（分层混合效应模型），例如，如果城镇级别的组别捕捉到一种相关变化，而州级别的组别捕捉到另一种。混合效应模型的多种变体将超出本书的讨论范围，但我们建议感兴趣的读者参考以下参考文献（West,
    Brady T., Kathleen B. Welch, and Andrzej T. Galecki. *线性混合模型：使用统计软件的实用指南*. CRC
    Press, 2014；Stroup, Walter W. *广义线性混合模型：现代概念、方法和应用*. CRC press, 2012）。与GEE一样，我们可以通过包含组别变量使用以下命令来拟合此模型：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Time series data
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据
- en: The last category of model assumptions that we will consider are where clustered
    data is temporally correlated, for example if a given customer has periodic buying
    activity based on the day of the week. While GEE and mixed effects models generally
    deal with data in which the inter-group correlations are exchangeable (the order
    does not matter), in time series data, the order is important for the interpretation
    of the data. If we assume exchangeability, then we may incorrectly estimate the
    error in our model, since we will assume the best fit goes through the middle
    of the data in a given group, rather than following the correlation structure
    of repeated measurements in a time series.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要考虑的模型假设的最后一类是集群数据在时间上是相关的，例如，如果某个客户基于一周中的某一天有周期性的购买活动。虽然GEE和混合效应模型通常处理组间相关性可交换的数据（顺序不重要），但在时间序列数据中，顺序对于数据的解释很重要。如果我们假设可交换性，那么我们可能会错误地估计模型中的误差，因为我们假设最佳拟合线穿过给定组别数据中的中间部分，而不是遵循时间序列中重复测量的相关性结构。
- en: 'A particularly flexible model for time series data uses a formula known as
    the Kalman filter. Superficially, the Kalman filter resembles the equation for
    mixed effects models; consider an observation that at a given point in time has
    an unobserved state which we want to infer in the model (such as whether a given
    stock is increasing or decreasing in price), which is obscured by noise (such
    as market variation in stock price). The state of the data point is given by:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别灵活的时间序列数据模型使用称为卡尔曼滤波的公式。表面上，卡尔曼滤波类似于混合效应模型的方程；考虑一个在特定时间点有一个未观察到的状态，我们希望在模型中推断这个状态（例如，某个股票的价格是上升还是下降），它被噪声（例如股票价格的市场变化）所掩盖。数据点的状态由以下公式给出：
- en: '![Time series data](img/B04881_04_18.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![时间序列数据](img/B04881_04_18.jpg)'
- en: 'Where `F` represents the matrix of transition probabilities between states,
    *xt-1* is the state at the last time step, *w[t]* is noise, and *B[t]* and *u[t]*
    represent regression variables that could incorporate, for example, seasonal effects.
    In this case, *u* would be a binary indicator of a season or time of day, and
    *β* the amount we should add or subtract from *x* based on this indicator. The
    state *x* is used to predict the observed response using:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `F` 代表状态之间的转移概率矩阵，*xt-1* 是最后一个时间步的状态，*w[t]* 是噪声，而 *B[t]* 和 *u[t]* 代表回归变量，例如可以包含季节效应。在这种情况下，*u*
    将是一个季节或一天中的时间的二元指示符，而 *β* 是根据这个指示符我们应该从 *x* 中添加或减去的数量。状态 *x* 用于使用以下方法预测观察到的响应：
- en: '![Time series data](img/B04881_04_19.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![时间序列数据](img/B04881_04_19.jpg)'
- en: Where *xt* is the state from the previous equation, *H* is a set of coefficients
    for each underlying state, and *vt* is noise. The Kalman filter uses the observations
    at time *t-1* to update our estimates of both the underlying state *x* and the
    response *y* at time *t*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *xt* 是前一个方程中的状态，*H* 是每个潜在状态的系数集，*vt* 是噪声。卡尔曼滤波使用时间 *t-1* 的观察值来更新我们对时间 *t*
    的潜在状态 *x* 和响应 *y* 的估计。
- en: 'The family of equations given previously is also known by the more general
    term "Structural Time Series Equations". For the derivations of the update equations
    and further details on "Structural Time Series Models", we refer the reader to
    more advanced references (Simon, Dan. *Optimal state estimation: Kalman, H infinity,
    and nonlinear approaches*. John Wiley & Sons, 2006; Harvey, Andrew C. *Forecasting,
    structural time series models and the Kalman filter*. Cambridge University Press,
    1990).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 之前给出的方程组也被称为更通用的术语“结构时间序列方程”。对于更新方程的推导和有关“结构时间序列模型”的更多细节，我们建议读者参考更高级的参考资料（Simon,
    Dan. *最优状态估计：卡尔曼，H无穷，和非线性方法*。John Wiley & Sons，2006；Harvey, Andrew C. *预测，结构时间序列模型和卡尔曼滤波*。Cambridge
    University Press，1990）。
- en: 'In the `statsmodels` package, the Kalman filter is used in **auto-regressive
    moving average** (**ARMA**) models, which are fit with the following command:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `statsmodels` 包中，卡尔曼滤波用于**自回归移动平均**（**ARMA**）模型，使用以下命令进行拟合：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Generalized linear models
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'In most of the preceding examples, we assume that the response variable may
    be modeled as a linear combination of the responses. However, we can often relax
    this assumption by fitting a generalized linear model. Instead of the formula:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数先前的例子中，我们假设响应变量可能被建模为响应的线性组合。然而，我们可以通过拟合广义线性模型来放宽这个假设。而不是以下公式：
- en: '![Generalized linear models](img/B04881_04_20.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/B04881_04_20.jpg)'
- en: 'We substitute a `link` function (*G*) that transforms the nonlinear output
    into a linear response:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个 `链接` 函数（*G*）替换，该函数将非线性输出转换为线性响应：
- en: '![Generalized linear models](img/B04881_04_21.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/B04881_04_21.jpg)'
- en: 'Examples of `link` functions include:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`链接`函数的例子包括：'
- en: '**Logit**: This `link` function maps the responses in the range `0` to `1`
    to a linear scale using the function *Xβ=ln(y/1-y)*, where *y* is usually a probability
    between `0` and `1`. This `link` function is used in logistic and multinomial
    regression, covered in [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place
    – Classification Methods and Analysis"), *Putting Data in its Place – Classification
    Methods and Analysis*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logit**：这个 `链接` 函数将范围在 `0` 到 `1` 之间的响应映射到线性尺度，使用函数 *Xβ=ln(y/1-y)*，其中 *y*
    通常是在 `0` 到 `1` 之间的概率。这个 `链接` 函数用于逻辑回归和多项式回归，在[第5章](ch05.html "第5章。将数据放在合适的位置——分类方法和分析")中介绍，*将数据放在合适的位置——分类方法和分析*。'
- en: '**Poisson**: This `link` function maps count data to a linear scale using the
    relationship *Xβ=ln(y)*, where *y* is count data.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Poisson**：这个 `链接` 函数使用关系 *Xβ=ln(y)* 将计数数据映射到线性尺度，其中 *y* 是计数数据。'
- en: '**Exponential**: This `link` function maps data from an exponential scale to
    a linear one with the formula *Xβ=y-1*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数**：这个`链接`函数将指数尺度上的数据映射到线性尺度，公式为 *Xβ=y-1*。'
- en: While these sorts of transformations make it possible to transform many nonlinear
    problems into linear ones, they also make it more difficult to estimate parameters
    of the model. Indeed, the matrix algebra used to derive the coefficients for simple
    linear regression do not apply, and the equations do not have any closed solution
    we could represent by a single step or calculation. Instead, we need iterative
    update equations such as those used for GEE and mixed effects models. We will
    cover these sorts of methods in more detail in [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这类变换使得将许多非线性问题转化为线性问题成为可能，但它们也使得估计模型参数变得更加困难。确实，用于推导简单线性回归系数的矩阵代数不再适用，而且方程也没有任何封闭解，我们无法通过单一步骤或计算来表示。相反，我们需要像用于广义估计方程（GEE）和混合效应模型那样的迭代更新方程。我们将在[第5章](ch05.html
    "第5章。将数据放在合适的位置——分类方法和分析")中更详细地介绍这类方法，*将数据放在合适的位置——分类方法和分析*。
- en: Now we have now covered some of the diverse cases of fitting models to data
    that violate the linear regression assumptions in order to correctly interpret
    coefficients. Let us return now to the task of trying to improve the predictive
    performance of our linear model by selecting a subset of variables in the hope
    of removing correlated inputs and reducing over-fitting, an approach known as
    *regularization*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些将模型拟合到违反线性回归假设的数据的多样情况，以便正确解释系数。现在让我们回到尝试通过选择变量子集来提高线性模型的预测性能的任务中，希望移除相关输入并减少过拟合，这种方法被称为*正则化*。
- en: Applying regularization to linear models
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将正则化应用于线性模型
- en: After observing that the performance of our linear model is not optimal, one
    relevant question is whether all of the features in this model are necessary,
    or whether the coefficients we have estimated are suboptimal. For instance, two
    columns may be highly correlated with one another, meaning that the matrix `XTX`
    can be rank-deficient and thus not invertible, leading to numerical instability
    in calculating the coefficients. Alternatively, we may have included enough input
    variables to make an excellent fit on the training data, but this fit may not
    generalize to the test data as it precisely captures nuanced patterns present
    only in the training data. The high number of variables gives us great flexibility
    to make the predicted responses exactly match the observed responses in the training
    set, leading to overfitting. In both cases, it may be helpful to apply regularization
    to the model. Using regularization, we try to apply a penalty to the magnitude
    and/or number of coefficients, in order to control for over-fitting and multicollinearity.
    For regression models, two of the most popular forms of regularization are Ridge
    and Lasso Regression.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察到我们的线性模型性能不佳后，一个相关的问题是，这个模型中的所有特征是否都是必要的，或者我们所估计的系数是否次优。例如，两列可能高度相关，这意味着矩阵
    `XTX` 可能是秩亏的，因此不可逆，导致计算系数时出现数值不稳定性。或者，我们可能已经包含了足够多的输入变量，使得在训练数据上拟合得非常好，但这种拟合可能无法推广到测试数据，因为它精确地捕捉了仅在训练数据中存在的细微模式。变量的高数量使我们能够有很大的灵活性，使预测响应与训练集中的观察响应完全匹配，从而导致过拟合。在这两种情况下，对模型应用正则化可能是有帮助的。使用正则化，我们试图对系数的大小和/或数量施加惩罚，以控制过拟合和多重共线性。对于回归模型，两种最流行的正则化形式是岭回归和Lasso回归。
- en: 'In Ridge Regression, we want to constrain the coefficient magnitude to a reasonable
    level, which is accomplished by applying a squared penalty to the size of the
    coefficients in the loss function equation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在岭回归中，我们希望将系数的大小限制在合理的水平，这是通过在损失函数方程中对系数的大小应用平方惩罚来实现的：
- en: '![Applying regularization to linear models](img/B04881_04_22.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![将正则化应用于线性模型](img/B04881_04_22.jpg)'
- en: Note
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note this L(β), though using the same symbols, is not the same as the
    likelihood equations discussed earlier.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管使用了相同的符号，但这个L(β)与之前讨论的似然方程不同。
- en: 'In other words, by applying the penalty *α* to the sum of squares of the coefficients,
    we constrain the model not only to approximate *y* as well as possible, using
    the slopes *β* multiplied by the features, but also constrain the size of the
    coefficients *β*. The effect of this penalty is controlled by the weighting factor
    *α*. When alpha is `0`, the model is just normal linear regression. Models with
    *α > 0* increasingly penalizes large *β* values. How can we choose the right value
    for *α*? The `scikit-learn` library offers a helpful cross-validation function
    that can find the optimal value for *α* on the training set using the following
    commands:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过将惩罚*α*应用于系数的平方和，我们不仅约束模型尽可能好地近似*y*，使用斜率*β*乘以特征，而且还约束系数*β*的大小。这种惩罚的效果由加权因子*α*控制。当alpha为`0`时，模型就是普通的线性回归。具有*α
    > 0*的模型会越来越多地惩罚大的*β*值。我们如何选择*α*的正确值？`scikit-learn`库提供了一个有用的交叉验证函数，可以使用以下命令在训练集上找到*α*的最优值：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Which gives the optimal `α` value as `0.100`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了最优的`α`值为`0.100`。
- en: 'However, making this change does not seem to influence predictive accuracy
    on the test set when we evaluate the new *R2* value using the following commands:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们使用以下命令评估新的*R2*值时，这种改变似乎不会影响测试集上的预测准确性：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In fact, we obtain the same result as the original linear model, which gave
    a test set *R2* of `0.109`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们得到了与原始线性模型相同的结果，该模型给出了测试集*R2*为`0.109`。
- en: Another method of regularization is referred to as Lasso, where we minimize
    the following equation. It is similar to the Ridge Regression formula above, except
    that the squared penalty on the values of *β* have been replaced with an absolute
    value term.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种正则化方法被称为Lasso，其中我们最小化以下方程。它与上面的岭回归公式类似，不同之处在于*β*值的平方惩罚已被绝对值项所取代。
- en: '![Applying regularization to linear models](img/B04881_04_23.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![将正则化应用于线性模型](img/B04881_04_23.jpg)'
- en: 'This absolute value penalty has the practical effect that many of the slopes
    are optimized to be zero. This could be useful if we have many inputs and wish
    to select only the most important to try and derive insights. It may also help
    in cases where two variables are closely correlated with one another, and we will
    select one to include in the model. Like Ridge Regression, we can find the optimal
    value of *α* using the following cross validation commands:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这种绝对值惩罚的实际效果是许多斜率被优化为零。如果我们有很多输入并且希望只选择最重要的输入以尝试得出见解，这可能是有用的。它也可能有助于在两个变量彼此高度相关的情况下，我们选择其中一个变量包含在模型中。像岭回归一样，我们可以使用以下交叉验证命令找到*α*的最优值：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Which suggests an optimal *α* value of *6.25e-5*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明最优的*α*值为*6.25e-5*。
- en: In this case, there does not seem to be much value in applying this kind of
    penalization to the model, as the optimal *α* is near zero. Taken together, the
    analyses above suggest that modifying the coefficients themselves is not helping
    our model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，将这种类型的惩罚应用于模型似乎没有太多价值，因为最优的*α*值接近于零。综合上述分析，上述分析表明，修改系数本身并没有帮助我们的模型。
- en: What might help us decide whether we would use Ridge or Lasso, besides the improvement
    in goodness of fit? One tradeoff is that while Lasso may generate a sparser model
    (more coefficients set to `0`), the values of the resulting coefficients are hard
    to interpret. Given two highly correlated variables, Lasso will select one, while
    shrinking the other to `0`, meaning with some modification to the underlying data
    (and thus bias to select one of these variables) we might have selected a different
    variable into the model. While Ridge regression does not suffer from this problem,
    the lack of sparsity may make it harder to interpret the outputs as well, as it
    does not tend to remove variables from the model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拟合优度的改进，我们可能会使用岭回归还是Lasso，还有什么可以帮助我们做出决定？一个权衡是，虽然Lasso可能会生成一个更稀疏的模型（更多系数被设置为`0`），但结果系数的值难以解释。给定两个高度相关的变量，Lasso会选择其中一个，而将另一个缩小到`0`，这意味着通过对基础数据进行一些修改（从而对选择这些变量之一产生偏差）我们可能会选择一个不同的变量进入模型。虽然岭回归不会出现这个问题，但缺乏稀疏性可能会使得解释输出更加困难，因为它不倾向于从模型中移除变量。
- en: 'A balance between these two choices is provided by Elastic Net Regression (Zou,
    Hui, and Trevor Hastie. "Regularization and variable selection via the elastic
    net." *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*
    67.2 (2005): 301-320.). In Elastic Net, our penalty term becomes a blend of Ridge
    and Lasso, with the optimal β minimizing:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '弹性网络回归（Zou, Hui, 和 Trevor Hastie. "通过弹性网络进行正则化和变量选择." *《皇家统计学会会刊：系列B（统计方法）》*
    67.2 (2005): 301-320）在这两种选择之间提供了平衡。在弹性网络中，我们的惩罚项变成了岭回归和Lasso的混合，最优的β值最小化：'
- en: '![Applying regularization to linear models](img/B04881_04_24.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![对线性模型应用正则化](img/B04881_04_24.jpg)'
- en: 'Because of this modification, Elastic Net can select groups of correlated variables,
    while still shrinking many to zero. Like Ridge and Lasso, Elastic Net has a CV
    function to choose the optimal value of the two penalty terms α using:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种修改，弹性网络可以选出相关变量组，同时将许多变量缩小到零。像岭回归和Lasso一样，弹性网络有一个CV函数来选择两个惩罚项α的最优值，使用以下方法：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'However, this still is not significantly improving the performance of our model,
    as the test *R2* is still unmoved from our original least squares regression.
    It may be the response is not captured well by a linear trend involving a combination
    of the inputs. There may be interactions between features that are not represented
    by coefficients of any single variable, and some variables might have nonlinear
    responses, such as:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仍然没有显著提高我们模型的性能，因为测试 *R2* 仍然没有从我们的原始最小二乘回归中移动。可能是因为响应没有被涉及输入组合的线性趋势很好地捕捉。可能存在某些特征之间的交互作用，这些交互作用不是任何单个变量的系数所表示的，并且某些变量可能有非线性响应，例如：
- en: Nonlinear trends, such as a logarithmic increase in the response for a linear
    increase in the predictor
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性趋势，例如预测变量线性增加时响应的对数增加
- en: Non-monotonic (increasing or decreasing) functions such as a parabola, with
    a lower response in the middle of the range of predictor values and higher values
    at the minimum and maximum
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非单调（增加或减少）函数，例如抛物线，在预测变量值范围的中间有较低的响应，在最小值和最大值处有较高的值
- en: More complex multi-modal responses, such as cubic polynomials
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更复杂的多模态响应，例如三次多项式
- en: While we could attempt to use generalized linear models, as described above,
    to capture these patterns, in large datasets we may struggle to find a transformation
    that effectively captures all these possibilities. We might also start constructing
    "interaction features" by, for example, multiplying each of our input variables
    to generate *N(N-1)/2* additional variables (for the pairwise products between
    all input variables). While this approach, sometimes called "polynomial expansion,"
    can sometimes capture nonlinear relationships missed in the original model, with
    larger feature sets this can ultimately become unwieldy. Instead, we might try
    to explore methods that can efficiently explore the space of possible variable
    interactions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以尝试使用上面描述的广义线性模型来捕捉这些模式，但在大型数据集中，我们可能难以找到一个能够有效捕捉所有这些可能性的转换。我们可能还会通过例如将每个输入变量相乘来生成“交互特征”，从而产生
    *N(N-1)/2* 个额外的变量（对于所有输入变量之间的成对乘积）。虽然这种方法，有时被称为“多项式展开”，有时可以捕捉到原始模型中遗漏的非线性关系，但随着特征集的增大，这最终可能变得难以控制。相反，我们可能尝试探索可以高效探索可能变量交互空间的方法。
- en: Tree methods
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树方法
- en: In many datasets, the relationship between our inputs and output may not be
    a straight line. For example, consider the relationship between hour of the day
    and probability of posting on social media. If you were to draw a plot of this
    probability, it would likely increase during the evening and lunch break, and
    decline during the night, morning and workday, forming a sinusoidal wave pattern.
    A linear model cannot represent this kind of relationship, as the value of the
    response does not strictly increase or decrease with the hour of the day. What
    models, then, could we use to capture this relationship? In the specific case
    of time series models we could use approaches such as the Kalman filter described
    above, using the components of the structural time series equation to represent
    the cyclical 24-hour pattern of social media activity. In the following section
    we examine more general approaches that will apply both to time series data and
    to more general non-linear relationships.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多数据集中，我们的输入和输出之间的关系可能不是一条直线。例如，考虑一天中的小时数和社交媒体发帖概率之间的关系。如果你绘制这个概率的图表，它可能会在傍晚和午餐时间增加，在夜间、早晨和工作日减少，形成一个正弦波模式。线性模型无法表示这种关系，因为响应的值并不严格随着一天中的小时数增加或减少。那么，我们可以使用哪些模型来捕捉这种关系呢？在特定的时间序列模型中，我们可以使用上述描述的卡尔曼滤波器等方法，使用结构时间序列方程的组成部分来表示社交媒体活动的24小时循环模式。在下一节中，我们将探讨更通用的方法，这些方法将适用于时间序列数据以及更通用的非线性关系。
- en: Decision trees
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Consider a case where we assign a probability of posting on social media when
    hour `> 11 am` and `< 1 pm`, `> 1 pm` and `< 6 pm`, and so forth. We could visualize
    these as the branches of a tree, where at each branch point we have a condition
    (such as hour `< 6 pm`), and assign our input data to one branch or another. We
    continue this sort of branching until we reach the end of a series of such selections,
    called a "leaf" of the tree; the predicted response for the tree is then the average
    of the value of training data points in this last group. To predict the response
    of new data points, we follow the branches of the tree to the bottom. To compute
    a decision tree, then, we need the following steps:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个案例，当我们为在社交媒体上发帖的概率分配一个值，当小时数大于上午11点且小于下午1点，大于下午1点且小于下午6点，以此类推。我们可以将这些看作是一棵树的分支，在每个分支点上都有一个条件（例如小时数小于下午6点），并将我们的输入数据分配到树的某个分支。我们继续这种分支，直到达到一系列此类选择的末端，称为树的“叶子”；树的预测响应是最后这个组中训练数据点值的平均值。为了预测新数据点的响应，我们沿着树的分支走到底部。因此，要计算决策树，我们需要以下步骤：
- en: Start with a training set of features *X* and responses *y*.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征*X*和响应*y*的训练集开始。
- en: Find the column of *X*, along with the dividing point, which optimizes the split
    between the data points. There are several criteria we could optimize, such as
    the variance of the target response on each side of the decision boundary (see
    split functions, later). (Breiman, Leo, et al. Classification and regression trees.
    CRC press, 1984.). We assign training data points to two new branches based on
    this rule.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到*X*的列以及分割点，这可以优化数据点之间的分割。我们可以优化几个标准，例如决策边界两侧目标响应的方差（参见后续的分割函数）。(Breiman, Leo,
    等人. 分类与回归树. CRC出版社，1984年)。我们根据这个规则将训练数据点分配到两个新的分支。
- en: Repeat step 2 until a stopping rule is reached, or there is only a single value
    in each of the final branches of the tree.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2，直到达到停止规则，或者树的最终分支中只剩下一个值。
- en: The predicted response is then given by the average response of the training
    points that end up in a particular branch of the tree.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测响应是最终落在树中特定分支的训练点的平均响应。
- en: As mentioned previously, every time we select a split point in the tree model,
    we need a principled way of determining whether one candidate variable is better
    than another for dividing the data into groups that have more correlated responses.
    There are several options.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每次我们在树模型中选择一个分割点时，我们需要一个原则性的方法来确定哪个候选变量比另一个变量更适合将数据分成具有更多相关响应的组。有几个选项。
- en: '*Variance Reduction* measures weather the two groups formed after splitting
    the data have lower variance in the response variable y than the data as a whole,
    and is used in the **Classification and Regression Trees** (CART) algorithm for
    decision trees (Breiman, Leo, et al. Classification and regression trees. CRC
    press, 1984.). It may be calculated by:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*方差减少*衡量在分割数据后形成的两个组在响应变量y中的方差是否比整体数据低，并在决策树的**分类和回归树**（CART）算法中使用。它可以按以下方式计算：'
- en: '![Decision trees](img/B04881_04_25.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B04881_04_25.jpg)'
- en: Where *A* is the set of all data points before the split, *L* is the set of
    values that fall to the left side of the split, and R is the set of points that
    falls to the right side. This formula is optimized when the combined variance
    of the two sides of the split point are less than the variance in the original
    data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 是分割前的所有数据点的集合，*L* 是落在分割左侧的值的集合，而 *R* 是落在分割右侧的点的集合。当分割点的两侧的联合方差小于原始数据的方差时，此公式得到优化。
- en: 'Variance reduction will work best for problems like the one we are examining
    in this chapter, where the output is a continuous variable. However, in classification
    problems with categorical outcomes, such as we will examine in [Chapter 5](ch05.html
    "Chapter 5. Putting Data in its Place – Classification Methods and Analysis"),
    *Putting Data in its Place – Classification Methods and Analysis* the variance
    becomes less meaningful because the data can only assume fixed number of values
    (1 or 0 for a particular class). Another statistic we might optimize is the "information
    gain," which is used in the Iterative Dichotomiser 3 (ID3) and C4.5 algorithms
    for building decision trees (Quinlan, J. Ross. *C4\. 5: programs for machine learning*.
    Elsevier, 2014; Quinlan, J. Ross. "Induction of decision trees." *Machine learning*
    1.1 (1986): 81-106). The information gain statistic asks whether the data on the
    left and right sides of the decision split become more or less similar after being
    partitioned. If we considered the response y to be a probability, then the information
    gain is calculated as:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '减少方差对于本章所探讨的问题将最为有效，其中输出是一个连续变量。然而，在具有分类结果的分类问题中，例如我们将在[第五章](ch05.html "第五章.
    数据定位 – 分类方法和分析")中探讨的，*数据定位 – 分类方法和分析*，方差变得不那么有意义，因为数据只能假设固定数量的值（对于特定类别为1或0）。我们可能还需要优化的另一个统计量是“信息增益”，它在构建决策树的迭代二分器3（ID3）和C4.5算法中使用（Quinlan,
    J. Ross. *C4\. 5: programs for machine learning*. Elsevier, 2014; Quinlan, J.
    Ross. "Induction of decision trees." *Machine learning* 1.1 (1986): 81-106）。信息增益统计量询问在决策分割后，左侧和右侧的数据是否变得更加相似或不同。如果我们认为响应y是一个概率，那么信息增益的计算如下：'
- en: '![Decision trees](img/B04881_04_26.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B04881_04_26.jpg)'
- en: 'Where α is the fraction of data that is divided to the left side of the split,
    and f[Ak], f[Lk], and f[Rk] are the fraction of elements in class *k* among all
    data points, the Left side, and the Right side of the split. The three terms of
    this equation are known as Entropies (Borda, Monica. *Fundamentals in information
    theory and coding*. Springer Science & Business Media, 2011). Why does the entropy
    reflect a good split of the data? To see this, plot the values from `0` to `1`
    for the function `ylog2y` using the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 α 是分割到分割左侧的数据的分数，f[Ak]，f[Lk]，和 f[Rk] 是在所有数据点、分割的左侧和右侧中类别 *k* 的元素分数。这个方程式的三个项被称为熵（Borda,
    Monica. *Fundamentals in information theory and coding*. Springer Science & Business
    Media, 2011）。为什么熵反映了数据的良好分割？为了看到这一点，使用以下方式绘制函数 `ylog2y` 从 `0` 到 `1` 的值：
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Look at the result:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 看看结果：
- en: '![Decision trees](img/B04881_04_38.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B04881_04_38.jpg)'
- en: You can appreciate that the entropy drops as **y** approaches `0` or `1`. This
    corresponds to a very high probability or low probability of a particular class
    in a classification problem, and thus trees which split data according to information
    gain will maximize the extent to which the left and right branches tend towards
    or against probability for a given class in the response.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以欣赏到，当 **y** 接近 `0` 或 `1` 时，熵会下降。这对应于在分类问题中特定类别的非常高的概率或低概率，因此根据信息增益分割数据的树将最大化左右分支趋向或反对给定类别的概率程度。
- en: 'Similarly, the CART algorithm (Breiman, Leo, et al. Classification and regression
    trees. CRC press, 1984.). Also use the *Gini impurity* to decide the split points,
    calculated as:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，CART算法（Breiman, Leo, et al. 分类和回归树. CRC press, 1984年。）也使用*基尼不纯度*来决定分割点，计算如下：
- en: '![Decision trees](img/B04881_04_27.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B04881_04_27.jpg)'
- en: Inspecting this formula, you can see it will be maximized when one class is
    near a value of *f = 1*, while all others are `0`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个公式，你可以看到当一类接近值*f = 1*时，它将被最大化，而所有其他类都是`0`。
- en: How could we deal with null values and missing data in such a model? In scikit-learn,
    the current decision tree implementation does not accommodate missing values,
    so we either need to insert a placeholder value (such as `-1`), drop missing records,
    or impute them (for example, replacing with the column mean) (see Aside for more
    details). However, some implementations (such as the R statistical programming
    language's `gbm` package) treat missing data as a third branch into which to sort
    data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理此类模型中的空值和缺失数据？在scikit-learn中，当前的决策树实现不兼容缺失值，因此我们需要插入一个占位符值（例如`-1`），删除缺失记录，或者进行插补（例如，用列均值替换）（详见旁注以获取更多详细信息）。然而，某些实现（如R统计编程语言的`gbm`包）将缺失数据视为一个第三分支，数据将被排序到这个分支中。
- en: Similar diversity is present in the treatment of categorical data. The current
    scikit-learn implementation expects only numerical columns, meaning that categorical
    features such as gender or country need to be encoded as binary indicators. However,
    other packages, such as the implementation in R, treat categorical data by assigning
    data into buckets based on their feature value, then sorting the buckets by average
    response to determine which buckets to assign to the left and right branches of
    a tree.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类数据时，也存在类似的多样性。当前的scikit-learn实现只期望数值列，这意味着性别或国家等分类特征需要被编码为二进制指示符。然而，其他包，如R语言中的实现，通过根据特征值将数据分配到桶中，然后按平均响应对桶进行排序，来处理分类数据，以确定将哪些桶分配给树的左右分支。
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**提示**'
- en: '**Aside**: **dealing with missing data**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁注**：**处理缺失数据**'
- en: When dealing with missing values in data, we need to consider several possibilities.
    One is whether the data is *missing at random*, or *missing not at random*. In
    the first case, there is a correlation between the response variable and the fact
    that the data is missing. We could assign a dummy value (such as `-1`), remove
    the whole row with missing data from our analysis, or assign the column mean or
    median as a placeholder. We could also think of more sophisticated approaches,
    such as training a regression model that uses all the other input variables as
    predictors and the column with the missing data as the output response, and derive
    imputed values using the predictions from this model. If data is missing not at
    random, then simply encoding the data with a placeholder is probably not sufficient,
    as the placeholder value is correlated with the response. In this scenario, we
    may remove the rows with missing data, or if this is not possible, employ the
    model-based approach. This will be preferable to infer the value of the missing
    elements in the data as it should predict values that follow the same distribution
    as the rest of the column.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据中的缺失值时，我们需要考虑几种可能性。一种是数据是否是*随机缺失*，或者*非随机缺失*。在前一种情况下，响应变量与数据缺失之间存在相关性。我们可以分配一个虚拟值（例如`-1`），从我们的分析中删除包含缺失数据的整个行，或者分配列均值或中位数作为占位符。我们还可以考虑更复杂的方法，例如训练一个回归模型，使用所有其他输入变量作为预测变量，将包含缺失数据的列作为输出响应，并使用该模型的预测来推导出插补值。如果数据是非随机缺失的，那么简单地用占位符编码数据可能是不够的，因为占位符值与响应相关。在这种情况下，我们可能会删除包含缺失数据的行，或者如果这不可能，则采用基于模型的方法。这将更适合推断数据中缺失元素的价值，因为它应该预测与列中其余部分相同的分布。
- en: In practice, while constructing the tree, we usually have some stopping rule,
    such as the minimum number of observations that are needed to form a leaf node
    (otherwise the predicted response could come from a small number of data points,
    which usually increases the error of the predictions).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，在构建树的过程中，我们通常有一些停止规则，例如形成叶节点所需的最小观察数（否则预测响应可能来自少数几个数据点，这通常会增加预测误差）。
- en: 'It is not clear at the outset how many times the tree should branch. If there
    are too few splits (decision points), then few rules can be applied to subdivide
    the dataset and the resulting accuracy of the model may be low. If there are too
    many splits in a very deep tree, then the model may not generalize well to a new
    set of data. For our example, let us try fitting the tree to a number of different
    depths:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始并不清楚树应该分支多少次。如果分支太少（决策点），那么可以应用于细分数据集的规则就很少，模型的最终准确率可能较低。如果在一棵非常深的树中有太多的分支，那么模型可能无法很好地推广到新的数据集。对于我们的例子，让我们尝试将树拟合到不同的深度：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we can evaluate the results by plotting the R2 against the tree depth
    for each model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过绘制每个模型的R2值与树深度之间的关系来评估结果：
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Looking at the performance on the test set, we can see that the gains in performance
    drop quickly once we make the tree too deep:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 查看测试集上的性能，我们可以看到，一旦我们将树做得太深，性能提升就会迅速下降：
- en: '![Decision trees](img/B04881_04_39.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B04881_04_39.jpg)'
- en: Unfortunately, the tree model is still not performing much better than our basic
    linear regression. To try and improve this, we can try to increase the number
    of trees rather than the depth of the trees. The intuition here is that a set
    of shallower trees may in combination capture relationships that it is difficult
    for a single deep tree to approximate. This approach, of using a combination of
    smaller models to fit complex relationships, is used both in the Random Forest
    algorithm discussed in the following section, and in Gradient Boosted Decision
    Trees ([Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification
    Methods and Analysis"), *Putting Data in its Place – Classification Methods and
    Analysis*) and, in a sense, in the deep learning models we will discuss in [Chapter
    7](ch07.html "Chapter 7. Learning from the Bottom Up – Deep Networks and Unsupervised
    Features"), *Learning from the Bottom Up – Deep Networks and Unsupervised Features*.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，树模型的表现仍然没有比我们的基本线性回归好多少。为了尝试改进这一点，我们可以尝试增加树的数量而不是树的深度。这里的直觉是，一组较浅的树结合起来可能能够捕捉到单个深树难以近似的复杂关系。这种方法，即使用组合的小模型来拟合复杂关系，在下一节讨论的随机森林算法中，以及在梯度提升决策树（[第5章](ch05.html
    "第5章. 将数据放在合适的位置 – 分类方法和分析"), *将数据放在合适的位置 – 分类方法和分析*）和从下往上学习（[第7章](ch07.html "第7章.
    从底部学习 – 深度网络和无监督特征"), *从底部学习 – 深度网络和无监督特征*）中，以及在某种程度上，在我们将在第7章讨论的深度学习模型中都有应用。
- en: Random forest
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'While the idea of capturing non-linear relationships seems reasonable, it is
    possible that it is difficult to construct a single tree that captures such complex
    relationships between input and output. What if we were to average over many simpler
    decision trees? This is the essence of the Random Forest algorithm (Ho, Tin Kam.
    "Random decision forests." *Document Analysis and Recognition, 1995., Proceedings
    of the Third International Conference on*. Vol. 1\. IEEE, 1995.; Breiman, Leo.
    "Random forests." *Machine learning* 45.1 (2001): 5-32.), in which we construct
    several trees to try and explore the space of possible nonlinear interactions.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然捕捉非线性关系的想法看起来合理，但可能很难构建一个能够捕捉输入和输出之间如此复杂关系的单个树。如果我们对许多简单的决策树进行平均呢？这就是随机森林算法的精髓（Ho,
    Tin Kam. "随机决策森林." *《文档分析与识别，1995年，第三届国际会议论文集》* 第1卷. IEEE, 1995年；Breiman, Leo.
    "随机森林." *《机器学习》* 45.1 (2001): 5-32），在这个算法中，我们构建多个树来尝试探索可能的非线性交互空间。'
- en: 'Random Forests are an innovation on the concept of Bootstrap Aggregation (Bagging)
    for tree models (Breiman, Leo. "Bagging predictors." *Machine learning* 24.2 (1996):
    123-140.). In the generic Bagging algorithm, we construct a large number of trees
    by sampling, with replacement from the training data a small number of data points
    and building a tree only on this subset of data. While individual trees will be
    relatively weak models, by averaging over a large number of trees we can often
    achieve better prediction performance. Conceptually this is because instead of
    trying to fit a single model (such as a single line) through the response, we
    are approximating the response using an ensemble of small models that each fit
    a single simpler pattern in the input data.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '随机森林是对树模型（Breiman, Leo. "Bagging predictors." *Machine learning* 24.2 (1996):
    123-140.）的Bootstrap Aggregation（Bagging）概念的进一步创新。在通用的Bagging算法中，我们通过从训练数据中采样（有放回地）选取少量数据点，并仅在这部分数据上构建一棵树，来构建大量树。虽然单个树可能相对较弱，但通过平均大量树，我们通常可以实现更好的预测性能。从概念上讲，这是因为我们不是试图通过单个模型（如单条线）来拟合响应，而是使用多个小模型来近似响应，每个小模型拟合输入数据中的单个简单模式。'
- en: 'Random Forests are a further development on the idea of Bagging by randomizing
    not just the data used to build each tree, but the variables as well. At each
    step we also only consider a random subset (for example, of size equal to the
    square root of the total number of columns) of the columns of X while constructing
    the splits in the tree (Hastie, Trevor, et al. "The elements of statistical learning:
    data mining, inference and prediction." *The Mathematical Intelligencer* 27.2
    (2005): 83-85.). If we used all input columns at each round of training, we would
    tend to select the same variables that are most strongly correlated with the response.
    By instead randomly selecting a subset of variables, we can also find patterns
    among weaker predictors and more widely cover the space of possible feature interactions.
    As with Bagging, we follow this process of random data and variable selection
    many times, and then average together the predictions of all trees to reach an
    overall prediction. Again, we can explore whether varying a parameter (the number
    of trees) improves performance on the test set:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过随机化不仅用于构建每棵树的每个数据点，还包括变量，对Bagging的概念进行了进一步的发展。在构建树中的分割时，我们也在每一步只考虑X的列的随机子集（例如，大小等于总列数的平方根）。如果我们每个训练轮次都使用所有输入列，我们往往会选择与响应最强烈相关的变量。通过随机选择变量子集，我们还可以发现较弱预测器之间的模式，并更广泛地覆盖可能的特征交互空间。与Bagging一样，我们多次遵循随机数据和变量选择的过程，然后将所有树的预测平均在一起以得到总体预测。同样，我们可以探索是否改变一个参数（树的数量）可以提高测试集上的性能：
- en: '[PRE34]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we can start to see some increases in the accuracy of the model when
    we plot the results using the following code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们使用以下代码绘制结果时，我们可以开始看到模型准确性的某些提高：
- en: '[PRE35]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Random forest](img/B04881_04_40.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B04881_04_40.jpg)'
- en: Like the linear regression model, we can get a ranking of feature importance.
    While for the linear regression, it is simply the magnitude of the slopes, in
    the random forest model the importance of features is determined in a more complex
    manner. Intuitively, if we were to shuffle the values of a particular column among
    the rows in the dataset, it should decrease the performance of the model if the
    column is important. By measuring the average effect of this permutation across
    all trees and dividing by the standard deviation of this effect, we can get can
    a ranking of the magnitude and consistency of the impact of a variable on the
    performance of the model. By ranking variables by the degree this randomization
    has on accuracy, we can derive a measure of feature significance. We can examine
    the important variables using the following commands to select the feature importance
    values of the largest random forest. Since the `np.argsort` command will by default
    return a list in ascending order, we use the [`::-1`] slice to invert the list
    order to place the large coefficient values at the beginning.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归模型一样，我们可以得到特征重要性的排名。对于线性回归来说，它只是斜率的幅度，而在随机森林模型中，特征的重要性是以更复杂的方式确定的。直观地说，如果我们对数据集中特定列的行值进行洗牌，如果该列很重要，它应该会降低模型的性能。通过测量这种排列的平均效应，并将其除以这种效应的标准差，我们可以得到一个变量对模型性能影响的幅度和一致性的排名。通过按这种随机化对准确度的影响程度对变量进行排名，我们可以得出特征显著性的度量。我们可以使用以下命令检查重要变量，以选择最大的随机森林的特征重要性值。由于`np.argsort`命令默认按升序返回列表，我们使用`[::-1]`切片来反转列表顺序，将大系数值放在前面。
- en: '[PRE36]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This gives the following result:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '[PRE37]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Interestingly, if you compare this list with the linear regression model, the
    order is quite different. Promisingly, this suggests that the random forest was
    able to incorporate patterns that a linear regression cannot capture, resulting
    in the gains in *R²* seen in this section.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果你将这个列表与线性回归模型进行比较，顺序相当不同。令人鼓舞的是，这表明随机森林能够结合线性回归无法捕捉到的模式，从而导致了本节中看到的*R²*增益。
- en: There is also a somewhat subtle problem in this dataset, in the sense that all
    the categorical variables have been encoded using a binary flag. The variable
    importance is thus applied individually to each member of a category. If one member
    of a category is highly correlated with the response while another is not, these
    individual variables' importance measures give an inaccurate picture of the true
    variable importance. One solution is to average the resulting values over all
    categories, a correction which we will not apply for now but raise as a consideration
    for your future analyses.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中也存在一个相对微妙的问题，即所有分类变量都使用二进制标志进行编码。因此，变量重要性是单独应用于每个类别的成员。如果一个类别的成员与响应高度相关，而另一个则不是，这些个别变量的重要性度量将给出一个关于真实变量重要性的不准确图景。一个解决方案是对所有类别中的结果值进行平均，这是一个我们现在不会应用但会作为你未来分析考虑的修正。
- en: 'Here we provide a visual flowchart illustrating many of the tradeoffs we have
    discussed in this chapter on regression analysis. While it is difficult to provide
    comprehensive rules-of-thumb for all scenarios, it can serve as a starting point
    for diagnosing which method to apply for a given problem:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一个视觉流程图，说明了我们在本章关于回归分析中讨论的许多权衡。虽然很难为所有场景提供全面的规则，但它可以作为诊断给定问题应应用哪种方法的起点：
- en: '![Random forest](img/B04881_04_41.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B04881_04_41.jpg)'
- en: Flowchart for regression analysis
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析流程图
- en: Scaling out with PySpark – predicting year of song release
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark进行扩展 - 预测歌曲发布年份
- en: 'To close, let us look at another example using PySpark. With this dataset,
    which is a subset of the Million Song dataset (Bertin-Mahieux, Thierry, et al.
    "The million song dataset." *ISMIR 2011: Proceedings of the 12th International
    Society for Music Information Retrieval Conference, October 24-28, 2011, Miami,
    Florida*. University of Miami, 2011), the goal is to predict the year of a song''s
    release based on the features of the track. The data is supplied as a comma-separated
    text file, which we can convert into an RDD using the Spark `textFile()` function.
    As before in our clustering example, we also define a parsing function with a
    `try…catch` block so that we do not fail on a single error in a large dataset:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，让我们通过另一个使用PySpark的例子来结束。在这个数据集（Bertin-Mahieux, Thierry, et al. "The million
    song dataset." *ISMIR 2011: Proceedings of the 12th International Society for
    Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida*.
    University of Miami, 2011）中，这是一个百万首歌曲数据集的子集，目标是根据歌曲的轨道特征预测歌曲的发行年份。数据以逗号分隔的文本文件的形式提供，我们可以使用Spark的`textFile()`函数将其转换为RDD。像我们之前的聚类示例一样，我们也定义了一个带有`try…catch`块的解析函数，这样我们就不至于在一个大型数据集中因为单个错误而失败：'
- en: '[PRE38]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We then use this function to map each line to the parsed format, which splits
    the comma delimited text into individual fields and converts these rows into a
    Spark DataFrame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用此函数将每一行映射到解析格式，该格式将逗号分隔的文本拆分为单个字段，并将这些行转换为Spark DataFrame：
- en: '[PRE39]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Since we convert the resulting RDD into a DataFrame, so that we can access
    its elements like a list or vector in Python. Next, we want to turn this into
    a `LabeledPoint` RDD, just as we did with the Streaming K-Means example in the
    previous chapter:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将结果RDD转换为DataFrame，以便我们可以像在Python中访问列表或向量一样访问其元素。接下来，我们想要将其转换为`LabeledPoint`
    RDD，就像我们在上一章的Streaming K-Means示例中所做的那样：
- en: '[PRE40]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As part of the documentation for this dataset, we assume that the training
    data (excluding tracks from artists appearing in the test set) is contained in
    the first 463,715 rows, while the rest is the test data. To split it, we can use
    the `zipWithIndex` function, which assigns an index to each element in a partition,
    and across partitions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该数据集文档的一部分，我们假设训练数据（不包括测试集中出现的艺术家的曲目）包含在前463,715行中，其余的是测试数据。为了分割它，我们可以使用`zipWithIndex`函数，该函数为分区中的每个元素分配一个索引，并在分区之间：
- en: '[PRE41]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we can train a random forest model on this data using the following
    commands:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令在此数据上训练一个随机森林模型：
- en: '[PRE42]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To evaluate the accuracy of the resulting model, we can use the `RegressionMetrics`
    module:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估结果的模型准确性，我们可以使用`RegressionMetrics`模块：
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The distributed nature of PySpark means that this analysis will run on both
    the single example file on your computer, and on a much larger dataset (such as
    the full million songs), all using the same code. If we wanted to save the random
    forest model (for example, if we want to store a particular day's model for future
    reference in a database, or distribute this model across multiple machines where
    it will be loaded from a serialized format), we can use to the `toString()` function,
    which can be potentially compressed using gzip.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的分布式特性意味着这项分析将在您电脑上的单个示例文件上运行，同时在一个更大的数据集（例如完整的百万首歌曲）上运行，所有这些都将使用相同的代码。如果我们想保存随机森林模型（例如，如果我们想将特定日期的模型存储在数据库中以供将来参考，或者将此模型分发到多台机器上，从序列化格式中加载），我们可以使用`toString()`函数，该函数可以使用gzip进行潜在压缩。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we examined the fitting of several regression models, including
    transforming input variables to the correct scale and accounting for categorical
    features correctly. In interpreting the coefficients of these models, we examined
    both cases where the classical assumptions of linear regression are fulfilled
    and broken. In the latter cases, we examined generalized linear models, GEE, mixed
    effects models, and time series models as alternative choices for our analyses.
    In the process of trying to improve the accuracy of our regression model, we fit
    both simple and regularized linear models. We also examined the use of tree-based
    regression models and how to optimize parameter choices in fitting them. Finally,
    we examined an example of using random forest in PySpark, which can be applied
    to larger datasets.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了几个回归模型的拟合，包括将输入变量转换为正确的尺度以及正确考虑分类特征。在解释这些模型的系数时，我们考察了线性回归的经典假设得到满足和被违反的情况。在后一种情况下，我们考察了广义线性模型、广义估计方程（GEE）、混合效应模型和时间序列模型作为我们分析的替代选择。在尝试提高回归模型准确性的过程中，我们拟合了简单和正则化的线性模型。我们还考察了基于树的回归模型的使用以及如何优化拟合这些模型时的参数选择。最后，我们考察了在PySpark中使用随机森林的例子，这可以应用于更大的数据集。
- en: In the next chapter, we will examine data that has a discrete categorical outcome,
    instead of a continuous response. In the process, we will examine in more detail
    how the likelihood functions of different models are optimized, as well as various
    algorithms for classification problems.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨具有离散分类结果的而非连续响应的数据。在这个过程中，我们将更详细地研究不同模型的似然函数是如何优化的，以及用于分类问题的各种算法。
