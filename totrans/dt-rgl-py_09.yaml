- en: '*Chapter 9*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application of Data Wrangling in Real Life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform data wrangling on multiple full-fledged datasets from renowned sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a unified dataset that can be passed on to a data science team for machine
    learning and predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relate data wrangling to version control, containerization, cloud services for
    data analytics, and big data technologies such as Apache Spark and Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will apply your gathered knowledge on real-life datasets
    and investigate various aspects of it.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned about databases in the previous chapter, so now it is time to combine
    the knowledge of data wrangling and Python with a real-world scenario. In the
    real world, data from one source is often inadequate to perform analysis. Generally,
    a data wrangler has to distinguish between relevant and non-relevant data and
    combine data from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: The primary job of a data wrangling expert is to pull data from multiple sources,
    format and clean it (impute the data if it is missing), and finally combine it
    in a coherent manner to prepare a dataset for further analysis by data scientists
    or machine learning engineers.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we will try to mimic such a typical task flow by downloading
    and using two different datasets from reputed web portals. Each of the datasets
    contains partial data pertaining to the key question that is being asked. Let's
    examine it more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Your Knowledge to a Real-life Data Wrangling Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you are asked this question: **In India, did the enrollment in primary/secondary/tertiary
    education increase with the improvement of per capita GDP in the past 15 years?**
    The actual modeling and analysis will be done by some senior data scientist, who
    will use machine learning and data visualization for analysis. As a data wrangling
    expert, **your job will be to acquire and provide a clean dataset that contains
    educational enrollment and GDP data side by side**.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a link for a dataset from the United Nations and you can download
    the dataset of education (for all the nations around the world). But this dataset
    has some missing values and moreover it does not have any GDP information. Someone
    has also given you another separate CSV file (downloaded from the World Bank site)
    which contains GDP data but in a messy format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this activity, we will examine how to handle these two separate sources
    and clean the data to prepare a simple final dataset with the required data and
    save it to the local drive as a SQL database file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Pictorial representation of the merging of education and economic
    data](img/C11065_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Pictorial representation of the merging of education and economic
    data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You are encouraged to follow along with the code and results in the notebook
    and try to understand and internalize the nature of the data wrangling flow. You
    are also encouraged to try extracting various data from these files and answer
    your own questions about a nations' socio-economic factors and their inter-relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Coming up with interesting questions about social, economic, technological,
    and geo-political topics and then answering them using freely available data and
    a little bit of programming knowledge is one of most fun ways to learn about any
    data science topic. You will get a flare of that process in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Imputation**'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we are missing some data. Let's say we decide to impute these data
    points by simple linear interpolation between the available data points. We can
    take out a pen and paper or a calculator and compute those values and manually
    create a dataset. But being a data wrangler, we will of course take advantage
    of Python programming, and use pandas imputation methods for this task.
  prefs: []
  type: TYPE_NORMAL
- en: But to do that, we first need to create a DataFrame with missing values in it,
    that is, we need to append another DataFrame with missing values to the current
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: Data Wrangling Task – Fixing UN Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose the agenda of the data analysis is to find out whether the enrolment
    in primary, secondary, or tertiary education has increased with the improvement
    of per capita GDP in the past 15 years. For this task, we will first need to clean
    or wrangle the two datasets, that is, the Education Enrolment and GDP data.
  prefs: []
  type: TYPE_NORMAL
- en: The UN data is available on [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/SYB61_T07_Education.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/SYB61_T07_Education.csv).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you download the CSV file and open it using Excel, then you will see that
    the `Footnotes` column sometimes contains useful notes. We may not want to drop
    it in the beginning. If we are interested in a particular country's data (like
    we are in this task), then it may well turn out that `Footnotes` will be `NaN`,
    that is, blank. In that case, we can drop it at the end. But for some countries
    or regions, it may contain information.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will guide you to find the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset from the UN data from GitHub from the following link:
    [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity13/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity13/India_World_Bank_Info.csv).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The UN data has missing values. Clean the data to prepare a simple final dataset
    with the required data and save it to the local drive as a SQL database file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the `pd.read_csv` method of pandas to create a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the first row does not contain useful information, skip it using the `skiprows`
    parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the column region/country/area and source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assign the following names as columns of DataFrame: Region/County/Area, Year,
    Data, Value, and Footnotes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check how many unique values are present in the `Footnotes` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the type of value column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function to convert the value column into a floating-point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `apply` method to apply this function to a value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the unique values in the data column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 338.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 13: Data Wrangling Task – Cleaning GDP Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GDP data is available on [https://data.worldbank.org/](https://data.worldbank.org/)
    and it is available on GitHub at [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/India_World_Bank_Info.csv).
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we will clean the GDP data.
  prefs: []
  type: TYPE_NORMAL
- en: Create three DataFrames from the original DataFrame using filtering. Create
    the `df_primary, df_secondary,` and `df_tertiary DataFrames` for students enrolled
    in primary education, secondary education, and tertiary education in thousands,
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot bar charts of the enrollment of primary students in a low-income country
    like India and a higher-income country like the USA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since there is missing data, use pandas imputation methods to impute these data
    points by simple linear interpolation between data points. To do that, create
    a DataFrame with missing values inserted and append a new DataFrame with missing
    values to the current DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (For India) Append the rows corresponding to the missing years - **2004 - 2009**,
    **2011 – 2013**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dictionary of values with `np.nan`. Note that there are 9 missing data
    points, so we need to create a list with identical values repeated 9 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a DataFrame of missing values (from the preceding dictionary) that we
    can append.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the DataFrames together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort by Year and reset the indices using `reset_index`. Use `inplace=True` to
    execute the changes on the DataFrame itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the interpolate method for linear interpolation. It fills all the NaNs
    by linearly interpolated values. See the following link for more details about
    this method: [http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the same steps for USA (or other countries).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are values that are unfilled, use the `limit` and `limit_direction`
    parameters with the interpolate method to fill them in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the final graph using the new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the GDP data using the pandas `read_csv` method. It will generally throw
    an error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To avoid errors, try the `error_bad_lines` `=` `False` option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since there is no delimiter in the file, add the `\t` delimiter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `skiprows` function to remove rows that are not useful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the dataset. Filter the dataset with information that states that it
    is similar to the previous education dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reset the index for this new dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the not useful rows and re-index the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the columns properly. This is necessary for merging the two datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will concentrate only on the data from 2003 to 2016\. Eliminate the remaining
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new DataFrame called `df_gdp` with rows 43 to 56.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 338.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 14: Data Wrangling Task – Merging UN Data and GDP Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to merge the databases is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Reset the indexes for merging.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the two DataFrames, `primary_enrollment_india` and `df_gdp`, on the Year
    column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the data, footnotes, and region/county/area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rearrange the columns for proper viewing and presentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 345.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 15: Data Wrangling Task – Connecting the New Data to the Database'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to connect the data to the database is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `sqlite3` module of Python and use the `connect` function to connect
    to the database. The main database engine is embedded. But for a different database
    like `Postgresql` or `MySQL`, we will need to connect to them using those credentials.
    We designate `Year` as the `PRIMARY` `KEY` of this table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, run a loop with the dataset rows one by one to insert them into the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we look at the current folder, we should see a file called `Education_GDP.db`,
    and if we examine that using a database viewer program, we can see the data transferred
    there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 347.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this notebook, we examined a complete data wrangling flow, including reading
    data from the web and local drive, filtering, cleaning, quick visualization, imputation,
    indexing, merging, and writing back to a database table. We also wrote custom
    functions to transform some of the data and saw how to handle situations where
    we may get errors when reading the file.
  prefs: []
  type: TYPE_NORMAL
- en: An Extension to Data Wrangling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the concluding chapter of our book, where we want to give you a broad
    overview of some of the exciting technologies and frameworks that you may need
    to learn beyond data wrangling to work as a full-stack data scientist. Data wrangling
    is an essential part of the whole data science and analytics pipeline, but it
    is not the whole enterprise. You have learned invaluable skills and techniques
    in this book, but it is always good to broaden your horizons and look beyond to
    see what other tools that are out there can give you an edge in this competitive
    and ever-changing world.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Skills Required to Become a Data Scientist
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To practice as a fully qualified data scientist/analyst, you should have some
    basic skills in your repertoire, irrespective of the particular programming language
    you choose to focus on. These skills and know-hows are language agnostic and can
    be utilized with any framework that you have to embrace, depending on your organization
    and business needs. We describe them in brief here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git and version control**: Git to version control is what RDBMS is to data
    storage and query. It simply means that there is a huge gap between the pre and
    post Git era of version controlling your code. As you may have noticed, all the
    notebooks for this book/book are hosted on GitHub, and this was done to take advantage
    of the powerful Git VCS. It gives you, out of the box, version control, history,
    branching facilities for different code, merging different code branches, and
    advanced operations like cherry picking, diff, and so on. It is an very essential
    tool to master as you can be almost sure that you will face it at one point of
    time in your journey. Packt has a very good book on it. You can check that out
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linux command line**: People coming from a Windows background (or even Mac,
    if you have not done any development before) are not very familiar, usually, with
    the command line. The superior UI of those OSes hides the low level details of
    interaction with the OS using a command line. However, as a data professional,
    it is important that you know the command line well. There are so many operations
    that you can do by simply using the command line that it is astonishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL and basic relational database concepts**: We dedicated an entire chapter
    to SQL and RDBMS. However, as we already mentioned there, it was really not enough.
    This is a vast subject and needs years of study to master it. Try to read more
    about it (Including Theory and Practical) from books and online sources. Do not
    forget that, despite all the other sources of data being used nowadays, we still
    have hundreds of millions of bytes of structured data stored in legacy database
    systems. You can be sure to come across one, sooner or later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker and containerization**: Since its first release in 2013, Docker has
    changed the way we distribute and deploy software in server-based applications.
    It gives you a clean and lightweight abstraction over the underlying OS and lets
    you iterate fast on development without the headache of creating and maintaining
    a proper environment. It is very useful in both the development and production
    phases. Without virtually no competitor present, they are becoming the default
    in the industry very fast. We strongly advise you to explore it in great detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Familiarity with Big Data and Cloud Technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Big data and cloud platforms are the latest trend. We will introduce them here
    with one or two short sentences and we encourage you to go ahead and learn about
    them as much as you can. If you are planning to grow as a data professional, then
    you can be sure that without these necessary skills it will be hard for you to
    transition to the next level:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fundamental characteristics of big data**: Big data is simply data that is
    very big in size. The term size is a bit ambiguous here. It can mean one static
    chunk of data (like the detail census data of a big country like India or the
    US) or data that is dynamically generated as time passes, and each time it is
    huge. To give an example for the second category, we can think of how much data
    is generated by Facebook per day. It''s about 500+ Terabytes per day. You can
    easily imagine that we will need specialized tools to deal with that amount of
    data. There are three different categories of big data, that is, Structured, Unstructured,
    and Semi-Structured. The main features that define big data are Volume, Variety,
    Velocity, and Variability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop ecosystem**: Apache Hadoop (and the related ecosystem) is a software
    framework that aims to use the Map-Reduce programming model to simplify the storage
    and processing of big data. It has since become one of the backbones of big data
    processing in the industry. The modules in Hadoop are designed keeping in mind
    that hardware failures are common occurrences, and they should be automatically
    handled by the framework. The four base modules of Hadoop are common, HDFS, YARN,
    and MapReduce. The Hadoop ecosystem consists of Apache Pig, Apache Hive, Apache
    Impala, Apache Zookeeper, Apache HBase, and more. They are very important bricks
    in many high demand and cutting-edge data pipelines. We encourage you to study
    more about them. They are essential in any industry that aims to leverage data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Spark**: Apache Spark is a general purpose Cluster Computing framework
    that was initially developed at the University of California, Barkley, and released
    in 2014\. It gives you an interface to program an entire cluster of computers
    with built-in data parallelism and fault tolerance. It contains Spark Core, Spark
    SQL, Spark Streaming, MLib (for machine learning), and GraphX. It is now one of
    the main frameworks that''s used in the industry to process a huge amount of data
    in real time based on streaming data. We encourage you to read and master it if
    you want to go toward real time data engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Web service (AWS)**: Amazon Web Services (often abbreviated as AWS)
    are a bunch of managed services offered by Amazon ranging from infrastructure-as-a-Service,
    Database-as-a-Service, MachineLearning-as-a-Service, Cache, Load Balancer, NoSQL
    database, to Message Queues and several other types. They are very useful for
    all sorts of applications. It can be a simple web app or a multi-cluster data
    pipeline. Many famous companies run their entire infrastructure on AWS (such as
    Netflix). They give us on-demand provision, easy scaling, a managed environment,
    a slick UI to control everything, and also a very powerful command-line client.
    They also expose a rich set of APIs and we can find an AWS API client in virtually
    any programming language. The Python one is called Boto3\. If you are planning
    to become a data professional, then it can be said with near certainty that you
    will end up using many of their services at one point or another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Goes with Data Wrangling?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned in *Chapter 1*, *Introduction to Data Wrangling with Python*, that
    the process of data wrangling lies in-between data gathering and advanced analytics,
    including visualization and machine learning. However, the boundaries that exist
    in-between these processes may not always be strict and rigid. It depends largely
    on the organizational culture and team composition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we need to not only be aware of the data wrangling but also the
    other components of the data science platform to wrangle data effectively. Even
    if you are performing pure data wrangling tasks, having a good grasp over how
    data is sourced and utilized will give you an edge for coming up with unique and
    efficient solutions to complex data wrangling problems and enhance the value of
    those solutions to the machine learning scientist or the business domain expert:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Process of data wrangling](img/Figure_1.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Process of data wrangling'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we have, in fact, already laid out a solid groundwork in this book for
    the data platform part, assuming that it is an integral part of data wrangling
    workflow. For example, we have covered web scraping, working with RESTful APIs,
    and database access and manipulation using Python libraries in detail.
  prefs: []
  type: TYPE_NORMAL
- en: We have also touched on basic visualization techniques and plotting functions
    in Python using matplotlib. However, there are other advanced statistical plotting
    libraries such as **Seaborn** that you can master for more sophisticated visualization
    for data science tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Business logic and domain expertise is the most varied topic and it can only
    be learned on the job, however it will come eventually with experience. If you
    have an academic background and/or work experience in any domain such as finance,
    medicine and healthcare, and engineering, that knowledge will come in handy in
    your data science career.
  prefs: []
  type: TYPE_NORMAL
- en: The fruit of the hard work of data wrangling is realized fully in the domain
    of machine learning. It is the science and engineering of making machines learn
    patterns and insights from data for predictive analytics and intelligent, automated
    decision-making with a deluge of data, which cannot be analyzed efficiently by
    humans. Machine learning has become one of the most sought-after skills in the
    modern technology landscape. It has truly become one of the most exciting and
    promising intellectual fields, with applications ranging from e-commerce to healthcare
    and virtually everything in-between. Data wrangling is intrinsically linked with
    machine learning as it prepares the data so that it's suitable for intelligent
    algorithms to process. Even if you start your career in data wrangling, it could
    be a natural progression to move to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Packt has published numerous books and books on this topic that you should explore.
    In the next section, we will touch upon some approaches to adopt and Python libraries
    to check out for giving you a boost in your learning.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and Tricks for Mastering Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Machine learning is difficult to start with. We have listed some structured
    MOOCs and incredible free resources that are available so that you can begin your
    journey:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the definition of and differentiation between the buzzwords — artificial
    intelligence, machine learning, deep learning, and data science. Cultivate the
    habit of reading great posts or listening to the expert talks, on these topics,
    and understand their true reach and applicability in some business problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stay updated with the recent trends by watching videos, reading books like
    *The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake
    Our World*, and articles and following influential blogs like KDnuggets, Brandon
    Rohrer''s blog, Open AI''s blog about their research, Towards Data Science publication
    on Medium, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you learn new algorithms or concepts, pause and analyze how you can apply
    these machine learning concepts or algorithm in your daily work. This is the best
    method for learning and expanding your knowledge base.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you choose Python as your preferred language for machine learning tasks,
    you have a great ML library in **scikit-learn**. It is the most widely used general
    machine learning package in the Python ecosystem. scikit-learn has a wide variety
    of supervised and unsupervised learning algorithms, which are exposed via a stable
    consistent interface. Moreover, it is specifically designed to interface seamlessly
    with other popular data wrangling and numerical libraries such as NumPy and pandas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another hot skill in today's job market is deep learning. Packt has many books
    and books on this topic and there are excellent MOOC books from Bookra where you
    can study deep learning. For Python libraries, you can learn and practice with
    **TensorFlow**, **Keras**, or **PyTorch** for deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is everywhere and it is all around us. In these nine chapters, we have
    learned about how data from different types and sources can be cleaned, corrected,
    and combined. Using the power of Python and the knowledge of data wrangling and
    applying the tricks and tips that you have studied in this book, you are ready
    to be a data wrangler.
  prefs: []
  type: TYPE_NORMAL
