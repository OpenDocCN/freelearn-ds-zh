- en: '*Chapter 9*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*'
- en: Application of Data Wrangling in Real Life
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理在现实生活中的应用
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Perform data wrangling on multiple full-fledged datasets from renowned sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对多个来自知名来源的完整数据集进行数据处理
- en: Create a unified dataset that can be passed on to a data science team for machine
    learning and predictive analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个统一的数据库，可以传递给数据科学团队进行机器学习和预测分析
- en: Relate data wrangling to version control, containerization, cloud services for
    data analytics, and big data technologies such as Apache Spark and Hadoop
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据处理与版本控制、容器化、数据分析云服务以及大数据技术（如Apache Spark和Hadoop）联系起来
- en: In this chapter, you will apply your gathered knowledge on real-life datasets
    and investigate various aspects of it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将应用你在现实生活中的数据集上所积累的知识，并调查其各个方面。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: We learned about databases in the previous chapter, so now it is time to combine
    the knowledge of data wrangling and Python with a real-world scenario. In the
    real world, data from one source is often inadequate to perform analysis. Generally,
    a data wrangler has to distinguish between relevant and non-relevant data and
    combine data from different sources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章学习了数据库，所以现在是将数据处理和Python知识与实际场景相结合的时候了。在现实世界中，来自单一来源的数据通常不足以进行分析。通常，数据处理员必须区分相关数据和非相关数据，并从不同来源组合数据。
- en: The primary job of a data wrangling expert is to pull data from multiple sources,
    format and clean it (impute the data if it is missing), and finally combine it
    in a coherent manner to prepare a dataset for further analysis by data scientists
    or machine learning engineers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理专家的主要工作是从多个来源提取数据，格式化和清理它（如果数据缺失，则进行数据插补），并最终以连贯的方式将其组合起来，为数据科学家或机器学习工程师准备进一步分析的数据集。
- en: In this topic, we will try to mimic such a typical task flow by downloading
    and using two different datasets from reputed web portals. Each of the datasets
    contains partial data pertaining to the key question that is being asked. Let's
    examine it more closely.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将尝试通过下载和使用来自知名网络门户的两个不同数据集来模拟这样一个典型的任务流程。每个数据集都包含与被询问的关键问题相关的部分数据。让我们更仔细地检查一下。
- en: Applying Your Knowledge to a Real-life Data Wrangling Task
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将你的知识应用于实际生活中的数据处理任务
- en: 'Suppose you are asked this question: **In India, did the enrollment in primary/secondary/tertiary
    education increase with the improvement of per capita GDP in the past 15 years?**
    The actual modeling and analysis will be done by some senior data scientist, who
    will use machine learning and data visualization for analysis. As a data wrangling
    expert, **your job will be to acquire and provide a clean dataset that contains
    educational enrollment and GDP data side by side**.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被问到这个问题：**在过去15年里，印度的初等/中等/高等教育入学率是否随着人均GDP的提高而增加？** 实际的建模和分析将由一些资深数据科学家完成，他们将使用机器学习和数据可视化进行分析。作为数据处理专家，**你的工作将是获取并提供一个包含教育入学率和GDP数据的干净数据集，这些数据并排排列**。
- en: Suppose you have a link for a dataset from the United Nations and you can download
    the dataset of education (for all the nations around the world). But this dataset
    has some missing values and moreover it does not have any GDP information. Someone
    has also given you another separate CSV file (downloaded from the World Bank site)
    which contains GDP data but in a messy format.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个来自联合国的数据集链接，你可以下载包含全球所有国家教育数据的数据集。但这个数据集有一些缺失值，而且它没有包含任何GDP信息。还有人给了你另一个单独的CSV文件（从世界银行网站下载），它包含GDP数据，但格式很混乱。
- en: 'In this activity, we will examine how to handle these two separate sources
    and clean the data to prepare a simple final dataset with the required data and
    save it to the local drive as a SQL database file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将检查如何处理这两个不同的来源，清理数据以准备一个包含所需数据的简单最终数据集，并将其保存为本地驱动器上的SQL数据库文件：
- en: '![Figure 9.1: Pictorial representation of the merging of education and economic
    data](img/C11065_09_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1：教育和经济数据合并的图示表示](img/C11065_09_01.jpg)'
- en: 'Figure 9.1: Pictorial representation of the merging of education and economic
    data'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.1：教育和经济数据合并的图示表示
- en: You are encouraged to follow along with the code and results in the notebook
    and try to understand and internalize the nature of the data wrangling flow. You
    are also encouraged to try extracting various data from these files and answer
    your own questions about a nations' socio-economic factors and their inter-relationships.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您跟随笔记本中的代码和结果，尝试理解和内化数据处理流程的本质。您还被鼓励尝试从这些文件中提取各种数据，并回答您自己对一个国家的社会经济因素及其相互关系的问题。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Coming up with interesting questions about social, economic, technological,
    and geo-political topics and then answering them using freely available data and
    a little bit of programming knowledge is one of most fun ways to learn about any
    data science topic. You will get a flare of that process in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提出关于社会、经济、技术和地缘政治主题的有趣问题，然后使用免费数据和一点编程知识来回答这些问题，这是学习任何数据科学主题最有趣的方式之一。您将在本章中了解到这个过程的一些内容。
- en: '**Data Imputation**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据插补**'
- en: Clearly, we are missing some data. Let's say we decide to impute these data
    points by simple linear interpolation between the available data points. We can
    take out a pen and paper or a calculator and compute those values and manually
    create a dataset. But being a data wrangler, we will of course take advantage
    of Python programming, and use pandas imputation methods for this task.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们缺少一些数据。假设我们决定通过在可用数据点之间进行简单线性插值来插补这些数据点。我们可以拿出笔和纸或计算器来计算这些值，并手动创建一个数据集。但作为一个数据处理员，我们当然会利用Python编程，并使用pandas插补方法来完成这项任务。
- en: But to do that, we first need to create a DataFrame with missing values in it,
    that is, we need to append another DataFrame with missing values to the current
    DataFrame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但要这样做，我们首先需要创建一个包含缺失值的DataFrame，也就是说，我们需要将另一个包含缺失值的DataFrame附加到当前DataFrame上。
- en: 'Activity 12: Data Wrangling Task – Fixing UN Data'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动十二：数据处理任务 - 修复联合国数据
- en: Suppose the agenda of the data analysis is to find out whether the enrolment
    in primary, secondary, or tertiary education has increased with the improvement
    of per capita GDP in the past 15 years. For this task, we will first need to clean
    or wrangle the two datasets, that is, the Education Enrolment and GDP data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据分析的议程是找出在过去15年中，人均GDP的提高是否导致了小学、中学或高等教育入学率的增加。为此任务，我们首先需要清理或整理两个数据集，即教育入学率和GDP数据。
- en: The UN data is available on [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/SYB61_T07_Education.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/SYB61_T07_Education.csv).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 联合国数据可在以下链接中找到：[https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/SYB61_T07_Education.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/SYB61_T07_Education.csv)。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If you download the CSV file and open it using Excel, then you will see that
    the `Footnotes` column sometimes contains useful notes. We may not want to drop
    it in the beginning. If we are interested in a particular country's data (like
    we are in this task), then it may well turn out that `Footnotes` will be `NaN`,
    that is, blank. In that case, we can drop it at the end. But for some countries
    or regions, it may contain information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您下载CSV文件并使用Excel打开它，您将看到“脚注”列有时包含有用的注释。我们可能不想一开始就删除它。如果我们对特定国家或地区的数据感兴趣（就像在这个任务中一样），那么“脚注”可能就是`NaN`，即空白。在这种情况下，我们可以在最后删除它。但对于某些国家或地区，它可能包含信息。
- en: 'These steps will guide you to find the solution:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将指导您找到解决方案：
- en: 'Download the dataset from the UN data from GitHub from the following link:
    [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity13/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity13/India_World_Bank_Info.csv).'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接下载联合国数据集：[https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity13/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity13/India_World_Bank_Info.csv)。
- en: The UN data has missing values. Clean the data to prepare a simple final dataset
    with the required data and save it to the local drive as a SQL database file.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合国数据存在缺失值。清理数据，准备一个包含所需数据的简单最终数据集，并将其保存到本地驱动器上的SQL数据库文件中。
- en: Use the `pd.read_csv` method of pandas to create a DataFrame.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`pd.read_csv`方法创建一个DataFrame。
- en: Since the first row does not contain useful information, skip it using the `skiprows`
    parameter.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于第一行不包含有用的信息，使用`skiprows`参数跳过它。
- en: Drop the column region/country/area and source.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除区域/国家/地区和来源列。
- en: 'Assign the following names as columns of DataFrame: Region/County/Area, Year,
    Data, Value, and Footnotes.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下名称分配为DataFrame的列：区域/县/地区、年份、数据、值和脚注。
- en: Check how many unique values are present in the `Footnotes` column.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查“脚注”列中有多少个独特的值。
- en: Check the type of value column.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查值列的类型。
- en: Create a function to convert the value column into a floating-point.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数将值列转换为浮点数。
- en: Use the `apply` method to apply this function to a value.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`apply`方法将此函数应用于一个值。
- en: Print the unique values in the data column.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据列中的独特值。
- en: 'Note:'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意：
- en: The solution for this activity can be found on page 338.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可在第338页找到。
- en: 'Activity 13: Data Wrangling Task – Cleaning GDP Data'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动十三：数据整理任务 - 清理GDP数据
- en: The GDP data is available on [https://data.worldbank.org/](https://data.worldbank.org/)
    and it is available on GitHub at [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/India_World_Bank_Info.csv).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GDP数据可在[https://data.worldbank.org/](https://data.worldbank.org/)找到，并在GitHub上可用，地址为[https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter09/Activity12-15/India_World_Bank_Info.csv](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson09/Activity12-15/India_World_Bank_Info.csv)。
- en: In this activity, we will clean the GDP data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将清理GDP数据。
- en: Create three DataFrames from the original DataFrame using filtering. Create
    the `df_primary, df_secondary,` and `df_tertiary DataFrames` for students enrolled
    in primary education, secondary education, and tertiary education in thousands,
    respectively.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据帧中通过筛选创建三个数据帧：`df_primary`、`df_secondary`和`df_tertiary`，分别对应小学、中学和大学的学生人数（单位为千）。
- en: Plot bar charts of the enrollment of primary students in a low-income country
    like India and a higher-income country like the USA.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制印度这类低收入国家和美国这类高收入国家小学学生入学人数的条形图。
- en: Since there is missing data, use pandas imputation methods to impute these data
    points by simple linear interpolation between data points. To do that, create
    a DataFrame with missing values inserted and append a new DataFrame with missing
    values to the current DataFrame.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于存在缺失数据，使用pandas的插补方法通过简单线性插值在数据点之间插补这些数据点。为此，创建一个包含缺失值的DataFrame，并将一个包含缺失值的新的DataFrame附加到当前DataFrame中。
- en: (For India) Append the rows corresponding to the missing years - **2004 - 2009**,
    **2011 – 2013**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （针对印度）添加对应缺失年份的行 - **2004 - 2009**，**2011 – 2013**。
- en: Create a dictionary of values with `np.nan`. Note that there are 9 missing data
    points, so we need to create a list with identical values repeated 9 times.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.nan`创建一个包含值的字典。请注意，有9个缺失数据点，因此我们需要创建一个包含相同值重复9次的列表。
- en: Create a DataFrame of missing values (from the preceding dictionary) that we
    can append.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含缺失值的DataFrame（来自前面的字典），我们可以将其附加。
- en: Append the DataFrames together.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据帧拼接在一起。
- en: Sort by Year and reset the indices using `reset_index`. Use `inplace=True` to
    execute the changes on the DataFrame itself.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按年份排序并使用`reset_index`重置索引。使用`inplace=True`在DataFrame本身上执行更改。
- en: 'Use the interpolate method for linear interpolation. It fills all the NaNs
    by linearly interpolated values. See the following link for more details about
    this method: [http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html).'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用线性插值方法进行线性插值。它通过线性插值值填充所有NaN。有关此方法的更多详细信息，请参阅以下链接：[http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html)。
- en: Repeat the same steps for USA (or other countries).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对美国（或其他国家）重复相同的步骤。
- en: If there are values that are unfilled, use the `limit` and `limit_direction`
    parameters with the interpolate method to fill them in.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有未填写的值，使用`limit`和`limit_direction`参数与插值方法一起填充它们。
- en: Plot the final graph using the new data.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新数据绘制最终图形。
- en: Read the GDP data using the pandas `read_csv` method. It will generally throw
    an error.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`read_csv`方法读取GDP数据。它通常会引发错误。
- en: To avoid errors, try the `error_bad_lines` `=` `False` option.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了避免错误，尝试使用`error_bad_lines` `=` `False`选项。
- en: Since there is no delimiter in the file, add the `\t` delimiter.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于文件中没有分隔符，添加`\t`分隔符。
- en: Use the `skiprows` function to remove rows that are not useful.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`skiprows`函数删除无用的行。
- en: Examine the dataset. Filter the dataset with information that states that it
    is similar to the previous education dataset.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集。使用表示与先前教育数据集相似的信息来过滤数据集。
- en: Reset the index for this new dataset.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这个新数据集重置索引。
- en: Drop the not useful rows and re-index the dataset.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除无用的行并重新索引数据集。
- en: Rename the columns properly. This is necessary for merging the two datasets.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确重命名列。这对于合并两个数据集是必要的。
- en: We will concentrate only on the data from 2003 to 2016\. Eliminate the remaining
    data.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将只关注2003年至2016年的数据。消除剩余的数据。
- en: Create a new DataFrame called `df_gdp` with rows 43 to 56.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的DataFrame，名为`df_gdp`，包含第43行到第56行的数据。
- en: Note
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 338.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第338页找到。
- en: 'Activity 14: Data Wrangling Task – Merging UN Data and GDP Data'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动十四：数据处理任务 – 合并联合国数据和GDP数据
- en: 'The steps to merge the databases is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 合并数据库的步骤如下：
- en: Reset the indexes for merging.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置合并的索引。
- en: Merge the two DataFrames, `primary_enrollment_india` and `df_gdp`, on the Year
    column.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在年份列上合并两个DataFrame，`primary_enrollment_india`和`df_gdp`。
- en: Drop the data, footnotes, and region/county/area.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除数据、脚注以及地区/县/区域。
- en: Rearrange the columns for proper viewing and presentation.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排列列以进行适当的查看和展示。
- en: Note
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 345.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第345页找到。
- en: 'Activity 15: Data Wrangling Task – Connecting the New Data to the Database'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动十五：数据处理任务 – 将新数据连接到数据库
- en: 'The steps to connect the data to the database is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据连接到数据库的步骤如下：
- en: Import the `sqlite3` module of Python and use the `connect` function to connect
    to the database. The main database engine is embedded. But for a different database
    like `Postgresql` or `MySQL`, we will need to connect to them using those credentials.
    We designate `Year` as the `PRIMARY` `KEY` of this table.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Python的`sqlite3`模块，并使用`connect`函数连接到数据库。主要数据库引擎是嵌入式的。但对于像`Postgresql`或`MySQL`这样的不同数据库，我们需要使用那些凭据来连接它们。我们将`Year`指定为该表的`PRIMARY
    KEY`。
- en: Then, run a loop with the dataset rows one by one to insert them into the table.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，运行一个循环，逐行将数据集的行插入到表中。
- en: If we look at the current folder, we should see a file called `Education_GDP.db`,
    and if we examine that using a database viewer program, we can see the data transferred
    there.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们查看当前文件夹，我们应该看到一个名为`Education_GDP.db`的文件，如果我们使用数据库查看程序检查它，我们可以看到数据已传输到那里。
- en: Note
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 347.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第347页找到。
- en: In this notebook, we examined a complete data wrangling flow, including reading
    data from the web and local drive, filtering, cleaning, quick visualization, imputation,
    indexing, merging, and writing back to a database table. We also wrote custom
    functions to transform some of the data and saw how to handle situations where
    we may get errors when reading the file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，我们检查了一个完整的数据处理流程，包括从网络和本地驱动器读取数据，过滤、清洗、快速可视化、插补、索引、合并，并将数据写回数据库表。我们还编写了自定义函数来转换一些数据，并展示了在读取文件时可能遇到错误的情况。
- en: An Extension to Data Wrangling
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理的扩展
- en: This is the concluding chapter of our book, where we want to give you a broad
    overview of some of the exciting technologies and frameworks that you may need
    to learn beyond data wrangling to work as a full-stack data scientist. Data wrangling
    is an essential part of the whole data science and analytics pipeline, but it
    is not the whole enterprise. You have learned invaluable skills and techniques
    in this book, but it is always good to broaden your horizons and look beyond to
    see what other tools that are out there can give you an edge in this competitive
    and ever-changing world.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章，我们希望向您提供一个广泛的概述，介绍一些您可能需要学习的令人兴奋的技术和框架，以便在数据处理之外工作，成为一名全栈数据科学家。数据处理是整个数据科学和数据分析流程的一个基本部分，但它不是全部。您在这本书中学到了宝贵的技能和技术，但总是好的，拓宽视野，看看其他工具可以在这个竞争激烈且不断变化的世界中给您带来优势。
- en: Additional Skills Required to Become a Data Scientist
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成为数据科学家所需的其他技能
- en: 'To practice as a fully qualified data scientist/analyst, you should have some
    basic skills in your repertoire, irrespective of the particular programming language
    you choose to focus on. These skills and know-hows are language agnostic and can
    be utilized with any framework that you have to embrace, depending on your organization
    and business needs. We describe them in brief here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为一名合格的数据科学家/分析师，你应该在你的技能库中拥有一些基本技能，无论你选择专注于哪种特定的编程语言。这些技能和知识是语言无关的，可以根据你的组织和企业需求，在任何你必须接受的框架中使用。我们在这里简要描述它们：
- en: '**Git and version control**: Git to version control is what RDBMS is to data
    storage and query. It simply means that there is a huge gap between the pre and
    post Git era of version controlling your code. As you may have noticed, all the
    notebooks for this book/book are hosted on GitHub, and this was done to take advantage
    of the powerful Git VCS. It gives you, out of the box, version control, history,
    branching facilities for different code, merging different code branches, and
    advanced operations like cherry picking, diff, and so on. It is an very essential
    tool to master as you can be almost sure that you will face it at one point of
    time in your journey. Packt has a very good book on it. You can check that out
    for more information.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git 和版本控制**：Git 版本控制对于代码管理来说，就像 RDBMS 对于数据存储和查询一样。这意味着在 Git 时代之前和之后，代码版本控制之间存在巨大的差距。正如你可能已经注意到的，这本书的所有笔记本都托管在
    GitHub 上，这是为了利用强大的 Git VCS。它为你提供了开箱即用的版本控制、历史记录、不同代码的分支功能、合并不同代码分支以及 cherry picking、diff
    等高级操作。这是一个非常必要的工具，你几乎可以肯定，在你的旅途中你会在某个时刻遇到它。Packt 有关于它的非常好的书籍。你可以查看以获取更多信息。'
- en: '**Linux command line**: People coming from a Windows background (or even Mac,
    if you have not done any development before) are not very familiar, usually, with
    the command line. The superior UI of those OSes hides the low level details of
    interaction with the OS using a command line. However, as a data professional,
    it is important that you know the command line well. There are so many operations
    that you can do by simply using the command line that it is astonishing.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux 命令行**：来自 Windows 背景（或者甚至 Mac，如果你之前没有进行过任何开发）的人通常不太熟悉命令行。那些操作系统的优越 UI
    隐藏了使用命令行与操作系统交互的底层细节。然而，作为一名数据专业人士，了解命令行是非常重要的。你可以通过命令行执行的操作如此之多，以至于令人惊讶。'
- en: '**SQL and basic relational database concepts**: We dedicated an entire chapter
    to SQL and RDBMS. However, as we already mentioned there, it was really not enough.
    This is a vast subject and needs years of study to master it. Try to read more
    about it (Including Theory and Practical) from books and online sources. Do not
    forget that, despite all the other sources of data being used nowadays, we still
    have hundreds of millions of bytes of structured data stored in legacy database
    systems. You can be sure to come across one, sooner or later.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQL 和基本的关系数据库概念**：我们专门用了一整章来介绍 SQL 和 RDBMS。然而，正如我们之前提到的，这远远不够。这是一个庞大的主题，需要多年的学习才能掌握。尝试从书籍和在线资源中了解更多关于它的内容（包括理论和实践）。不要忘记，尽管现在使用了所有其他数据来源，我们仍然有数亿字节的结构化数据存储在传统的数据库系统中。你可以确信，迟早你会遇到这样的系统。'
- en: '**Docker and containerization**: Since its first release in 2013, Docker has
    changed the way we distribute and deploy software in server-based applications.
    It gives you a clean and lightweight abstraction over the underlying OS and lets
    you iterate fast on development without the headache of creating and maintaining
    a proper environment. It is very useful in both the development and production
    phases. Without virtually no competitor present, they are becoming the default
    in the industry very fast. We strongly advise you to explore it in great detail.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker 和容器化**：自从 2013 年首次发布以来，Docker 已经改变了我们在基于服务器的应用程序中分发和部署软件的方式。它为底层操作系统提供了一个干净且轻量级的抽象，让你能够快速迭代开发，而无需为创建和维护适当的环境而烦恼。它在开发和生产阶段都非常有用。由于几乎没有竞争对手，它们正在迅速成为行业中的默认选择。我们强烈建议你深入了解它。'
- en: Basic Familiarity with Big Data and Cloud Technologies
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对大数据和云计算技术的基本了解
- en: 'Big data and cloud platforms are the latest trend. We will introduce them here
    with one or two short sentences and we encourage you to go ahead and learn about
    them as much as you can. If you are planning to grow as a data professional, then
    you can be sure that without these necessary skills it will be hard for you to
    transition to the next level:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据和云平台是当前最新的趋势。我们将用一两句话来介绍它们，并鼓励你尽可能多地了解它们。如果你计划成为一名数据专业人士，那么你可以确信，没有这些必要的技能，你将很难过渡到下一个层次：
- en: '**Fundamental characteristics of big data**: Big data is simply data that is
    very big in size. The term size is a bit ambiguous here. It can mean one static
    chunk of data (like the detail census data of a big country like India or the
    US) or data that is dynamically generated as time passes, and each time it is
    huge. To give an example for the second category, we can think of how much data
    is generated by Facebook per day. It''s about 500+ Terabytes per day. You can
    easily imagine that we will need specialized tools to deal with that amount of
    data. There are three different categories of big data, that is, Structured, Unstructured,
    and Semi-Structured. The main features that define big data are Volume, Variety,
    Velocity, and Variability.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大数据的基本特征**: 大数据仅仅是非常大规模的数据。这里的“规模”一词有点模糊。它可以指一个静态的数据块（如像印度或美国这样的大国的详细人口普查数据）或者随着时间的推移动态生成的大量数据。为了举例说明第二类，我们可以想想
    Facebook 每天生成多少数据。大约是每天 500+ 太字节。你可以很容易地想象，我们将需要专门的工具来处理这么多的数据。大数据有三种不同的类别，即结构化、非结构化和半结构化。定义大数据的主要特征是体积、种类、速度和可变性。'
- en: '**Hadoop ecosystem**: Apache Hadoop (and the related ecosystem) is a software
    framework that aims to use the Map-Reduce programming model to simplify the storage
    and processing of big data. It has since become one of the backbones of big data
    processing in the industry. The modules in Hadoop are designed keeping in mind
    that hardware failures are common occurrences, and they should be automatically
    handled by the framework. The four base modules of Hadoop are common, HDFS, YARN,
    and MapReduce. The Hadoop ecosystem consists of Apache Pig, Apache Hive, Apache
    Impala, Apache Zookeeper, Apache HBase, and more. They are very important bricks
    in many high demand and cutting-edge data pipelines. We encourage you to study
    more about them. They are essential in any industry that aims to leverage data.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 生态系统**: Apache Hadoop（及其相关生态系统）是一个旨在使用 Map-Reduce 编程模型简化大数据存储和处理的软件框架。它已经成为行业大数据处理的主要支柱之一。Hadoop
    的模块设计时考虑到硬件故障是常见现象，并且应该由框架自动处理。Hadoop 的四个基础模块是 Common、HDFS、YARN 和 MapReduce。Hadoop
    生态系统包括 Apache Pig、Apache Hive、Apache Impala、Apache Zookeeper、Apache HBase 等。它们是许多高需求和前沿数据管道中非常重要的基石。我们鼓励你更多地了解它们。它们在旨在利用数据的任何行业中都是必不可少的。'
- en: '**Apache Spark**: Apache Spark is a general purpose Cluster Computing framework
    that was initially developed at the University of California, Barkley, and released
    in 2014\. It gives you an interface to program an entire cluster of computers
    with built-in data parallelism and fault tolerance. It contains Spark Core, Spark
    SQL, Spark Streaming, MLib (for machine learning), and GraphX. It is now one of
    the main frameworks that''s used in the industry to process a huge amount of data
    in real time based on streaming data. We encourage you to read and master it if
    you want to go toward real time data engineering.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**: Apache Spark 是一个通用的集群计算框架，最初由加州大学伯克利分校开发，并于 2014 年发布。它为你提供了一个接口，可以编程整个计算机集群，内置数据并行性和容错性。它包含
    Spark Core、Spark SQL、Spark Streaming、MLib（用于机器学习）和 GraphX。现在，它是工业界用于基于流数据的实时处理大量数据的主要框架之一。如果你想要走向实时数据工程，我们鼓励你阅读并掌握它。'
- en: '**Amazon Web service (AWS)**: Amazon Web Services (often abbreviated as AWS)
    are a bunch of managed services offered by Amazon ranging from infrastructure-as-a-Service,
    Database-as-a-Service, MachineLearning-as-a-Service, Cache, Load Balancer, NoSQL
    database, to Message Queues and several other types. They are very useful for
    all sorts of applications. It can be a simple web app or a multi-cluster data
    pipeline. Many famous companies run their entire infrastructure on AWS (such as
    Netflix). They give us on-demand provision, easy scaling, a managed environment,
    a slick UI to control everything, and also a very powerful command-line client.
    They also expose a rich set of APIs and we can find an AWS API client in virtually
    any programming language. The Python one is called Boto3\. If you are planning
    to become a data professional, then it can be said with near certainty that you
    will end up using many of their services at one point or another.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务（AWS）**：亚马逊网络服务（通常缩写为AWS）是由亚马逊提供的一系列托管服务，从基础设施即服务（IaaS）、数据库即服务（DBaaS）、机器学习即服务（MLaaS）、缓存、负载均衡器、NoSQL数据库，到消息队列等多种类型。它们对各种应用都非常有用。它可以是一个简单的Web应用，也可以是一个多集群数据管道。许多知名公司都在AWS上运行其整个基础设施（如Netflix）。他们提供按需提供、易于扩展、管理环境、流畅的用户界面来控制一切，以及一个非常强大的命令行客户端。他们还公开了一系列丰富的API，我们几乎可以在任何编程语言中找到AWS
    API客户端。Python的一个叫做Boto3。如果你计划成为一名数据专业人士，那么几乎可以肯定地说，你最终会在某个时候使用他们的许多服务。'
- en: What Goes with Data Wrangling?
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据处理需要什么？
- en: We learned in *Chapter 1*, *Introduction to Data Wrangling with Python*, that
    the process of data wrangling lies in-between data gathering and advanced analytics,
    including visualization and machine learning. However, the boundaries that exist
    in-between these processes may not always be strict and rigid. It depends largely
    on the organizational culture and team composition.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第1章*，*使用Python进行数据处理入门*中，我们了解到数据处理过程位于数据收集和高级分析（包括可视化和机器学习）之间。然而，存在于这些过程之间的边界可能并不总是严格和固定的。这很大程度上取决于组织文化和团队构成。
- en: 'Therefore, we need to not only be aware of the data wrangling but also the
    other components of the data science platform to wrangle data effectively. Even
    if you are performing pure data wrangling tasks, having a good grasp over how
    data is sourced and utilized will give you an edge for coming up with unique and
    efficient solutions to complex data wrangling problems and enhance the value of
    those solutions to the machine learning scientist or the business domain expert:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不仅需要了解数据处理，还需要了解数据科学平台的其他组件，以有效地处理数据。即使你正在执行纯粹的数据处理任务，对数据来源和利用的良好掌握也将为你提供优势，以便提出独特且高效的解决方案来解决复杂的数据处理问题，并提高这些解决方案对机器学习科学家或业务领域专家的价值：
- en: '![Figure 9.2: Process of data wrangling](img/Figure_1.1.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：数据处理过程](img/Figure_1.1.jpg)'
- en: 'Figure 9.2: Process of data wrangling'
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.2：数据处理过程
- en: Now, we have, in fact, already laid out a solid groundwork in this book for
    the data platform part, assuming that it is an integral part of data wrangling
    workflow. For example, we have covered web scraping, working with RESTful APIs,
    and database access and manipulation using Python libraries in detail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实际上，我们已经在这本书中为数据处理平台部分打下了坚实的基础，假设它是数据处理工作流程的一个组成部分。例如，我们详细介绍了网络爬取、使用RESTful
    API以及使用Python库进行数据库访问和操作。
- en: We have also touched on basic visualization techniques and plotting functions
    in Python using matplotlib. However, there are other advanced statistical plotting
    libraries such as **Seaborn** that you can master for more sophisticated visualization
    for data science tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还简要介绍了使用matplotlib在Python中的基本可视化技术和绘图函数。然而，还有其他高级统计绘图库，如**Seaborn**，你可以掌握它来进行更复杂的数据科学任务的可视化。
- en: Business logic and domain expertise is the most varied topic and it can only
    be learned on the job, however it will come eventually with experience. If you
    have an academic background and/or work experience in any domain such as finance,
    medicine and healthcare, and engineering, that knowledge will come in handy in
    your data science career.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 商业逻辑和领域专业知识是最多样化的主题，而且只能在工作中学习，然而随着经验的积累，它最终会到来。如果你在金融、医学和医疗保健、工程等任何领域有学术背景和/或工作经验，那么这些知识将在你的数据科学职业生涯中派上用场。
- en: The fruit of the hard work of data wrangling is realized fully in the domain
    of machine learning. It is the science and engineering of making machines learn
    patterns and insights from data for predictive analytics and intelligent, automated
    decision-making with a deluge of data, which cannot be analyzed efficiently by
    humans. Machine learning has become one of the most sought-after skills in the
    modern technology landscape. It has truly become one of the most exciting and
    promising intellectual fields, with applications ranging from e-commerce to healthcare
    and virtually everything in-between. Data wrangling is intrinsically linked with
    machine learning as it prepares the data so that it's suitable for intelligent
    algorithms to process. Even if you start your career in data wrangling, it could
    be a natural progression to move to machine learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的辛勤工作在机器学习领域得到了充分的体现。它是使机器从数据中学习模式和洞察力，以进行预测分析和智能、自动决策的科学和工程，这些数据量巨大，人类无法有效分析。机器学习已成为现代技术景观中最受欢迎的技能之一。它确实已成为最具激动人心和最有希望的智力领域之一，其应用范围从电子商务到医疗保健，几乎涵盖所有领域。数据清洗与机器学习内在相关，因为它准备数据，使其适合智能算法处理。即使你从数据清洗开始你的职业生涯，也可能自然过渡到机器学习。
- en: Packt has published numerous books and books on this topic that you should explore.
    In the next section, we will touch upon some approaches to adopt and Python libraries
    to check out for giving you a boost in your learning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Packt已经出版了大量的书籍，你应该探索这些书籍。在下一节中，我们将讨论一些可以采用的方法和Python库，以帮助你提高学习效率。
- en: Tips and Tricks for Mastering Machine Learning
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掌握机器学习的技巧和窍门
- en: 'Machine learning is difficult to start with. We have listed some structured
    MOOCs and incredible free resources that are available so that you can begin your
    journey:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习入门有一定难度。我们列出了一些结构化的MOOCs和令人难以置信的免费资源，以便你可以开始你的旅程：
- en: Understand the definition of and differentiation between the buzzwords — artificial
    intelligence, machine learning, deep learning, and data science. Cultivate the
    habit of reading great posts or listening to the expert talks, on these topics,
    and understand their true reach and applicability in some business problem.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人工智能、机器学习、深度学习和数据科学等术语的定义和区别。培养阅读优秀帖子或聆听专家关于这些主题的演讲的习惯，并了解它们在解决某些商业问题中的真正影响力和适用性。
- en: 'Stay updated with the recent trends by watching videos, reading books like
    *The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake
    Our World*, and articles and following influential blogs like KDnuggets, Brandon
    Rohrer''s blog, Open AI''s blog about their research, Towards Data Science publication
    on Medium, and so on.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过观看视频、阅读像《终极学习机：对终极学习机的追求将如何重塑我们的世界》这样的书籍，以及阅读文章和关注像KDnuggets、Brandon Rohrer的博客、Open
    AI关于他们研究的博客、Medium上的《数据科学》出版物等有影响力的博客，来保持对最新趋势的了解。
- en: As you learn new algorithms or concepts, pause and analyze how you can apply
    these machine learning concepts or algorithm in your daily work. This is the best
    method for learning and expanding your knowledge base.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你学习新的算法或概念时，请暂停并分析如何将这些机器学习概念或算法应用到你的日常工作中。这是学习和扩展你的知识库的最佳方法。
- en: If you choose Python as your preferred language for machine learning tasks,
    you have a great ML library in **scikit-learn**. It is the most widely used general
    machine learning package in the Python ecosystem. scikit-learn has a wide variety
    of supervised and unsupervised learning algorithms, which are exposed via a stable
    consistent interface. Moreover, it is specifically designed to interface seamlessly
    with other popular data wrangling and numerical libraries such as NumPy and pandas.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你选择Python作为机器学习任务的优先语言，你将拥有一个出色的ML库**scikit-learn**。它是Python生态系统中使用最广泛的通用机器学习包。scikit-learn拥有各种监督和非监督学习算法，这些算法通过一个稳定一致的接口公开。此外，它专门设计用于与其他流行的数据清洗和数值库无缝接口，如NumPy和pandas。
- en: Another hot skill in today's job market is deep learning. Packt has many books
    and books on this topic and there are excellent MOOC books from Bookra where you
    can study deep learning. For Python libraries, you can learn and practice with
    **TensorFlow**, **Keras**, or **PyTorch** for deep learning.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当今的就业市场上，深度学习是另一种热门技能。Packt有许多关于这个主题的书籍，Bookra也有关于深度学习的优秀MOOC书籍，你可以学习并练习。对于Python库，你可以学习并使用**TensorFlow**、**Keras**或**PyTorch**进行深度学习。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data is everywhere and it is all around us. In these nine chapters, we have
    learned about how data from different types and sources can be cleaned, corrected,
    and combined. Using the power of Python and the knowledge of data wrangling and
    applying the tricks and tips that you have studied in this book, you are ready
    to be a data wrangler.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在，它环绕着我们。在这九章中，我们学习了如何清理、纠正和合并来自不同类型和来源的数据。利用Python的力量以及你在本书中学到的数据处理知识和技巧，你已准备好成为一名数据整理师。
