<html><head></head><body>
		<div><h1 id="_idParaDest-175" class="chapter-number"><a id="_idTextAnchor194"/>10</h1>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor195"/>Time Series Analysis: Statistics, Plots, and Forecasting</h1>
			<p>In the realm of mathematical analysis, particularly in the study of data and trends, time series charts play a pivotal role. A time series chart is a graphical representation that displays data points collected over a sequence of time intervals. This tool is indispensable in various fields, including economics, finance, environmental science, and social sciences, for analyzing patterns, trends, and fluctuations in data over time.</p>
			<p>A typical time series chart comprises two essential components: the time axis and the data axis. The time axis represents the progression of time, which can be measured in various units, such as seconds, minutes, hours, days, months, or years. The data axis displays the values of the variable being studied, which can be anything from stock prices and temperature readings to population counts and sales figures.</p>
			<p>To construct a time series chart, you must do the following:</p>
			<ul>
				<li><strong class="bold">Data collection</strong>: Gather data points at regular time intervals or time stamps. These intervals should be consistent to accurately capture temporal patterns.</li>
				<li><strong class="bold">Data representation</strong>: Plot the collected data points on the graph, aligning each point with its corresponding time stamp on the time axis.</li>
				<li><strong class="bold">Axis scaling</strong>: Choose appropriate scaling for both the time axis and the data axis. This ensures that the patterns are visible and accurately represented.</li>
				<li><strong class="bold">Connect data points</strong>: Depending on the context, data points can be connected with lines, curves, or bars. Connecting points can reveal trends more effectively.</li>
			</ul>
			<p>A time series chart’s primary function is to enable the analysis of trends, patterns, and anomalies in the data. Several insights can be gained from such analysis:</p>
			<ul>
				<li><strong class="bold">Trend identification</strong>: Time series charts help identify long-term trends, such as gradual increases or decreases in values over time. These trends can provide valuable information for decision-making.</li>
				<li><strong class="bold">Seasonal variations</strong>: Seasonal patterns, such as spikes in sales during holiday seasons, can be identified by observing regular fluctuations in the data over specific periods.</li>
				<li><strong class="bold">Cyclic patterns</strong>: In addition to seasonal variations, cyclic patterns – repeating but irregular fluctuations – can be observed. These might be influenced by factors such as economic cycles or environmental changes.</li>
				<li><strong class="bold">Volatility and outliers</strong>: Sudden spikes or dips in the data can indicate volatility or outliers, drawing attention to events or factors affecting the variable being measured.</li>
			</ul>
			<p>Time series charts also serve as a foundation for forecasting and predictive analysis. Mathematical models can be applied to historical data to make predictions about future trends and values. It is important to note that when I say make predictions, I do mean that we are making inferences about what could be likely to happen if all things remain the same. Techniques such as moving averages, exponential <a id="_idIndexMarker778"/>smoothing, and <strong class="bold">autoregressive integrated moving average</strong> (<strong class="bold">ARIMA</strong>) models are commonly used for this purpose.</p>
			<p>In the realm of mathematical analysis, a time series chart is a powerful tool that aids in understanding the dynamics of data over time. By visually representing data points and trends, it allows researchers, analysts, and decision-makers to extract valuable insights, identify patterns, and make informed predictions. Through the lens of mathematical analysis, time series charts provide a structured approach to comprehending temporal data, thereby contributing significantly to fields that rely on data-driven decision-making.</p>
			<p>In this chapter, we’ll cover the following topics:</p>
			<ul>
				<li>Generating random time series objects in R</li>
				<li>Time series plotting with R</li>
				<li>Auto ARIMA modeling with <code>healthyR.ts</code></li>
				<li>Creating a Brownian motion with <code>healthyR.ts</code></li>
				<li>Time series plotting – basic plots and ACF/PACF plots</li>
				<li>Time series statistics and statistical forecasting</li>
				<li>Time series forecasting with deep learning – LSTM</li>
			</ul>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor196"/>Technical requirements</h1>
			<p>There are a few technical requirements for this chapter. Note that the code for this chapter can be found at  <a href="https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%2010">https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%2010</a>.</p>
			<p>Some of the packages that we will be using in this chapter are as follows:</p>
			<ul>
				<li><code>healthyR.ts</code></li>
				<li><code>forecast</code></li>
				<li><code>timetk</code></li>
				<li><code>Modeltime</code></li>
				<li><code>prophet (</code><code>for Python)</code></li>
				<li><code>keras</code></li>
				<li><code>tensorflow</code></li>
			</ul>
			<p>We will start by creating time series objects in base R. The basic object class for a time series object in R is <code>ts</code> and objects can be coerced to that object by either using the <code>ts()</code> function directly or calling <code>as.ts()</code> on an object such as a vector.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor197"/>Generating random time series objects in R</h1>
			<p>We are going to generate some<a id="_idIndexMarker779"/> random time<a id="_idIndexMarker780"/> series objects in base R. Doing this is very simple as base R comes with some distribution functions already packed in. We will make use of the random normal distribution by making calls to the <code>rnorm()</code> function. This function has three parameters to provide arguments to:</p>
			<ul>
				<li><code>n</code>: The number of points to be generated</li>
				<li><code>mean</code>: The mean of the distribution, with a default of 0</li>
				<li><code>sd</code>: The standard deviation of the distribution, with the default being 1</li>
			</ul>
			<p>Let’s go ahead and generate our first random vector. We will call it <code>x</code>:</p>
			<pre class="source-code">
# Generate a Random Time Series
# Set seed to make results reproducible
set.seed(123)
# Generate Random Points using a gaussian distribution with mean 0 and sd = 1
n &lt;- 25
x &lt;- rnorm(n)
head(x)
[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499</pre>			<p>In the preceding code, we did the following:</p>
			<ul>
				<li><code>set.seed(123)</code>: This line is all about ensuring that the random numbers that are generated in your code are consistent each time you run it. By setting a seed value (in this case, <code>123</code>), you ensure that the random numbers that are generated by your code will be the same every time you run it. This is useful for reproducibility in your analysis.</li>
				<li><code>n &lt;- 25</code>: Here, you’re defining a variable called <code>n</code> and setting its value to <code>25</code>. This variable represents the number of data points you want to generate in your random time series.</li>
				<li><code>x &lt;- rnorm(n)</code>: This is where the actual data generation happens. You’re creating a new variable, <code>x</code>, and using the <code>rnorm()</code> function to generate random numbers. These numbers are drawn from a Gaussian (or normal) distribution, which is often<a id="_idIndexMarker781"/> called a bell curve. <code>n</code> specifies the number of random data points to generate, which in this case is 25.</li>
				<li><code>head(x)</code>: Finally, you’re using the <code>head()</code> function to display the first few values of the <code>x</code> variable. This helps you quickly inspect what the generated data looks like. It’s a handy way to get a glimpse of your data without printing the entire dataset.</li>
			</ul>
			<p>To summarize, this code sets a random seed for reproducibility, specifies the number of data points you want (<code>25</code>), and generates these data points from a Gaussian distribution, storing them in the <code>x</code> variable. Then, it shows the first few values of <code>x</code> using the <code>head()</code> function. This code is often used in data analysis and <a id="_idIndexMarker782"/>statistics when <a id="_idIndexMarker783"/>you need random data to work with or simulate real-world scenarios.</p>
			<p>Now, let’s convert this vector, <code>x</code>, into a time series object using the <code>ts()</code> function:</p>
			<pre class="source-code">
# Make x a ts object
ts_obj &lt;- ts(x)</pre>			<p>Let’s check the class of the newly created object:</p>
			<pre class="source-code">
class(ts_obj)
[1] "ts"</pre>			<p>We must do the same for its structure and attributes:</p>
			<pre class="source-code">
str(ts_obj)
 Time-Series [1:25] from 1 to 25: -0.5605 -0.2302 1.5587 0.0705 0.1293 ...
attributes(ts_obj)
$tsp
[1]  1 25  1
$class
[1] "ts"</pre>			<p>So, what happened exactly? Let’s go over it in a simple yet concise manner.</p>
			<p>This R code does the following:</p>
			<ul>
				<li><code>ts_obj &lt;- ts(x)</code>: This creates a time series object (<code>ts_obj</code>) from a vector or data series, <code>x</code>. This step converts <code>x</code> into a time series format.</li>
				<li><code>class(ts_obj)</code>: This checks and displays the class of <code>ts_obj</code>. This should return <code>ts</code>, indicating that <code>ts_obj</code> is indeed a time series.</li>
				<li><code>str(ts_obj)</code>: This line displays the structure of <code>ts_obj</code>, providing information about the time series. In this case, it shows that the time series has 25 data points ranging from 1 to 25, along with the values themselves.</li>
				<li><code>attributes(ts_obj)</code>: This shows the attributes of the time series object. In this case, it displays the time span (<code>tsp</code>) with values of <code>1 25 1</code>, which means that the time series starts from period 1 and ends at period 25 with a frequency of 1.</li>
			</ul>
			<p>So, this code essentially takes a vector, <code>x</code>, converts it into a time series object, and then provides information about the<a id="_idIndexMarker784"/> class and structure of the resulting time series. Now, let’s visualize the time series by using the <code>plot</code> function. We can do this with <code>plot(ts_obj)</code>:</p>
			<div><div><img src="img/B19142_10_1.jpg" alt="Figure 10.1 – Plot of a time series object from rnorm(25)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Plot of a time series object from rnorm(25)</p>
			<p>Now that we have covered <a id="_idIndexMarker785"/>how to <a id="_idIndexMarker786"/>coerce a vector into a time series object, we can discuss how to change the start, end, and frequency parameters.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor198"/>Manipulating the time series parameters</h2>
			<p>Now, we are going to use the<a id="_idIndexMarker787"/> previous vector we created from <code>rnorm()</code> and convert it into different time series objects with different starts, ends, and frequencies. This sort of thing is very simple to do in base R.</p>
			<p>Let’s see some code manipulation first; then, we will go through and explain the examples:</p>
			<pre class="source-code">
# Change Start
ts(x, start = 1980)
ts(x, start = c(1980, 05))
ts(x, start = 1980, frequency = 12)
ts(x, start = 1980, frequency = 12/3)
# Change End
ts(x, end = 2023)
ts(x, end = 2023, frequency = 12)
ts(x, end = 2023, frequency = 12/3)
      Qtr1         Qtr2         Qtr3         Qtr4
2017 -0.56047565  -0.23017749   1.55870831   0.07050839
2018  0.12928774   1.71506499   0.46091621  -1.26506123
2019 -0.68685285  -0.44566197   1.22408180   0.35981383
2020  0.40077145   0.11068272  -0.55584113   1.78691314
2021  0.49785048  -1.96661716   0.70135590  -0.47279141
2022 -1.06782371  -0.21797491  -1.02600445  -0.72889123
2023 -0.62503927</pre>			<p>Here are the explanations for each of the preceding examples. The last one in the preceding code block shows the output of the last manipulation:</p>
			<ul>
				<li><code>ts(x, start = 1980)</code>: This creates a time series with a start date in <code>1980</code>. The exact month and day are not specified, so the default is January 1, 1980.</li>
				<li><code>ts(x, start = c(1980, 05))</code>: This sets the start date to May 1980 (year and month) explicitly.</li>
				<li><code>ts(x, start = 1980, frequency = 12)</code>: This line creates a time series with a start date in <code>1980</code> and a monthly frequency, indicating that each data point represents one month.</li>
				<li><code>ts(x, start = 1980, frequency = 12/3)</code>: This sets the start year to <code>1980</code> and specifies a frequency of 4, which means each data point is a quarter (three months) apart.</li>
				<li><code>ts(x, end = 2023)</code>: This creates a time series with an end date in <code>2023</code>. The start date is not specified here, so it defaults to the beginning of the series.</li>
				<li><code>ts(x, end = 2023, frequency = 12)</code>: This line indicates that the end date is <code>2023</code> with a monthly frequency, assuming each data point represents one month.</li>
				<li><code>ts(x, end = 2023, frequency = 12/3)</code>: This sets the end year to <code>2023</code> and specifies a frequency of 4, meaning each data point is a quarter (three months) apart.</li>
			</ul>
			<p>These variations allow you to control<a id="_idIndexMarker788"/> the temporal characteristics of your time series data, such as the start and end points and the frequency of observations. Now that we have generated some data, let’s plot it out.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor199"/>Time series plotting</h1>
			<p>In this section, we will cover<a id="_idIndexMarker789"/> plotting time series objects, along with plotting some diagnostics such as decomposition. These plots include time series plots <a id="_idIndexMarker790"/>themselves, <code>readxl</code> package:</p>
			<pre class="source-code">
# Read the airpassengers.xlsx file in and convert to a ts object starting at 1949
ap_ts &lt;- read_xlsx("./Chapter 10/airpassengers.xlsx")  |&gt;
  ts(start = 1949, frequency = 12)
# Plot the ts object
plot(ap_ts)</pre>			<p>This produces the following chart:</p>
			<div><div><img src="img/B19142_10_2.jpg" alt="Figure 10.2 – Visualizing the AirPassengers time series dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Visualizing the AirPassengers time series dataset</p>
			<p>From here, it is easy to see that the data has a trend and a seasonal cycle component. This observation will lead us to our next visual. We will decompose the data into its parts and visualize the decomposition. The decomposition of the time series breaks the data down into the following parts:</p>
			<ul>
				<li>The observed data</li>
				<li>The trend of the data</li>
				<li>The seasonal cycle of the data</li>
				<li>The remaining “randomness” or “residual/remainder”</li>
			</ul>
			<p>Let’s go ahead and see what it looks like. First, we have the code:</p>
			<pre class="source-code">
plot(decompose(ap_ts))</pre>			<p>Here’s the plot:</p>
			<div><div><img src="img/B19142_10_3.jpg" alt="Figure 10.3 – Plot of a decomposed time series"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Plot of a decomposed time series</p>
			<p>Now that we have<a id="_idIndexMarker792"/> visualized the decomposition, we can start analyzing the ACF and PACF plots.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor200"/>Creating ACF and PACF plots in R</h2>
			<p>In this <a id="_idIndexMarker793"/>section, we <a id="_idIndexMarker794"/>are going to<a id="_idIndexMarker795"/> cover ACF and PACF <a id="_idIndexMarker796"/>plots. An ACF plot is an autocorrelation function plot. An autocorrelation function is the relationship between a current observation in time against a previous observation in time. It tells you how correlated a current observation is compared to different lags of the same time series. So, if there is a strong seasonal demand for beer, you will see a strong correlation for them at the same seasonal period before it.</p>
			<p>First, let’s look at the output of the <code>acf()</code> function:</p>
			<pre class="source-code">
acf(ap_ts)</pre>			<p>Here’s the plot for it:</p>
			<div><div><img src="img/B19142_10_4.jpg" alt="Figure 10.4 – ACF plot of the AirPassengers data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – ACF plot of the AirPassengers data</p>
			<p>Here, we can see<a id="_idIndexMarker797"/> that all of the<a id="_idIndexMarker798"/> points are<a id="_idIndexMarker799"/> significantly related to their previous points because the data itself is trending upward. However<a id="_idIndexMarker800"/>, we can also see that there are peaks and valleys in the data, which is representative of the seasonal correlations.</p>
			<p>Now, let’s take a look at the PACF plot, which is also generated by the <code>acf()</code> function where the type is set to “partial.”</p>
			<p>The partial autocorrelation is the relationship between an observation, <em class="italic">t</em>, and some observation, <em class="italic">t-n</em>. With a relationship between <em class="italic">t</em> and the previous observations through <em class="italic">t-n</em> removed, the partial autocorrelation is the result of removing the effects of the terms at shorter lags. Now, let’s look at the PACF of the same time series, <code>acf(ap_ts, type = "</code><code>partial")</code>:</p>
			<div><div><img src="img/B19142_10_5.jpg" alt="Figure 10.5 – PACF of the AirPassengers data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – PACF of the AirPassengers data</p>
			<p>With that, we’ve gone over how to quickly create ACF and PACF plots in R. We have also provided a very quick <a id="_idIndexMarker801"/>overview<a id="_idIndexMarker802"/> of what<a id="_idIndexMarker803"/> they<a id="_idIndexMarker804"/> are. Now, let’s learn how to model the time series with the <code>healthyR.ts</code> library.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor201"/>Auto ARIMA modeling with healthyR.ts</h1>
			<p>Time series, just like <a id="_idIndexMarker805"/>any other set <a id="_idIndexMarker806"/>of data, can be modeled. The methods are vast, both old and new. In this section, we are going to discuss ARIMA modeling, and more specifically building an automatic ARIMA model with the <code>healthyR.ts</code> library in R. ARIMA models themselves attempt to describe the autocorrelations in the data.</p>
			<p>In this section, we will use a workflow that ends with the <code>ts_auto_arima()</code> function creating and fitting a tuned model. This model requires our data to be in tibble format. So, to do this, we will use the AirPassengers dataset and make sure it is a tibble.</p>
			<p>Let’s get started with the dataset we have already brought in and coerce it into a tibble:</p>
			<pre class="source-code">
library(healthyR.ts)
library(dplyr)
library(timetk)
ap_tbl &lt;- ts_to_tbl(ap_ts) |&gt;
  select(-index)
&gt; class(ap_tbl)
[1] "tbl_df"      "tbl"           "data.frame"</pre>			<p>In the preceding code, we loaded the <code>healthyR.ts</code> and <code>dplyr</code> libraries and then took the already existing <code>ap_ts</code> time series object and converted it into a tibble using the <code>ts_to_tbl()</code> function from the <code>healthyR.ts </code>library. The next thing we have to do is create a train/test split using the <code>time_series_split()</code> function from the <code>timetk</code> library:</p>
			<pre class="source-code">
# Time Series Split
splits &lt;- time_series_split(
  data = ap_tbl
  , date_var = date_col
  , assess = 12
  , skip = 3
  , cumulative = TRUE
)
&gt; splits
&lt;Analysis/Assess/Total&gt;
&lt;132/12/144&gt;</pre>			<p>Now that we have created the analysis/assess data splits, we are ready to run the main function. This function is intended to be a boilerplate function in that it will do most things automatically. However, you can always take the model and do what you wish with it. These boilerplate functions from <code>healthyR.ts</code> are <a id="_idIndexMarker807"/>not meant <a id="_idIndexMarker808"/>to be the end-all-be-all.</p>
			<p>Let’s run the function now and then take a look at the outputs:</p>
			<pre class="source-code">
Library(modeltime)
ts_auto_arima &lt;- ts_auto_arima(
  .data = ap_tbl,
  .num_cores = 10,
  .date_col = date_col,
  .value_col = x,
  .rsamp_obj = splits,
  .formula = x ~ .,
  .grid_size = 20,
  .cv_slice_limit = 5,
  .tune = TRUE
)</pre>			<p>In the preceding, we have provided the tibble that holds the data and the number of cores that we want to use – in our case, <code>10</code>. Next, we supplied the column that holds the date – in this case, <code>date_col</code> – with the value column being <code>x</code> from our supplied tibble. Next up was our resampling object, splits, and then our formula, which gets passed to the recipe function internally to the <code>ts_auto_arima()</code> function. In this case, it is <code>x</code> against the date. We supply a grid size of <code>20</code> to make the tuning grid and a slice limit of <code>5</code> so that no more than five slices of data are generated. Most importantly, we set the <code>.tune</code> parameter to <code>TRUE</code>. This instructs the function to go ahead with model tuning. Model tuning can result in it taking a few seconds/minutes for data to be returned.</p>
			<p>Let’s look at the output. The first thing that we will look at is the recipe information:</p>
			<pre class="console">
&gt; ts_auto_arima$recipe_info
$recipe_call
recipe(.data = ap_tbl, .date_col = date_col, .value_col = x,
     .formula = x ~ ., .rsamp_obj = splits, .tune = TRUE, .grid_size = 20,
     .num_cores = 10, .cv_slice_limit = 5)
$recipe_syntax
[1] "ts_arima_recipe &lt;-"
[2] "\n  recipe(.data = ap_tbl, .date_col = date_col, .value_col = x, .formula = x ~ \n     ., .rsamp_obj = splits, .tune = TRUE, .grid_size = 20, .num_cores = 10, \n     .cv_slice_limit = 5)"
$rec_obj
── Recipe ────────────────────────────────────────────────────────────────────────────────────────
── Inputs
Number of variables by role
outcome:   1
predictor: 1</pre>			<p>So, from the preceding<a id="_idIndexMarker809"/> output, we<a id="_idIndexMarker810"/> can see that we have both a single outcome and a predictor variable. This is all we need for an auto ARIMA model with no exogenous regressors.</p>
			<p>Now, let’s look at the model information:</p>
			<pre class="source-code">
ts_auto_arima
&gt; ts_auto_arima$model_info
$model_spec
ARIMA Regression Model Specification (regression)
Main Arguments:
  seasonal_period = tune::tune()
  non_seasonal_ar = tune::tune()
  non_seasonal_differences = tune::tune()
  non_seasonal_ma = tune::tune()
  seasonal_ar = tune::tune()
  seasonal_differences = tune::tune()
  seasonal_ma = tune::tune()
Computational engine: arima</pre>			<p>In the preceding code, we can see that all the parameters of the model are set to <code>tune::tune()</code>. This will allow the model to be run through the tuning grid:</p>
			<pre class="source-code">
$wflw
══ Workflow
Preprocessor: Recipe
Model: arima_reg()
── Preprocessor
0 Recipe Steps
── Model
ARIMA Regression Model Specification (regression)
Main Arguments:
  seasonal_period = tune::tune()
  non_seasonal_ar = tune::tune()
  non_seasonal_differences = tune::tune()
  non_seasonal_ma = tune::tune()
  seasonal_ar = tune::tune()
  seasonal_differences = tune::tune()
  seasonal_ma = tune::tune()
Computational engine: arima</pre>			<p>The following workflow <a id="_idIndexMarker811"/>object that’s <a id="_idIndexMarker812"/>created shows that the recipe has no steps to it as no transformations of any type are being performed:</p>
			<pre class="console">
$fitted_wflw
…
── Model
Series: outcome
ARIMA(4,1,2)(1,0,1)[12]
Coefficients:
       ar1    ar2     ar3      ar4     ma1      ma2     sar1     sma1
     -0.221  0.9020  0.0894  -0.2144  0.0477  -0.9523  0.9695  -0.0869
s.e.  0.092  0.0996  0.0958   0.0875  0.0367   0.0365  0.0143   0.0927
sigma^2 = 99.46:  log likelihood = -497.36
AIC=1012.72   AICc=1014.21   BIC=1038.6
$was_tuned
[1] "tuned"</pre>			<p>The fitted workflow object shows that the best model that was selected was the <code>ARIMA(4,1,2)(1,0,1)[12]</code> model. It also gives us our coefficients and model AIC.</p>
			<p>Next, we will take a look at the <code>model_calibration</code> object of the return output:</p>
			<pre class="console">
&gt; ts_auto_arima$model_calibration
$plot
$calibration_tbl
# Modeltime Table
# A tibble: 1 × 5
  .model_id .model.model_desc.type .calibration_data
        &lt;int&gt; &lt;list&gt;      &lt;chr&gt;                           &lt;chr&gt; &lt;list&gt;
1            1 &lt;workflow&gt; ARIMA(4,1,2)(1,0,1)[12] Test  &lt;tibble [12 × 4]&gt;
$model_accuracy
# A tibble: 1 × 9
  .model_id .model_desc.type   mae  mape  mase smape  rmse   rsq
        &lt;int&gt; &lt;chr&gt;                           &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1            1 ARIMA(4,1,2)(1,0,1)[12] Test   16.2  3.35 0.335  3.35  19.5 0.960</pre>			<p>Here’s the <a id="_idIndexMarker813"/>resulting<a id="_idIndexMarker814"/> plot:</p>
			<div><div><img src="img/B19142_10_6.jpg" alt="Figure 10.6 – Auto ARIMA calibration plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Auto ARIMA calibration plot</p>
			<p>Lastly, we are going to go over the <code>tuned_model</code> object of the return:</p>
			<pre class="source-code">
&gt; ts_auto_arima$tuned_info
$tuning_grid
# A tibble: 20 × 7
   seasonal_period non_seasonal_ar non_seasonal_differences non_seasonal_ma seasonal_ar
   &lt;chr&gt;         &lt;int&gt;           &lt;int&gt;         &lt;int&gt;       &lt;int&gt;
 1 weekly          3               0            1             2
 2 yearly          5               1            4               0
# ℹ 2 more variables: seasonal_differences &lt;int&gt;, seasonal_ma &lt;int&gt;
$tscv
# Time Series Cross Validation Plan
# A tibble: 5 × 2
  splits               id
  &lt;list&gt;               &lt;chr&gt;
1 &lt;split [120/12]&gt; Slice1
2 &lt;split [117/12]&gt; Slice2
3 &lt;split [114/12]&gt; Slice3
4 &lt;split [111/12]&gt; Slice4
5 &lt;split [108/12]&gt; Slice5
$tuned_results
# Tuning results
# NA
# A tibble: 5 × 4
  splits               id      .metrics                 .notes
  &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;                    &lt;list&gt;
1 &lt;split [120/12]&gt; Slice1 &lt;tibble [120 × 11]&gt; &lt;tibble [1 × 3]&gt;
2 …
$grid_size
[1] 20
$best_metric
[1] "rmse"
$best_result_set
# A tibble: 1 × 13
  seasonal_period non_seasonal_ar non_seasonal_differences non_seasonal_ma seasonal_ar
  &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;         &lt;int&gt;         &lt;int&gt;
1 yearly             4                1              2               1
# ℹ 8 more variables: seasonal_differences &lt;int&gt;, seasonal_ma &lt;int&gt;, .metric &lt;chr&gt;,
#   .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;
$tuning_grid_plot
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
$plotly_grid_plot</pre>			<p>From the preceding code, we can see that there is a great deal of information that comes from this portion of what is returned from the function. This should contain everything you need to make an informed decision about whether you should keep tinkering or move on.</p>
			<p>Let’s look at the last plot, which is returned as both a static <code>ggplot2</code> object and a <code>plotly</code> object:</p>
			<div><div><img src="img/B19142_10_7.jpg" alt="Figure 10.7 – The tuning grid of the auto ARIMA model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – The tuning grid of the auto ARIMA model</p>
			<p>With this done, let<a id="_idIndexMarker815"/> us now<a id="_idIndexMarker816"/> see how Brownian motion concept works.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor202"/>Creating a Brownian motion with healthyR.ts</h1>
			<p>The final time series<a id="_idIndexMarker817"/> plot that <a id="_idIndexMarker818"/>we are going to showcase is the <strong class="bold">Brownian motion</strong>. Brownian motion, also known as the <em class="italic">Wiener process</em>, is a fundamental <a id="_idIndexMarker819"/>concept in finance and mathematics that describes the random movement of particles in a fluid. In the context of finance, it is often used to model the price movement of financial instruments such as stocks, commodities, and currencies.</p>
			<p>Here are some of the key<a id="_idIndexMarker820"/> characteristics of Brownian motion:</p>
			<ul>
				<li><strong class="bold">Randomness</strong>: Brownian motion is inherently random. The future direction and magnitude of movement at any point in time cannot be predicted with certainty.</li>
				<li><strong class="bold">Continuous path</strong>: The path of a Brownian motion is continuous, meaning that the asset’s price can move smoothly without sudden jumps or gaps.</li>
				<li><strong class="bold">Independent increments</strong>: The changes in the asset’s price over non-overlapping time intervals are independent of each other. In other words, the price movement in one interval does not affect the price movement in another.</li>
				<li><strong class="bold">Gaussian distribution</strong>: The increments of a Brownian motion (that is, the changes in price) are normally distributed, following a Gaussian or normal distribution. This is in line with the notion that in financial markets, small price changes are more common than large ones.</li>
				<li><strong class="bold">Constant variance</strong>: The variance of the price increments remains constant over time. This is sometimes referred <a id="_idIndexMarker821"/>to as the <strong class="bold">homoscedastic</strong> property.</li>
			</ul>
			<p>Mathematically, the movement of an asset’s price, <em class="italic">S(t)</em>, over time, <em class="italic">t</em>, can be described using the stochastic differential equation:</p>
			<p>Here, we have the following:</p>
			<ul>
				<li><em class="italic">dS(t)</em> is the infinitesimal change in the asset’s price over a small time interval, <em class="italic">dt</em></li>
				<li><em class="italic">μ</em> is the drift or expected average rate of return of the asset over time</li>
				<li><em class="italic">σ</em> is the volatility of the asset, representing the standard deviation of its price changes</li>
				<li><em class="italic">dW(t)</em> is the Wiener process, representing a random increment</li>
			</ul>
			<p>The Brownian motion<a id="_idIndexMarker822"/> is a cornerstone <a id="_idIndexMarker823"/>of financial models such as the Black-Scholes option pricing model and the Vasicek interest rate model, among others. It helps in understanding and estimating the behavior of financial instruments by capturing their inherent randomness and volatility. However, it’s important to note that real financial markets can deviate from a perfect Brownian motion due to factors such as market sentiment, news, and other external influences.</p>
			<p>We can quickly generate a Brownian motion and plot its output with the <code>healthyR.ts</code> library. Here’s the code:</p>
			<pre class="source-code">
library(healthyR.ts)
ts_brownian_motion() |&gt;
  ts_brownian_motion_plot(t, y)</pre>			<p>Here’s the output of the preceding code. This is a random process, so your output will likely not match:</p>
			<div><div><img src="img/B19142_10_8.jpg" alt="Figure 10.8 – Creating and viewing a Brownian motion with the healthyR.ts library"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Creating and viewing a Brownian motion with the healthyR.ts library</p>
			<p>Now that we have <a id="_idIndexMarker824"/>gone over <a id="_idIndexMarker825"/>time series data in R, let’s go over to Python and see how it is done there.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor203"/>Time series analysis in Python – statistics, plots, and forecasting</h1>
			<p>Before diving <a id="_idIndexMarker826"/>into time series analysis, it’s <a id="_idIndexMarker827"/>crucial to have data to work with. In this section, we’ll walk through the process of creating mock time series data, saving it to an Excel file, and then reading it back into pandas. This will serve as our foundation for the upcoming time series analysis.</p>
			<p>As always, we’ll start by loading the relevant libraries:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt</pre>			<p>Then, we must create the sample data and save it to Excel so that it can be used in the rest of this chapter:</p>
			<pre class="source-code">
# Create a date range
date_rng = pd.date_range(start='2022-01-01', end='2023-12-31',
    freq='D')
# Create a trend component
trend = 0.05 * np.arange(len(date_rng))
# Create a seasonal component (cyclicality)
seasonal = 2.5 * np.sin(2 * np.pi * np.arange(len(date_rng)) / 365)
# Add some random noise
noise = np.random.normal(0, 0.5, len(date_rng))
# Combine all components to create the time series
time_series = trend + seasonal + noise
# Create a DataFrame
df = pd.DataFrame({'Date': date_rng, 'Value': time_series})
# Save the data to an Excel file
df.to_excel('time_series_data.xlsx', index=False)</pre>			<p>Finally, we must load the data from Excel and have a look at the loaded data, as follows:</p>
			<pre class="source-code">
# Read the data back into pandas
loaded_df = pd.read_excel('time_series_data.xlsx')
# Display the first few rows
print(loaded_df.head())</pre>			<p>In this example, we generate a synthetic time series with a linear trend, sine-wave seasonal component, and random noise. This <a id="_idIndexMarker828"/>kind of<a id="_idIndexMarker829"/> dataset is more representative of real-world time series data, where patterns often involve a combination of these elements. The dataset is then saved to an Excel file, and you can read it back into Python for analysis as needed.</p>
			<p>As we discussed in <a href="B19142_06.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>, the first step of analysis is plotting (insert sinister laugh)!</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor204"/>Time series plotting – basic plots and ACF/PACF plots</h1>
			<p>Visualizing time series<a id="_idIndexMarker830"/> data is a crucial step in understanding its underlying patterns and trends. In this section, we’ll explore various time series plots and how to create them using Python. These visualizations help us gain insights into seasonality, trends, and autocorrelation within the time series data.</p>
			<p>We’ll start by loading the required libraries:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf</pre>			<p>Then, we must load the data from Excel and ensure the date information is converted correctly:</p>
			<pre class="source-code">
# Load time series data (replace 'time_series_data.xlsx' with your data file)
data = pd.read_excel('time_series_data.xlsx')
# Convert the 'Date' column to datetime format and set it as the index
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)</pre>			<p>Now, we can create a basic plot of the time series for a first look:</p>
			<pre class="source-code">
# Plot the time series
plt.figure(figsize=(12, 6))
plt.plot(data['Value'])
plt.title('Time Series Plot')
plt.xlabel('Date')
plt.ylabel('Value')
plt.grid(True)
plt.show()</pre>			<p>This is what the<a id="_idIndexMarker831"/> plot looks like:</p>
			<div><div><img src="img/B19142_10_9.jpg" alt="Figure 10.9 - Caption"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 - Caption</p>
			<p>Next, we must create the more advanced ACF and PACF plots with the following code:</p>
			<pre class="source-code">
# ACF and PACF plots
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
# ACF plot
plot_acf(data['Value'], lags=10, ax=ax1)
ax1.set_title('Autocorrelation Function (ACF)')
# PACF plot
plot_pacf(data['Value'], lags=40, ax=ax2)
ax2.set_title('Partial Autocorrelation Function (PACF)')
plt.tight_layout()
plt.show()</pre>			<p>The resulting plots are shown here:</p>
			<div><div><img src="img/B19142_10_10.jpg" alt="Figure 10.10 – ACF and PACF plots for lags 1 and 2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – ACF and PACF plots for lags 1 and 2</p>
			<p>Here, we started by<a id="_idIndexMarker832"/> loading the time series data from an Excel file and converting the <strong class="bold">Date</strong> column into a datetime index. Then, we created a time series plot to visualize the data over time. Additionally, we generated ACF and PACF plots to explore autocorrelation patterns within the time series. These plots are valuable for identifying potential lag values for time series models.</p>
			<p>Now, let’s have a look at how ACF and PACF plots help with gaining insights into time series data. In particular, we’ll see how they can be used to understand seasonality, trends, and autocorrelation.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor205"/>Autocorrelation function (ACF) plot</h2>
			<p>The ACF plot shows the <a id="_idIndexMarker833"/>correlation between a time series and its lagged values. In other words, it quantifies how well the current value of the series is correlated with its past values.</p>
			<p>Peaks in the ACF plot at specific lags indicate potential seasonality or periodic patterns in the data. For instance, a significant peak at lag 7 in a daily series suggests a weekly seasonality, while a peak at lag 12 in a monthly series may indicate a yearly seasonality.</p>
			<p>By analyzing the ACF plot, you can identify the lag values at which correlations significantly deviate from zero. These lags can provide insights into the seasonality of the data. Additionally, identifying exponential decay in ACF can suggest an autoregressive component in your time series model.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor206"/>Partial autocorrelation function (PACF) plot</h2>
			<p>The PACF plot measures the<a id="_idIndexMarker834"/> direct relationship between a time series and its lagged values while removing the effects of intermediate lags.</p>
			<p>Peaks in the PACF plot at specific lags indicate the number of lagged values that directly impact the current value of the series. For example, a significant peak at lag 1 and no significant peaks beyond that suggest a<a id="_idIndexMarker835"/> first-order <strong class="bold">autoregressive</strong> (<strong class="bold">AR</strong>) process.</p>
			<p>The PACF plot helps in determining the order of autoregressive terms (p) in ARIMA models. It can also reveal abrupt changes in the time series, indicating structural breaks or shifts.</p>
			<p>By analyzing these plots in conjunction with your time series data, you can gain valuable insights into the presence of seasonality, trends, and autocorrelation. These insights are instrumental in selecting appropriate time series models and making accurate forecasts.</p>
			<p>The plots shown in <em class="italic">Figure 10</em><em class="italic">.9</em> show that for the PACF plot, only lag 1 has a strong autocorrelation, while the ACF plot is tricked by the way the random data was generated and shows a high autocorrelation across all lags investigated.</p>
			<p>With the insights gained from these plots, we can move on to the meat of the matter: statistical analysis and forecasting</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor207"/>Time series statistics and statistical forecasting</h1>
			<p>Data exploration and <a id="_idIndexMarker836"/>statistical analysis are<a id="_idIndexMarker837"/> crucial steps in understanding the characteristics of time series data. In this section, we’ll walk you through how to perform data exploration and apply statistical analysis techniques in Python to gain valuable insights into your time series.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor208"/>Statistical analysis for time series data</h2>
			<p>After exploring the data <a id="_idIndexMarker838"/>using the plots in the previous section, let’s move on to statistical analysis to gain a deeper understanding. This section focuses on two areas:</p>
			<ul>
				<li><strong class="bold">The Augmented Dickey-Fuller (ADF) test</strong>: This statistical test is used to determine whether the time series data is stationary. Stationary data is easier to model and forecast.</li>
				<li><strong class="bold">Time series decomposition</strong>: Time series decomposition separates the data into its constituent components: trend, seasonality, and residuals. This decomposition aids in isolating patterns for forecasting.</li>
			</ul>
			<p>We’ll understand both of these in the following sections.</p>
			<h3>The ADF test</h3>
			<p>The ADF test is a<a id="_idIndexMarker839"/> statistical <a id="_idIndexMarker840"/>method that’s used to evaluate the <em class="italic">stationarity</em> of a time series. Stationarity is a fundamental concept in time series analysis as it simplifies the modeling process. A stationary time series has statistical properties, such as constant mean and variance, that do not change over time. Non-stationary data, on the other hand, exhibits trends or seasonality, making it challenging to model and forecast accurately.</p>
			<p>In the ADF test, the null hypothesis (H0) assumes that the data is non-stationary. The alternative hypothesis (H1) suggests that the data is stationary. By analyzing the p-value obtained from the test, you can determine whether to reject the null hypothesis. A low p-value (typically less than 0.05) indicates that the data is stationary, while a high p-value suggests non-stationarity. Therefore, when conducting the ADF test, a p-value less than 0.05 is an indicator of a stationary time series.</p>
			<p>Here is a code sample that implements the ADF test in Python using our trusty <code>statsmodels</code> library:</p>
			<pre class="source-code">
from statsmodels.tsa.stattools import adfuller
import pandas as pd
# Read the data back into pandas
df = pd.read_excel('time_series_data.xlsx')
# Augmented Dickey-Fuller Test
adf_result = adfuller(df['Value'])
print("\nAugmented Dickey-Fuller Test:")
print(f"ADF Statistic: {adf_result[0]}")
print(f"P-value: {adf_result[1]}")
print("Null Hypothesis (H0): Data is non-stationary")
print("Alternative Hypothesis (H1): Data is stationary")
if adf_result[1] &lt;= 0.05:
     print("Result: Reject the null hypothesis. Data is stationary.")
else:
     print("Result: Failed to reject the null hypothesis. Data is non-stationary.")</pre>			<p>Not surprisingly, we failed to reject the null hypothesis as the time series data we generated earlier has a clear linear trend and hence it is not stationary.</p>
			<p>For further insights, we can analyze the detailed output further:</p>
			<div><div><img src="img/B19142_10_11.jpg" alt="Figure 10.11 – ADF test results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – ADF test results</p>
			<p>As you can see, the<a id="_idIndexMarker841"/> result<a id="_idIndexMarker842"/> of the ADF test, as returned by <code>adfuller</code>, contains several components:</p>
			<ul>
				<li><code>-0.24536819384067887</code>, in our case) is the test statistic. This statistic is used to assess the null hypothesis that a unit root is present in a time series dataset. The more negative (or less positive) the test statistic, the stronger the evidence against the null hypothesis.</li>
				<li><code>0.9328933413659218</code>, in our case) is the p-value associated with the test statistic. It represents the probability of observing the given test statistic if the null hypothesis were true. A small p-value (typically less than 0.05) suggests rejecting the null hypothesis in favor of stationarity.</li>
				<li><code>9</code>, in our case) represents the number of lags used in the regression when estimating the ADF test.</li>
				<li><code>720</code>, in our case) represents the number of observations used in the ADF regression.</li>
				<li><strong class="bold">Dictionary</strong>: The next part of the result is a dictionary containing critical values for different confidence levels (1%, 5%, and 10%). These critical values are used to determine the significance of the test statistic.</li>
				<li><code>1208.8292254446185</code>, in our case) is the maximized IC. It represents a measure of the goodness-of-fit of the model. Lower values of the IC indicate <a id="_idIndexMarker844"/>a better fit.</li>
			</ul>
			<p>In summary, you would typically interpret the ADF test result by focusing on the test statistic and p-value. If the test statistic <a id="_idIndexMarker845"/>is more <a id="_idIndexMarker846"/>negative (or less positive) and the p-value is small (typically less than 0.05), it suggests rejecting the null hypothesis of a unit root and concluding that the time series is stationary.</p>
			<h3>Time series decomposition</h3>
			<p>Time series decomposition<a id="_idIndexMarker847"/> is a<a id="_idIndexMarker848"/> technique that’s used to break down a time series dataset into its key components: trend, seasonality, and residuals. These components provide valuable insights into the underlying patterns of the time series, making it easier to understand and forecast.</p>
			<p>Let’s understand each of these<a id="_idIndexMarker849"/> components:</p>
			<ul>
				<li><strong class="bold">Trend</strong>: The trend component represents the underlying long-term movement or tendency in the data. It captures the overall direction – that is, whether data is increasing or decreasing over time.</li>
				<li><strong class="bold">Seasonality</strong>: Seasonality refers to the repeating patterns in the data at fixed intervals. These could be daily, weekly, monthly, or yearly patterns, depending on the data. Detecting seasonality is crucial for understanding periodic trends and adjusting for them during forecasting.</li>
				<li><strong class="bold">Residuals</strong>: Residuals are the irregular or random components of the time series data. They represent what remains after removing the trend and seasonality. Analyzing residuals helps identify any leftover patterns or unusual events in the data.</li>
			</ul>
			<p>In our code, we apply time series decomposition to break down the time series into its constituent components, and we visualize each component. This process allows us to understand the structure of the data, making it easier to identify underlying patterns for forecasting.</p>
			<p>Let’s have a look at an implementation for time series decomposition in Python:</p>
			<pre class="source-code">
from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.pyplot as plt
# Time Series Decomposition
decomposition = seasonal_decompose(df['Value'],
    model='additive',  period=365)
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
# Plot the decomposition components
plt.figure(figsize=(12, 8))
plt.subplot(411)
plt.plot(df['Date'], df['Value'], label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(df['Date'], trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(df['Date'], seasonal, label='Seasonal')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(df['Date'], residual, label='Residual')
plt.legend(loc='best')
plt.suptitle("Time Series Decomposition")
plt.show()</pre>			<p>The resulting plot is <a id="_idIndexMarker850"/>self-explanatory (especially as <a id="_idIndexMarker851"/>it reflects how the data was artificially generated):</p>
			<p class="IMG---Figure">/</p>
			<div><div><img src="img/B19142_10_12.jpg" alt="Figure 10.12 – The time series decomposed"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – The time series decomposed</p>
			<p>In this section, we have covered the basic statistics that can help you understand your time series and the core techniques to handle the most typical challenges to modeling: decomposition and how to interpret the<a id="_idIndexMarker852"/> components. With a deeper understanding of our time series<a id="_idIndexMarker853"/> thanks to the plots provided<a id="_idIndexMarker854"/> and statistical analysis, we <a id="_idIndexMarker855"/>can move on to the highest value-added step: prediction.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor209"/>Understanding predictive modeling approaches</h1>
			<p>In this section, we’ll delve into predictive modeling approaches<a id="_idIndexMarker856"/> using two powerful Python libraries – <code>statsmodels</code> and <code>prophet</code>.</p>
			<p>These libraries provide diverse tools to tackle time series forecasting, enabling you to make informed decisions and predictions based on your time series data.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor210"/>Forecasting with statsmodels</h2>
			<p><code>statsmodels</code> is a popular library<a id="_idIndexMarker857"/> in the Python ecosystem<a id="_idIndexMarker858"/> that offers a wide range of statistical tools, including time series analysis. For forecasting, it provides functionality for building ARIMA models. ARIMA models are a staple in time series analysis, allowing you to capture and model complex patterns within your data.</p>
			<p>Building an ARIMA model with <code>statsmodels</code> involves selecting the appropriate order of differencing, autoregressive components, and moving average components to best represent the underlying patterns of the data. Once the model has been established, you can make forecasts and evaluate its performance.</p>
			<p>Finally, it is very important to note that since the time series we have is not stationary, we should be modeling the changes or differences in our time series.</p>
			<p>Let’s have a look at the workflow in code!</p>
			<ol>
				<li>First, import the necessary libraries and load the data:<pre class="source-code">
# Import necessary libraries
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import norm
import matplotlib.pyplot as plt
# Load the time series data (replace with your data)
time_series_data = pd.read_excel('time_series_data.xlsx')['Value']</pre></li>				<li>Then, check for stationarity<a id="_idIndexMarker859"/> and decide if we want<a id="_idIndexMarker860"/> to model the time series itself or the differences:<pre class="source-code">
# Perform the Augmented Dickey-Fuller test to check for stationarity
result = sm.tsa.adfuller(time_series_data, autolag='AIC')
# If the p-value is greater than a threshold (e.g., 0.05), perform differencing to make the data stationary
if result[1] &gt; 0.05:
        differenced_data = np.diff(time_series_data, n=1)
else:
        differenced_data = time_series_data</pre></li>				<li>Now (with the differenced data since our time series is not stationary), we can build the actual model and fit it:<pre class="source-code">
# Build an ARIMA model
order = (1, 1, 1)  # Values based on ACF and PACF analysis
model = sm.tsa.ARIMA(differenced_data, order=order)
# Fit the ARIMA model
model_fit = model.fit()</pre></li>				<li>Optionally, we could perform<a id="_idIndexMarker861"/> some hypertuning on the order<a id="_idIndexMarker862"/> parameters instead of basing our decision on the graphs only. However, for this example, this will suffice. Hypertuning is the action of tuning the parameters defining the model rather than the parameters fitted to the data, such as the order of the ARIMA model.</li>
				<li>Now, we can create our forecast with the trained model.<p class="list-inset">Note that since we modeled the differenced data, we need to translate the forecasts back into the actual time series:</p><pre class="source-code">
# Make forecasts
forecast_steps = 50  # Adjust the number of forecast steps as needed
forecast = model_fit.forecast(steps=forecast_steps)
# If the p-value is greater than a threshold (e.g., 0.05), perform differencing to make the data stationary
if result[1] &gt; 0.05:
        # The model was trained on the differenced data so the forecasts have to be added to the last data point
        cumsum_forecasts = np.cumsum(forecast)
        # Add this cumulative sum to the last observed value in your raw data
        real_forecasts = cumsum_forecasts + time_series_data[len(time_series_data)-1]
else:
        real_forecasts = forecast</pre></li>				<li>Then, we must calculate<a id="_idIndexMarker863"/> the basic statistics<a id="_idIndexMarker864"/> and use them to calculate a confidence interval:<pre class="source-code">
# Retrieve ARIMA model parameters
params = model_fit.params
p, d, q = order
resid = model_fit.resid
# Compute the standard errors
stderr = np.std(resid)
# Calculate the confidence intervals
z_score = norm.ppf(0.975)  # For a 95% confidence interval
conf_int = np.column_stack((real_forecasts - z_score * stderr,
        real_forecasts + z_score * stderr))
# Separate the forecasts into point forecasts and confidence intervals
point_forecasts = real_forecasts  # The point forecasts
forecast_stderr = stderr  # The standard errors of the forecasts
lower_bound = conf_int[:, 0]  # Lower confidence interval bounds
upper_bound = conf_int[:, 1]  # Upper confidence interval bounds</pre></li>				<li>Finally, we must plot the forecast<a id="_idIndexMarker865"/> together with the original<a id="_idIndexMarker866"/> time series for visual evaluation of the model:<pre class="source-code">
# Visualize the original time series and forecasts
plt.figure(figsize=(12, 6))
plt.plot(time_series_data, label='Original Time Series', color='blue')
plt.plot(range(len(time_series_data),
    len(time_series_data) + forecast_steps),
    real_forecasts, label='Forecast', color='red')
plt.fill_between(range(len(time_series_data),
    len(time_series_data) + forecast_steps),
    lower_bound, upper_bound, color='pink', alpha=0.5)
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.title('ARIMA Time Series Forecast')
plt.legend()
plt.show()</pre><p class="list-inset">The resulting plot gives us some nice insights:</p></li>			</ol>
			<div><div><img src="img/B19142_10_13.jpg" alt="Figure 10.13 – The time series and the statsmodel forecast"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – The time series and the statsmodel forecast</p>
			<p>We can see that while the trend<a id="_idIndexMarker867"/> is nicely captured and some<a id="_idIndexMarker868"/> of the seasonality also appears, the model is too simplistic to fully capture the nature of the time series.</p>
			<p>To remedy this, we can use a more complex model from a dedicated time series library: <code>prophet</code>.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor211"/>Time series forecasting with Facebook’s prophet</h2>
			<p>The specialized <code>prophet</code> library is designed<a id="_idIndexMarker869"/> explicitly for time<a id="_idIndexMarker870"/> series forecasting tasks. It is known for its ease of use and extensive model selection for a wide variety of forecasting scenarios.</p>
			<p><code>prophet</code> provides an intuitive way to model time series data. It also features tools for hyperparameter optimization and forecasting evaluation.</p>
			<p>The code we will use is also visibly simpler:</p>
			<ol>
				<li>First, we must load the necessary libraries, read the Excel data in, and prepare it by creating a DataFrame with the columns <code>prophet</code> expects:<pre class="source-code">
# Import necessary libraries
import pandas as pd
from prophet import Prophet
from prophet.plot import plot
# Load the time series data (replace with your data)
time_series_data = pd.read_excel('time_series_data.xlsx')
# Create a DataFrame with 'ds' and 'y' columns
df = pd.DataFrame({'ds': time_series_data['Date'], 
    'y': time_series_data['Value']})</pre></li>				<li>Then, we must customize<a id="_idIndexMarker871"/> the model<a id="_idIndexMarker872"/> a little using domain knowledge (in this case, coming from the fact that we generated the data this way):<pre class="source-code">
# Initialize and fit the Prophet model without weekly seasonality
model = Prophet(weekly_seasonality=False)
# Add custom seasonality obtained from domain knowledge (in this case: we generated the data so)
model.add_seasonality(name='custom_season', period=365, 
    fourier_order=5)
# Fit the customized model
model.fit(df)</pre></li>				<li>Finally, we can create our forecast and plot it and its components:<pre class="source-code">
# Create a dataframe for future dates
forecast_steps = 150  # Adjust the number of forecast steps as needed
future = model.make_future_dataframe(periods=forecast_steps,
    freq='D')
# Make predictions
forecast = model.predict(future)
# Plot the forecast
fig = model.plot(forecast)
fig.show()
# Plot components of the forecast (trend, yearly, and weekly seasonality)
fig2 = model.plot_components(forecast)
fig2.show()</pre><p class="list-inset">The resulting plot<a id="_idIndexMarker873"/> shows a much higher<a id="_idIndexMarker874"/> quality forecast than <code>statsmodel</code> with a tight confidence interval. The seasonality is captured visibly, along with the trend:</p></li>			</ol>
			<div><div><img src="img/B19142_10_14.jpg" alt="Figure 10.14 – The time series and the prophet forecast"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – The time series and the prophet forecast</p>
			<p>To dig deeper into the<a id="_idIndexMarker875"/> model fit, let’s have<a id="_idIndexMarker876"/> a look at the components of the fitted time series model:</p>
			<div><div><img src="img/B19142_10_15.jpg" alt="Figure 10.15 – The components of the time series model fitted by prophet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – The components of the time series model fitted by prophet</p>
			<p>As you can see, <code>prophet</code> fits a model in a very simple way without having to guess what the starting parameters are. You can and should use any domain knowledge to improve the initial model.</p>
			<p>In the preceding code, we switched off the default weekly seasonality and added a custom annual seasonality. We can do much more if necessary:</p>
			<ul>
				<li>We can add a custom holiday calendar, for example, using the <code>model.add_country_holidays(country_name='US')</code> command</li>
				<li>We can play around with change points if we are sure they are present but unsure exactly where</li>
				<li>We can hypertune the Fourier order of the custom seasonality</li>
			</ul>
			<p>With the forecasting done with <code>statsmodel</code> and <code>prophet</code>, it’s time to wrap up this section and move on to the next.</p>
			<p>In this section, you built a strong foundation in statistical analysis and forecasting for your time series data. These skills<a id="_idIndexMarker877"/> are crucial for understanding<a id="_idIndexMarker878"/> past trends and making predictions, enabling data-driven decisions and insights.</p>
			<p>In the next section, we will have a look at deep learning models for time series analysis.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor212"/>Time series forecasting with deep learning – LSTM</h1>
			<p>This section will give you insights<a id="_idIndexMarker879"/> into advanced time series forecasting<a id="_idIndexMarker880"/> techniques using deep learning models. Whether you’re working with traditional time series data or more complex, high-dimensional data, these deep learning models<a id="_idIndexMarker881"/> can help you make more accurate predictions. In particular, we will cover the <code>keras</code>.</p>
			<p>We will be using <code>keras</code> with a <code>tensorflow</code> backend, so you need to install both libraries:</p>
			<ol>
				<li>As always, let’s load the necessary libraries and preprocess some time series data:<pre class="source-code">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
# Load the time series data (replace with your data)
time_series_data = pd.read_excel('time_series_data.xlsx')
# Normalize the data to be in the range [0, 1]
scaler = MinMaxScaler()
data = scaler.fit_transform(
    time_series_data['Value'].to_numpy().reshape(-1, 1))</pre></li>				<li>With the dataset<a id="_idIndexMarker882"/> prepared, we will split<a id="_idIndexMarker883"/> it into training<a id="_idIndexMarker884"/> and testing sets and reshape them for LSTM input:<pre class="source-code">
# Split the data into training and testing sets
train_size = int(len(data) * 0.67)
train, test = data[0:train_size, :], data[train_size:len(data), :]
# Create sequences and labels for training
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)
look_back = 3
X_train, Y_train = create_dataset(train, look_back)
X_test, Y_test = create_dataset(test, look_back)
# Reshape the data for LSTM input
X_train = np.reshape(X_train, (X_train.shape[0], 1, 
    X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, 
    X_test.shape[1]))</pre></li>				<li>Now that the data<a id="_idIndexMarker885"/> has been<a id="_idIndexMarker886"/> prepared, let’s create<a id="_idIndexMarker887"/> and train an LSTM model:<pre class="source-code">
model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)</pre><p class="list-inset">This code defines a simple LSTM model, compiles it, and trains it using our training data. You can customize the model architecture and training parameters as needed for your specific time series analysis. Note that the training time is significantly longer than with the simpler models we used in the previous section.</p></li>				<li>Once the model has been trained, we can make predictions and scale them back so that we can compare them with the original dataset:<pre class="source-code">
# Make predictions:
trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)
# Inverse transform the predictions to the original scale
trainPredict = scaler.inverse_transform(trainPredict)
testPredict = scaler.inverse_transform(testPredict)</pre></li>				<li>Finally, visualize<a id="_idIndexMarker888"/> the actual<a id="_idIndexMarker889"/> dataset and the prediction<a id="_idIndexMarker890"/> for the test set:<pre class="source-code">
# Plot the training predictions
trainPredictPlot = np.empty_like(data)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict) + look_back, :] =\
    trainPredict
# Plot the test predictions
testPredictPlot = np.empty_like(data)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict) + (look_back * 2):len(data),
    :] = testPredict
# Plot the training data in blue
plt.plot(time_series_data['Value'], color='blue', label=
    'Actual Data')
# Create shaded regions for the training and test data
plt.fill_between(range(len(data)), 0, 
    trainPredictPlot.reshape(-1),
    color='lightgray', label='Training Data')
plt.fill_between(range(len(data)), 0, 
    testPredictPlot.reshape(-1),
    color='lightcoral', label='Test Data')
# Overlay the predictions in green
plt.plot(testPredictPlot, color='green', label='Predictions')
plt.title('Time Series Analysis with LSTM')
plt.legend()
plt.show()</pre><p class="list-inset">The resulting image<a id="_idIndexMarker891"/> is quite impressive<a id="_idIndexMarker892"/> for such a small<a id="_idIndexMarker893"/> dataset and simple model:</p></li>			</ol>
			<div><div><img src="img/B19142_10_16.jpg" alt="Figure 10.16 – The actual data (in blue) and the prediction for the test set (in green)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – The actual data (in blue) and the prediction for the test set (in green)</p>
			<p>Remember that this is a basic<a id="_idIndexMarker894"/> example. LSTM models<a id="_idIndexMarker895"/> can be significantly<a id="_idIndexMarker896"/> more complex, and you might need to explore techniques such as stacked LSTMs, hyperparameter tuning, and more, depending on your specific time series data.</p>
			<p>This concludes our journey into time series analysis with Python. Let’s recap what you have learned in this chapter!</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor213"/>Summary</h1>
			<p>In this chapter, we delved into the fascinating world of time series analysis. We began by exploring time series plotting, mastering essential plots, and understanding the significance of ACF/PACF plots.</p>
			<p>Moving forward, we ventured into time series statistics, including the ADF test, time series decomposition, and statistical forecasting with tools such as <code>statsmodels</code> and <code>prophet</code>.</p>
			<p>To elevate our forecasting game, we embraced deep learning, employing LSTM networks using Python’s <code>keras</code> library. We learned to develop accurate time series forecasts and create insightful visualizations for data-driven insights.</p>
			<p>This chapter equipped us with a comprehensive set of skills for time series analysis, enabling us to unravel the hidden patterns and insights within time-based data, from plotting to statistical analysis and deep learning forecasting.</p>
			<p>In the next chapter, we will discuss a different integration method – that is, calling R and Python from Excel directly.</p>
		</div>
	

		<div><h1 id="_idParaDest-195" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor214"/>Part 4: The Other Way Around – Calling R and Python from Excel</h1>
		</div>
		<div><p>In this part, you willl unlock the power of R and Python within Excel with comprehensive solutions designed for seamless integration. You will discover how to leverage BERT and <code>xlwings</code> for streamlined communication between Excel and your preferred programming language locally. You will explore open source API solutions such as Plumber and FastAPI, along with commercial offerings such as Posit Connect and ownR, to expand the capabilities of your Excel workflow with API-based integration.</p>
			<p>This part has the following chapter:</p>
			<ul>
				<li><a href="B19142_11.xhtml#_idTextAnchor215"><em class="italic">Chapter 11</em></a>, <em class="italic">Calling R/Python Locally from Excel Directly or via an API</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>