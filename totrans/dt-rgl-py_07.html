<html><head></head><body><div><div><h1 id="_idParaDest-195"><em class="italics"><a id="_idTextAnchor239"/>Chapter 7</em></h1>
		</div>
		<div><h1 id="_idParaDest-196"><a id="_idTextAnchor240"/>Advanced Web Scraping and Data Gathering</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Make use of <code>requests</code> and <code>BeautifulSoup</code> to read various web pages and gather data from them</li>
				<li class="bullets">Perform read operations on XML files and the web using an Application Program Interface (API)</li>
				<li class="bullets">Make use of regex techniques to scrape useful information from a large and messy text corpus</li>
			</ul>
			<p>In this chapter, you will learn how to gather data from web pages, XML files, and APIs.</p>
		</div>
		<div><h2 id="_idParaDest-197"><a id="_idTextAnchor241"/>Introduction</h2>
			<p>The previous chapter covered how to create a successful data wrangling pipeline. In this chapter, we will build a real-life web scraper using all of the techniques that we have learned so far. This chapter builds on the foundation of <code>BeautifulSoup</code> and introduces various methods for scraping a web page and using an API to gather data.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor242"/>The Basics of Web Scraping and the Beautiful Soup Library</h2>
			<p><a id="_idTextAnchor243"/>In today's connected world, one of the most valued and widely used skill for a data wrangling professional is the ability to extract and read data from web pages and databases hosted on the web. Most organizations host data on the cloud (public or private), and the majority of web microservices these days provide some kind of API for the external users to access data:</p>
			<div><div><img src="img/C11065_07_01.jpg" alt="Figure 7.1: Data Wrangling HTTP request and XML/JSON reply" width="1800" height="1000"/>
				</div>
			</div>
			<h6>Figure 7.1: Data wrangling HTTP request and an XML/JSON reply</h6>
			<p>It is necessary that, as a data wrangling engineer, you know about the structure of web pages and Python libraries so that you are able to extract data from a web page. The World Wide Web is an ever-growing, ever-changing universe, in which different data exchange protocols and formats are used. A few of these are widely used and have become standard.</p>
			<h3 id="_idParaDest-199"><a id="_idTextAnchor244"/>Libraries in Python</h3>
			<p>Python comes equipped with built-in modules, such as <code>urllib 3</code>, which that can place HTTP requests over the internet and receive data from the cloud. However, these modules operate at a lower level and require deeper knowledge of HTTP protocols, encoding, and requests.</p>
			<p>We will take advantage of two Python libraries in this chapter: <code>Requests</code> and <code>BeautifulSoup</code>. To avoid dealing with HTTP methods on a lower level, we will use the <code>Requests</code> library. It is an API built on top of pure Python web utility libraries, which makes placing HTTP requests easy and intuitive.</p>
			<p><strong class="keyword">BeautifulSoup</strong> is one of the most popular HTML parser packages. It parses the HTML content you pass on and builds a detailed tree of all tags and markups within the page for easy and intuitive traversal. This tree can be used by a programmer to look for certain markup elements (for example, a table, a hyperlink, or a blob of text within a particular div ID) to scrape useful data.</p>
			<h3 id="_idParaDest-200">E<a id="_idTextAnchor245"/>xercise 81: Using the Requests Library to Get a Response from the Wikipedia Home Page</h3>
			<p>The Wikipedia home page consists of many elements and scripts, all of which are a mix of HTML, CSS, and JavaScript code blocks. To read the home page of Wikipedia and extract some useful textual information, we need to move step by step, as we are not interested in all of the code or markup tags; only some selected portions of text.</p>
			<p>In this exercise, we will peel off the layers of HTML/CSS/JavaScript to pry away the information we are interested in.</p>
			<ol>
				<li>Import the <code>requests</code> library:<pre>import requests</pre></li>
				<li>Assign the home page URL to a variable, <code>wiki_home</code>:<pre># First assign the URL of Wikipedia home page to a strings
wiki_home = "https://en.wikipedia.org/wiki/Main_Page"</pre></li>
				<li>Use the <code>get</code> method from the <code>requests</code> library to get a response from this page:<pre>response = requests.get(wiki_home)</pre></li>
				<li>To get information about the response object, enter the following code:<pre>type(response)</pre><p>The output is as follows:</p><pre>requests.models.Response</pre></li>
			</ol>
			<p>It is a model data structure that's defined in the <code>requests</code> library.</p>
			<p>T<a id="_idTextAnchor246"/>he web is an extremely dynamic place. It is possible that the home page of Wikipedia will have changed by the time somebody uses your code, or that a particular web server will be down and your request will essentially fail. If you proceed to write more complex and elaborate code without checking the status of your request, then all that subsequent work will be fruitless.</p>
			<p>A web page request generally comes back with various codes. Here are some of the common codes you may encounter:</p>
			<div><div><img src="img/C11065_07_02.jpg" alt="Figure 7.2: Web requests and their description" width="1800" height="779"/>
				</div>
			</div>
			<h6>Figure 7.2: Web requests and their description</h6>
			<p>So, we write a function to check the code and print out messages as needed. These kinds of small helper/utility functions are incredibly useful for complex projects.</p>
			<h3 id="_idParaDest-201"><a id="_idTextAnchor247"/>Exercise 82: Checking the Status of the Web Request</h3>
			<p>Next, we will write a small utility function to check the status of the response.</p>
			<p>We will start by getting into the a habit of writing small functions to accomplish small modular tasks, instead of writing long scripts, which are hard to debug and track:</p>
			<ol>
				<li value="1">Create a <code>status_check</code> function by using the following command:<pre>def status_check(r):
  <a id="_idTextAnchor248"/>  if r.status_code==200:
        print("Success!")
        return 1
    else:
        print("Failed!")
        return -1</pre><p>Note that, along with printing the appropriate message, we are returning either 1 or -1 from this function. This is important.</p></li>
				<li>Check the response using the <code>status_check</code> command:<pre>status_check(response)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_07_03.jpg" alt="Figure 7.3 : The output of status_check" width="723" height="100"/>
				</div>
			</div>
			<h6>Figure 7.3 : The output of status_check</h6>
			<p>In this chapter, we will not use these returned values, but later, for more complex programming activity, you will proceed only if you get one as the return value for this function, that is, you will write a conditional statement to check the return value and then execute the subsequent code based on that.</p>
			<h3 id="_idParaDest-202">Che<a id="_idTextAnchor249"/>cking the Encoding of the Web Page</h3>
			<p>We can also write a utility function to check the encoding of the web page. Various encodings are possible with any HTML document, although the most popular is UTF-8. Some of the most popular encodings are ASCII, Unicode, and UTF-8. ASCII is the simplest, but it cannot capture the complex symbols used in various spoken and written languages all over the world, so UTF-8 has become the almost universal standard in web development these days.</p>
			<p>When we run this function on the Wikipedia home page, we get back the particular encoding type that's used for that page. This function, like the previous one, takes the <code>requests</code> response object as an argument and returns a value:</p>
			<pre>def encoding_check(r):
    return (r.encoding)</pre>
			<p>Check the response:</p>
			<pre>encoding_check(response)</pre>
			<p>The output is as follows:</p>
			<pre>'UTF-8'</pre>
			<p>Here, UTF-8 denotes the most popular character encoding scheme that's used in the digital medium and on the web today. It employs variable-length encoding with 1-4 bytes, thereby representing all Unicode characters in various languages around the world.</p>
			<h3 id="_idParaDest-203">Exe<a id="_idTextAnchor250"/>rcise 83: Creating a Function to Decode the Contents of the Response and Check its Length</h3>
			<p>The final aim of this series of steps is to get a page's contents as a blob of text or as a string object that Python can process afterward. Over the internet, data streams move in an encoded format. Therefore, we need to decode the content of the response object. For this purpose, we need to perform the following steps:</p>
			<ol>
				<li value="1">Write a utility function to decode the contents of the response:<pre>def decode_content(r,encoding):
    return (r.content.decode(encoding))
contents = decode_content(response,encoding_check(response))</pre></li>
				<li>Check the type of the decoded object:<pre>type(contents)</pre><p>The output is as follows:</p><pre>str</pre><p>We finally got a string object by reading the HTML page!</p><h4>Note</h4><p class="callout">Note that the answer in this chapter and in the exercise in Jupyter notebook may vary because of updates that have been made to the Wikipedia page.</p></li>
				<li>Check the length of the object and try printing some of it:<pre>len(contents)</pre><p>The output is as follows:</p><pre>74182</pre><p>If you print the first 10,000 characters of this string, it will look some similar to this:</p></li>
			</ol>
			<div><div><img src="img/C11065_07_04.jpg" alt="Figure 7.4: Output showing a mixed blob of HTML markup tags, text and element names, and properties" width="1249" height="302"/>
				</div>
			</div>
			<h6>Figure 7.4: Output showing a mixed blob of HTML markup tags, text and element names, and properties</h6>
			<p>Obviously, this is a mixed blob of various HTML markup tags, text, and elements names/properties. We cannot hope to extract meaningful information from this without using sophisticated functions or methods. Fortunately, the <code>BeautifulSoup</code> library provides such methods, and we will see how to use them next.</p>
			<h3 id="_idParaDest-204">Exer<a id="_idTextAnchor251"/>cise 84: Extracting Human-Readable Text From a BeautifulSoup Object</h3>
			<p>It turns out that a <code>BeautifulSoup</code> object has a <code>text</code> method, which can be used just to extract text:</p>
			<ol>
				<li value="1">Import the package and then pass on the whole string (HTML content) to a method for parsing:<pre>from bs4 import BeautifulSoup
soup = BeautifulSoup(contents, 'html.parser')</pre></li>
				<li>Execute the following code in your notebook:<pre>txt_dump=soup.text</pre></li>
				<li>Find the type of the <code>txt_dmp:</code><pre>type(txt_dump)</pre><p>The output is as follows:</p><pre>str</pre></li>
				<li>Find the length of the <code>txt_dmp:</code><pre>len(txt_dump)</pre><p>The output is as follows:</p><pre>15326</pre></li>
				<li>Now, the length of the text dump is much smaller than the raw HTML's string length. This is because <code>bs4</code> has parsed through the HTML and extracted only human-readable text for further processing.</li>
				<li>Print the initial portion of this text. <pre>print(txt_dump[10000:11000])</pre><p>You will see something similar to the following:</p></li>
			</ol>
			<div><div><img src="img/C11065_07_05.jpg" alt="Figure 7.5: Output showing the initial portion of text" width="931" height="258"/>
				</div>
			</div>
			<h6>Figure 7.5: Output showing the initial portion of text</h6>
			<h3 id="_idParaDest-205">Extra<a id="_idTextAnchor252"/>cting Text from a Section</h3>
			<p>Now, let's move on to a more exciting data wrangling task. If you open the Wikipedia home page, you are likely to see a section called <strong class="bold">From today's featured article</strong>. This is an excerpt from the day's prominent article, which is randomly selected and promoted on the home page. In fact, this article can also change throughout the day:</p>
			<div><div><img src="img/C11065_07_06.jpg" alt="Figure 7.6: Sample Wikipedia page highlighting the “From today’s featured article” section" width="1280" height="806"/>
				</div>
			</div>
			<h6>Figure 7.6: Sample Wikipedia page highlighting the "From today's featured article" section</h6>
			<p>You need to extract the text from this section. There are number of ways to accomplish this task. We will go through a simple and intuitive method for doing so here.</p>
			<p>First, we try to identify two indices – the start index and end index of the string, which demarcate the start and end of the text we are interested in. In the next screenshot, the indices are shown:</p>
			<div><div><img src="img/C11065_07_07.jpg" alt="Figure 7.7: Wikipedia page highlighting the text to be extracted" width="1324" height="816"/>
				</div>
			</div>
			<h6>Figure 7.7: Wikipedia page highlighting the text to be extracted</h6>
			<p>The following code accomplishes the extraction:</p>
			<pre>idx1=txt_dump.find("From today's featured article")
idx2=txt_dump.find("Recently featured")
print(txt_dump[idx1+len("From today's featured article"):idx2])</pre>
			<p>Note, that we have to add the length of the <code>From today's featured article</code> string to <code>idx1</code> and then pass that as the starting index. This is because idx1 finds where the <em class="italics">From today's featured article</em> string starts, not ends.</p>
			<p>It prints out something like this (this is a sample output):</p>
			<div><div><img src="img/C11065_07_08.jpg" alt="Figure 7.8: The extracted text" width="790" height="174"/>
				</div>
			</div>
			<h6>Figure 7.8: The extracted text</h6>
			<h3 id="_idParaDest-206">Extracti<a id="_idTextAnchor253"/>ng Important Historical Events that Happened on Today's Date</h3>
			<p>Next, we will try to extract the text corresponding to the important historical events that happened on today's date. This can generally be found at the bottom-right corner as shown in the following screenshot:</p>
			<div><div><img src="img/C11065_07_09.jpg" alt="Figure 7.9: Wikipedia page highlighting the “On this day” section" width="1140" height="641"/>
				</div>
			</div>
			<h6>Figure 7.9: Wikipedia page highlighting the "On this day" section</h6>
			<p>So, can we apply the same technique as we did for "<strong class="bold">From today's featured article</strong>"? Apparently not, because there is text just below where we want our extraction to end, which is not fixed, unlike in the previous case. Note that, in the previous exercise, the fixed string "<strong class="bold">Recently featured</strong>" occurs at the exact place where we want the extraction to stop. So, we could use it in our code. However, we cannot do that in this case, and the reason for this is illustrated in the following screenshot:</p>
			<div><div><img src="img/C11065_07_10.jpg" alt="Figure 7.10: Wikipedia page highlighting the text to be extracted" width="1212" height="594"/>
				</div>
			</div>
			<h6>Figure 7.10: Wikipedia page highlighting the text to be extracted</h6>
			<p>So, in this section, we just want to find out what the text looks like around the main content we are interested in. For that, we must find out the start of the string "On this day" and print out the next 1,000 characters, using the following command:</p>
			<pre>idx3=txt_dump.find("On this day")
print(txt_dump[idx3+len("On this day"):idx3+len("On this day")+1000])</pre>
			<p>This looks as follows:</p>
			<div><div><img src="img/C11065_07_11.jpg" alt="Figure 7.11: Output of the &quot;On this day&quot; section from Wikipedia" width="1223" height="586"/>
				</div>
			</div>
			<h6>Figure 7.11: Output of the "On this day" section from Wikipedia</h6>
			<p>To address this issue, we need to think differently and use some other methods from BeautifulSoup (and write another utility function).</p>
			<h3 id="_idParaDest-207"><a id="_idTextAnchor254"/>Exercise 85: Using Advanced BS4 Techniques to Extract Relevant Text</h3>
			<p>HTML pages are made of many markup tags, such as &lt;div&gt;, which denotes a division of text/images, or &lt;ul&gt;, which denotes lists. We can take advantage of this structure and look at the element that contains the text we are interested in. In the Mozilla Firefox browser, we can easily do this by right-clicking and selecting the "<strong class="bold">Inspect Element</strong>" option:</p>
			<div><div><img src="img/C11065_07_12.jpg" alt="Figure 7.12: Inspecting elements on Wikipedia" width="1116" height="687"/>
				</div>
			</div>
			<h6>Figure 7.12: Inspecting elements on Wikipedia</h6>
			<p>As you hover over this with the mouse, you will see different portions of the page being highlighted. By doing this, it is easy to discover the precise block of markup text, that is responsible for the textual information we are interested in. Here, we can see that a certain <code>&lt;ul&gt;</code> block contains the text:</p>
			<div><div><img src="img/C11065_07_13.jpg" alt="" width="1271" height="715"/>
				</div>
			</div>
			<h6>Figure 7.13: Identifying the HTML block that contains text</h6>
			<p>Now, it is prudent to find the <code>&lt;div&gt;</code> tag that contains this <code>&lt;ul&gt;</code> block within it. By looking around the same screen as before, we find the <code>&lt;div&gt;</code> and also its ID:</p>
			<div><div><img src="img/C11065_07_14.jpg" alt="Figure 7.14: The tag ul containing the text" width="1144" height="548"/>
				</div>
			</div>
			<h6>Figure 7.14: The &lt;ul&gt; tag containing the text</h6>
			<ol>
				<li value="1">Use the <code>find_all</code> method from BeautifulSoup, which scans all the tags of the HTML page (and their sub-elements) to find and extract the text associated with this particular <code>&lt;div&gt;</code> element.<h4>Note</h4><p class="callout">Note how we are utilizing the 'mp-otd' ID of the &lt;div&gt; to identify it among tens of other &lt;div&gt; elements.</p><p>The <code>find_all</code> method returns a <code>NavigableString</code> class, which has a useful <code>text</code> method associated with it for extraction.</p></li>
				<li>To put these ideas together, we will create an empty list and append the text from the <code>NavigableString</code> class to this list as we traverse the page:<pre>text_list=[] #Empty list
for d in soup.find_all('div'):
        if (d.get('id')=='mp-otd'):
            for i in d.find_all('ul'):
                text_list.append(i.text)</pre></li>
				<li>Now, if we examine the <code>text_list</code> list, we will see that it has three elements. If we print the elements, separated by a marker, we will see that the text we are interested in appears as the first element!<pre>for i in text_list:
    print(i)
    print('-'*100)</pre><h4>Note</h4><p class="callout">In this example, it is the first element of the list that we are interested in. However, the exact position will depend on the web page.</p><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_07_15.jpg" alt="Figure 7.15: The text highlighted" width="1432" height="510"/>
				</div>
			</div>
			<h6>Figure 7.15: The text highlighted</h6>
			<h3 id="_idParaDest-208"><a id="_idTextAnchor255"/>Exercise 86: Creating a Compact Function to Extract the "On this Day" Text from the Wikipedia Home Page</h3>
			<p>As we discussed before, it is always good to try to functionalize specific tasks, particularly in a web scraping application:</p>
			<ol>
				<li value="1">Create a function, whose only job is to take the URL (as a string) and to return the text corresponding to the <strong class="bold">On this day</strong> section. The benefit of such a functional approach is that you can call this function from any Python script and use it anywhere in another program as a standalone module:<pre>def wiki_on_this_day(url="https://en.wikipedia.org/wiki/Main_Page"):
    """</pre></li>
				<li>Extract the text from the "On this day" section on the Wikipedia home page. Accept the Wikipedia home page URL as a string. A default URL is provided:<pre>    """
    import requests
    from bs4 import BeautifulSoup
    
    wiki_home = str(url)
    response = requests.get(wiki_home)
    
    def status_check(r):
        if r.status_code==200:
            return 1
        else:
            return -1
    
    status = status_check(response)
    if status==1:
        contents = decode_content(response,encoding_check(response))
    else:
        print("Sorry could not reach the web page!")
        return -1
    
    soup = BeautifulSoup(contents, 'html.parser')
    text_list=[]
    
    for d in soup.find_all('div'):
            if (d.get('id')=='mp-otd'):
                for i in d.find_all('ul'):
                    text_list.append(i.text)
    
    return (text_list[0])</pre></li>
				<li>Note how this function utilizes the status check and prints out an error message if the request failed. When we test this function with an intentionally incorrect URL, it behaves as expected:<pre>print(wiki_on_this_day("https://en.wikipedia.org/wiki/Main_Page1"))
 Sorry could not reach the web page!</pre></li>
			</ol>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor256"/>Reading Data from XML</h2>
			<p>XML, or Extensible Markup Language, is a web markup language that's similar to HTML but with significant flexibility (on the part of the user) built in, such as the ability to define your own tags. It was one of the most hyped technologies in the 1990s and early 2000s. It is a meta-language, that is, a language that allows us to define other languages using its mechanics, such as RSS, MathML (a mathematical markup language widely used for web publication and the display of math-heavy technical information), and so on. XML is also heavily used in regular data exchanges over the web, and as a data wrangling professional, you should have enough familiarity with its basic features to tap into the data flow pipeline whenever you need to extract data for your project.</p>
			<h3 id="_idParaDest-210">Exercise 87: C<a id="_idTextAnchor257"/>reating an XML File and Reading XML Element Objects</h3>
			<p>Let's create some random data to understand the XML data format better. Type in the following code snippets:</p>
			<ol>
				<li value="1">Create an XML file using the following command:<pre>data = '''
&lt;person&gt;
  &lt;name&gt;Dave&lt;/name&gt;
  &lt;surname&gt;Piccardo&lt;/surname&gt;
  &lt;phone type="intl"&gt;
     +1 742 101 4456
   &lt;/phone&gt;
   &lt;email hide="yes"&gt;
   dave.p@gmail.com&lt;/email&gt;
&lt;/person&gt;'''</pre></li>
				<li>This is a triple-quoted string or multiline string. If you print this object, you will get the following output. This is an XML-formatted data string in a tree structure, as we will see soon, when we parse the structure and tease apart the individual parts:<div><img src="img/C11065_07_16.jpg" alt="Figure 7.16: The XML file output" width="747" height="245"/></div><h6>Figure 7.16: The XML file output</h6></li>
				<li>To process and wrangle with the data, we have to read it as an <code>Element</code> object using the Python XML parser engine:<pre>import xml.etree.ElementTree as ET
tree = ET.fromstring(data)
type (tree)</pre><p>The output is as follows:</p><pre> xml.etree.ElementTree.Element</pre></li>
			</ol>
			<h3 id="_idParaDest-211">Exercise 88: Fi<a id="_idTextAnchor258"/>nding Various Elements of Data within a Tree (Element)</h3>
			<p>We can use the <code>find</code> method to search for various pieces of useful data within an XML element object and print them (or use them in whatever processing code we want) using the <code>text</code> method. We can also use the <code>get</code> method to extract the specific attribute we want:</p>
			<ol>
				<li value="1">Use the <code>find</code> method to find <code>Name</code>:<pre># Print the name of the person
print('Name:', tree.find('name').text)
 Dave</pre></li>
				<li>Use the <code>find</code> method to find <code>Surname</code>:<pre># Print the surname
print('Surname:', tree.find('surname').text)
 Piccardo</pre></li>
				<li>Use the <code>find</code> method to find <code>Phone</code>. Note the use of the <code>strip</code> method to strip away any trailing spaces/blanks:<pre># Print the phone number
print('Phone:', tree.find('phone').text.strip())</pre><p>The output will be as follows:</p><pre>+1 742 101 4456</pre></li>
				<li>Use the <code>find</code> method to find <code>email status</code> and <code>actual email</code>. Note the use of the <code>get</code> method to extract the status:<pre># Print email status and the actual email
print('Email hidden:', tree.find('email').get('hide'))
print('Email:', tree.find('email').text.strip())</pre><p>The output will be as follows:</p><pre> Email hidden: yes
 Email: dave.p@gmail.com</pre></li>
			</ol>
			<h3 id="_idParaDest-212">Reading from a L<a id="_idTextAnchor259"/>ocal XML File into an ElementTree Object</h3>
			<p>We can also read from an XML file (saved locally on disk).</p>
			<p>This is a fairly common situation where a frontend web scraping module has already downloaded a lot of XML files by reading a table of data on the web and now the data wrangler needs to parse through this XML file to extract meaningful pieces of numerical and textual data.</p>
			<p>We have a file associated with this chapter, called "<code>xml1.xml</code>". Please make sure you have the file in the same directory that you are running your Jupyter Notebook from:</p>
			<pre>tree2=ET.parse('xml1.xml')
type(tree2)
The output will be as follows: xml.etree.ElementTree.ElementTree</pre>
			<p>Note how we use the <code>parse</code> method to read this XML file. This is slightly different than using the <code>fromstring</code> method used in the previous exercise, where we were directly reading from a string object. This produces an <code>ElementTree</code> object instead of a simple <code>Element</code>.</p>
			<p>The idea of building a tree-like object is the same as in the domains of computer science and programming:</p>
			<ul>
				<li>There is a root</li>
				<li>There are children objects attached to the root</li>
				<li>There could be multiple levels, that is, children of children recursively going down</li>
				<li>All of the nodes of the tree (root and children alike) have attributes attached to them that contain data</li>
				<li>Tree traversal algorithms can be used to search for a particular attribute</li>
				<li>If provided, special methods can be used to probe a node deeper</li>
			</ul>
			<h3 id="_idParaDest-213">Exercise 89: Tr<a id="_idTextAnchor260"/>aversing the Tree, Finding the Root, and Exploring all Child Nodes and their Tags and Attributes</h3>
			<p>Every node in the XML tree has tags and attributes. The idea is as follows:</p>
			<div><div><img src="img/C11065_07_17.jpg" alt="" width="1445" height="993"/>
				</div>
			</div>
			<h6>Figure 7.17: Finding the root and child nodes of an XML tag</h6>
			<ol>
				<li value="1">Explore these tags and attributes using the following code:<pre>root=tree2.getroot()
for child in root:
    print ("Child:",child.tag, "| Child attribute:",child.attrib)</pre><p>The output will be as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_07_18.jpg" alt="Figure 7.18: The output showing the extracted XML tags" width="820" height="101"/>
				</div>
			</div>
			<h6>Figure 7.18: The output showing the extracted XML tags</h6>
			<h4>Note</h4>
			<p class="callout">Remember that every XML data file could follow a different naming or structural format, but using an element tree approach puts the data into a somewhat structured flow that can be explored systematically. Still, it is best to examine the raw XML file structure once and understand (even if at a high level) the data format before attempting automatic extractions.</p>
			<h3 id="_idParaDest-214">Exercise 90: Usin<a id="_idTextAnchor261"/>g the <code>text</code> Method to Extract Meaningful Data</h3>
			<p>We can almost think of the XML tree as a <strong class="bold">list of lists</strong> and index it accordingly:</p>
			<ol>
				<li value="1">Access the element <code>root[0][2]</code> by using the following code:<pre>root[0][2]</pre><p>The output will be as follows:</p><pre>&lt;Element 'gdppc' at 0x00000000051FF278&gt;</pre><p>So, this points to the '<code>gdppc</code>' piece of data. Here, '<code>gdppc</code>' is the tag and the actual GDP/per capita data is attached to this tag.</p></li>
				<li>Use the <code>text</code> method to access the data:<pre>root[0][2].text</pre><p>The output will be as follows:</p><pre> '70617'</pre></li>
				<li>Use the <code>tag</code> method to access <code>gdppc</code>:<pre>root[0][2].tag</pre><p>The output will be as follows:</p><pre> 'gdppc'</pre></li>
				<li>Check <code>root[0]</code>:<pre>root[0]</pre><p>The output will be as follows:</p><pre> &lt;Element 'country1' at 0x00000000050298B8&gt;</pre></li>
				<li>Check the tag:<pre>root[0].tag</pre><p>The output will be as follows:</p><pre>  'country1'</pre><p>We can use the <code>attrib</code> method to access it:</p><pre>root[0].attrib</pre><p>The output will be as follows:</p><pre> {'name': 'Norway'}</pre><p>So, <code>root[0]</code> is again an element, but it has different a set of tags and attributes than <code>root[0][2]</code>. This is expected because they are all part of the tree as nodes, but each is associated with a different level of data.</p></li>
			</ol>
			<p>This last piece of code output is interesting because it returns a dictionary object. Therefore, we can just index it by its keys. We will do that in the next exercise.</p>
			<h3 id="_idParaDest-215">Extracting and Pri<a id="_idTextAnchor262"/>nting the GDP/Per Capita Information Using a Loop</h3>
			<p>Now that we know how to read the GDP/per capita data and how to get a dictionary back from the tree, we can easily construct a simple dataset by running a loop over the tree:</p>
			<pre>for c in root:
    country_name=c.attrib['name']
    gdppc = int(c[2].text)
    print("{}: {}".format(country_name,gdppc))</pre>
			<p>The output is as follows:</p>
			<pre> Norway: 70617
 Austria: 44857
 Israel: 38788</pre>
			<p>We can put these in a DataFrame or CSV file for saving to a local disk or further processing, such as a simple plot!</p>
			<h3 id="_idParaDest-216">Exercise 91: Find<a id="_idTextAnchor263"/>ing All the Neighboring Countries for each Country and Printing Them</h3>
			<p>As we mentioned before, there are efficient search algorithms for tree structures, and one such method for XML trees is <code>findall</code>. We can use this, for this example, to find all the neighbors a country has and print them out.</p>
			<p>Why do we need to use <code>findall</code> over find? Well, because not all the countries have an equal number of neighbors and <code>findall</code> searches for all the data with that tag that is associated with a particular node, and we want to traverse all of them:</p>
			<pre>for c in root:
    ne=c.findall('neighbor') # Find all the neighbors
    print("Neighbors\n"+"-"*25)
    for i in ne: # Iterate over the neighbors and print their 'name' attribute
        print(i.attrib['name'])
    print('\n')</pre>
			<p>The output looks something like this:</p>
			<div><div><img src="img/C11065_07_19.jpg" alt="" width="858" height="420"/>
				</div>
			</div>
			<h6>Figure 7.19: The output that's generated by using findall</h6>
			<h3 id="_idParaDest-217">Exercise 92: A Sim<a id="_idTextAnchor264"/>ple Demo of Using XML Data Obtained by Web Scraping</h3>
			<p>In the last topic of this chapter, we learned about simple web scraping using the <code>requests</code> library. So far, we have worked with static XML data, that is, data from a local file or a string object we've scripted. Now, it is time to combine our learning and read XML data directly over the internet (as you are expected to do almost all the time):</p>
			<ol>
				<li value="1">We will try to read a cooking recipe from a website called <a href="http://www.recipepuppy.com/">http://www.recipepuppy.com/</a>, which aggregates links to various other sites with the recipe:<pre>import urllib.request, urllib.parse, urllib.error
serviceurl = 'http://www.recipepuppy.com/api/?'
item = str(input('Enter the name of a food item (enter \'quit\' to quit): '))
url = serviceurl + urllib.parse.urlencode({'q':item})+'&amp;p=1&amp;format=xml'
uh = urllib.request.urlopen(url)
data = uh.read().decode()
print('Retrieved', len(data), 'characters')
tree3 = ET.fromstring(data)</pre></li>
				<li>This code will ask the user for input. You have to enter the name of a food item. For example, 'chicken tikka':<div><img src="img/C11065_07_20.jpg" alt="Figure 7.20: Demo of scraping from XML data" width="1147" height="144"/></div><h6>Figure 7.20: Demo of scraping from XML data</h6></li>
				<li>We get back data in XML format and read and decode it before creating an XML tree out of it:<pre>data = uh.read().decode()
print('Retrieved', len(data), 'characters')
tree3 = ET.fromstring(data)</pre></li>
				<li>Now, we can use another useful method, called <code>iter</code>, which basically iterates over the nodes under an element. If we traverse the tree and extract the text, we get the following output:<pre>for elem in tree3.iter():
    print(elem.text)</pre><p>The output is as follows:</p><div><img src="img/C11065_07_21.jpg" alt="Figure 7.21: The output that’s generated by using iter" width="1044" height="520"/></div><h6>Figure 7.21: The output that's generated by using iter</h6></li>
				<li>We can use the find method to search for the appropriate attribute and extract its content. This is the reason it is important to scan through the XML data manually and check what attributes are used. Remember, this means scanning the raw string data, not the tree structure.</li>
				<li>Print the raw string data:<div><img src="img/C11065_07_22.jpg" alt="Figure 7.22: The output showing the extracted href tags&#13;&#10;" width="1232" height="693"/></div><h6>Figure 7.22: The output showing the extracted href tags</h6><p>Now we know what tags to search for.</p></li>
				<li>Print all the hyperlinks in the XML data:<pre>for e in tree3.iter():
    h=e.find('href')
    t=e.find('title')
    if h!=None and t!=None:
        print("Receipe Link for:",t.text)
        print(h.text)
        print("-"*100)</pre><p>Note the use of <code>h!=None</code> and <code>t!=None</code>. These are difficult to expect when you first run this kind of code. You may get an error because some of the tags may return a <code>None</code> object, that is, they were empty for some reason in this XML data stream. This kind of situation is fairly common and cannot be anticipated beforehand. You have to use your Python knowledge and programming intuition to get around it if you receive such an error. Here, we are just checking for the type of the object and if it is not a <code>None</code>, then we need to extract the text associated with it.</p><p>The final output is as follows:</p><div><img src="img/C11065_07_23.jpg" alt="Figure 7.23: The output showing the final output" width="836" height="375"/></div></li>
			</ol>
			<h6>Figure 7.23: The output showing the final output</h6>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor265"/>Reading Data from an API</h2>
			<p>Fundamentally, an API or Application Programming Interface is some kind of interface to a computing resource (for example, an operating system or database table), which has a set of exposed methods (function calls) that allow a programmer to access particular data or internal features of that resource.</p>
			<p>A web API is, as the name suggests, an API over the web. Note that it is not a specific technology or programming framework, but an architectural concept. Think of an API like a fast food restaurant's customer service center. Internally, there are many food items, raw materials, cooking resources, and recipe management systems, but all you see are fixed menu items on the board and you can only interact through those items. It is like a port that can be accessed using an HTTP protocol and is able to deliver data and services if used properly.</p>
			<p>Web APIs are extremely popular these days for all kinds of data services. In the very first chapter, we talked about how UC San Diego's data science team pulls data from Twitter feeds to analyze occurrence of forest fires. For this, they do not go to twitter.com and scrape the data by looking at HTML pages and text. Instead, they use the Twitter API, which sends this data continuously in a streaming format.</p>
			<p>Therefore, it is very important for a data wrangling professional to understand the basics of data extraction from a web API as you are extremely likely to find yourself in a situation where large quantities of data must be read through an API interface for processing and wrangling. These days, most APIs stream data out in JSON format. In this chapter, we will use a free API to read some information about various countries around the world in JSON format and process it.</p>
			<p>We will use Python's built-in <code>urllib</code> module for this topic, along with pandas to make a DataFrame. So, we can import them now. We will also import Python's <code>JSON</code> module:</p>
			<pre>import urllib.request, urllib.parse
from urllib.error import HTTPError,URLError
import json
import pandas as pd</pre>
			<h3 id="_idParaDest-219">Defining the Base URL (<a id="_idTextAnchor266"/>or API Endpoint)</h3>
			<p>First, we need to set the base URL. When we are dealing with API microservices, this is often called the <strong class="bold">API endpoint</strong>. Therefore, look for such a phrase in the web service portal you are interested in and use the endpoint URL they give you:</p>
			<pre>serviceurl = 'https://restcountries.eu/rest/v2/name/'</pre>
			<p>API-based microservices are extremely dynamic in nature in terms of what and how they offer their service and data. It can change at any time. At the time of this chapter planning, we found this particular API to be a nice choice for extracting data easily and without using authorization keys (login or special API keys).</p>
			<p>For most APIs, however, you need to have your own API key. You get that by registering with their service. A basic usage (up to a fixed number of requests or a data flow limit) is often free, but after that you will be charged. To register for an API key, you often need to enter credit card information.</p>
			<p>We wanted to avoid all that hassle to teach you the basics and that's why we chose this example, which does not require such authorization. But, depending on what kind of data you will encounter in your work, please be prepared to learn about using an API key.</p>
			<h3 id="_idParaDest-220">Exercise 93: Defining <a id="_idTextAnchor267"/>and Testing a Function to Pull Country Data from an API</h3>
			<p>This particular API serves basic information about countries around the world:</p>
			<ol>
				<li value="1">Define a function to pull out data when we pass the name of a country as an argument. The crux of the operation is contained in the following two lines of code:<pre>url = serviceurl + country_name
uh = urllib.request.urlopen(url)</pre></li>
				<li>The first line of code appends the country name as a string to the base URL and the second line sends a <code>get</code> request to the API endpoint. If all goes well, we get back the data, decode it, and read it as a JSON file. This whole exercise is coded in the following function, along with some error-handling code wrapped around the basic actions we talked about previously:<pre>def get_country_data(country):
    """
    Function to get data about country from "https://restcountries.eu" API
    """
    country_name=str(country)
    url = serviceurl + country_name
    
    try: 
        uh = urllib.request.urlopen(url)
    except HTTPError as e:
        print("Sorry! Could not retrieve anything on {}".format(country_name))
        return None
    except URLError as e:
        print('Failed to reach a server.')
        print('Reason: ', e.reason)
        return None
    else:
        data = uh.read().decode()
        print("Retrieved data on {}. Total {} characters read.".format(country_name,len(data)))
        return data</pre></li>
				<li>Test this function by passing some arguments. We pass a correct name and an erroneous name. The response is as follows:<h4>Note</h4><p class="callout">This is an example of rudimentary error handling. You have to think about various possibilities and put in such code to catch and gracefully respond to user input when you are building a real-life web or enterprise application.</p></li>
			</ol>
			<div><div><img src="img/C11065_07_24.jpg" alt="Figure 7.24: Input arguments" width="1266" height="484"/>
				</div>
			</div>
			<h6>Figure 7.24: Input arguments</h6>
			<h3 id="_idParaDest-221">Using the Built-In JSON <a id="_idTextAnchor268"/>Library to Read and Examine Data</h3>
			<p>As we have already mentioned, JSON looks a lot like a Python dictionary.</p>
			<p>In this exercise, we will use Python's <code>json</code> module to read raw data in that format and see what we can process further:</p>
			<pre>x=json.loads(data)
y=x[0]
type(y)</pre>
			<p>The output will be as follows:</p>
			<pre> dict</pre>
			<p>So, we get a list back when we use the <code>loads</code> method from the <code>json</code> module. It reads a string datatype into a list of dictionaries. In this case, we get only one element in the list, so we extract that and check its type to make sure it is a dictionary.</p>
			<p>We can quickly check the keys of the dictionary, that is the JSON data (note that a full screenshot is not shown here). We can see the relevant country data, such as calling codes, population, area, time zones, borders, and so on:</p>
			<div><div><img src="img/C11065_07_25.jpg" alt="Figure 7.25: The output of dict_keys" width="874" height="110"/>
				</div>
			</div>
			<h6>Figure 7.25: The output of dict_keys</h6>
			<h3 id="_idParaDest-222">Printing All the Data Ele<a id="_idTextAnchor269"/>ments</h3>
			<p>This task is extremely simple given that we have a dictionary at our disposal! All we have to do is iterate over the dictionary and print the keys/items pair one by one:</p>
			<pre>for k,v in y.items():
    print("{}: {}".format(k,v))</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_07_26.jpg" alt="Figure 7.26: The output using dict" width="1067" height="600"/>
				</div>
			</div>
			<h6>Figure 7.26: The output using dict</h6>
			<p>Note that the items in the dictionary are not of the same type, that is, they are not similar objects. Some are floating-point numbers, such as the area, many are simple strings, but some are lists or even lists of dictionaries!</p>
			<p>This is fairly common with JSON data. The internal data structure of JSON can be arbitrarily complex and multilevel, that is, you can have a dictionary of lists of dictionaries of dictionaries of lists of lists…. and so on.</p>
			<h4>Note</h4>
			<p class="callout">It is clear, therefore, that there is no universal method or processing function for JSON data format, and you have to write custom loops and functions to extract data from such a dictionary object based on your particular needs.</p>
			<p>Now, we will write a small loop to extract the languages spoken in Switzerland. First, let's examine the dictionary closely and see where the language data is:</p>
			<div><div><img src="img/C11065_07_27.jpg" alt="" width="1428" height="365"/>
				</div>
			</div>
			<h6>Figure 7.27: The tags</h6>
			<p>So, the data is embedded inside a list of dictionaries, which is accessed by a particular key of the main dictionary.</p>
			<p>We can write simple two-line code to extract this data:</p>
			<pre>for lang in y['languages']:
    print(lang['name'])</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_07_28.jpg" alt="Figure 7.28: The output showing the languages" width="726" height="178"/>
				</div>
			</div>
			<h6>Figure 7.28: The output showing the languages</h6>
			<h3 id="_idParaDest-223">Using a Function that Extrac<a id="_idTextAnchor270"/>ts a DataFrame Containing Key Information</h3>
			<p>Here, we are interested in writing a function that can take a list of countries and return a pandas DataFrame with some key information:</p>
			<ul>
				<li>Capital</li>
				<li>Region</li>
				<li>Sub-region</li>
				<li>Population</li>
				<li>Latitude/longitude</li>
				<li>Area</li>
				<li>Gini index</li>
				<li>Time zones</li>
				<li>Currencies</li>
				<li>Languages<h4>Note</h4><p class="callout">This is the kind of wrapper function you are generally expected to write in real-life data wrangling tasks, that is, a utility function that can take a user argument and output a useful data structure (or a mini database type object) with key information extracted over the internet about the item the user is interested in.</p></li>
			</ul>
			<p>We will show you the whole function first and then discuss some key points about it. It is a slightly complex and long piece of code. However, based on your Python- based data wrangling knowledge, you should be able to examine this function closely and understand what it is doing:</p>
			<pre>import pandas as pd
import json
def build_country_database(list_country):
    """
    Takes a list of country names.
    Output a DataFrame with key information about those countries.
    """
    # Define an empty dictionary with keys
    country_dict={'Country':[],'Capital':[],'Region':[],'Sub-region':[],'Population':[],
                  'Lattitude':[],'Longitude':[],'Area':[],'Gini':[],'Timezones':[],
                  'Currencies':[],'Languages':[]}</pre>
			<h4>Note</h4>
			<p class="callout">The code has been truncated here. Please find the entire code at the following GitHub link and code bundle folder link <a href="https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson07/Exercise93-94/Lesson%207%20Topic%203%20Exercises.ipynb">https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter07/Exercise93-94/Chapter%207%20Topic%203%20Exercises.ipynb</a>.</p>
			<p>Here are some of the key points about this function:</p>
			<ul>
				<li>It starts by building an empty dictionary of lists. This is the chosen format for finally passing to the pandas <code>DataFrame</code> method, which can accept such a format and returns a nice DataFrame with column names set to the dictionary keys' names.</li>
				<li>We use the previously defined <code>get_country_data</code> function to extract data for each country in the user-defined list. For this, we simply iterate over the list and call this function.</li>
				<li>We check the output of the <code>get_country_data</code> function. If, for some reason, it returns a <code>None</code> object, we will know that the API reading was not successful, and we will print out a suitable message. Again, this is an example of an error-handling mechanism and you must have them in your code. Without such small error checking code, your application won't be robust enough for the occasional incorrect input or API malfunction!</li>
				<li>For many data types, we simply extract the data from the main JSON dictionary and append it to the corresponding list in our data dictionary.</li>
				<li>However, for special data types, such as time zones, currencies, and languages, we write a special loop to extract the data without error.</li>
				<li>We also take care of the fact that these special data types can have a variable length, that is, some countries may have multiple spoken languages, but most will have only one entry. So, we check whether the length of the list is greater than one and handle the data accordingly.</li>
			</ul>
			<h3 id="_idParaDest-224">Exercise 94: Testing the Fu<a id="_idTextAnchor271"/>nction by Building a Small Database of Countries' Information</h3>
			<p>Finally, we test this function by passing a list of country names:</p>
			<ol>
				<li value="1">To test its robustness, we pass in an erroneous name – such as 'Turmeric' in this case!<p>See the output… it detected that it did not get any data back for the incorrect entry and printed out a suitable message. The key is that, if you do not have the error checking and handling code in your function, then it will stop execution on that entry and will not return the expected mini database. To avoid this behavior, such error handling code is invaluable:</p><div><img src="img/C11065_07_29.jpg" alt="" width="1200" height="564"/></div><h6>Figure 7.29: The incorrect entry highlighted</h6></li>
				<li>Finally, the output is a pandas DataFrame, which is as follows:</li>
			</ol>
			<div><div><img src="img/C11065_07_30.jpg" alt="Figure 7.30: The data extracted correctly" width="1432" height="630"/>
				</div>
			</div>
			<h6>Figure 7.30: The data extracted correctly</h6>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor272"/>Fundamentals of Regular Expressions (RegEx)</h2>
			<p><strong class="bold">Reg</strong>ular <strong class="bold">ex</strong>pressions or <strong class="bold">regex</strong> are used to identify whether a pattern exists in a given sequence of characters a (string) or not. They help in manipulating textual data, which is often a prerequisite for data science projects that involve text mining.</p>
			<h3 id="_idParaDest-226"><a id="_idTextAnchor273"/>Regex in the Context of Web Scraping</h3>
			<p>Web pages are often full of text and while there are some methods in <code>BeautifulSoup</code> or XML parser to extract raw text, there is no method for the intelligent analysis of that text. If, as a data wrangler, you are looking for a particular piece of data (for example, email IDs or phone numbers in a special format), you have to do a lot of string manipulation on a large corpus to extract email IDs or phone numbers. RegEx are very powerful and save data wrangling professional a lot of time and effort with string manipulation because they can search for complex textual patterns with wildcards of an arbitrary length.</p>
			<p>RegEx is like a mini-programming language in itself and common ideas are used not only Python, but in all widely used web app languages like JavaScript, PHP, Perl, and so on. The RegEx module is in-built in Python, and you can import it by using the following code:</p>
			<pre>import re</pre>
			<h3 id="_idParaDest-227">Exercise 95: Using the match <a id="_idTextAnchor274"/>Method to Check Whether a Pattern matches a String/Sequence</h3>
			<p>One of the most common regex methods is <code>match</code>. This is used to check for an exact or partial match at a beginning of the string (by default):</p>
			<ol>
				<li value="1">Import the RegEx module:<pre>import re</pre></li>
				<li>Define a string and a pattern:<pre>string1 = 'Python'
pattern = r"Python"</pre></li>
				<li>Write a conditional expression to check for a match:<pre>if re.match(pattern,string1):
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The preceding code should give an affirmative answer, that is, "Matches!".</p></li>
				<li>Test this with a string that only differs in the first letter by making it lowercase:<pre>string2 = 'python'
if re.match(pattern,string2):
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output is as follows:</p><pre> Doesn't match.</pre></li>
			</ol>
			<h3 id="_idParaDest-228">Using the Compile Method to Cr<a id="_idTextAnchor275"/>eate a Regex Program</h3>
			<p>In a program or module, if we are making heavy use of a particular pattern, then it is better to use the <code>compile</code> method and create a regex program and then call methods on this program.</p>
			<p>Here is how you compile a regex program:</p>
			<pre>prog = re.compile(pattern)
prog.match(string1)</pre>
			<p>The output is as follows:</p>
			<pre> &lt;_sre.SRE_Match object; span=(0, 6), match='Python'&gt;</pre>
			<p>This code produced an <code>SRE.Match</code> object that has a <code>span</code> of (0,6) and the matched string of 'Python'. The span here simply denotes the start and end indices of the pattern that was matched. These indices may come in handy in a text mining program where the subsequent code uses the indices for further search or decision-making purposes. We will see some examples of that later.</p>
			<h3 id="_idParaDest-229">Exercise 96: Compiling Progra<a id="_idTextAnchor276"/>ms to Match Objects</h3>
			<p>Compiled objects act like functions in that they return <code>None</code> if the pattern does not match. Here, we are going to check that by writing a simple conditional. This concept will come in handy later when we write a small utility function to check for the type of the returned object from regex-compiled programs and act accordingly. We cannot be sure whether a pattern will match a given string or whether it will appear in a corpus of the text (if we are searching for the pattern anywhere within the text). Depending on the situation, we may encounter <code>Match</code> objects or <code>None</code> as the returned value, and we have to handle this gracefully:</p>
			<pre>#string1 = 'Python'
#string2 = 'python'
#pattern = r"Python"</pre>
			<ol>
				<li value="1">Use the <code>compile</code> function in RegEx:<pre>prog = re.compile(pattern)</pre></li>
				<li>Match it with the first string:<pre>if prog.match(string1)!=None:
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output is as follows:</p><pre> Matches!</pre></li>
				<li>Match it with the second string:<pre>if prog.match(string2)!=None:
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output is as follows:</p><pre> Doesn't match.</pre></li>
			</ol>
			<h3 id="_idParaDest-230">Exercise 97: Using Additional<a id="_idTextAnchor277"/> Parameters in Match to Check for Positional Matching</h3>
			<p>By default, <code>match</code> looks for pattern matching at the beginning of the given string. But sometimes, we need to check matching at a specific location in the string:</p>
			<ol>
				<li value="1">Match <code>y</code> for the second position:<pre>prog = re.compile(r'y')
prog.match('Python',pos=1)</pre><p>The output is as follows:</p><pre> &lt;_sre.SRE_Match object; span=(1, 2), match='y'&gt;</pre></li>
				<li>Check for a pattern called <code>thon</code> starting from <code>pos=2</code>, that is, the third character:<pre>prog = re.compile(r'thon')
prog.match('Python',pos=2)</pre><p>The output is as follows:</p><pre> &lt;_sre.SRE_Match object; span=(2, 6), match='thon'&gt;</pre></li>
				<li>Find a match in a different string by using the following command:<pre>prog.match('Marathon',pos=4)</pre><p>The output is as follows:</p><pre>&lt;_sre.SRE_Match object; span=(4, 8), match='thon'&gt;</pre></li>
			</ol>
			<h3 id="_idParaDest-231">Finding the Number of Words in<a id="_idTextAnchor278"/> a List That End with "ing"</h3>
			<p>Suppose we want to find out if a given string has the last three letters: 'ing'. This kind of query may come up in a text analytics/text mining program where somebody is interested in finding instances of present continuous tense words, which are highly likely to end with 'ing'. However, other nouns may also end with 'ing' (as we will see in this example):</p>
			<pre>prog = re.compile(r'ing')
words = ['Spring','Cycling','Ringtone']</pre>
			<p>Create a <code>for</code> loop to find words ending with 'ing':</p>
			<pre>for w in words:
    if prog.match(w,pos=len(w)-3)!=None:
        print("{} has last three letters 'ing'".format(w))
    else:
        print("{} does not have last three letter as 'ing'".format(w))</pre>
			<p>The output is as follows:</p>
			<pre> Spring has last three letters 'ing'
 Cycling has last three letters 'ing'
 Ringtone does not have last three letter as 'ing'</pre>
			<h4>Note</h4>
			<p class="callout">It looks plain and simple, and you may well wonder what the purpose of using a special regex module for this is. A simple string method should have been sufficient. Yes, it would have been OK for this particular example, but the whole point of using regex is to be able to use very complex string patterns that are not at all obvious when it comes to how they are written using simple string methods. We will see the real power of regex compared to string methods shortly. But before that, let's explore another of the most commonly used methods, called <code>search</code>.</p>
			<h3 id="_idParaDest-232">Exercise 98: The search Metho<a id="_idTextAnchor279"/>d in Regex</h3>
			<p><code>Search</code> and <code>match</code> are related concepts and they both return the same Match object. The real difference between them is that <strong class="bold">match works for only the first match</strong> (either at the beginning of the string or at a specified position, as we saw in the previous exercises), whereas <strong class="bold">search looks for the pattern anywhere in the string</strong> and returns the appropriate position if it finds a match:</p>
			<ol>
				<li value="1">Use the <code>compile</code> method to find matching strings:<pre>prog = re.compile('ing')
if prog.match('Spring')==None:
    print("None")</pre></li>
				<li>The output is as follows:<pre> None</pre></li>
				<li>Search the string by using the following command:<pre>prog.search('Spring')
&lt;_sre.SRE_Match object; span=(3, 6), match='ing'&gt;
prog.search('Ringtone')
&lt;_sre.SRE_Match object; span=(1, 4), match='ing'&gt;</pre><p>As you can see, the <code>match</code> method returns <code>None</code> for the input <code>spring</code>, and we had to write code to print that out explicitly (because in Jupyter notebook, nothing will show up for a None object). But <code>search</code> returns a <code>Match</code> object with <code>span=(3,6)</code> as it finds the <code>ing </code>pattern spanning those positions.</p></li>
			</ol>
			<p>Similarly, for the <code>Ringtone</code> string, it finds the correct position of the match and returns <code>span=(1,4)</code>.</p>
			<h3 id="_idParaDest-233">Exercise 99: Using the span M<a id="_idTextAnchor280"/>ethod of the <code>Match</code> Object to Locate the Position of the Matched Pattern</h3>
			<p>As you will understand by now, the <code>span</code> contained in the <code>Match</code> object is useful for locating the exact position of the pattern as it appears in the string.</p>
			<ol>
				<li value="1">Intitialize <code>prog</code> with pattern ing.<pre>prog = re.compile(r'ing')
words = ['Spring','Cycling','Ringtone']</pre></li>
				<li>Create a function to return a tuple of start and end positions of match.<pre>for w in words:
    mt = prog.search(w)
    # Span returns a tuple of start and end positions of the match
    start_pos = mt.span()[0] # Starting position of the match
    end_pos = mt.span()[1] # Ending position of the match</pre></li>
				<li>Print the words ending with ing in the start or end position.<pre>    print("The word '{}' contains 'ing' in the position {}-{}".format(w,start_pos,end_pos))</pre></li>
			</ol>
			<p>The output is as follows:</p>
			<pre> The word 'Spring' contains 'ing' in the position 3-6
 The word 'Cycling' contains 'ing' in the position 4-7
 The word 'Ringtone' contains 'ing' in the position 1-4</pre>
			<h3 id="_idParaDest-234"><a id="_idTextAnchor281"/>Exercise 100: Examples of  Single Character Pattern Matching with search</h3>
			<p>Now, we will start getting into the real usage of regex with examples of various useful pattern matching. First, we will explore single-character matching. We will also use the <code>group</code> method, which essentially returns the matched pattern in a string format so that we can print and process it easily:</p>
			<ol>
				<li value="1">Dot (.) matches any single character except a newline character:<pre>prog = re.compile(r'py.')
print(prog.search('pygmy').group())
print(prog.search('Jupyter').group())</pre><p>The output is as follows:</p><pre> pyg
 pyt</pre></li>
				<li><code>\w</code> (lowercase w) matches any single letter, digit, or underscore:<pre>prog = re.compile(r'c\wm')
print(prog.search('comedy').group())
print(prog.search('camera').group())
print(prog.search('pac_man').group())
print(prog.search('pac2man').group())</pre><p>The output is as follows:</p><pre> com
 cam
 c_m
 c2m</pre></li>
				<li><code>\W</code> (uppercase W) matches anything not covered with <code>\w:</code><pre>prog = re.compile(r'4\W1')
print(prog.search('4/1 was a wonderful day!').group())
print(prog.search('4-1 was a wonderful day!').group())
print(prog.search('4.1 was a wonderful day!').group())
print(prog.search('Remember the wonderful day 04/1?').group())</pre><p>The output is as follows:</p><pre> 4/1
 4-1
 4.1
 4/1</pre></li>
				<li><code>\s</code> (lowercase s) matches a single whitespace character, such as a space, newline, tab, or return:<pre>prog = re.compile(r'Data\swrangling')
print(prog.search("Data wrangling is cool").group())
print("-"*80)
print("Data\twrangling is the full string")
print(prog.search("Data\twrangling is the full string").group())
print("-"*80)
print("Data\nwrangling is the full string")
print(prog.search("Data\nwrangling").group())</pre><p>The output is as follows:</p><pre>Data wrangling
----------------------------------------------------------------------
Data	wrangling is the full string
Data	wrangling
----------------------------------------------------------------------
Data
wrangling is the full string
Data
wrangling</pre></li>
				<li><code>\d</code> matches numerical digits 0 – 9:<pre>prog = re.compile(r"score was \d\d")
print(prog.search("My score was 67").group())
print(prog.search("Your score was 73").group())</pre><p>The output is as follows:</p><pre> score was 67
 score was 73</pre></li>
			</ol>
			<h3 id="_idParaDest-235">Exercise 101: Examples of Patt<a id="_idTextAnchor282"/>ern Matching at the Start or End of a String</h3>
			<p>In this exercise, we will match patterns with strings. The focus is to find out whether the pattern is present at the start or the end of the string:</p>
			<ol>
				<li value="1">Write a function to handle cases where match is not found, that is, to handle <code>None</code> objects as returns:<pre>def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())</pre></li>
				<li>Use <code>^</code> (Caret) to match a pattern at the start of the string:<pre>prog = re.compile(r'^India')
print_match("Russia implemented this law")
print_match("India implemented that law")
print_match("This law was implemented by India")
The output is as follows: No match
 India
 No match</pre></li>
				<li>Use <code>$</code> (dollar sign) to match a pattern at the end of the string:<pre>prog = re.compile(r'Apple$')
print_match("Patent no 123456 belongs to Apple")
print_match("Patent no 345672 belongs to Samsung")
print_match("Patent no 987654 belongs to Apple")</pre><p>The output is as follows:</p><pre> Apple
 No match
 Apple</pre></li>
			</ol>
			<h3 id="_idParaDest-236">Exercise 102: Examples of Patt<a id="_idTextAnchor283"/>ern Matching with Multiple Characters</h3>
			<p>Now, we will turn to more exciting and useful pattern matching with examples of multiple characters matching. You should start seeing and appreciating the real power of regex by now.</p>
			<h4>Note:</h4>
			<p class="callout">For these examples and exercises, also try to think how you would implement them without regex, that is, by using simple string methods and any other logic that you can think of. Then, compare that solution to the ones implemented with regex for brevity and efficiency.</p>
			<ol>
				<li value="1">Use <code>*</code> to match 0 or more repetitions of the preceding <code>RE</code>:<pre>prog = re.compile(r'ab*')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre> a
 ab
 abbb
 No match
 ab
 abb</pre></li>
				<li>Using <code>+</code> causes the resulting RE to match 1 or more repetitions of the preceding RE:<pre>prog = re.compile(r'ab+')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre> No match
 ab
 abbb
 No match
 ab
 abb</pre></li>
				<li><code>?</code> causes the resulting RE to match precisely 0 or 1 repetitions of the preceding RE:<pre>prog = re.compile(r'ab?')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre> a
 ab
 ab
 No match
 ab
 ab</pre></li>
			</ol>
			<h3 id="_idParaDest-237">Exercise 103: Greedy versus No<a id="_idTextAnchor284"/>n-Greedy Matching</h3>
			<p>The standard (default) mode of pattern matching in regex is greedy, that is, the program tries to match as much as it can. Sometimes, this behavior is natural, but, in some cases, you may want to match minimally:</p>
			<ol>
				<li value="1">The greedy way of matching a string is as follows:<pre>prog = re.compile(r'&lt;.*&gt;')
print_match('&lt;a&gt; b &lt;c&gt;')</pre><p>The output is as follows:</p><pre> &lt;a&gt; b &lt;c&gt;</pre></li>
				<li>So, the preceding regex found both tags with the &lt;&gt; pattern, but what if we wanted to match the first tag only and stop there. We can use <code>?</code> by inserting it after any regex expression to make it non-greedy:<pre>prog = re.compile(r'&lt;.*?&gt;')
print_match('&lt;a&gt; b &lt;c&gt;')</pre><p>The output is as follows:</p><pre> &lt;a&gt;</pre></li>
			</ol>
			<h3 id="_idParaDest-238"><a id="_idTextAnchor285"/>Exercise 104: Controlling Repetitions to Match</h3>
			<p>In many situations, we want to have precise control over how many repetitions of the pattern we want to match in a text. This can be done in a few ways, which we will show examples of here:</p>
			<ol>
				<li value="1"><code>{m}</code> specifies exactly <code>m</code> copies of RE to match. Fewer matches cause a non-match and returns <code>None:</code><pre>prog = re.compile(r'A{3}')
print_match("ccAAAdd")
print_match("ccAAAAdd")
print_match("ccAAdd")</pre><p>The output is as follows:</p><pre> AAA
 AAA
 No match</pre></li>
				<li><code>{m,n}</code> specifies exactly <code>m</code> to <code>n</code> copies of <code>RE</code> to match:<pre>prog = re.compile(r'A{2,4}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre> AAAB
 No match
 AAB
 AAAAB</pre></li>
				<li>Omitting <code>m</code> specifies a lower bound of zero:<pre>prog = re.compile(r'A{,3}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre> AAAB
 AB
 AAB
 AAAB</pre></li>
				<li>Omitting <code>n</code> specifies an infinite upper bound:<pre>prog = re.compile(r'A{3,}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre> AAAB
 No match
 No match
 AAAAAAAB</pre></li>
				<li><code>{m,n}?</code> specifies <code>m</code> to <code>n</code> copies of RE to match in a non-greedy fashion:<pre>prog = re.compile(r'A{2,4}')
print_match("AAAAAAA")
prog = re.compile(r'A{2,4}?')
print_match("AAAAAAA")</pre><p>The output is as follows:</p><pre> AAAA
 AA</pre></li>
			</ol>
			<h3 id="_idParaDest-239">Exercise 105: Sets of Matching<a id="_idTextAnchor286"/> Characters</h3>
			<p>To match an arbitrarily complex pattern, we need to be able to include a logical combination of characters together as a bunch. Regex gives us that kind of capability:</p>
			<ol>
				<li value="1">The following examples demonstrate such uses of regex. <code>[x,y,z]</code> matches x, y, or z:<pre>prog = re.compile(r'[A,B]')
print_match("ccAd")
print_match("ccABd")
print_match("ccXdB")
print_match("ccXdZ")</pre><p>The output will be as follows:</p><pre> A
 A
 B
 No match</pre><p>A range of characters can be matched inside the set using -. This is one of the most widely used regex techniques!</p></li>
				<li>Suppose we want to pick out an email address from a text. Email addresses are generally of the form <code>&lt;some name&gt;@&lt;some domain name&gt;.&lt;some domain identifier&gt;</code>:<pre>prog = re.compile(r'[a-zA-Z]+@+[a-zA-Z]+\.com')
print_match("My email is coolguy@xyz.com")
print_match("My email is coolguy12@xyz.com")</pre><p>The output is as follows:</p><pre> coolguy@xyz.com
 No match</pre><p>Look at the regex pattern inside the [ … ]. It is '<code>a-zA-Z'</code>. This covers all alphabets, including lowercase and uppercase! With this one simple regex, you are able to match any (pure) alphabetical string for that part of the email. Now, the next pattern is '<code>@</code>', which is added to the previous regex by a '<code>+</code>' character. This is the way to build up a complex regex: by adding/stacking up individual regex patterns. We also use the same <code>[a-zA-Z]</code> for the email domain name and add a '<code>.com</code>' at the end to complete the pattern as a valid email address. Why \.? Because, by itself, DOT (.) is used as a special modifier in regex, but here we want to use DOT (.) just as DOT (.), not as a modifier. So, we need to precede it by a '\'.</p></li>
				<li>So, with this regex, we could extract the first email address perfectly but got '<code>No match</code>' with the second one.</li>
				<li>What happened with the second email ID?</li>
				<li>The regex could not capture it because it had the number '12' in the name! That pattern is not captured by the expression [a-zA-Z].</li>
				<li>Let's change that and add the digits as well:<pre>prog = re.compile(r'[a-zA-Z0-9]+@+[a-zA-Z]+\.com')
print_match("My email is coolguy12@xyz.com")
print_match("My email is <a href="mailto:coolguy12@xyz.org">coolguy12@xyz.org</a>")</pre><p>The output is as follows:</p><pre> coolguy12@xyz.com
 No match</pre><p>Now, we catch the first email ID perfectly. But what's going on with the second one? Again, we got a mismatch. The reason is that we changed the .com to .org in that email, and in our regex expression, that portion was hardcoded as <code>.com</code>, so it did not find a match.</p></li>
				<li>Let's try to address this in the following regex:<pre>prog = re.compile(r'[a-zA-Z0-9]+@+[a-zA-Z]+\.+[a-zA-Z]{2,3}')
print_match("My email is coolguy12@xyz.org")
print_match("My email is coolguy12[AT]xyz[DOT]org")</pre><p>The output is as follows:</p><pre> coolguy12@xyz.org
 No match</pre></li>
				<li>In this regex, we used the fact that most domain identifiers have 2 or 3 characters, so we used <code>[a-zA-Z]{2,3}</code> to capture that.</li>
			</ol>
			<p>What happened with the second email ID? This is an example of the small tweaks that you can make to stay ahead of telemarketers who want to scrape online forums or any other corpus of text and extract your email ID. If you do not want your email to be found, you can change <code>@</code> to <code>[AT]</code> and . to <code>[DOT]</code> ,and hopefully that can beat some regex techniques (but not all)!</p>
			<h3 id="_idParaDest-240">Exercise 106: The use of OR in<a id="_idTextAnchor287"/> Regex using the OR Operator</h3>
			<p>Because regex patterns are like complex and compact logical constructors themselves, it makes perfect sense that we want to combine them to construct even more complex programs when needed. We can do that by using the <code>|</code> operator:</p>
			<ol>
				<li value="1">The following example demonstrates the use of the OR operator:<pre>prog = re.compile(r'[0-9]{10}')
print_match("3124567897")
print_match("312-456-7897")</pre><p>The output is as follows:</p><pre> 3124567897
 No match</pre><p>So, here, we are trying to extract patterns of 10-digit numbers that could be phone numbers. Note the use of <code>{10}</code> to denote exactly 10-digit numbers in the pattern. But the second number could not be matched for obvious reasons – it had '-' symbols inserted in between groups of numbers.</p></li>
				<li>Use multiple smaller regexes and logically combine them by using the following command:<pre>prog = re.compile(r'[0-9]{10}|[0-9]{3}-[0-9]{3}-[0-9]{4}')
print_match("3124567897")
print_match("312-456-7897")</pre><p>The output is as follows:</p><pre> 3124567897
 312-456-7897</pre><p>Phone numbers are written in a myriad of ways and if you search on the web, you will see examples of very complex regexes (written not only in Python but other widely used languages, for web apps such as JavaScript, C++, PHP, Perl, and so on) for capturing phone numbers.</p></li>
				<li>Create four strings and execute <code>print_match</code> on them:<pre>p1= r'[0-9]{10}'
p2=r'[0-9]{3}-[0-9]{3}-[0-9]{4}'
p3 = r'\([0-9]{3}\)[0-9]{3}-[0-9]{4}'
p4 = r'[0-9]{3}\.[0-9]{3}\.[0-9]{4}'
pattern= p1+'|'+p2+'|'+p3+'|'+p4
prog = re.compile(pattern)
print_match("3124567897")
print_match("312-456-7897")
print_match("(312)456-7897")
print_match("312.456.7897")</pre><p>The output is as follows:</p><pre> 3124567897
 312-456-7897
 (312)456-7897
 312.456.7897</pre></li>
			</ol>
			<h3 id="_idParaDest-241"><code><a id="_idTextAnchor288"/>The findall</code> Method</h3>
			<p>The last regex method that we will learn in this chapter is <code>findall</code>. Essentially, it is a <strong class="bold">search-and-aggregate</strong> method, that is, it puts all the instances that match with the regex pattern in a given text and returns them in a list. This is extremely useful, as we can just count the length of the returned list to count the number of occurrences or pick and use the returned pattern-matched words one by one as we see fit.</p>
			<p>Note that, although we are giving short examples of single sentences in this chapter, you will often deal with a large corpus of text when using a RegEx.</p>
			<p>In those cases you are likely to get many matches from a single regex pattern search. For all of those cases, the <code>findall</code> method is going to be the most useful:</p>
			<pre>ph_numbers = """Here are some phone numbers.
Pick out the numbers with 312 area code: 
312-423-3456, 456-334-6721, 312-5478-9999, 
312-Not-a-Number,777.345.2317, 312.331.6789"""
print(ph_numbers)
re.findall('312+[-\.][0-9-\.]+',ph_numbers)</pre>
			<p>The output is as follows:</p>
			<pre> Here are some phone numbers.
Pick out the numbers with 312 area code: 
312-423-3456, 456-334-6721, 312-5478-9999, 
312-Not-a-Number,777.345.2317, 312.331.6789
 ['312-423-3456', '312-5478-9999', '312.331.6789']</pre>
			<h3 id="_idParaDest-242"><a id="_idTextAnchor289"/>Activity 9: Extracting the Top 100 eBooks from Gutenberg</h3>
			<p>Project Gutenberg encourages the creation and distribution of eBooks by encouraging volunteer efforts to digitize and archive cultural works. This activity aims to scrape the URL of Project Gutenberg's Top 100 eBooks to identify the eBooks' links. It uses BeautifulSoup4 to parse the HTML and regular expression code to identify the Top 100 eBook file numbers.</p>
			<p>You can use those book ID numbers to download the book into your local drive if you want.</p>
			<p>Head over to the supplied Jupyter notebook (in the GitHub repository) to work on this activity.</p>
			<p>These are the steps that will help you solve this activity:</p>
			<ol>
				<li value="1">Import the necessary libraries, including <code>regex</code> and <code>beautifulsoup</code>.</li>
				<li>Check the SSL certificate.</li>
				<li>Read the HTML from the URL.</li>
				<li>Write a small function to check the status of the web request.</li>
				<li>Decode the response and pass this on to BeautifulSoup for HTML parsing.</li>
				<li>Find all the <code>href</code> tags and store them in the list of links. Check what the list looks like – print the first 30 elements.</li>
				<li>Use a regular expression to find the numeric digits in these links. These are the file numbers for the top 100 eBooks.</li>
				<li>Initialize the empty list to hold the file numbers over an appropriate range and use <code>regex</code> to find the numeric digits in the link <code>href</code> string. Use the <code>findall</code> method.</li>
				<li>What does the <code>soup</code> object's text look like? Use the .<code>text</code> method and print only the first 2,000 characters (do not print the whole thing, as it is too long).</li>
				<li>Search in the extracted text (using a regular expression) from the soup object to find the names of the top 100 eBooks (yesterday's ranking).</li>
				<li>Create a starting index. It should point at the text <em class="italics">Top 100 Ebooks yesterday</em>. Use the <code>splitlines</code> method of soup.text. It splits the lines of text of the soup object.</li>
				<li>Loop 1-100 to add the strings of the next 100 lines to this temporary list. Hint: use the <code>splitlines</code> method.</li>
				<li>Use a regular expression to extract only text from the name strings and append it to an empty list. Use <code>match</code> and <code>span</code> to find the indices and use them.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 315.</p></li>
			</ol>
			<h3 id="_idParaDest-243"><a id="_idTextAnchor290"/>Activity 10: Building Your Own Movie Database by Reading an API</h3>
			<p>In this activity, you will build a complete movie database by communicating and interfacing with a free API. You will learn about obtaining a unique user key that must be used when your program tries to access the API. This activity will teach you general chapters about working with an API, which are fairly common for other highly popular API services such as Google or Twitter. Therefore, after doing this exercise, you will be confident about writing more complex programs to scrape data from such services.</p>
			<p>The aims of this activity are as follows:</p>
			<ul>
				<li>To retrieve and print basic data about a movie (the title is entered by the user) from the web (OMDb database)</li>
				<li>If a poster of the movie can be found, it downloads the file and saves it at a user-specified location</li>
			</ul>
			<p>These are the steps that will help you solve this activity:</p>
			<ol>
				<li value="1">Import <code>urllib.request</code>, <code>urllib.parse</code>, <code>urllib.error</code>, and <code>json</code>.</li>
				<li>Load the secret API key (you have to get one from the OMDb website and use that; it has a daily limit of 1,000) from a JSON file stored in the same folder in a variable, by using <code>json.loads.</code></li>
				<li>Obtain a key and store it in JSON as <code>APIkeys.json</code>.</li>
				<li>Open the <code>APIkeys.json</code> file.</li>
				<li>Assign the OMDb portal (<a href="http://www.omdbapi.com/?">http://www.omdbapi.com/?</a>) as a string to a variable.</li>
				<li>Create a variable called <code>apikey</code> with the last portion of the URL (<code>&amp;apikey=secretapikey</code>), where <code>secretapikey</code> is your own API key.</li>
				<li>Write a utility function called <code>print_json</code> to print the movie data from a JSON file (which we will get from the portal).</li>
				<li>Write a utility function to download a poster of the movie based on the information from the JSON dataset and save it  in your local folder. Use the <code>os</code> module. The poster data is stored in the JSON key <code>Poster</code>. Use the Python command to open a file and write the poster data. Close the file after you're done. This function will save the poster data as an image file.</li>
				<li>Write a utility function called <code>search_movie</code> to search for a movie by its name, print the downloaded <code>JSON</code> data, and save the movie poster in the local folder. Use a <code>try-except</code> loop for this. Use the previously created <code>serviceurl</code> and <code>apikey</code> variables. You have to pass on a dictionary with a key, <code>t</code>, and the movie name as the corresponding value to the <code>urllib.parse.urlencode()</code> function and then add the <code>serviceurl</code> and <code>apikey</code> to the output of the function to construct the full URL. This URL will be used to access the data. The <code>JSON</code> data has a key called <code>Response</code>. If it is <code>True</code>, that means the read was successful. Check this before processing the data. If it's not successful, then print the <code>JSON</code> key <code>Error</code>, which will contain the appropriate error message returned by the movie database.</li>
				<li>Test the <code>search_movie</code> function by entering <code>Titanic</code>.</li>
				<li>Test the <code>search_movie</code> function by entering <code>"Random_error"</code> (obviously, this will not be found, and you should be able to check whether your error catching code is working properly).<h4>Note:</h4><p class="callout">The solution for this activity can be found on page 320.</p></li>
			</ol>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor291"/>Summary</h2>
			<p>In this chapter, we went through several important concepts and learning modules related to advanced data gathering and web scraping. We started by reading data from web pages using two of the most popular Python libraries – <code>requests</code> and <code>BeautifulSoup</code>. In this task, we utilized the previous chapter's knowledge about the general structure of HTML pages and their interaction with Python code. We extracted meaningful data from the Wikipedia home page during this process.</p>
			<p>Then, we learned how to read data from XML and JSON files, two of the most widely used data streaming/exchange formats on the web. For the XML part, we showed you how to traverse the tree-structure data string efficiently to extract key information. For the JSON part, we mixed it with reading data from the web using an API (Application Program Interface). The API we consumed was RESTful, which is one of the major standards in Web API. </p>
			<p>At the end of this chapter, we went through a detailed exercise of using regex techniques in tricky string-matching problems to scrape useful information from a large and messy text corpus, parsed from HTML. This chapter should come in extremely handy for string and text processing tasks in your data wrangling career.</p>
			<p>In the next chapter, we will learn about databases with Python.</p>
		</div>
	</div>



  </body></html>