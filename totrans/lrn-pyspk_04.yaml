- en: Chapter 4. Prepare Data for Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All data is dirty, irrespective of what the source of the data might lead you
    to believe: it might be your colleague, a telemetry system that monitors your
    environment, a dataset you download from the web, or some other source. Until
    you have tested and proven to yourself that your data is in a clean state (we
    will get to what clean state means in a second), you should neither trust it nor
    use it for modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Your data can be stained with duplicates, missing observations and outliers,
    non-existent addresses, wrong phone numbers and area codes, inaccurate geographical
    coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases,
    trailing spaces, and many other more subtle problems. It is your job to clean
    it, irrespective of whether you are a data scientist or data engineer, so you
    can build a statistical or machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Your dataset is considered technically clean if none of the aforementioned problems
    can be found. However, to clean the dataset for modeling purposes, you also need
    to check the distributions of your features and confirm they fit the predefined
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you can expect to spend 80-90% of your time *massaging*
    your data and getting familiar with all the features. This chapter will guide
    you through that process, leveraging Spark capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Recognize and handle duplicates, missing observations, and outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate descriptive statistics and correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize your data with matplotlib and Bokeh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking for duplicates, missing observations, and outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until you have fully tested the data and proven it worthy of your time, you
    should neither trust it nor use it. In this section, we will show you how to deal
    with duplicates, missing observations, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Duplicates are observations that appear as distinct rows in your dataset, but
    which, upon closer inspection, look the same. That is, if you looked at them side
    by side, all the features in these two (or more) rows would have exactly the same
    values.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if your data has some form of an ID to distinguish between
    records (or associate them with certain users, for example), then what might initially
    appear as a duplicate may not be; sometimes systems fail and produce erroneous
    IDs. In such a situation, you need to either check whether the same ID is a real
    duplicate, or you need to come up with a new ID system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have several issues here:'
  prefs: []
  type: TYPE_NORMAL
- en: We have two rows with IDs equal to `3` and they are exactly the same
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows with IDs `1` and `4` are the same — the only thing that's different is
    their IDs, so we can safely assume that they are the same person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have two rows with IDs equal to `5`, but that seems to be a recording issue,
    as they do not seem to be the same person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a very easy dataset with only seven rows. What do you do when you have
    millions of observations? The first thing I normally do is to check if I have
    any duplicates: I compare the counts of the full dataset with the one that I get
    after running a `.distinct()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what you get back for our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If these two numbers differ, then you know you have, what I like to call, pure
    duplicates: rows that are exact copies of each other. We can drop these rows by
    using the `.dropDuplicates(...)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dataset will then look as follows (once you run `df.show()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We dropped one of the rows with ID `3`. Now let''s check whether there are
    any duplicates in the data irrespective of ID. We can quickly repeat what we have
    done earlier, but using only columns other than the ID column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see one more row that is a duplicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can still use the `.dropDuplicates(...)`, but will add the `subset` parameter
    that specifies only the columns other than the `id` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `subset` parameter instructs the `.dropDuplicates(...)` method to look
    for duplicated rows using only the columns specified via the `subset` parameter;
    in the preceding example, we will drop the duplicated records with the same `weight`,
    `height`, `age`, and `gender` but not `id`. Running the `df.show()`, we get the
    following cleaner dataset as we dropped the row with `id = 1` since it was identical
    to the record with `id = 4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know there are no full rows duplicated, or any identical rows differing
    only by ID, let''s check if there are any duplicated IDs. To calculate the total
    and distinct number of IDs in one step, we can use the `.agg(...)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous example, we first import all the functions from the `pyspark.sql`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This gives us access to a vast array of various functions, too many to list
    here. However, we strongly encourage you to study the PySpark's documentation
    at [http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `.count(...)` and `.countDistinct(...)` to, respectively, calculate
    the number of rows and the number of distinct `ids` in our DataFrame. The `.alias(...)`
    method allows us to specify a friendly name to the returned column.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have five rows in total, but only four distinct IDs. Since
    we have already dropped all the duplicates, we can safely assume that this might
    just be a fluke in our ID data, so we will give each row a unique ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet produced the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Duplicates](img/B05793_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `.monotonicallymonotonically_increasing_id()` method gives each record a
    unique and increasing ID. According to the documentation, as long as your data
    is put into less than roughly 1 billion partitions with less than 8 billions records
    in each, the ID is guaranteed to be unique.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A word of caution: in earlier versions of Spark the `.monotonicallymonotonically_increasing_id()`
    method would not necessarily return the same IDs across multiple evaluations of
    the same DataFrame. This, however, has been fixed in Spark 2.0.'
  prefs: []
  type: TYPE_NORMAL
- en: Missing observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will frequently encounter datasets with *blanks* in them. The missing values
    can happen for a variety of reasons: systems failure, people error, data schema
    changes, just to name a few.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to deal with missing values, if your data can afford it, is
    to drop the whole observation when any missing value is found. You have to be
    careful not to drop too many: depending on the distribution of the missing values
    across your dataset it might severely affect the usability of your dataset. If,
    after dropping the rows, I end up with a very small dataset, or find that the
    reduction in data size is more than 50%, I start checking my data to see what
    features have the most holes in them and perhaps exclude those altogether; if
    a feature has most of its values missing (unless a missing value bears a meaning),
    from a modeling point of view, it is fairly useless.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other way to deal with the observations with missing values is to impute
    some value in place of those `Nones`. Given the type of your data, you have several
    options to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: If your data is a discrete Boolean, you can turn it into a categorical variable
    by adding a third category — `Missing`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your data is already categorical, you can simply extend the number of levels
    and add the `Missing` category as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're dealing with ordinal or numerical data, you can impute either mean,
    median, or some other predefined value (for example, first or third quartile,
    depending on the distribution shape of your data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a similar example to the one we presented previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we deal with a number of missing values categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing *rows*, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The row with ID `3` has only one useful piece of information—the `height`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The row with ID `6` has only one missing value—the `age`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing *columns*, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `income` column, since it is a very personal thing to disclose, has most
    of its values missing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `weight` and `gender` columns have only one missing value each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `age` column has two missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To find the number of missing observations per row, we can use the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It tells us that, for example, the row with ID `3` has four missing observations,
    as we observed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what values are missing so that when we count missing observations
    in columns, we can decide whether to drop the observation altogether or impute
    some of the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now check what percentage of missing observations are there in each
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `*` argument to the `.count(...)` method (in place of a column name) instructs
    the method to count all rows. On the other hand, the `*` preceding the list declaration
    instructs the `.agg(...)` method to treat the list as a set of separate parameters
    passed to the function.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have 14% of missing observations in the `weight` and `gender` columns,
    twice as much in the `height` column, and almost 72% of missing observations in
    the `income` column. Now we know what to do.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will drop the `'income'` feature, as most of its values are missing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We now see that we do not need to drop the row with ID `3` as the coverage in
    the `'weight'` and `'age'` columns has enough observations (in our simplified
    example) to calculate the mean and impute it in the place of the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you decide to drop the observations instead, you can use the `.dropna(...)`
    method, as shown here. Here, we will also use the `thresh` parameter, which allows
    us to specify a threshold on the number of missing observations per row that would
    qualify the row to be dropped. This is useful if you have a dataset with tens
    or hundreds of features and you only want to drop those rows that exceed a certain
    threshold of missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, if you wanted to impute the observations, you can use the
    `.fillna(...)` method. This method accepts a single integer (long is also accepted),
    float, or string; all missing values in the whole dataset will then be filled
    in with that value. You can also pass a dictionary of a form `{''<colName>'':
    <value_to_impute>}`. This has the same limitation, in that, as the `<value_to_impute>`,
    you can only pass an integer, float, or string.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to impute a mean, median, or other calculated value, you need to
    first calculate the value, create a dictionary with such values, and then pass
    it to the `.fillna(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We omit the `gender` column as one cannot calculate a mean of a categorical
    variable, obviously.
  prefs: []
  type: TYPE_NORMAL
- en: We use a double conversion here. Taking the output of the `.agg(...)` method
    (a PySpark DataFrame), we first convert it into a pandas' DataFrame and then once
    more to a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that calling the `.toPandas()` can be problematic, as the method works
    essentially in the same way as `.collect()` in RDDs. It collects all the information
    from the workers and brings it over to the driver. It is unlikely to be a problem
    with the preceding dataset, unless you have thousands upon thousands of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `records` parameter to the `.to_dict(...)` method of pandas instructs it
    to create the following dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Missing observations](img/B05793_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since we cannot calculate the average (or any other numeric metric of a categorical
    variable), we added the `missing` category to the dictionary for the `gender`
    feature. Note that, even though the mean of the age column is 40.40, when imputed,
    the type of the `df_miss_no_income.age` column was preserved—it is still an integer.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers are those observations that deviate significantly from the distribution
    of the rest of your sample. The definitions of *significance* vary, but in the
    most general form, you can accept that there are no outliers if all the values
    are roughly within the Q1−1.5IQR and Q3+1.5IQR range, where IQR is the interquartile
    range; the IQR is defined as a difference between the upper- and lower-quartiles,
    that is, the 75th percentile (the Q3) and 25th percentile (the Q1), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s, again, consider a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use the definition we outlined previously to flag the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate the lower and upper cut off points for each feature. We
    will use the `.approxQuantile(...)` method. The first parameter specified is the
    name of the column, the second parameter can be either a number between `0` or
    `1` (where `0.5` means to calculated median) or a list (as in our case), and the
    third parameter specifies the acceptable level of an error for each metric (if
    set to `0`, it will calculate an exact value for the metric, but it can be really
    expensive to do so):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bounds` dictionary holds the lower and upper bounds for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outliers](img/B05793_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now use it to flag our outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outliers](img/B05793_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have two outliers in the `weight` feature and two in the `age` feature.
    By now you should know how to extract these, but here is a snippet that lists
    the values significantly differing from the rest of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outliers](img/B05793_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Equipped with the methods described in this section, you can quickly clean up
    even the biggest of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we would strongly discourage such behavior, you can build a model without
    knowing your data; it will most likely take you longer, and the quality of the
    resulting model might be less than optimal, but it is doable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will use the dataset we downloaded from [http://packages.revolutionanalytics.com/datasets/ccFraud.csv](http://packages.revolutionanalytics.com/datasets/ccFraud.csv).
    We did not alter the dataset itself, but it was GZipped and uploaded to [http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz](http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz).
    Please download the file first and save it in the same folder that contains your
    notebook for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The head of the dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting familiar with your data](img/B05793_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, any serious data scientist or data modeler will become acquainted with
    the dataset before starting any modeling. As a first thing, we normally start
    with some descriptive statistics to get a feeling for what we are dealing with.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Descriptive statistics, in the simplest sense, will tell you the basic information
    about your dataset: how many non-missing observations there are in your dataset,
    the mean and the standard deviation for the column, as well as the min and max
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, first things first—let''s load our data and convert it to a Spark
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: First, we load the only module we will need. The `pyspark.sql.types` exposes
    all the data types we can use, such as `IntegerType()` or `FloatType()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a full list of available types check [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we read the data in and remove the header line using the `.filter(...)`
    method. This is followed by splitting the row on each comma (since this is a `.csv`
    file) and converting each element to an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the schema for our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Having created our `fraud_df` DataFrame, we can calculate the basic descriptive
    statistics for our dataset. However, you need to remember that even though all
    of our features appear as numeric in nature, some of them are categorical (for
    example, `gender` or `state`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the schema of our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The representation is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, no information would be gained from calculating the mean and standard
    deviation of the `custId` column, so we will not be doing that.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better understanding of categorical columns, we will count the frequencies
    of their values using the `.groupby(...)` method. In this example, we will count
    the frequencies of the `gender` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are dealing with a fairly imbalanced dataset. What you would
    expect to see is an equal distribution for both genders.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It goes beyond the scope of this chapter, but if we were building a statistical
    model, you would need to take care of these kinds of biases. You can read more
    at [http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf](http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the truly numerical features, we can use the `.describe()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `.show()` method will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Even from these relatively few numbers we can tell quite a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: All of the features are positively skewed. The maximum values are a number of
    times larger than the average.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coefficient of variation (the ratio of mean to standard deviation) is very
    high (close or greater than `1`), suggesting a wide spread of observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s how you check the `skeweness` (we will do it for the `''balance''`
    feature only):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A list of aggregation functions (the names are fairly self-explanatory) includes:
    `avg()`, `count()`, `countDistinct()`, `first()`, `kurtosis()`, `max()`, `mean()`,
    `min()`, `skewness()`, `stddev()`, `stddev_pop()`, `stddev_samp()`, `sum()`, `sumDistinct()`,
    `var_pop()`, `var_samp()` and `variance()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another highly useful measure of mutual relationships between features is correlation.
    Your model would normally include only those features that are highly correlated
    with your target. However, it is almost equally important to check the correlation
    between the features; including features that are highly correlated among them
    (that is, are *collinear*) may lead to unpredictable behavior of your model, or
    might unnecessarily complicate it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I talk more about multicollinearity in my other book, *Practical Data Analysis
    Cookbook, Packt Publishing* ([https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)),
    in [Chapter 5](ch05.html "Chapter 5. Introducing MLlib"), *Introducing MLlib*,
    under the section titled *Identifying and tackling multicollinearity*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating correlations in PySpark is very easy once your data is in a DataFrame
    form. The only difficulties are that the `.corr(...)` method supports the Pearson
    correlation coefficient at the moment, and it can only calculate pairwise correlations,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to create a correlations matrix, you can use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will create the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlations](img/B05793_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the correlations between the numerical features in the credit
    card fraud dataset are pretty much non-existent. Thus, all these features can
    be used in our models, should they turn out to be statistically sound in explaining
    our target.
  prefs: []
  type: TYPE_NORMAL
- en: Having checked the correlations, we can now move on to visually inspecting our
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple visualization packages, but in this section we will be using
    `matplotlib` and Bokeh exclusively to give you the best tools for your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of the packages come preinstalled with Anaconda. First, let''s load the
    modules and set them up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `%matplotlib inline` and the `output_notebook()` commands will make every
    chart generated with `matplotlib` or Bokeh, respectively, appear within the notebook
    and not as a separate window.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Histograms are by far the easiest way to visually gauge the distribution of
    your features. There are three ways you can generate histograms in PySpark (or
    a Jupyter notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate the data in workers and return an aggregated list of bins and counts
    in each bin of the histogram to the driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return all the data points to the driver and allow the plotting libraries' methods
    to do the job for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample your data and then return them to the driver for plotting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the number of rows in your dataset is counted in billions, then the second
    option might not be attainable. Thus, you need to aggregate the data first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the histogram, you can simply call `matplotlib`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms](img/B05793_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar manner, a histogram can be created with Bokeh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Since Bokeh uses D3.js in the background, the resulting chart is interactive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms](img/B05793_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If your data is small enough to fit on the driver (although we would argue
    it would normally be faster to use the previous method), you can bring the data
    and use the `.hist(...)` (from `matplotlib`) or `.Histogram(...)` (from Bokeh)
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following chart for `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms](img/B05793_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For Bokeh, the following chart will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms](img/B05793_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interactions between features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scatter charts allow us to visualize interactions between up to three variables
    at a time (although we will be only presenting a 2D interaction in this section).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should rarely revert to 3D visualizations unless you are dealing with some
    temporal data and you want to observe changes over time. Even then, we would rather
    discretize the time data and present a series of 2D charts, as interpreting 3D
    charts is somewhat more complicated and (most of the time) confusing.
  prefs: []
  type: TYPE_NORMAL
- en: Since PySpark does not offer any visualization modules on the server side, and
    trying to plot billions of observations at the same time would be highly impractical,
    in this section we will sample the dataset at 0.02% (roughly 2,000 observations).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unless you chose a stratified sampling, you should create at least three to
    five samples at a predefined sampling fraction so you can check if your sample
    is somewhat representative of your dataset—that is, that the differences between
    your samples are not big.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will sample our fraud dataset at 0.02% given `''gender''`
    as a strata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To put multiple 2D charts in one go, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Interactions between features](img/B05793_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are plenty of fraudulent transactions that had 0 balance
    but many transactions—that is, a fresh card and big spike of transactions. However,
    no specific pattern can be shown apart from some *banding* occurring at $1,000
    intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to clean and prepare your dataset for modeling
    by identifying and tackling datasets with missing values, duplicates, and outliers.
    We also looked at how to get a bit more familiar with your data using tools from
    PySpark (although this is by no means a full manual on how to analyze your datasets).
    Finally, we showed you how to chart your data.
  prefs: []
  type: TYPE_NORMAL
- en: We will use these (and more) techniques in the next two chapters, where we will
    be building machine learning models.
  prefs: []
  type: TYPE_NORMAL
