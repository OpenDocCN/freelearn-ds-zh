- en: Chapter 4. Prepare Data for Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 准备数据以进行建模
- en: 'All data is dirty, irrespective of what the source of the data might lead you
    to believe: it might be your colleague, a telemetry system that monitors your
    environment, a dataset you download from the web, or some other source. Until
    you have tested and proven to yourself that your data is in a clean state (we
    will get to what clean state means in a second), you should neither trust it nor
    use it for modeling.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是脏的，无论数据的来源可能让您相信什么：可能是您的同事、一个监控您环境的遥测系统、您从网络上下载的数据集，或者其他来源。直到您已经测试并证明自己数据处于干净状态（我们将在下一节中解释干净状态的含义），您都不应该信任它或将其用于建模。
- en: Your data can be stained with duplicates, missing observations and outliers,
    non-existent addresses, wrong phone numbers and area codes, inaccurate geographical
    coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases,
    trailing spaces, and many other more subtle problems. It is your job to clean
    it, irrespective of whether you are a data scientist or data engineer, so you
    can build a statistical or machine learning model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据可能会受到重复项、缺失观测值和异常值、不存在的地址、错误的电话号码和区号、不准确的地标坐标、错误的日期、不正确的标签、大小写混合、尾部空格以及许多其他更微妙的问题的影响。无论您是数据科学家还是数据工程师，您的任务是清理它，以便您可以构建统计或机器学习模型。
- en: Your dataset is considered technically clean if none of the aforementioned problems
    can be found. However, to clean the dataset for modeling purposes, you also need
    to check the distributions of your features and confirm they fit the predefined
    criteria.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集中没有发现上述任何问题，则您的数据集被认为是技术上干净的。然而，为了建模目的清理数据集，您还需要检查您特征的分布，并确认它们符合预定义的标准。
- en: As a data scientist, you can expect to spend 80-90% of your time *massaging*
    your data and getting familiar with all the features. This chapter will guide
    you through that process, leveraging Spark capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，您可以预期将花费80-90%的时间对数据进行“按摩”并熟悉所有特征。本章将引导您通过这个过程，利用Spark的能力。
- en: 'In this chapter, you will learn how to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: Recognize and handle duplicates, missing observations, and outliers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和处理重复项、缺失观测值和异常值
- en: Calculate descriptive statistics and correlations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算描述性统计和相关性
- en: Visualize your data with matplotlib and Bokeh
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用matplotlib和Bokeh可视化您的数据
- en: Checking for duplicates, missing observations, and outliers
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查重复项、缺失观测值和异常值
- en: Until you have fully tested the data and proven it worthy of your time, you
    should neither trust it nor use it. In this section, we will show you how to deal
    with duplicates, missing observations, and outliers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在您完全测试数据并证明它值得您的时间之前，您都不应该信任它或使用它。在本节中，我们将向您展示如何处理重复项、缺失观测值和异常值。
- en: Duplicates
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重复项
- en: Duplicates are observations that appear as distinct rows in your dataset, but
    which, upon closer inspection, look the same. That is, if you looked at them side
    by side, all the features in these two (or more) rows would have exactly the same
    values.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重复项是在您的数据集中作为独立行出现的观测值，但在仔细检查后看起来是相同的。也就是说，如果您将它们并排查看，这两行（或更多）中的所有特征将具有完全相同的值。
- en: On the other hand, if your data has some form of an ID to distinguish between
    records (or associate them with certain users, for example), then what might initially
    appear as a duplicate may not be; sometimes systems fail and produce erroneous
    IDs. In such a situation, you need to either check whether the same ID is a real
    duplicate, or you need to come up with a new ID system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您的数据具有某种形式的ID来区分记录（或将其与某些用户关联，例如），那么最初可能看起来是重复项的，可能不是；有时系统会失败并产生错误的ID。在这种情况下，您需要检查相同的ID是否是真正的重复项，或者您需要提出一个新的ID系统。
- en: 'Consider the following example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, we have several issues here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们这里有几个问题：
- en: We have two rows with IDs equal to `3` and they are exactly the same
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有两行ID等于`3`，它们完全相同
- en: Rows with IDs `1` and `4` are the same — the only thing that's different is
    their IDs, so we can safely assume that they are the same person
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID为`1`和`4`的行是相同的——唯一不同的是它们的ID，因此我们可以安全地假设它们是同一个人
- en: We have two rows with IDs equal to `5`, but that seems to be a recording issue,
    as they do not seem to be the same person
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有两行ID等于`5`，但看起来这似乎是一个录音问题，因为它们似乎不是同一个人
- en: 'This is a very easy dataset with only seven rows. What do you do when you have
    millions of observations? The first thing I normally do is to check if I have
    any duplicates: I compare the counts of the full dataset with the one that I get
    after running a `.distinct()` method:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的数据集，只有七行。当你有数百万个观察值时，你该怎么办？我通常做的第一件事是检查是否有任何重复：我将完整数据集的计数与运行 `.distinct()`
    方法后得到的计数进行比较：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here''s what you get back for our DataFrame:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的 DataFrame 返回的内容：
- en: '![Duplicates](img/B05793_04_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_01.jpg)'
- en: 'If these two numbers differ, then you know you have, what I like to call, pure
    duplicates: rows that are exact copies of each other. We can drop these rows by
    using the `.dropDuplicates(...)` method:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两个数字不同，那么你就知道你有了，我喜欢称之为，纯重复：彼此完全相同的行。我们可以通过使用 `.dropDuplicates(...)` 方法来删除这些行：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our dataset will then look as follows (once you run `df.show()`):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集将如下所示（一旦运行 `df.show()`）：
- en: '![Duplicates](img/B05793_04_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_02.jpg)'
- en: 'We dropped one of the rows with ID `3`. Now let''s check whether there are
    any duplicates in the data irrespective of ID. We can quickly repeat what we have
    done earlier, but using only columns other than the ID column:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了具有 ID `3` 的一行。现在让我们检查数据中是否存在任何与 ID 无关的重复项。我们可以快速重复之前所做的操作，但仅使用除 ID 列之外的其他列：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should see one more row that is a duplicate:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一行额外的重复项：
- en: '![Duplicates](img/B05793_04_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_03.jpg)'
- en: 'We can still use the `.dropDuplicates(...)`, but will add the `subset` parameter
    that specifies only the columns other than the `id` column:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以使用 `.dropDuplicates(...)`, 但会添加一个 `subset` 参数，该参数指定除了 `id` 列之外的其他列：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `subset` parameter instructs the `.dropDuplicates(...)` method to look
    for duplicated rows using only the columns specified via the `subset` parameter;
    in the preceding example, we will drop the duplicated records with the same `weight`,
    `height`, `age`, and `gender` but not `id`. Running the `df.show()`, we get the
    following cleaner dataset as we dropped the row with `id = 1` since it was identical
    to the record with `id = 4`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`subset` 参数指示 `.dropDuplicates(...)` 方法仅使用通过 `subset` 参数指定的列来查找重复行；在上面的例子中，我们将删除具有相同
    `weight`、`height`、`age` 和 `gender` 但不是 `id` 的重复记录。运行 `df.show()`，我们得到以下更干净的数据集，因为我们删除了
    `id = 1` 的行，因为它与 `id = 4` 的记录完全相同：'
- en: '![Duplicates](img/B05793_04_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_04.jpg)'
- en: 'Now that we know there are no full rows duplicated, or any identical rows differing
    only by ID, let''s check if there are any duplicated IDs. To calculate the total
    and distinct number of IDs in one step, we can use the `.agg(...)` method:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道没有完整的行重复，或者只有 ID 不同的相同行，让我们检查是否有任何重复的 ID。为了在一步中计算总数和不同 ID 的数量，我们可以使用 `.agg(...)`
    方法：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here''s the output of the preceding code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Duplicates](img/B05793_04_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_05.jpg)'
- en: In the previous example, we first import all the functions from the `pyspark.sql`
    module.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们首先从 `pyspark.sql` 模块导入所有函数。
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This gives us access to a vast array of various functions, too many to list
    here. However, we strongly encourage you to study the PySpark's documentation
    at [http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够访问各种函数，太多以至于无法在此列出。然而，我们强烈建议您研究 PySpark 的文档，网址为 [http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)。
- en: Next, we use the `.count(...)` and `.countDistinct(...)` to, respectively, calculate
    the number of rows and the number of distinct `ids` in our DataFrame. The `.alias(...)`
    method allows us to specify a friendly name to the returned column.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `.count(...)` 和 `.countDistinct(...)` 分别计算 DataFrame 中的行数和不同 `ids`
    的数量。`.alias(...)` 方法允许我们为返回的列指定一个友好的名称。
- en: 'As you can see, we have five rows in total, but only four distinct IDs. Since
    we have already dropped all the duplicates, we can safely assume that this might
    just be a fluke in our ID data, so we will give each row a unique ID:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们总共有五行，但只有四个不同的 ID。由于我们已经删除了所有重复项，我们可以安全地假设这可能是 ID 数据中的一个偶然错误，因此我们将为每一行分配一个唯一的
    ID：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code snippet produced the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段生成了以下输出：
- en: '![Duplicates](img/B05793_04_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Duplicates](img/B05793_04_06.jpg)'
- en: The `.monotonicallymonotonically_increasing_id()` method gives each record a
    unique and increasing ID. According to the documentation, as long as your data
    is put into less than roughly 1 billion partitions with less than 8 billions records
    in each, the ID is guaranteed to be unique.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`.monotonicallymonotonically_increasing_id()`方法为每条记录分配一个唯一且递增的ID。根据文档，只要你的数据被放入少于大约10亿个分区，每个分区少于80亿条记录，ID就可以保证是唯一的。'
- en: Note
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'A word of caution: in earlier versions of Spark the `.monotonicallymonotonically_increasing_id()`
    method would not necessarily return the same IDs across multiple evaluations of
    the same DataFrame. This, however, has been fixed in Spark 2.0.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个警告：在Spark的早期版本中，`.monotonicallymonotonically_increasing_id()`方法在多次评估同一个DataFrame时可能不会返回相同的ID。然而，这已经在Spark
    2.0中得到了修复。
- en: Missing observations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失的观测值
- en: 'You will frequently encounter datasets with *blanks* in them. The missing values
    can happen for a variety of reasons: systems failure, people error, data schema
    changes, just to name a few.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会遇到包含*空白*的数据集。缺失值可能由多种原因造成：系统故障、人为错误、数据模式变更，仅举几例。
- en: 'The simplest way to deal with missing values, if your data can afford it, is
    to drop the whole observation when any missing value is found. You have to be
    careful not to drop too many: depending on the distribution of the missing values
    across your dataset it might severely affect the usability of your dataset. If,
    after dropping the rows, I end up with a very small dataset, or find that the
    reduction in data size is more than 50%, I start checking my data to see what
    features have the most holes in them and perhaps exclude those altogether; if
    a feature has most of its values missing (unless a missing value bears a meaning),
    from a modeling point of view, it is fairly useless.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据可以承受，处理缺失值的最简单方法是在发现任何缺失值时删除整个观测值。你必须小心不要删除太多：根据缺失值在你数据集中的分布，这可能会严重影响数据集的可用性。如果删除行后，我最终得到一个非常小的数据集，或者发现数据量减少了50%以上，我开始检查我的数据，看看哪些特征有最多的空缺，也许可以完全排除它们；如果一个特征的大部分值都是缺失的（除非缺失值有特定的含义），从建模的角度来看，它几乎是毫无用处的。
- en: 'The other way to deal with the observations with missing values is to impute
    some value in place of those `Nones`. Given the type of your data, you have several
    options to choose from:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 处理具有缺失值的观测值的另一种方法是，用某些值代替那些`Nones`。根据你的数据类型，你有几个选项可以选择：
- en: If your data is a discrete Boolean, you can turn it into a categorical variable
    by adding a third category — `Missing`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据是离散布尔值，你可以通过添加第三个类别——`缺失`来将其转换为分类变量
- en: If your data is already categorical, you can simply extend the number of levels
    and add the `Missing` category as well
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据已经是分类的，你可以简单地扩展级别数量，并添加`缺失`类别
- en: If you're dealing with ordinal or numerical data, you can impute either mean,
    median, or some other predefined value (for example, first or third quartile,
    depending on the distribution shape of your data)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在处理有序或数值数据，你可以用均值、中位数或其他预定义的值（例如，第一或第三四分位数，取决于你数据的分布形状）来插补。
- en: 'Consider a similar example to the one we presented previously:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个与我们之前展示的类似的例子：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In our example, we deal with a number of missing values categories.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们处理了多个缺失值类别。
- en: 'Analyzing *rows*, we see the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 分析*行*，我们可以看到以下：
- en: The row with ID `3` has only one useful piece of information—the `height`
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID为`3`的行只有一个有用的信息——`身高`
- en: The row with ID `6` has only one missing value—the `age`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID为`6`的行只有一个缺失值——`年龄`
- en: 'Analyzing *columns*, we can see the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 分析*列*，我们可以看到以下：
- en: The `income` column, since it is a very personal thing to disclose, has most
    of its values missing
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`收入`列，由于它是一个非常私人的信息，大部分值都是缺失的'
- en: The `weight` and `gender` columns have only one missing value each
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`体重`和`性别`列各有只有一个缺失值'
- en: The `age` column has two missing values
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`年龄`列有两个缺失值'
- en: 'To find the number of missing observations per row, we can use the following
    snippet:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到每行的缺失观测值数量，我们可以使用以下代码片段：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It produces the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以下输出：
- en: '![Missing observations](img/B05793_04_07.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![缺失的观测值](img/B05793_04_07.jpg)'
- en: It tells us that, for example, the row with ID `3` has four missing observations,
    as we observed earlier.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉我们，例如，ID为`3`的行有四个缺失观测值，正如我们之前观察到的。
- en: 'Let''s see what values are missing so that when we count missing observations
    in columns, we can decide whether to drop the observation altogether or impute
    some of the observations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪些值是缺失的，这样当我们计算列中的缺失观测值时，我们可以决定是否删除整个观测值或对某些观测值进行插补：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here''s what we get:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![Missing observations](img/B05793_04_08.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![缺失观测值](img/B05793_04_08.jpg)'
- en: 'Let''s now check what percentage of missing observations are there in each
    column:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查每列中缺失观测值的百分比是多少：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This generates the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下输出：
- en: '![Missing observations](img/B05793_04_09.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![缺失观测值](img/B05793_04_09.jpg)'
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `*` argument to the `.count(...)` method (in place of a column name) instructs
    the method to count all rows. On the other hand, the `*` preceding the list declaration
    instructs the `.agg(...)` method to treat the list as a set of separate parameters
    passed to the function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`.count(...)`方法的`*`参数（代替列名）指示该方法计算所有行。另一方面，列表声明前的`*`指示`.agg(...)`方法将列表视为一组单独的参数传递给函数。'
- en: So, we have 14% of missing observations in the `weight` and `gender` columns,
    twice as much in the `height` column, and almost 72% of missing observations in
    the `income` column. Now we know what to do.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在`weight`和`gender`列中有14%的缺失观测值，在`height`列中有两倍于此，在`income`列中有近72%的缺失观测值。现在我们知道该做什么了。
- en: First, we will drop the `'income'` feature, as most of its values are missing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将删除`'income'`特征，因为其中大部分值是缺失的。
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We now see that we do not need to drop the row with ID `3` as the coverage in
    the `'weight'` and `'age'` columns has enough observations (in our simplified
    example) to calculate the mean and impute it in the place of the missing values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到，我们不需要删除ID为`3`的行，因为在`'weight'`和`'age'`列中的观测值覆盖足够（在我们的简化示例中）来计算平均值并将其插补到缺失值的位置。
- en: 'However, if you decide to drop the observations instead, you can use the `.dropna(...)`
    method, as shown here. Here, we will also use the `thresh` parameter, which allows
    us to specify a threshold on the number of missing observations per row that would
    qualify the row to be dropped. This is useful if you have a dataset with tens
    or hundreds of features and you only want to drop those rows that exceed a certain
    threshold of missing values:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你决定删除观测值，你可以使用`.dropna(...)`方法，如下所示。在这里，我们还将使用`thresh`参数，它允许我们指定每行缺失观测值的阈值，以确定该行是否应该被删除。这对于你拥有具有数十或数百个特征的dataset来说很有用，你只想删除那些超过一定缺失值阈值的行：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code produces the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![Missing observations](img/B05793_04_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![缺失观测值](img/B05793_04_10.jpg)'
- en: 'On the other hand, if you wanted to impute the observations, you can use the
    `.fillna(...)` method. This method accepts a single integer (long is also accepted),
    float, or string; all missing values in the whole dataset will then be filled
    in with that value. You can also pass a dictionary of a form `{''<colName>'':
    <value_to_impute>}`. This has the same limitation, in that, as the `<value_to_impute>`,
    you can only pass an integer, float, or string.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，如果你想插补观测值，你可以使用`.fillna(...)`方法。此方法接受单个整数（长整型也接受），浮点数或字符串；然后整个dataset中的所有缺失值都将用该值填充。你也可以传递一个形式为`{''<colName>'':
    <value_to_impute>}`的字典。这有一个相同的限制，即，作为`<value_to_impute>`，你只能传递整数、浮点数或字符串。'
- en: If you want to impute a mean, median, or other calculated value, you need to
    first calculate the value, create a dictionary with such values, and then pass
    it to the `.fillna(...)` method.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要插补平均值、中位数或其他计算值，你需要首先计算该值，创建一个包含这些值的字典，然后将它传递给`.fillna(...)`方法。
- en: 'Here''s how we do it:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的做法：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code will produce the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下输出：
- en: '![Missing observations](img/B05793_04_11.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![缺失观测值](img/B05793_04_11.jpg)'
- en: We omit the `gender` column as one cannot calculate a mean of a categorical
    variable, obviously.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了`性别`列，因为显然无法对分类变量计算平均值。
- en: We use a double conversion here. Taking the output of the `.agg(...)` method
    (a PySpark DataFrame), we first convert it into a pandas' DataFrame and then once
    more to a dictionary.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用双重转换。首先将`.agg(...)`方法的输出（一个PySpark DataFrame）转换为pandas DataFrame，然后再将其转换为字典。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that calling the `.toPandas()` can be problematic, as the method works
    essentially in the same way as `.collect()` in RDDs. It collects all the information
    from the workers and brings it over to the driver. It is unlikely to be a problem
    with the preceding dataset, unless you have thousands upon thousands of features.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调用`.toPandas()`可能会有问题，因为这个方法基本上与RDD中的`.collect()`方法以相同的方式工作。它会从工作者那里收集所有信息，并将其带到驱动器上。除非你有成千上万的特征，否则这不太可能成为前一个数据集的问题。
- en: 'The `records` parameter to the `.to_dict(...)` method of pandas instructs it
    to create the following dictionary:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的`.to_dict(...)`方法的`records`参数指示它创建以下字典：
- en: '![Missing observations](img/B05793_04_12.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![缺失观测值](img/B05793_04_12.jpg)'
- en: Since we cannot calculate the average (or any other numeric metric of a categorical
    variable), we added the `missing` category to the dictionary for the `gender`
    feature. Note that, even though the mean of the age column is 40.40, when imputed,
    the type of the `df_miss_no_income.age` column was preserved—it is still an integer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法计算分类变量的平均值（或任何其他数值指标），我们在`gender`特征的字典中添加了`missing`类别。注意，尽管年龄列的平均值是40.40，但在插补时，`df_miss_no_income.age`列的类型仍然被保留——它仍然是一个整数。
- en: Outliers
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: Outliers are those observations that deviate significantly from the distribution
    of the rest of your sample. The definitions of *significance* vary, but in the
    most general form, you can accept that there are no outliers if all the values
    are roughly within the Q1−1.5IQR and Q3+1.5IQR range, where IQR is the interquartile
    range; the IQR is defined as a difference between the upper- and lower-quartiles,
    that is, the 75th percentile (the Q3) and 25th percentile (the Q1), respectively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是那些与你的样本中其余部分分布显著偏离的观测值。*显著性*的定义各不相同，但最一般的形式，你可以接受如果没有异常值，所有值都大致在Q1−1.5IQR和Q3+1.5IQR范围内，其中IQR是四分位距；IQR定义为上四分位数和下四分位数的差，即75百分位数（Q3）和25百分位数（Q1）。
- en: 'Let''s, again, consider a simple example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑一个简单的例子：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we can use the definition we outlined previously to flag the outliers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用我们之前概述的定义来标记异常值。
- en: 'First, we calculate the lower and upper cut off points for each feature. We
    will use the `.approxQuantile(...)` method. The first parameter specified is the
    name of the column, the second parameter can be either a number between `0` or
    `1` (where `0.5` means to calculated median) or a list (as in our case), and the
    third parameter specifies the acceptable level of an error for each metric (if
    set to `0`, it will calculate an exact value for the metric, but it can be really
    expensive to do so):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算每个特征的上下限。我们将使用`.approxQuantile(...)`方法。指定的第一个参数是列名，第二个参数可以是`0`到`1`之间的数字（其中`0.5`表示计算中位数）或列表（如我们的情况），第三个参数指定每个指标的容错水平（如果设置为`0`，它将为指标计算一个精确值，但这可能非常昂贵）：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `bounds` dictionary holds the lower and upper bounds for each feature:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`bounds`字典包含每个特征的上下限：'
- en: '![Outliers](img/B05793_04_13.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![异常值](img/B05793_04_13.jpg)'
- en: 'Let''s now use it to flag our outliers:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用它来标记我们的异常值：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code produces the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码产生以下输出：
- en: '![Outliers](img/B05793_04_14.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![异常值](img/B05793_04_14.jpg)'
- en: 'We have two outliers in the `weight` feature and two in the `age` feature.
    By now you should know how to extract these, but here is a snippet that lists
    the values significantly differing from the rest of the distribution:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`weight`特征和`age`特征中各有两个异常值。到现在你应该知道如何提取这些值，但这里有一个列出与整体分布显著不同的值的代码片段：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code will give you the following output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码将给出以下输出：
- en: '![Outliers](img/B05793_04_15.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![异常值](img/B05793_04_15.jpg)'
- en: Equipped with the methods described in this section, you can quickly clean up
    even the biggest of datasets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有了本节中描述的方法，你可以快速清理甚至最大的数据集。
- en: Getting familiar with your data
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉你的数据
- en: Although we would strongly discourage such behavior, you can build a model without
    knowing your data; it will most likely take you longer, and the quality of the
    resulting model might be less than optimal, but it is doable.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们强烈反对这种行为，但你可以在不了解数据的情况下构建模型；这可能会花费你更长的时间，并且生成的模型的质量可能不如最佳，但这是可行的。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In this section, we will use the dataset we downloaded from [http://packages.revolutionanalytics.com/datasets/ccFraud.csv](http://packages.revolutionanalytics.com/datasets/ccFraud.csv).
    We did not alter the dataset itself, but it was GZipped and uploaded to [http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz](http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz).
    Please download the file first and save it in the same folder that contains your
    notebook for this chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用我们从 [http://packages.revolutionanalytics.com/datasets/ccFraud.csv](http://packages.revolutionanalytics.com/datasets/ccFraud.csv)
    下载的数据集。我们没有更改数据集本身，但它被 GZipped 并上传到 [http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz](http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz)。请首先下载文件，并将其保存在包含你本章笔记本的同一文件夹中。
- en: 'The head of the dataset looks as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的头部看起来如下所示：
- en: '![Getting familiar with your data](img/B05793_04_16.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉你的数据](img/B05793_04_16.jpg)'
- en: Thus, any serious data scientist or data modeler will become acquainted with
    the dataset before starting any modeling. As a first thing, we normally start
    with some descriptive statistics to get a feeling for what we are dealing with.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何严肃的数据科学家或数据模型师在开始任何建模之前都会熟悉数据集。作为第一步，我们通常从一些描述性统计开始，以了解我们正在处理的内容。
- en: Descriptive statistics
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述性统计
- en: 'Descriptive statistics, in the simplest sense, will tell you the basic information
    about your dataset: how many non-missing observations there are in your dataset,
    the mean and the standard deviation for the column, as well as the min and max
    values.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性统计，在最简单的意义上，会告诉你关于你的数据集的基本信息：你的数据集中有多少非缺失观测值，列的均值和标准差，以及最小值和最大值。
- en: 'However, first things first—let''s load our data and convert it to a Spark
    DataFrame:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首先的事情是——让我们加载数据并将其转换为 Spark DataFrame：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: First, we load the only module we will need. The `pyspark.sql.types` exposes
    all the data types we can use, such as `IntegerType()` or `FloatType()`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载我们需要的唯一模块。`pyspark.sql.types` 暴露了我们可以使用的数据类型，例如 `IntegerType()` 或 `FloatType()`。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a full list of available types check [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用类型的完整列表，请检查[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)。
- en: 'Next, we read the data in and remove the header line using the `.filter(...)`
    method. This is followed by splitting the row on each comma (since this is a `.csv`
    file) and converting each element to an integer:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `.filter(...)` 方法读取数据并删除标题行。这之后，我们将行按每个逗号分割（因为这是一个 `.csv` 文件），并将每个元素转换为整数：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create the schema for our DataFrame:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为我们的 DataFrame 创建模式：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we create our DataFrame:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建我们的 DataFrame：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Having created our `fraud_df` DataFrame, we can calculate the basic descriptive
    statistics for our dataset. However, you need to remember that even though all
    of our features appear as numeric in nature, some of them are categorical (for
    example, `gender` or `state`).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的 `fraud_df` DataFrame 之后，我们可以计算数据集的基本描述性统计。然而，你需要记住，尽管我们的所有特征在本质上都表现为数值型，但其中一些是分类的（例如，`gender`
    或 `state`）。
- en: 'Here''s the schema of our DataFrame:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的 DataFrame 的模式：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The representation is shown here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表示如下所示：
- en: '![Descriptive statistics](img/B05793_04_17.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![描述性统计](img/B05793_04_17.jpg)'
- en: Also, no information would be gained from calculating the mean and standard
    deviation of the `custId` column, so we will not be doing that.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从计算 `custId` 列的均值和标准差中不会获得任何信息，所以我们不会进行这项操作。
- en: 'For a better understanding of categorical columns, we will count the frequencies
    of their values using the `.groupby(...)` method. In this example, we will count
    the frequencies of the `gender` column:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解分类列，我们将使用 `.groupby(...)` 方法计算其值的频率。在这个例子中，我们将计算 `gender` 列的频率：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code will produce the following output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下输出：
- en: '![Descriptive statistics](img/B05793_04_18.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![描述性统计](img/B05793_04_18.jpg)'
- en: As you can see, we are dealing with a fairly imbalanced dataset. What you would
    expect to see is an equal distribution for both genders.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在处理一个相当不平衡的数据集。你可能会期望看到性别分布是相等的。
- en: Note
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It goes beyond the scope of this chapter, but if we were building a statistical
    model, you would need to take care of these kinds of biases. You can read more
    at [http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf](http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这超出了本章的范围，但如果我们在构建统计模型，就需要注意这些类型的偏差。您可以在[http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf](http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf)了解更多信息。
- en: 'For the truly numerical features, we can use the `.describe()` method:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于真正的数值特征，我们可以使用`.describe()`方法：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `.show()` method will produce the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`.show()`方法将产生以下输出：'
- en: '![Descriptive statistics](img/B05793_04_19.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![描述性统计](img/B05793_04_19.jpg)'
- en: 'Even from these relatively few numbers we can tell quite a bit:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 即使从这些相对较少的数字中，我们也可以得出很多结论：
- en: All of the features are positively skewed. The maximum values are a number of
    times larger than the average.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征都是正偏的。最大值是平均值的数倍。
- en: The coefficient of variation (the ratio of mean to standard deviation) is very
    high (close or greater than `1`), suggesting a wide spread of observations.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变异系数（平均值与标准差的比率）非常高（接近或大于`1`），表明观察值的分布范围很广。
- en: 'Here''s how you check the `skeweness` (we will do it for the `''balance''`
    feature only):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是检查`偏度`（我们只为`'balance'`特征做此操作）的方法：
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code produces the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![Descriptive statistics](img/B05793_04_20.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![描述性统计](img/B05793_04_20.jpg)'
- en: 'A list of aggregation functions (the names are fairly self-explanatory) includes:
    `avg()`, `count()`, `countDistinct()`, `first()`, `kurtosis()`, `max()`, `mean()`,
    `min()`, `skewness()`, `stddev()`, `stddev_pop()`, `stddev_samp()`, `sum()`, `sumDistinct()`,
    `var_pop()`, `var_samp()` and `variance()`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合函数列表（名称相当直观）包括：`avg()`、`count()`、`countDistinct()`、`first()`、`kurtosis()`、`max()`、`mean()`、`min()`、`skewness()`、`stddev()`、`stddev_pop()`、`stddev_samp()`、`sum()`、`sumDistinct()`、`var_pop()`、`var_samp()`和`variance()`。
- en: Correlations
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关系数
- en: Another highly useful measure of mutual relationships between features is correlation.
    Your model would normally include only those features that are highly correlated
    with your target. However, it is almost equally important to check the correlation
    between the features; including features that are highly correlated among them
    (that is, are *collinear*) may lead to unpredictable behavior of your model, or
    might unnecessarily complicate it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常有用的衡量特征之间相互关系的方法是相关系数。通常，您的模型只会包括与您的目标高度相关的特征。然而，检查特征之间的相关性几乎同样重要；包括彼此高度相关（即，*共线性*）的特征可能会导致模型的行为不可预测，或者可能不必要地使模型复杂化。
- en: Note
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: I talk more about multicollinearity in my other book, *Practical Data Analysis
    Cookbook, Packt Publishing* ([https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)),
    in [Chapter 5](ch05.html "Chapter 5. Introducing MLlib"), *Introducing MLlib*,
    under the section titled *Identifying and tackling multicollinearity*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的另一本书中更多地讨论了多重共线性，*《实用数据分析食谱，Packt出版社》* ([https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook))，在第5章[介绍MLlib](ch05.html
    "第5章。介绍MLlib")中，标题为*识别和解决多重共线性*的部分。
- en: 'Calculating correlations in PySpark is very easy once your data is in a DataFrame
    form. The only difficulties are that the `.corr(...)` method supports the Pearson
    correlation coefficient at the moment, and it can only calculate pairwise correlations,
    such as the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的数据以DataFrame形式存在，在PySpark中计算相关系数非常简单。唯一的困难是`.corr(...)`方法目前只支持皮尔逊相关系数，并且它只能计算成对的相关性，如下所示：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order to create a correlations matrix, you can use the following script:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建相关系数矩阵，您可以使用以下脚本：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code will create the following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下输出：
- en: '![Correlations](img/B05793_04_21.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![相关系数](img/B05793_04_21.jpg)'
- en: As you can see, the correlations between the numerical features in the credit
    card fraud dataset are pretty much non-existent. Thus, all these features can
    be used in our models, should they turn out to be statistically sound in explaining
    our target.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，信用卡欺诈数据集中数值特征之间的相关系数几乎不存在。因此，所有这些特征都可以用于我们的模型，如果它们在解释我们的目标时在统计上是有意义的。
- en: Having checked the correlations, we can now move on to visually inspecting our
    data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 检查完相关系数后，我们现在可以继续对数据进行视觉检查。
- en: Visualization
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: There are multiple visualization packages, but in this section we will be using
    `matplotlib` and Bokeh exclusively to give you the best tools for your needs.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在多个可视化包，但在这个部分，我们将专门使用 `matplotlib` 和 Bokeh，以提供最适合你需求的工具。
- en: 'Both of the packages come preinstalled with Anaconda. First, let''s load the
    modules and set them up:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个包都预装在 Anaconda 中。首先，让我们加载模块并设置它们：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `%matplotlib inline` and the `output_notebook()` commands will make every
    chart generated with `matplotlib` or Bokeh, respectively, appear within the notebook
    and not as a separate window.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`%matplotlib inline` 和 `output_notebook()` 命令将使 `matplotlib` 或 Bokeh 生成的每个图表都出现在笔记本中，而不是作为单独的窗口。'
- en: Histograms
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直方图
- en: 'Histograms are by far the easiest way to visually gauge the distribution of
    your features. There are three ways you can generate histograms in PySpark (or
    a Jupyter notebook):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图无疑是直观评估特征分布的最简单方法。在 PySpark（或 Jupyter notebook）中，你可以通过以下三种方式生成直方图：
- en: Aggregate the data in workers and return an aggregated list of bins and counts
    in each bin of the histogram to the driver
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工作节点上聚合数据，并将直方图中每个箱子的箱子和计数列表返回给驱动程序
- en: Return all the data points to the driver and allow the plotting libraries' methods
    to do the job for you
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有数据点返回给驱动程序，并允许绘图库的方法为你完成工作
- en: Sample your data and then return them to the driver for plotting.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本你的数据，然后将它们返回给驱动程序进行绘图。
- en: 'If the number of rows in your dataset is counted in billions, then the second
    option might not be attainable. Thus, you need to aggregate the data first:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集的行数以亿计，那么第二种方法可能不可行。因此，你需要首先聚合数据：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To plot the histogram, you can simply call `matplotlib`, as shown in the following
    code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制直方图，你可以简单地调用 `matplotlib`，如下面的代码所示：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will produce the following chart:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Histograms](img/B05793_04_22.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![直方图](img/B05793_04_22.jpg)'
- en: 'In a similar manner, a histogram can be created with Bokeh:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，可以使用 Bokeh 创建直方图：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since Bokeh uses D3.js in the background, the resulting chart is interactive:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Bokeh 在后台使用 D3.js，生成的图表是交互式的：
- en: '![Histograms](img/B05793_04_23.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![直方图](img/B05793_04_23.jpg)'
- en: 'If your data is small enough to fit on the driver (although we would argue
    it would normally be faster to use the previous method), you can bring the data
    and use the `.hist(...)` (from `matplotlib`) or `.Histogram(...)` (from Bokeh)
    methods:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据足够小，可以放在驱动程序上（尽管我们会争论通常使用前一种方法会更快），你可以将数据带进来，并使用 `.hist(...)`（来自 `matplotlib`）或
    `.Histogram(...)`（来自 Bokeh）方法：
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will produce the following chart for `matplotlib`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为 `matplotlib` 生成以下图表：
- en: '![Histograms](img/B05793_04_24.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![直方图](img/B05793_04_24.jpg)'
- en: 'For Bokeh, the following chart will be generated:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Bokeh，将生成以下图表：
- en: '![Histograms](img/B05793_04_25.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![直方图](img/B05793_04_25.jpg)'
- en: Interactions between features
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征之间的交互
- en: Scatter charts allow us to visualize interactions between up to three variables
    at a time (although we will be only presenting a 2D interaction in this section).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图允许我们同时可视化最多三个变量之间的交互（尽管在本节中我们将只展示二维交互）。
- en: Tip
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You should rarely revert to 3D visualizations unless you are dealing with some
    temporal data and you want to observe changes over time. Even then, we would rather
    discretize the time data and present a series of 2D charts, as interpreting 3D
    charts is somewhat more complicated and (most of the time) confusing.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你很少需要回退到三维可视化，除非你处理的是一些时间数据，并且你想观察随时间的变化。即使在这种情况下，我们宁愿将时间数据进行离散化，并展示一系列二维图表，因为解读三维图表相对复杂，并且（大多数时候）令人困惑。
- en: Since PySpark does not offer any visualization modules on the server side, and
    trying to plot billions of observations at the same time would be highly impractical,
    in this section we will sample the dataset at 0.02% (roughly 2,000 observations).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PySpark 在服务器端不提供任何可视化模块，并且同时尝试绘制数十亿个观察值将非常不切实际，因此在本节中，我们将对数据集进行 0.02%（大约
    2,000 个观察值）的抽样。
- en: Tip
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Unless you chose a stratified sampling, you should create at least three to
    five samples at a predefined sampling fraction so you can check if your sample
    is somewhat representative of your dataset—that is, that the differences between
    your samples are not big.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你选择了分层抽样，否则你应该创建至少三个到五个样本，在预定义的抽样比例下，以便检查你的样本是否在一定程度上代表了你的数据集——也就是说，样本之间的差异不大。
- en: 'In this example, we will sample our fraud dataset at 0.02% given `''gender''`
    as a strata:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将以 0.02% 的比例对欺诈数据集进行抽样，给定 `'gender'` 作为分层：
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To put multiple 2D charts in one go, you can use the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要一次性放入多个二维图表，你可以使用以下代码：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code will produce the following chart:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码将生成以下图表：
- en: '![Interactions between features](img/B05793_04_26.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![特征之间的交互](img/B05793_04_26.jpg)'
- en: As you can see, there are plenty of fraudulent transactions that had 0 balance
    but many transactions—that is, a fresh card and big spike of transactions. However,
    no specific pattern can be shown apart from some *banding* occurring at $1,000
    intervals.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，有许多欺诈交易余额为0，但许多交易——即新卡和交易的大幅增加。然而，除了在$1,000间隔发生的一些*带状*之外，没有特定的模式可以展示。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to clean and prepare your dataset for modeling
    by identifying and tackling datasets with missing values, duplicates, and outliers.
    We also looked at how to get a bit more familiar with your data using tools from
    PySpark (although this is by no means a full manual on how to analyze your datasets).
    Finally, we showed you how to chart your data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过识别和解决数据集中缺失值、重复值和异常值来清洁和准备数据集以进行建模。我们还探讨了如何使用PySpark工具（尽管这绝对不是如何分析数据集的完整手册）来更熟悉你的数据。最后，我们展示了如何绘制数据图表。
- en: We will use these (and more) techniques in the next two chapters, where we will
    be building machine learning models.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两章中，我们将使用这些（以及更多）技术来构建机器学习模型。
