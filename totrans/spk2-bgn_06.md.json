["```py\n$ man nc\n NC(1)          BSD General Commands Manual\nNC(1) \nNAME\n     nc -- arbitrary TCP and UDP connections and listens \nSYNOPSIS\n     nc [-46AcDCdhklnrtUuvz] [-b boundif] [-i interval] [-p source_port] [-s source_ip_address] [-w timeout] [-X proxy_protocol] [-x proxy_address[:port]]\n        [hostname] [port[s]]\n DESCRIPTION\n     The nc (or netcat) utility is used for just about anything under the sun involving TCP or UDP.  It can open TCP connections, send UDP packets, listen on\n     arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6.  Unlike telnet(1), nc scripts nicely, and separates error messages onto\n     standard error instead of sending them to standard output, as telnet(1) does with some. \n     Common uses include: \n           o   simple TCP proxies\n           o   shell-script based HTTP clients and servers\n           o   network daemon testing\n           o   a SOCKS or HTTP ProxyCommand for ssh(1)\n           o   and much, much more\n$ nc -lk 9999\n\n```", "```py\n$ cd Scala\n$ sbt\n> compile\n [success] Total time: 1 s, completed 24 Jul, 2016 8:39:04 AM \n > exit\n\t  $\n\n```", "```py\n#!/bin/bash\n\t  #-----------\n\t  # submit.sh\n\t  #-----------\n\t  # IMPORTANT - Assumption is that the $SPARK_HOME and $KAFKA_HOME environment variables are already set in the system that is running the application\n\t  # [FILLUP] Which is your Spark master. If monitoring is needed, use the desired Spark master or use local\n\t  # When using the local mode. It is important to give more than one cores in square brackets\n\t  #SPARK_MASTER=spark://Rajanarayanans-MacBook-Pro.local:7077\n\t  SPARK_MASTER=local[4]\n\t  # [OPTIONAL] Your Scala version\n\t  SCALA_VERSION=\"2.11\"\n\t  # [OPTIONAL] Name of the application jar file. You should be OK to leave it like that\n\t  APP_JAR=\"spark-for-beginners_$SCALA_VERSION-1.0.jar\"\n\t  # [OPTIONAL] Absolute path to the application jar file\n\t  PATH_TO_APP_JAR=\"target/scala-$SCALA_VERSION/$APP_JAR\"\n\t  # [OPTIONAL] Spark submit commandSPARK_SUBMIT=\"$SPARK_HOME/bin/spark-submit\"\n\t  # [OPTIONAL] Pass the application name to run as the parameter to this script\n\t  APP_TO_RUN=$1\n\t  sbt package\n\t  if [ $2 -eq 1 ]\n\t  then\n\t  $SPARK_SUBMIT --class $APP_TO_RUN --master $SPARK_MASTER --jars $KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar,$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar,$KAFKA_HOME/libs/metrics-core-2.2.0.jar,$KAFKA_HOME/libs/zkclient-0.3.jar,./lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar $PATH_TO_APP_JAR\n\t  else\n\t  $SPARK_SUBMIT --class $APP_TO_RUN --master $SPARK_MASTER --jars $PATH_TO_APP_JAR $PATH_TO_APP_JAR\n\t  fi\n\n```", "```py\n #!/usr/bin/env bash\n\t  #------------\n\t  # submitPy.sh\n\t  #------------\n\t  # IMPORTANT - Assumption is that the $SPARK_HOME and $KAFKA_HOME environment variables are already set in the system that is running the application\n\t  # Disable randomized hash in Python 3.3+ (for string) Otherwise the following exception will occur\n\t  # raise Exception(\"Randomness of hash of string should be disabled via PYTHONHASHSEED\")\n\t  # Exception: Randomness of hash of string should be disabled via PYTHONHASHSEED\n\t  export PYTHONHASHSEED=0\n\t  # [FILLUP] Which is your Spark master. If monitoring is needed, use the desired Spark master or use local\n\t  # When using the local mode. It is important to give more than one cores in square brackets\n\t  #SPARK_MASTER=spark://Rajanarayanans-MacBook-Pro.local:7077\n\t  SPARK_MASTER=local[4]\n\t  # [OPTIONAL] Pass the application name to run as the parameter to this script\n\t  APP_TO_RUN=$1\n\t  # [OPTIONAL] Spark submit command\n\t  SPARK_SUBMIT=\"$SPARK_HOME/bin/spark-submit\"\n\t  if [ $2 -eq 1 ]\n\t  then\n\t  $SPARK_SUBMIT --master $SPARK_MASTER --jars $KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar,$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar,$KAFKA_HOME/libs/metrics-core-2.2.0.jar,$KAFKA_HOME/libs/zkclient-0.3.jar,./lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar $APP_TO_RUN\n\t  else\n\t  $SPARK_SUBMIT --master $SPARK_MASTER $APP_TO_RUN\n\t  fi\n\n```", "```py\n $ cd $SPARK_HOME\n\t  $ ./sbin/start-all.sh\n       starting org.apache.spark.deploy.master.Master, logging to /Users/RajT/source-code/spark-source/spark-2.0/logs/spark-RajT-org.apache.spark.deploy.master.Master-1-Rajanarayanans-MacBook-Pro.local.out \n localhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/RajT/source-code/spark-source/spark-2.0/logs/spark-RajT-org.apache.spark.deploy.worker.Worker-1-Rajanarayanans-MacBook-Pro.local.out\n\n```", "```py\n /**\n\t  The following program can be compiled and run using SBT\n\t  Wrapper scripts have been provided with this\n\t  The following script can be run to compile the code\n\t  ./compile.sh\n\t  The following script can be used to run this application in Spark\n\t  ./submit.sh com.packtpub.sfb.StreamingApps\n\t  **/\n\t  package com.packtpub.sfb\n\t  import org.apache.spark.sql.{Row, SparkSession}\n\t  import org.apache.spark.streaming.{Seconds, StreamingContext}\n\t  import org.apache.spark.storage.StorageLevel\n\t  import org.apache.log4j.{Level, Logger}\n\t  object StreamingApps{\n\t  def main(args: Array[String]) \n\t  {\n\t  // Log level settings\n\t  \t  LogSettings.setLogLevels()\n\t  \t  // Create the Spark Session and the spark context\t  \n\t  \t  val spark = SparkSession\n\t  \t  .builder\n\t  \t  .appName(getClass.getSimpleName)\n\t  \t  .getOrCreate()\n\t     // Get the Spark context from the Spark session for creating the streaming context\n\t  \t  val sc = spark.sparkContext   \n\t      // Create the streaming context\n\t      val ssc = new StreamingContext(sc, Seconds(10))\n\t      // Set the check point directory for saving the data to recover when \n       there is a crash   ssc.checkpoint(\"/tmp\")\n\t      println(\"Stream processing logic start\")\n\t      // Create a DStream that connects to localhost on port 9999\n\t      // The StorageLevel.MEMORY_AND_DISK_SER indicates that the data will be \n       stored in memory and if it overflows, in disk as well\n\t      val appLogLines = ssc.socketTextStream(\"localhost\", 9999, \n       StorageLevel.MEMORY_AND_DISK_SER)\n\t      // Count each log message line containing the word ERROR\n\t      val errorLines = appLogLines.filter(line => line.contains(\"ERROR\"))\n\t      // Print the elements of each RDD generated in this DStream to the \n        console   errorLines.print()\n\t\t   // Count the number of messages by the windows and print them\n\t\t   errorLines.countByWindow(Seconds(30), Seconds(10)).print()\n\t\t   println(\"Stream processing logic end\")\n\t\t   // Start the streaming   ssc.start()   \n\t\t   // Wait till the application is terminated             \n\t\t   ssc.awaitTermination()    }\n\t\t}object LogSettings{\n\t\t  /** \n\t\t   Necessary log4j logging level settings are done \n\t\t  */  def setLogLevels() {\n\t\t    val log4jInitialized = \n         Logger.getRootLogger.getAllAppenders.hasMoreElements\n\t\t     if (!log4jInitialized) {\n\t\t        // This is to make sure that the console is clean from other INFO \n            messages printed by Spark\n\t\t\t       Logger.getRootLogger.setLevel(Level.WARN)\n\t\t\t    }\n\t\t\t  }\n\t\t\t}\n\n```", "```py\n $ cd Scala\n\t\t\t$ ./compile.sh\n\n      [success] Total time: 1 s, completed 24 Jan, 2016 2:34:48 PM\n\n\t$ ./submit.sh com.packtpub.sfb.StreamingApps\n\n      Stream processing logic start    \n\n      Stream processing logic end  \n\n      -------------------------------------------                                     \n\n      Time: 1469282910000 ms\n\n      -------------------------------------------\n\n      -------------------------------------------\n\n      Time: 1469282920000 ms\n\n      ------------------------------------------- \n\n```", "```py\n [Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/\n\t  [Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/\n\t  [Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test\n\t  [Fri Dec 20 01:54:34 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test\n\t  [Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] Client sent malformed Host header\n\t  [Fri Dec 20 02:25:55 2015] [WARN] [client 1.2.3.4.5.6] Client sent malformed Host header\n\t  [Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] user test: authentication failure for \"/~raj/test\": Password Mismatch\n\t  [Mon Dec 20 23:02:01 2015] [WARN] [client 1.2.3.4.5.6] user test: authentication failure for \"/~raj/test\": Password Mismatch \n\n```", "```py\n\t  -------------------------------------------\n\t  Time: 1469283110000 ms\n\t  -------------------------------------------\n\t  [Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index\n      forbidden by rule: /home/raj/\n\t  -------------------------------------------\n\t  Time: 1469283190000 ms\n\t  -------------------------------------------\n\t  -------------------------------------------\n\t  Time: 1469283200000 ms\n\t  -------------------------------------------\n\t  [Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index\n      forbidden by rule: /apache/web/test\n\t  -------------------------------------------\n\t  Time: 1469283250000 ms\n\t  -------------------------------------------\n\t  -------------------------------------------\n\t  Time: 1469283260000 ms\n\t  -------------------------------------------\n\t  [Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] Client sent \n      malformed Host header\n\t  -------------------------------------------\n\t  Time: 1469283310000 ms\n\t  -------------------------------------------\n\t  [Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] user test:\n      authentication failure for \"/~raj/test\": Password Mismatch\n\t  -------------------------------------------\n\t  Time: 1453646710000 ms\n\t  -------------------------------------------\n\n```", "```py\n # The following script can be used to run this application in Spark\n\t  # ./submitPy.sh StreamingApps.py\n\t  from __future__ import print_function\n\t  import sys\n\t  from pyspark import SparkContext\n\t  from pyspark.streaming import StreamingContext\n\t  if __name__ == \"__main__\":\n\t      # Create the Spark context\n\t      sc = SparkContext(appName=\"PythonStreamingApp\")\n\t      # Necessary log4j logging level settings are done \n\t      log4j = sc._jvm.org.apache.log4j\n\t      log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)\n\t      # Create the Spark Streaming Context with 10 seconds batch interval\n\t      ssc = StreamingContext(sc, 10)\n\t      # Set the check point directory for saving the data to recover when\n        there is a crash\n\t\t    ssc.checkpoint(\"\\tmp\")\n\t\t    # Create a DStream that connects to localhost on port 9999\n\t\t    appLogLines = ssc.socketTextStream(\"localhost\", 9999)\n\t\t    # Count each log messge line containing the word ERROR\n\t\t    errorLines = appLogLines.filter(lambda appLogLine: \"ERROR\" in appLogLine)\n\t\t    # // Print the elements of each RDD generated in this DStream to the console \n\t\t    errorLines.pprint()\n\t\t    # Count the number of messages by the windows and print them\n\t\t    errorLines.countByWindow(30,10).pprint()\n\t\t    # Start the streaming\n\t\t    ssc.start()\n\t\t    # Wait till the application is terminated   \n\t\t    ssc.awaitTermination()\n```", "```py\n $ cd Python\n\t\t$ ./submitPy.sh StreamingApps.py \n\n```", "```py\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:21:50\n\t\t-------------------------------------------\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:22:00\n\t\t-------------------------------------------\n\t\t[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] \n\t\tDirectory index forbidden by rule: /home/raj/\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:23:50\n\t\t-------------------------------------------\n\t\t[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] \n\t\tDirectory index forbidden by rule: /apache/web/test\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:25:10\n\t\t-------------------------------------------\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:25:20\n\t\t-------------------------------------------\n\t\t[Fri Dec 20 02:25:55 2015] [ERROR] [client 1.2.3.4.5.6] \n\t\tClient sent malformed Host header\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:26:50\n\t\t-------------------------------------------\n\t\t[Mon Dec 20 23:02:01 2015] [ERROR] [client 1.2.3.4.5.6] \n\t\tuser test: authentication failure for \"/~raj/test\": Password Mismatch\n\t\t-------------------------------------------\n\t\tTime: 2016-07-23 15:26:50\n\t\t-------------------------------------------\n\n```", "```py\nerrorLines.print()errorLines.countByWindow(Seconds(30), Seconds(10)).print()\n```", "```py\n[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test\n\n```", "```py\n-------------------------------------------\nTime: 1469284630000 ms\n-------------------------------------------\n[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index \n      forbidden by rule: /home/raj/\n-------------------------------------------\nTime: 1469284630000 ms\n      -------------------------------------------\n1\n-------------------------------------------\nTime: 1469284640000 ms\n-------------------------------------------\n[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index \n      forbidden by rule: /apache/web/test\n-------------------------------------------\nTime: 1469284640000 ms\n-------------------------------------------\n2\n-------------------------------------------\nTime: 1469284650000 ms\n-------------------------------------------\n2\n-------------------------------------------\nTime: 1469284660000 ms\n-------------------------------------------\n1\n-------------------------------------------\nTime: 1469284670000 ms\n-------------------------------------------\n0\n\n```", "```py\nssc.checkpoint(\"/tmp\") \n\n```", "```py\nerrorLines.pprint()\nerrorLines.countByWindow(30,10).pprint()\n```", "```py\n[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] \nDirectory index forbidden by rule: /home/raj/\n[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] \nDirectory index forbidden by rule: /home/raj/\n[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] \nDirectory index forbidden by rule: /apache/web/test\n\n```", "```py\n------------------------------------------- \nTime: 2016-07-23 15:29:40 \n------------------------------------------- \n[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ \n------------------------------------------- \nTime: 2016-07-23 15:29:40 \n------------------------------------------- \n1 \n------------------------------------------- \nTime: 2016-07-23 15:29:50 \n------------------------------------------- \n[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /apache/web/test \n------------------------------------------- \nTime: 2016-07-23 15:29:50 \n------------------------------------------- \n2 \n------------------------------------------- \nTime: 2016-07-23 15:30:00 \n------------------------------------------- \n------------------------------------------- \nTime: 2016-07-23 15:30:00 \n------------------------------------------- \n2 \n------------------------------------------- \nTime: 2016-07-23 15:30:10 \n------------------------------------------- \n------------------------------------------- \nTime: 2016-07-23 15:30:10 \n------------------------------------------- \n1 \n------------------------------------------- \nTime: 2016-07-23 15:30:20 \n------------------------------------------- \n------------------------------------------- \nTime: 2016-07-23 15:30:20 \n-------------------------------------------\n\n```", "```py\n$ cd $KAFKA_HOME \n$ $KAFKA_HOME/bin/zookeeper-server-start.sh \n$KAFKA_HOME/config/zookeeper.properties  \n[2016-07-24 09:01:30,196] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory) \n$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties  \n\n[2016-07-24 09:05:06,381] INFO 0 successfully elected as leader \n(kafka.server.ZookeeperLeaderElector) \n[2016-07-24 09:05:06,455] INFO [Kafka Server 0], started \n(kafka.server.KafkaServer) \n$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 \n--replication-factor 1 --partitions 1 --topic sfb \nCreated topic \"sfb\". \n$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list \nlocalhost:9092 --topic sfb\n\n```", "```py\n/** \nThe following program can be compiled and run using SBT \nWrapper scripts have been provided with this \nThe following script can be run to compile the code \n./compile.sh \n\nThe following script can be used to run this application in Spark. The second command line argument of value 1 is very important. This is to flag the shipping of the kafka jar files to the Spark cluster \n./submit.sh com.packtpub.sfb.KafkaStreamingApps 1 \n**/ \npackage com.packtpub.sfb \n\nimport java.util.HashMap \nimport org.apache.spark.streaming._ \nimport org.apache.spark.sql.{Row, SparkSession} \nimport org.apache.spark.streaming.kafka._ \nimport org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord} \n\nobject KafkaStreamingApps { \n  def main(args: Array[String]) { \n   // Log level settings \n   LogSettings.setLogLevels() \n   // Variables used for creating the Kafka stream \n   //The quorum of Zookeeper hosts \n    val zooKeeperQuorum = \"localhost\" \n   // Message group name \n   val messageGroup = \"sfb-consumer-group\" \n   //Kafka topics list separated by coma if there are multiple topics to be listened on \n   val topics = \"sfb\" \n   //Number of threads per topic \n   val numThreads = 1 \n   // Create the Spark Session and the spark context            \n   val spark = SparkSession \n         .builder \n         .appName(getClass.getSimpleName) \n         .getOrCreate() \n   // Get the Spark context from the Spark session for creating the streaming context \n   val sc = spark.sparkContext    \n   // Create the streaming context \n   val ssc = new StreamingContext(sc, Seconds(10)) \n    // Set the check point directory for saving the data to recover when there is a crash \n   ssc.checkpoint(\"/tmp\") \n   // Create the map of topic names \n    val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap \n   // Create the Kafka stream \n    val appLogLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2) \n   // Count each log messge line containing the word ERROR \n    val errorLines = appLogLines.filter(line => line.contains(\"ERROR\")) \n   // Print the line containing the error \n   errorLines.print() \n   // Count the number of messages by the windows and print them \n   errorLines.countByWindow(Seconds(30), Seconds(10)).print() \n   // Start the streaming \n    ssc.start()    \n   // Wait till the application is terminated             \n    ssc.awaitTermination()  \n  } \n} \n\n```", "```py\n # The following script can be used to run this application in Spark \n# ./submitPy.sh KafkaStreamingApps.py 1 \n\nfrom __future__ import print_function \nimport sys \nfrom pyspark import SparkContext \nfrom pyspark.streaming import StreamingContext \nfrom pyspark.streaming.kafka import KafkaUtils \n\nif __name__ == \"__main__\": \n    # Create the Spark context \n    sc = SparkContext(appName=\"PythonStreamingApp\") \n    # Necessary log4j logging level settings are done  \n    log4j = sc._jvm.org.apache.log4j \n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN) \n    # Create the Spark Streaming Context with 10 seconds batch interval \n    ssc = StreamingContext(sc, 10) \n    # Set the check point directory for saving the data to recover when there is a crash \n    ssc.checkpoint(\"\\tmp\") \n    # The quorum of Zookeeper hosts \n    zooKeeperQuorum=\"localhost\" \n    # Message group name \n    messageGroup=\"sfb-consumer-group\" \n    # Kafka topics list separated by coma if there are multiple topics to be listened on \n    topics = \"sfb\" \n    # Number of threads per topic \n    numThreads = 1     \n    # Create a Kafka DStream \n    kafkaStream = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, {topics: numThreads}) \n    # Create the Kafka stream \n    appLogLines = kafkaStream.map(lambda x: x[1]) \n    # Count each log messge line containing the word ERROR \n    errorLines = appLogLines.filter(lambda appLogLine: \"ERROR\" in appLogLine) \n    # Print the first ten elements of each RDD generated in this DStream to the console \n    errorLines.pprint() \n    errorLines.countByWindow(30,10).pprint() \n    # Start the streaming \n    ssc.start() \n    # Wait till the application is terminated    \n    ssc.awaitTermination()\n\n```", "```py\n $ cd Scala\n\t$ ./submit.sh com.packtpub.sfb.KafkaStreamingApps 1\n\n```", "```py\n $ cd Python\n\t$ \n\t./submitPy.sh KafkaStreamingApps.py 1\n\n```", "```py\n\t$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 \n\t--topic sfb \n\t[Fri Dec 20 01:46:23 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ \n\t[Fri Dec 20 01:46:23 2015] [WARN] [client 1.2.3.4.5.6] Directory index forbidden by rule: /home/raj/ \n\t[Fri Dec 20 01:54:34 2015] [ERROR] [client 1.2.3.4.5.6] Directory index forbidden by rule: \n\t/apache/web/test \n\n```", "```py\n ssc = StreamingContext(sc, 10) \n    ssc.checkpoint(\"\\tmp\")\n\n```", "```py\n\t /** \n  * The following function has to be used when the code is being restructured to have checkpointing and driver recovery \n  * The way it should be used is to use the StreamingContext.getOrCreate with this function and do a start of that \n  */ \n  def sscCreateFn(): StreamingContext = { \n   // Variables used for creating the Kafka stream \n   // The quorum of Zookeeper hosts \n    val zooKeeperQuorum = \"localhost\" \n   // Message group name \n   val messageGroup = \"sfb-consumer-group\" \n   //Kafka topics list separated by coma if there are multiple topics to be listened on \n   val topics = \"sfb\" \n   //Number of threads per topic \n   val numThreads = 1      \n   // Create the Spark Session and the spark context            \n   val spark = SparkSession \n         .builder \n         .appName(getClass.getSimpleName) \n         .getOrCreate() \n   // Get the Spark context from the Spark session for creating the streaming context \n   val sc = spark.sparkContext    \n   // Create the streaming context \n   val ssc = new StreamingContext(sc, Seconds(10)) \n   // Create the map of topic names \n    val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap \n   // Create the Kafka stream \n    val appLogLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2) \n   // Count each log messge line containing the word ERROR \n    val errorLines = appLogLines.filter(line => line.contains(\"ERROR\")) \n   // Print the line containing the error \n   errorLines.print() \n   // Count the number of messages by the windows and print them \n   errorLines.countByWindow(Seconds(30), Seconds(10)).print() \n   // Set the check point directory for saving the data to recover when there is a crash \n   ssc.checkpoint(\"/tmp\") \n   // Return the streaming context \n   ssc \n  } \n\n```"]