<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Deployment and Monitoring</h1></div></div></div><p>We have explored the development of the Stock Screener Application in previous chapters; it is now time to consider how to deploy it in the production environment. In this chapter, we will discuss the most important aspects of deploying a Cassandra database in production. These aspects include the selection of an appropriate combination of replication strategy, snitch, and replication factor to make up a fault-tolerant, highly available cluster. Then we will demonstrate the procedure to migrate our Cassandra development database of the Stock Screener Application to a production database. However, cluster maintenance is beyond the scope of this book.</p><p>Moreover, a live production system that continuously operates certainly requires monitoring of its health status. We will cover the basic tools and techniques of monitoring a Cassandra cluster, including the nodetool utility, JMX and MBeans, and system log.</p><p>Finally, we will explore ways of boosting the performance of Cassandra other than using the defaults. Actually, performance tuning can be made at several levels, from the lowest hardware and system configuration to the highest application coding techniques. We will focus on the <strong>Java Virtual Machine</strong> (<strong>JVM</strong>)<a id="id538" class="indexterm"/> level, because Cassandra heavily relies on its underlying performance. In addition, we will touch on how to tune caches for a table.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec36"/>Replication strategies</h1></div></div></div><p>This section<a id="id539" class="indexterm"/> is about the data replication configuration of a Cassandra cluster. It will cover replication strategies, snitches, and the configuration of the cluster for the Stock Screener Application.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec83"/>Data replication</h2></div></div></div><p>Cassandra, by<a id="id540" class="indexterm"/> design, can work in a huge cluster across <a id="id541" class="indexterm"/>multiple data centers all over the globe. In such a distributed environment, network bandwidth and latency must be critically considered in the architecture, and careful planning in advance is required, otherwise it would lead to catastrophic consequences. The most obvious issue is the time clock synchronization—the genuine means of resolving transaction conflicts that can threaten data integrity in the whole cluster. Cassandra <a id="id542" class="indexterm"/>relies on <a id="id543" class="indexterm"/>the underlying operating system platform to provide the time clock synchronization service. Furthermore, a node is highly likely to fail at some time and the cluster must be resilient to this typical node failure. These issues have to be thoroughly considered at the architecture level.</p><p>Cassandra adopts data replication to tackle these issues, based on the idea of using space in exchange of time. It simply consumes more storage space to make data replicas so as to minimize the complexities in resolving the previously mentioned issues in a cluster.</p><p>Data replication is configured by the so-called replication factor in a <a id="id544" class="indexterm"/>
<strong>keyspace</strong>. The replication factor refers to the total number of copies of each row across the cluster. So a replication factor of <code class="literal">1</code> (as seen in the examples in previous chapters) means that there is only one copy of each row on one node. For a replication factor of <code class="literal">2</code>, two copies of each row are on two different nodes. Typically, a replication factor of <code class="literal">3</code> is sufficient in most production scenarios.</p><p>All data replicas are equally important. There are neither master nor slave replicas. So data replication does not have scalability issues. The replication factor can be increased as more nodes are added. However, the replication factor should not be set to exceed the number of nodes in the cluster.</p><p>Another unique feature of Cassandra is its awareness of the physical location of nodes in a cluster and their proximity to each other. Cassandra can be configured to know the layout of the data centers and racks by a correct IP address assignment scheme. This setting is known as replication strategy and Cassandra provides two choices for us: <code class="literal">SimpleStrategy</code> and <code class="literal">NetworkTopologyStrategy</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec84"/>SimpleStrategy</h2></div></div></div><p>
<code class="literal">SimpleStrategy</code> is<a id="id545" class="indexterm"/> used on a single machine <a id="id546" class="indexterm"/>or on a cluster in a single data center. It places the first replica on a node determined by the partitioner, and then the additional replicas are placed on the next nodes in a clockwise fashion without considering the data center and rack locations. Even though this is the default replication strategy when creating a keyspace, if we ever intend to have more than one data center, we should use <code class="literal">NetworkTopologyStrategy</code> instead.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec85"/>NetworkTopologyStrategy</h2></div></div></div><p>
<code class="literal">NetworkTopologyStrategy</code><a id="id547" class="indexterm"/> becomes <a id="id548" class="indexterm"/>aware of the locations of data centers and racks by understanding the IP addresses of the nodes in the cluster. It places replicas in the same data center by the clockwise mechanism until the first node in another rack is reached. It attempts to place replicas on different racks because the nodes in the same rack often fail at the same time due to power, network issues, air conditioning, and so on.</p><p>As mentioned, Cassandra knows the physical location from the IP addresses of the nodes. The mapping of the IP addresses to the data centers and racks is referred to as a <a id="id549" class="indexterm"/>
<strong>snitch</strong>. Simply put, a snitch determines which data centers and racks the nodes belong to. It optimizes read operations by providing information about the network topology to Cassandra such that read requests can be routed efficiently. It also affects how replicas can be distributed in consideration of the physical location of data centers and racks.</p><p>There are many types of snitches available for different scenarios and each comes with its pros and cons. They are briefly described as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">SimpleSnitch</code>: This <a id="id550" class="indexterm"/>is used for single data center <a id="id551" class="indexterm"/>deployments only</li><li class="listitem" style="list-style-type: disc"><code class="literal">DynamicSnitch</code>: This <a id="id552" class="indexterm"/>monitors the performance of<a id="id553" class="indexterm"/> read operations from different replicas, and chooses the best replica based on historical performance</li><li class="listitem" style="list-style-type: disc"><code class="literal">RackInferringSnitch</code>: This <a id="id554" class="indexterm"/>determines the <a id="id555" class="indexterm"/>location of the nodes by data center and rack corresponding to the IP addresses</li><li class="listitem" style="list-style-type: disc"><code class="literal">PropertyFileSntich</code>: This <a id="id556" class="indexterm"/>determines the locations <a id="id557" class="indexterm"/>of the nodes by data center and rack</li><li class="listitem" style="list-style-type: disc"><code class="literal">GossipingPropertyFileSnitch</code>: This<a id="id558" class="indexterm"/> automatically <a id="id559" class="indexterm"/>updates all nodes using gossip when adding new nodes</li><li class="listitem" style="list-style-type: disc"><code class="literal">EC2Snitch</code>: This<a id="id560" class="indexterm"/> is used <a id="id561" class="indexterm"/>with Amazon EC2 in a single region</li><li class="listitem" style="list-style-type: disc"><code class="literal">EC2MultiRegionSnitch</code>: This <a id="id562" class="indexterm"/>is used with <a id="id563" class="indexterm"/>Amazon EC2 in multiple regions</li><li class="listitem" style="list-style-type: disc"><code class="literal">GoogleCloudSnitch</code>: This<a id="id564" class="indexterm"/> is used with Google Cloud <a id="id565" class="indexterm"/>Platform across one or more regions</li><li class="listitem" style="list-style-type: disc"><code class="literal">CloudstackSnitch</code>: This is<a id="id566" class="indexterm"/> used for Apache<a id="id567" class="indexterm"/> Cloudstack environments</li></ul></div><div><div><h3 class="title"><a id="note40"/>Note</h3><p>
<strong>Snitch Architecture</strong>
</p><p>For more detailed information, please refer to the documentation made by DataStax at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html">http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html</a>.</p></div></div><p>The following figure illustrates an example of a cluster of eight nodes in four racks across two data centers using<a id="id568" class="indexterm"/> <code class="literal">RackInferringSnitch</code> and a replication factor of three per data center:</p><div><img src="img/8884OS_07_01.jpg" alt="NetworkTopologyStrategy"/></div><div><div><h3 class="title"><a id="tip07"/>Tip</h3><p>All nodes in the cluster must use the same snitch setting.</p></div></div><p>Let us look at the IP address assignment in <strong>Data Center 1</strong> first. The IP addresses are grouped and assigned in a top-down fashion. All the nodes in <strong>Data Center 1</strong> are in the same <strong>123.1.0.0</strong> subnet. For those nodes in <strong>Rack 1</strong>, they are in the same <strong>123.1.1.0</strong> subnet. Hence, <strong>Node 1</strong> in <strong>Rack 1</strong> is assigned an IP address of <strong>123.1.1.1</strong> and <strong>Node 2</strong> in <strong>Rack 1</strong> is <strong>123.1.1.2</strong>. The same rule applies to <strong>Rack 2</strong> such that the IP addresses of <strong>Node 1</strong> and <strong>Node 2</strong> in <strong>Rack 2</strong> are <strong>123.1.2.1</strong> and <strong>123.1.2.2</strong>, respectively. For <strong>Data Center 2</strong>, we just change the subnet of the data center to <strong>123.2.0.0</strong> and the racks and nodes in <strong>Data Center 2</strong> are then changed similarly.</p><p>The <code class="literal">RackInferringSnitch</code> deserves<a id="id569" class="indexterm"/> a more detailed explanation. It assumes that the network topology is known by properly assigned IP addresses based on the following rule:</p><p>
<em>IP address = &lt;arbitrary octet&gt;.&lt;data center octet&gt;.&lt;rack octet&gt;.&lt;node octet&gt;</em>
</p><p>The formula for IP address assignment is shown in the previous paragraph. With this very structured assignment of IP addresses, Cassandra can understand the physical location of all nodes in the cluster.</p><p>Another thing that we need to understand is the replication factor of the three replicas that are shown in the previous figure. For a cluster with <code class="literal">NetworkToplogyStrategy</code>, the replication factor is set on a per data center basis. So in our example, three replicas are placed in <strong>Data Center 1</strong> as illustrated by the dotted arrows in the previous diagram. <strong>Data Center 2</strong> is another data center that must have three replicas. Hence, there are six replicas in total across the cluster.</p><p>We will not go through every combination of the replication factor, snitch and replication strategy here, but we should now understand the foundation of how Cassandra makes use of them to flexibly deal with different cluster scenarios in real-life production.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec86"/>Setting up the cluster for Stock Screener Application</h2></div></div></div><p>Let us <a id="id570" class="indexterm"/>return to the Stock Screener Application. The cluster it runs on in <a class="link" href="ch06.html" title="Chapter 6. Enhancing a Version">Chapter 6</a>, <em>Enhancing a Version</em>, is a single-node cluster. In this<a id="id571" class="indexterm"/> section, we will set up a cluster of two nodes that can be used in small-scale production. We will also migrate the existing data in the development database to the new fresh production cluster. It should be noted that for quorum reads/writes, it's usually best practice to use an odd number of nodes.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec24"/>System and network configuration</h3></div></div></div><p>The steps <a id="id572" class="indexterm"/>of installation and setup of the operating system and network configuration are assumed to be done. Moreover, both nodes should have Cassandra freshly installed. The system configuration of the two nodes is identical and shown as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">OS: Ubuntu 12.04 LTS 64-bit</li><li class="listitem" style="list-style-type: disc">Processor: Intel Core i7-4771 CPU @3.50GHz x 2</li><li class="listitem" style="list-style-type: disc">Memory: 2 GB</li><li class="listitem" style="list-style-type: disc">Disk: 20 GB</li></ul></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec25"/>Global settings</h3></div></div></div><p>The <a id="id573" class="indexterm"/>cluster is named <a id="id574" class="indexterm"/>
<strong>Test Cluster</strong>, in which both the<a id="id575" class="indexterm"/> <strong>ubtc01</strong> and <strong>ubtc02</strong> <a id="id576" class="indexterm"/>nodes are in the same rack, <code class="literal">RACK1</code>, and in the same data center, <code class="literal">NY1</code>. The logical architecture of the cluster to be set up is depicted in the following diagram:</p><div><img src="img/8884OS_07_02.jpg" alt="Global settings"/></div><p>In order to configure a Cassandra cluster, we need to modify a few properties in the main configuration file, <code class="literal">cassandra.yaml</code>, for Cassandra. Depending on the installation method of Cassandra, <code class="literal">cassandra.yaml</code> is located in different directories:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Package installation: <code class="literal">/etc/cassandra/</code></li><li class="listitem" style="list-style-type: disc">Tarball installation: <code class="literal">&lt;install_location&gt;/conf/</code></li></ul></div><p>The first thing<a id="id577" class="indexterm"/> to do is to set the properties in <code class="literal">cassandra.yaml</code> for each node. As the system configuration of both nodes is the same, the following modification on <code class="literal">cassandra.yaml</code> settings is identical to them:</p><div><pre class="programlisting">-seeds: ubtc01
listen_address:
rpc_address: 0.0.0.0
endpoint_snitch: GossipingPropertyFileSnitch
auto_bootstrap: false</pre></div><p>The reason for using <code class="literal">GossipingPropertyFileSnitch</code> is that we want the Cassandra cluster to automatically update all nodes with the gossip protocol when adding a new node.</p><p>Apart from <code class="literal">cassandra.yaml</code>, we also need to modify the data center and rack properties in <code class="literal">cassandra-rackdc.properties</code> in the same location as <code class="literal">cassandra.yaml</code>. In our case, the data center is <code class="literal">NY1</code> and the rack is <code class="literal">RACK1</code>, as shown in the following code:</p><div><pre class="programlisting">dc=NY1
rack=RACK1</pre></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Configuration procedure</h3></div></div></div><p>The configuration procedure<a id="id578" class="indexterm"/> of the cluster (refer to the following bash shell scripts: <code class="literal">setup_ubtc01.sh</code> and <code class="literal">setup_ubtc02.sh</code>) is enumerated as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Stop Cassandra service:<div><pre class="programlisting">
<strong>ubtc01:~$ sudo service cassandra stop</strong>
<strong>ubtc02:~$ sudo service cassandra stop</strong>
</pre></div></li><li class="listitem">Remove the system keyspace:<div><pre class="programlisting">
<strong>ubtc01:~$ sudo rm -rf /var/lib/cassandra/data/system/*</strong>
<strong>ubtc02:~$ sudo rm -rf /var/lib/cassandra/data/system/*</strong>
</pre></div></li><li class="listitem">Modify <code class="literal">cassandra.yaml</code> and <code class="literal">cassandra-rackdc.properties</code> in both nodes based on the global settings as specified in the previous section</li><li class="listitem">Start the seed node <code class="literal">ubtc01</code> first:<div><pre class="programlisting">
<strong>ubtc01:~$ sudo service cassandra start</strong>
</pre></div></li><li class="listitem">Then start <code class="literal">ubtc02</code>:<div><pre class="programlisting">
<strong>ubtc02:~$ sudo service cassandra start</strong>
</pre></div></li><li class="listitem">Wait for a minute and check if <code class="literal">ubtc01</code> and <code class="literal">ubtc02</code> are both up and running:<div><pre class="programlisting">
<strong>ubtc01:~$ nodetool status</strong>
</pre></div></li></ol></div><p>A successful <a id="id579" class="indexterm"/>result of setting up the cluster should resemble something similar to the following screenshot, showing that both nodes are up and running:</p><div><img src="img/8884OS_07_03.jpg" alt="Configuration procedure"/></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Legacy data migration procedure</h3></div></div></div><p>We now<a id="id580" class="indexterm"/> have the cluster ready but it is empty. We can simply rerun the Stock Screener Application to download and fill in the production database again. Alternatively, we can migrate the historical prices collected in the development single-node cluster to this production cluster. In the case of the latter approach, the following procedure can help us ease the<a id="id581" class="indexterm"/> data migration task:</p><div><ol class="orderedlist arabic"><li class="listitem">Take a snapshot of the <code class="literal">packcdma</code> keyspace in the development database (ubuntu is the hostname of the development machine):<div><pre class="programlisting">
<strong>ubuntu:~$ nodetool snapshot packtcdma</strong>
</pre></div></li><li class="listitem">Record the snapshot directory, in this example, <strong>1412082842986</strong></li><li class="listitem">To play it safe, copy all SSTables under the snapshot directory to a temporary location, say <code class="literal">~/temp/</code>:<div><pre class="programlisting">
<strong>ubuntu:~$ mkdir ~/temp/</strong>
<strong>ubuntu:~$ mkdir ~/temp/packtcdma/</strong>
<strong>ubuntu:~$ mkdir ~/temp/packtcdma/alert_by_date/</strong>
<strong>ubuntu:~$ mkdir ~/temp/packtcdma/alertlist/</strong>
<strong>ubuntu:~$ mkdir ~/temp/packtcdma/quote/</strong>
<strong>ubuntu:~$ mkdir ~/temp/packtcdma/watchlist/</strong>
<strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/alert_by_date/snapshots/1412082842986/* ~/temp/packtcdma/alert_by_date/</strong>
<strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/alertlist/snapshots/1412082842986/* ~/temp/packtcdma/alertlist/</strong>
<strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/quote/snapshots/1412082842986/* ~/temp/packtcdma/quote/</strong>
<strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/watchlist/snapshots/1412082842986/* ~/temp/packtcdma/watchlist/</strong>
</pre></div></li><li class="listitem">Open cqlsh to connect to <code class="literal">ubtc01</code> and create a keyspace with the appropriate replication strategy in the production cluster:<div><pre class="programlisting">
<strong>ubuntu:~$ cqlsh ubtc01</strong>
<strong>cqlsh&gt; CREATE KEYSPACE packtcdma WITH replication = {'class': 'NetworkTopologyStrategy',  'NY1': '2'};</strong>
</pre></div></li><li class="listitem">Create the <code class="literal">alert_by_date</code>, <code class="literal">alertlist</code>, <code class="literal">quote</code>, and <code class="literal">watchlist</code> tables:<div><pre class="programlisting">
<strong>cqlsh&gt; CREATE TABLE packtcdma.alert_by_date (</strong>
<strong>  price_time timestamp,</strong>
<strong>  symbol varchar,</strong>
<strong>  signal_price float,</strong>
<strong>  stock_name varchar,</strong>
<strong>  PRIMARY KEY (price_time, symbol));</strong>
<strong>cqlsh&gt; CREATE TABLE packtcdma.alertlist (</strong>
<strong>  symbol varchar,</strong>
<strong>  price_time timestamp,</strong>
<strong>  signal_price float,</strong>
<strong>  stock_name varchar,</strong>
<strong>  PRIMARY KEY (symbol, price_time));</strong>
<strong>cqlsh&gt; CREATE TABLE packtcdma.quote (</strong>
<strong>  symbol varchar,</strong>
<strong>  price_time timestamp,</strong>
<strong>  close_price float,</strong>
<strong>  high_price float,</strong>
<strong>  low_price float,</strong>
<strong>  open_price float,</strong>
<strong>  stock_name varchar,</strong>
<strong>  volume double,</strong>
<strong>  PRIMARY KEY (symbol, price_time));</strong>
<strong>cqlsh&gt; CREATE TABLE packtcdma.watchlist (</strong>
<strong>  watch_list_code varchar,</strong>
<strong>  symbol varchar,</strong>
<strong>  PRIMARY KEY (watch_list_code, symbol));</strong>
</pre></div></li><li class="listitem">Load the <a id="id582" class="indexterm"/>SSTables back to the production cluster using the <code class="literal">sstableloader</code> utility:<div><pre class="programlisting">
<strong>ubuntu:~$ cd ~/temp</strong>
<strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/alert_by_date</strong>
<strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/alertlist</strong>
<strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/quote</strong>
<strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/watchlist</strong>
</pre></div></li><li class="listitem">Check the legacy data in the production database on <code class="literal">ubtc02</code>:<div><pre class="programlisting">
<strong>cqlsh&gt; select * from packtcdma.alert_by_date;</strong>
<strong>cqlsh&gt; select * from packtcdma.alertlist;</strong>
<strong>cqlsh&gt; select * from packtcdma.quote;</strong>
<strong>cqlsh&gt; select * from packtcdma.watchlist;</strong>
</pre></div></li></ol></div><p>Although <a id="id583" class="indexterm"/>the previous steps look complicated, it is not difficult to understand what they want to achieve. It should be noted that we have set the replication factor per data center as <code class="literal">2</code> to provide data redundancy on both nodes, as shown in the <code class="literal">CREATE KEYSPACE</code> statement. The replication factor can be changed in future if needed.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec28"/>Deploying the Stock Screener Application</h3></div></div></div><p>As we <a id="id584" class="indexterm"/>have set up the production cluster and moved the legacy data into it, it is time to deploy the Stock Screener Application. The only thing needed to modify is the code to establish Cassandra connection to the production cluster. This is very easy to do with Python. The code in <code class="literal">chapter06_006.py</code> is modified to work with the production cluster as <code class="literal">chapter07_001.py</code>. A new test case named <code class="literal">testcase003()</code> is created to replace <code class="literal">testcase002()</code>. To save pages, the complete source code of <code class="literal">chapter07_001.py</code> is not shown here; only the <code class="literal">testcase003()</code> function is depicted as follows:</p><div><pre class="programlisting"># -*- coding: utf-8 -*-
# program: chapter07_001.py

# other functions are not shown for brevity

def testcase003():
    ## create Cassandra instance with multiple nodes
    cluster = Cluster(['ubtc01', 'ubtc02'])
    
    ## establish Cassandra connection, using local default
    session = cluster.connect('packtcdma')
    
    start_date = datetime.datetime(2012, 6, 28)
    end_date = datetime.datetime(2013, 9, 28)
    
    ## load the watch list
    stocks_watched = load_watchlist(session, "WS01")
    
    for symbol in stocks_watched:
        ## retrieve data
        data = retrieve_data(session, symbol, start_date, end_date)
        
        ## compute 10-Day SMA
        data = sma(data, 10)
        
        ## generate the buy-and-hold signals
        alerts = signal_close_higher_than_sma10(data)
        
        ## save the alert list
        for index, r in alerts.iterrows():
            insert_alert(session, symbol, index, \
                         Decimal(r['close_price']), \
                         r['stock_name'])
    
    ## close Cassandra connection
    cluster.shutdown()

testcase003()</pre></div><p>The cluster <a id="id585" class="indexterm"/>connection code right at the beginning of the <code class="literal">testcase003()</code> function is passed with an array of the nodes to be connected (<code class="literal">ubtc01</code> and <code class="literal">ubtc02</code>). Here we adopted the default <code class="literal">RoundRobinPolicy</code> as the connection load balancing policy. It is used to decide how to distribute requests among all possible coordinator nodes in the cluster. There are many other options which are described in the driver API documentation.</p><div><div><h3 class="title"><a id="note41"/>Note</h3><p>
<strong>Cassandra Driver 2.1 Documentation</strong>
</p><p>For the complete <a id="id586" class="indexterm"/>API documentation of the Python Driver 2.1 for Apache Cassandra, you can refer to <a class="ulink" href="http://datastax.github.io/python-driver/api/index.html">http://datastax.github.io/python-driver/api/index.html</a>.</p></div></div></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Monitoring</h1></div></div></div><p>As the<a id="id587" class="indexterm"/> application system goes live, we need to monitor its health day-by-day. Cassandra provides a number of tools for this purpose. We will introduce some of them with pragmatic recommendations. It is remarkable that each operating system also <a id="id588" class="indexterm"/>provides a bunch of tools and utilities for monitoring, for example, <code class="literal">top</code>, <code class="literal">df</code>, <code class="literal">du</code> on Linux and Task Manager on Windows. However, they are beyond the scope of this book.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec87"/>Nodetool</h2></div></div></div><p>The <a id="id589" class="indexterm"/>nodetool<a id="id590" class="indexterm"/> utility should not be new to us. It is a command-line interface used to monitor Cassandra and perform routine database operations. It includes the most important metrics for tables, server, and compaction statistics, and other useful commands for administration.</p><p>Here are the most commonly used <code class="literal">nodetool</code> options:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">status</code>: This<a id="id591" class="indexterm"/> provides a concise summary of the cluster, such as the state, load, and IDs</li><li class="listitem" style="list-style-type: disc"><code class="literal">netstats</code>: This <a id="id592" class="indexterm"/>gives the network information for a node, focusing on read repair operations</li><li class="listitem" style="list-style-type: disc"><code class="literal">info</code>: This <a id="id593" class="indexterm"/>gives valuable node information including token, on disk load, uptime, Java heap memory usage, key cache, and row cache</li><li class="listitem" style="list-style-type: disc"><code class="literal">tpstats</code>: This <a id="id594" class="indexterm"/>provides statistics about the number of active, pending, and completed tasks for each stage of Cassandra operations by thread pool</li><li class="listitem" style="list-style-type: disc"><code class="literal">cfstats</code>: This <a id="id595" class="indexterm"/>gets the statistics about one or more tables, such as read-and-write counts and latencies, metrics about SSTable, memtable, bloom filter, and compaction.</li></ul></div><div><div><h3 class="title"><a id="note42"/>Note</h3><p>A detailed <a id="id596" class="indexterm"/>documentation of nodetool can be referred to at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html</a>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec88"/>JMX and MBeans</h2></div></div></div><p>Cassandra <a id="id597" class="indexterm"/>is <a id="id598" class="indexterm"/>written in the Java language and so<a id="id599" class="indexterm"/> it natively supports <strong>Java Management Extensions</strong> (<strong>JMX</strong>). We may use JConsole, a JMX-compliant tool, to<a id="id600" class="indexterm"/> monitor Cassandra.</p><div><div><h3 class="title"><a id="note43"/>Note</h3><p>
<strong>JConsole</strong>
</p><p>JConsole<a id="id601" class="indexterm"/> is included with Sun JDK 5.0 and higher versions. However, it consumes a significant amount of system resources. It is recommended that you run it on a remote machine rather than on the same host as a Cassandra node.</p></div></div><p>We can <a id="id602" class="indexterm"/>launch JConsole by typing <code class="literal">jconsole</code> in a terminal. Assuming that we want to monitor the local node, when the <strong>New Connection</strong> dialog box pops up, we type <code class="literal">localhost:7199</code> (<code class="literal">7199</code> is the port number of JMX) in the <strong>Remote Process</strong> textbox, as depicted in the following screenshot:</p><div><img src="img/8884OS_07_04.jpg" alt="JMX and MBeans"/></div><p>After having connected to the local Cassandra instance, we will see a well-organized GUI showing six separate tabs placed horizontally on the top, as seen in the following screenshot:</p><div><img src="img/8884OS_07_05.jpg" alt="JMX and MBeans"/></div><p>The tabs of the <a id="id603" class="indexterm"/>GUI are explained as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Overview</strong>: This displays overview information about the JVM and monitored values</li><li class="listitem" style="list-style-type: disc"><strong>Memory</strong>: This displays information about heap and non-heap memory usage and garbage collection metrics</li><li class="listitem" style="list-style-type: disc"><strong>Threads</strong>: This displays information about thread use</li><li class="listitem" style="list-style-type: disc"><strong>Classes</strong>: This displays information about class loading</li><li class="listitem" style="list-style-type: disc"><strong>VM Summary</strong>: This displays information about the JVM</li><li class="listitem" style="list-style-type: disc"><strong>MBeans</strong>: This displays information about specific Cassandra metrics and operations</li></ul></div><p>Furthermore, Cassandra provides five MBeans<a id="id604" class="indexterm"/> for JConsole. They are briefly introduced as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.db</code>: This <a id="id605" class="indexterm"/>includes caching, table metrics, and compaction</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.internal</code>: These <a id="id606" class="indexterm"/>are internal server operations such as gossip and hinted handoff</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.metrics</code>: These<a id="id607" class="indexterm"/> are various metrics of the Cassandra instance such as cache and compaction</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.net</code>: This <a id="id608" class="indexterm"/>has Inter-node communication including FailureDetector, MessagingService and StreamingService</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.request</code>: These<a id="id609" class="indexterm"/> include tasks related to read, write, and replication operations</li></ul></div><div><div><h3 class="title"><a id="note44"/>Note</h3><p>
<strong>MBeans</strong>
</p><p>An <strong>Managed Bean</strong> (<strong>MBean</strong>) is <a id="id610" class="indexterm"/>a Java object that represents a manageable resource such as an application, a service, a component, or a device running in the JVM. It can be used to collect statistics on concerns such as performance, resource usage, or problems, for getting and setting application configurations or properties, and notifying events like faults or state changes.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec89"/>The system log</h2></div></div></div><p>The most <a id="id611" class="indexterm"/>rudimentary,<a id="id612" class="indexterm"/> yet the most powerful, monitoring tool is Cassandra's system log. The default location of the system log is named <code class="literal">system.log</code> under <code class="literal">/var/log/cassandra/</code>. It is simply a text file and can be viewed or edited by any text editor. The following screenshot shows an extract of <code class="literal">system.log</code>:</p><div><img src="img/8884OS_07_06.jpg" alt="The system log"/></div><p>This piece <a id="id613" class="indexterm"/>of <a id="id614" class="indexterm"/>log looks long and weird. However, if you are a Java developer and you are familiar with the standard log library, Log4j, it is pretty straightforward. The beauty of Log4j is the provision of different log levels for us to control the granularity of the log statements to be recorded in <code class="literal">system.log</code>. As shown in the previous figure, the first word of each line is <code class="literal">INFO</code>, meaning that the log statement is a piece of information. Other log level choices include <code class="literal">FATAL</code>, <code class="literal">ERROR</code>, <code class="literal">WARN</code>, <code class="literal">DEBUG</code>, and <code class="literal">TRACE</code>, from the least verbose to the most verbose.</p><p>The system log is very valuable in troubleshooting problems as well. We may increase the log level to <code class="literal">DEBUG</code> or <code class="literal">TRACE</code> for troubleshooting. However, running a production Cassandra cluster in the <code class="literal">DEBUG</code> or <code class="literal">TRACE</code> mode will degrade its performance significantly. We must use them with great care.</p><p>We can change the standard log level in Cassandra by adjusting the <code class="literal">log4j.rootLogger</code> property in <code class="literal">log4j-server.properties</code> in the Cassandra configuration directory. The following screenshot shows the content of <code class="literal">log4j-server.properties</code> in ubtc02:</p><div><img src="img/8884OS_07_07.jpg" alt="The system log"/></div><p>It is important <a id="id615" class="indexterm"/>to mention that <code class="literal">system.log</code> and <code class="literal">log4j-server.properties</code> are only responsible for a single node. For a cluster of two nodes, we will<a id="id616" class="indexterm"/> have two <code class="literal">system.log</code> and two <code class="literal">log4j-server.properties</code> on the respective nodes.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Performance tuning</h1></div></div></div><p>Performance tuning<a id="id617" class="indexterm"/> is a large and complex topic that in itself can be a whole course. We can only scratch the surface of it in this short section. Similar to monitoring in the last section, operating system-specific performance tuning techniques are beyond the scope of this book.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec90"/>Java virtual machine</h2></div></div></div><p>Based on<a id="id618" class="indexterm"/> the information given by the monitoring tools and <a id="id619" class="indexterm"/>the system log, we can discover opportunities for performance tuning. The first things we usually watch are the Java heap memory and garbage collection. JVM's configuration settings<a id="id620" class="indexterm"/> are controlled in the environment settings file for Cassandra, <code class="literal">cassandra-env.sh</code>, located in <code class="literal">/etc/cassandra/</code>. An example is shown in the following screenshot:</p><div><img src="img/8884OS_07_08.jpg" alt="Java virtual machine"/></div><p>Basically, it already <a id="id621" class="indexterm"/>has the boilerplate options calculated to be optimized for the host system. It is also accompanied with explanation for us to tweak specific JVM parameters and the startup options of a Cassandra instance when we experience real issues; otherwise, these boilerplate options should not be altered.</p><div><div><h3 class="title"><a id="note45"/>Note</h3><p>A detailed documentation on <a id="id622" class="indexterm"/>how to tune JVM for Cassandra can be found at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html</a>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec91"/>Caching</h2></div></div></div><p>Another <a id="id623" class="indexterm"/>area <a id="id624" class="indexterm"/>we should pay attention to is caching. Cassandra includes integrated caching and distributes cache data around the cluster. For a cache specific to a table, we will focus on the partition key cache and the row cache.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec29"/>Partition key cache</h3></div></div></div><p>The <a id="id625" class="indexterm"/>partition key cache, or <a id="id626" class="indexterm"/>key cache for short, is a cache of the partition index for a table. Using the key cache saves processor time and memory. However, enabling just the key cache makes the disk activity actually read the requested data rows.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec30"/>Row cache</h3></div></div></div><p>The <a id="id627" class="indexterm"/>row cache<a id="id628" class="indexterm"/> is similar to a traditional cache. When a row is accessed, the entire row is pulled into memory, merging from multiple SSTables when required, and cached. This prevents Cassandra from retrieving that row using disk I/O again, which can tremendously improve read performance.</p><p>When both row cache and partition key cache are configured, the row cache returns results whenever possible. In the event of a row cache miss, the partition key cache might still provide a hit that makes the disk seek much more efficient.</p><p>However, there is one caveat. Cassandra caches all the rows of a partition when reading that partition. So if the partition is large or only a small portion of the partition is read every time, the row cache might not be beneficial. It is very easy to be misused and consequently the JVM will be exhausted, causing Cassandra to fail. That is why the row cache is disabled by default.</p><div><div><h3 class="title"><a id="note46"/>Note</h3><p>We usually enable either the key or row cache for a table, not both at the same time.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Monitoring cache</h3></div></div></div><p>Either <a id="id629" class="indexterm"/>the <code class="literal">nodetool info</code> <a id="id630" class="indexterm"/>command or JMX MBeans can provide assistance in monitoring cache. We should make changes to cache options in small, incremental adjustments, and then monitor the effects of each change using the nodetool utility. The last two lines of output of the <code class="literal">nodetool info</code> command, as seen in the following figure, contain the <code class="literal">Row Cache</code> and <code class="literal">Key Cache</code> metrics of <code class="literal">ubtc02</code>:</p><div><img src="img/8884OS_07_09.jpg" alt="Monitoring cache"/></div><p>In the<a id="id631" class="indexterm"/> event <a id="id632" class="indexterm"/>of high memory consumption, we can consider tuning data caches.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec32"/>Enabling/disabling cache</h3></div></div></div><p>We <a id="id633" class="indexterm"/>use <a id="id634" class="indexterm"/>the<a id="id635" class="indexterm"/> CQL to enable or disable caching by altering the cache property of a table. For instance, we use the <code class="literal">ALTER TABLE</code> statement to enable the row cache for <code class="literal">watchlist</code>:</p><div><pre class="programlisting">
<strong>ALTER TABLE watchlist WITH caching=''ROWS_ONLY'';</strong>
</pre></div><p>Other available table<a id="id636" class="indexterm"/> caching options include <code class="literal">ALL</code>, <code class="literal">KEYS_ONLY</code> and <code class="literal">NONE</code>. They are quite self-explanatory and we do not go through each of them here.</p><div><div><h3 class="title"><a id="note47"/>Note</h3><p>Further information <a id="id637" class="indexterm"/>about data caching can be found at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html</a>.</p></div></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Summary</h1></div></div></div><p>This chapter highlights the most important aspects of deploying a Cassandra cluster into the production environment. Cassandra can be taught to understand the physical location of the nodes in the cluster in order to intelligently manage its availability, scalability and performance. We deployed the Stock Screener Application to the production environment, though the scale is small. It is also valuable for us to learn how to migrate legacy data from a non-production environment.</p><p>We then learned the basics of monitoring and performance tuning which are a must for a live running system. If you have experience in deploying other database and system, you may well appreciate the neatness and simplicity of Cassandra.</p><p>In the next chapter, we will have a look at the supplementary information pertinent to application design and development. We will also summarize of the essence of each chapter.</p></div></body></html>