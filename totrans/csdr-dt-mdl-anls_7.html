<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Deployment and Monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Deployment and Monitoring</h1></div></div></div><p>We have explored the development of the Stock Screener Application in previous chapters; it is now time to consider how to deploy it in the production environment. In this chapter, we will discuss the most important aspects of deploying a Cassandra database in production. These aspects include the selection of an appropriate combination of replication strategy, snitch, and replication factor to make up a fault-tolerant, highly available cluster. Then we will demonstrate the procedure to migrate our Cassandra development database of the Stock Screener Application to a production database. However, cluster maintenance is beyond the scope of this book.</p><p>Moreover, a live production system that continuously operates certainly requires monitoring of its health status. We will cover the basic tools and techniques of monitoring a Cassandra cluster, including the nodetool utility, JMX and MBeans, and system log.</p><p>Finally, we will explore ways of boosting the performance of Cassandra other than using the defaults. Actually, performance tuning can be made at several levels, from the lowest hardware and system configuration to the highest application coding techniques. We will focus on the <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>)<a id="id538" class="indexterm"/> level, because Cassandra heavily relies on its underlying performance. In addition, we will touch on how to tune caches for a table.</p><div class="section" title="Replication strategies"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec36"/>Replication strategies</h1></div></div></div><p>This section<a id="id539" class="indexterm"/> is about the data replication configuration of a Cassandra cluster. It will cover replication strategies, snitches, and the configuration of the cluster for the Stock Screener Application.</p><div class="section" title="Data replication"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec83"/>Data replication</h2></div></div></div><p>Cassandra, by<a id="id540" class="indexterm"/> design, can work in a huge cluster across <a id="id541" class="indexterm"/>multiple data centers all over the globe. In such a distributed environment, network bandwidth and latency must be critically considered in the architecture, and careful planning in advance is required, otherwise it would lead to catastrophic consequences. The most obvious issue is the time clock synchronization—the genuine means of resolving transaction conflicts that can threaten data integrity in the whole cluster. Cassandra <a id="id542" class="indexterm"/>relies on <a id="id543" class="indexterm"/>the underlying operating system platform to provide the time clock synchronization service. Furthermore, a node is highly likely to fail at some time and the cluster must be resilient to this typical node failure. These issues have to be thoroughly considered at the architecture level.</p><p>Cassandra adopts data replication to tackle these issues, based on the idea of using space in exchange of time. It simply consumes more storage space to make data replicas so as to minimize the complexities in resolving the previously mentioned issues in a cluster.</p><p>Data replication is configured by the so-called replication factor in a <a id="id544" class="indexterm"/>
<span class="strong"><strong>keyspace</strong></span>. The replication factor refers to the total number of copies of each row across the cluster. So a replication factor of <code class="literal">1</code> (as seen in the examples in previous chapters) means that there is only one copy of each row on one node. For a replication factor of <code class="literal">2</code>, two copies of each row are on two different nodes. Typically, a replication factor of <code class="literal">3</code> is sufficient in most production scenarios.</p><p>All data replicas are equally important. There are neither master nor slave replicas. So data replication does not have scalability issues. The replication factor can be increased as more nodes are added. However, the replication factor should not be set to exceed the number of nodes in the cluster.</p><p>Another unique feature of Cassandra is its awareness of the physical location of nodes in a cluster and their proximity to each other. Cassandra can be configured to know the layout of the data centers and racks by a correct IP address assignment scheme. This setting is known as replication strategy and Cassandra provides two choices for us: <code class="literal">SimpleStrategy</code> and <code class="literal">NetworkTopologyStrategy</code>.</p></div><div class="section" title="SimpleStrategy"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec84"/>SimpleStrategy</h2></div></div></div><p>
<code class="literal">SimpleStrategy</code> is<a id="id545" class="indexterm"/> used on a single machine <a id="id546" class="indexterm"/>or on a cluster in a single data center. It places the first replica on a node determined by the partitioner, and then the additional replicas are placed on the next nodes in a clockwise fashion without considering the data center and rack locations. Even though this is the default replication strategy when creating a keyspace, if we ever intend to have more than one data center, we should use <code class="literal">NetworkTopologyStrategy</code> instead.</p></div><div class="section" title="NetworkTopologyStrategy"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec85"/>NetworkTopologyStrategy</h2></div></div></div><p>
<code class="literal">NetworkTopologyStrategy</code><a id="id547" class="indexterm"/> becomes <a id="id548" class="indexterm"/>aware of the locations of data centers and racks by understanding the IP addresses of the nodes in the cluster. It places replicas in the same data center by the clockwise mechanism until the first node in another rack is reached. It attempts to place replicas on different racks because the nodes in the same rack often fail at the same time due to power, network issues, air conditioning, and so on.</p><p>As mentioned, Cassandra knows the physical location from the IP addresses of the nodes. The mapping of the IP addresses to the data centers and racks is referred to as a <a id="id549" class="indexterm"/>
<span class="strong"><strong>snitch</strong></span>. Simply put, a snitch determines which data centers and racks the nodes belong to. It optimizes read operations by providing information about the network topology to Cassandra such that read requests can be routed efficiently. It also affects how replicas can be distributed in consideration of the physical location of data centers and racks.</p><p>There are many types of snitches available for different scenarios and each comes with its pros and cons. They are briefly described as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">SimpleSnitch</code>: This <a id="id550" class="indexterm"/>is used for single data center <a id="id551" class="indexterm"/>deployments only</li><li class="listitem" style="list-style-type: disc"><code class="literal">DynamicSnitch</code>: This <a id="id552" class="indexterm"/>monitors the performance of<a id="id553" class="indexterm"/> read operations from different replicas, and chooses the best replica based on historical performance</li><li class="listitem" style="list-style-type: disc"><code class="literal">RackInferringSnitch</code>: This <a id="id554" class="indexterm"/>determines the <a id="id555" class="indexterm"/>location of the nodes by data center and rack corresponding to the IP addresses</li><li class="listitem" style="list-style-type: disc"><code class="literal">PropertyFileSntich</code>: This <a id="id556" class="indexterm"/>determines the locations <a id="id557" class="indexterm"/>of the nodes by data center and rack</li><li class="listitem" style="list-style-type: disc"><code class="literal">GossipingPropertyFileSnitch</code>: This<a id="id558" class="indexterm"/> automatically <a id="id559" class="indexterm"/>updates all nodes using gossip when adding new nodes</li><li class="listitem" style="list-style-type: disc"><code class="literal">EC2Snitch</code>: This<a id="id560" class="indexterm"/> is used <a id="id561" class="indexterm"/>with Amazon EC2 in a single region</li><li class="listitem" style="list-style-type: disc"><code class="literal">EC2MultiRegionSnitch</code>: This <a id="id562" class="indexterm"/>is used with <a id="id563" class="indexterm"/>Amazon EC2 in multiple regions</li><li class="listitem" style="list-style-type: disc"><code class="literal">GoogleCloudSnitch</code>: This<a id="id564" class="indexterm"/> is used with Google Cloud <a id="id565" class="indexterm"/>Platform across one or more regions</li><li class="listitem" style="list-style-type: disc"><code class="literal">CloudstackSnitch</code>: This is<a id="id566" class="indexterm"/> used for Apache<a id="id567" class="indexterm"/> Cloudstack environments</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note40"/>Note</h3><p>
<span class="strong"><strong>Snitch Architecture</strong></span>
</p><p>For more detailed information, please refer to the documentation made by DataStax at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html">http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html</a>.</p></div></div><p>The following figure illustrates an example of a cluster of eight nodes in four racks across two data centers using<a id="id568" class="indexterm"/> <code class="literal">RackInferringSnitch</code> and a replication factor of three per data center:</p><div class="mediaobject"><img src="graphics/8884OS_07_01.jpg" alt="NetworkTopologyStrategy"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>All nodes in the cluster must use the same snitch setting.</p></div></div><p>Let us look at the IP address assignment in <span class="strong"><strong>Data Center 1</strong></span> first. The IP addresses are grouped and assigned in a top-down fashion. All the nodes in <span class="strong"><strong>Data Center 1</strong></span> are in the same <span class="strong"><strong>123.1.0.0</strong></span> subnet. For those nodes in <span class="strong"><strong>Rack 1</strong></span>, they are in the same <span class="strong"><strong>123.1.1.0</strong></span> subnet. Hence, <span class="strong"><strong>Node 1</strong></span> in <span class="strong"><strong>Rack 1</strong></span> is assigned an IP address of <span class="strong"><strong>123.1.1.1</strong></span> and <span class="strong"><strong>Node 2</strong></span> in <span class="strong"><strong>Rack 1</strong></span> is <span class="strong"><strong>123.1.1.2</strong></span>. The same rule applies to <span class="strong"><strong>Rack 2</strong></span> such that the IP addresses of <span class="strong"><strong>Node 1</strong></span> and <span class="strong"><strong>Node 2</strong></span> in <span class="strong"><strong>Rack 2</strong></span> are <span class="strong"><strong>123.1.2.1</strong></span> and <span class="strong"><strong>123.1.2.2</strong></span>, respectively. For <span class="strong"><strong>Data Center 2</strong></span>, we just change the subnet of the data center to <span class="strong"><strong>123.2.0.0</strong></span> and the racks and nodes in <span class="strong"><strong>Data Center 2</strong></span> are then changed similarly.</p><p>The <code class="literal">RackInferringSnitch</code> deserves<a id="id569" class="indexterm"/> a more detailed explanation. It assumes that the network topology is known by properly assigned IP addresses based on the following rule:</p><p>
<span class="emphasis"><em>IP address = &lt;arbitrary octet&gt;.&lt;data center octet&gt;.&lt;rack octet&gt;.&lt;node octet&gt;</em></span>
</p><p>The formula for IP address assignment is shown in the previous paragraph. With this very structured assignment of IP addresses, Cassandra can understand the physical location of all nodes in the cluster.</p><p>Another thing that we need to understand is the replication factor of the three replicas that are shown in the previous figure. For a cluster with <code class="literal">NetworkToplogyStrategy</code>, the replication factor is set on a per data center basis. So in our example, three replicas are placed in <span class="strong"><strong>Data Center 1</strong></span> as illustrated by the dotted arrows in the previous diagram. <span class="strong"><strong>Data Center 2</strong></span> is another data center that must have three replicas. Hence, there are six replicas in total across the cluster.</p><p>We will not go through every combination of the replication factor, snitch and replication strategy here, but we should now understand the foundation of how Cassandra makes use of them to flexibly deal with different cluster scenarios in real-life production.</p></div><div class="section" title="Setting up the cluster for Stock Screener Application"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec86"/>Setting up the cluster for Stock Screener Application</h2></div></div></div><p>Let us <a id="id570" class="indexterm"/>return to the Stock Screener Application. The cluster it runs on in <a class="link" href="ch06.html" title="Chapter 6. Enhancing a Version">Chapter 6</a>, <span class="emphasis"><em>Enhancing a Version</em></span>, is a single-node cluster. In this<a id="id571" class="indexterm"/> section, we will set up a cluster of two nodes that can be used in small-scale production. We will also migrate the existing data in the development database to the new fresh production cluster. It should be noted that for quorum reads/writes, it's usually best practice to use an odd number of nodes.</p><div class="section" title="System and network configuration"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec24"/>System and network configuration</h3></div></div></div><p>The steps <a id="id572" class="indexterm"/>of installation and setup of the operating system and network configuration are assumed to be done. Moreover, both nodes should have Cassandra freshly installed. The system configuration of the two nodes is identical and shown as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">OS: Ubuntu 12.04 LTS 64-bit</li><li class="listitem" style="list-style-type: disc">Processor: Intel Core i7-4771 CPU @3.50GHz x 2</li><li class="listitem" style="list-style-type: disc">Memory: 2 GB</li><li class="listitem" style="list-style-type: disc">Disk: 20 GB</li></ul></div></div><div class="section" title="Global settings"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec25"/>Global settings</h3></div></div></div><p>The <a id="id573" class="indexterm"/>cluster is named <a id="id574" class="indexterm"/>
<span class="strong"><strong>Test Cluster</strong></span>, in which both the<a id="id575" class="indexterm"/> <span class="strong"><strong>ubtc01</strong></span> and <span class="strong"><strong>ubtc02</strong></span> <a id="id576" class="indexterm"/>nodes are in the same rack, <code class="literal">RACK1</code>, and in the same data center, <code class="literal">NY1</code>. The logical architecture of the cluster to be set up is depicted in the following diagram:</p><div class="mediaobject"><img src="graphics/8884OS_07_02.jpg" alt="Global settings"/></div><p>In order to configure a Cassandra cluster, we need to modify a few properties in the main configuration file, <code class="literal">cassandra.yaml</code>, for Cassandra. Depending on the installation method of Cassandra, <code class="literal">cassandra.yaml</code> is located in different directories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Package installation: <code class="literal">/etc/cassandra/</code></li><li class="listitem" style="list-style-type: disc">Tarball installation: <code class="literal">&lt;install_location&gt;/conf/</code></li></ul></div><p>The first thing<a id="id577" class="indexterm"/> to do is to set the properties in <code class="literal">cassandra.yaml</code> for each node. As the system configuration of both nodes is the same, the following modification on <code class="literal">cassandra.yaml</code> settings is identical to them:</p><div class="informalexample"><pre class="programlisting">-seeds: ubtc01
listen_address:
rpc_address: 0.0.0.0
endpoint_snitch: GossipingPropertyFileSnitch
auto_bootstrap: false</pre></div><p>The reason for using <code class="literal">GossipingPropertyFileSnitch</code> is that we want the Cassandra cluster to automatically update all nodes with the gossip protocol when adding a new node.</p><p>Apart from <code class="literal">cassandra.yaml</code>, we also need to modify the data center and rack properties in <code class="literal">cassandra-rackdc.properties</code> in the same location as <code class="literal">cassandra.yaml</code>. In our case, the data center is <code class="literal">NY1</code> and the rack is <code class="literal">RACK1</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">dc=NY1
rack=RACK1</pre></div></div><div class="section" title="Configuration procedure"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Configuration procedure</h3></div></div></div><p>The configuration procedure<a id="id578" class="indexterm"/> of the cluster (refer to the following bash shell scripts: <code class="literal">setup_ubtc01.sh</code> and <code class="literal">setup_ubtc02.sh</code>) is enumerated as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Stop Cassandra service:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubtc01:~$ sudo service cassandra stop</strong></span>
<span class="strong"><strong>ubtc02:~$ sudo service cassandra stop</strong></span>
</pre></div></li><li class="listitem">Remove the system keyspace:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubtc01:~$ sudo rm -rf /var/lib/cassandra/data/system/*</strong></span>
<span class="strong"><strong>ubtc02:~$ sudo rm -rf /var/lib/cassandra/data/system/*</strong></span>
</pre></div></li><li class="listitem">Modify <code class="literal">cassandra.yaml</code> and <code class="literal">cassandra-rackdc.properties</code> in both nodes based on the global settings as specified in the previous section</li><li class="listitem">Start the seed node <code class="literal">ubtc01</code> first:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubtc01:~$ sudo service cassandra start</strong></span>
</pre></div></li><li class="listitem">Then start <code class="literal">ubtc02</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubtc02:~$ sudo service cassandra start</strong></span>
</pre></div></li><li class="listitem">Wait for a minute and check if <code class="literal">ubtc01</code> and <code class="literal">ubtc02</code> are both up and running:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubtc01:~$ nodetool status</strong></span>
</pre></div></li></ol></div><p>A successful <a id="id579" class="indexterm"/>result of setting up the cluster should resemble something similar to the following screenshot, showing that both nodes are up and running:</p><div class="mediaobject"><img src="graphics/8884OS_07_03.jpg" alt="Configuration procedure"/></div></div><div class="section" title="Legacy data migration procedure"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Legacy data migration procedure</h3></div></div></div><p>We now<a id="id580" class="indexterm"/> have the cluster ready but it is empty. We can simply rerun the Stock Screener Application to download and fill in the production database again. Alternatively, we can migrate the historical prices collected in the development single-node cluster to this production cluster. In the case of the latter approach, the following procedure can help us ease the<a id="id581" class="indexterm"/> data migration task:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Take a snapshot of the <code class="literal">packcdma</code> keyspace in the development database (ubuntu is the hostname of the development machine):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubuntu:~$ nodetool snapshot packtcdma</strong></span>
</pre></div></li><li class="listitem">Record the snapshot directory, in this example, <span class="strong"><strong>1412082842986</strong></span></li><li class="listitem">To play it safe, copy all SSTables under the snapshot directory to a temporary location, say <code class="literal">~/temp/</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/</strong></span>
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/packtcdma/</strong></span>
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/packtcdma/alert_by_date/</strong></span>
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/packtcdma/alertlist/</strong></span>
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/packtcdma/quote/</strong></span>
<span class="strong"><strong>ubuntu:~$ mkdir ~/temp/packtcdma/watchlist/</strong></span>
<span class="strong"><strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/alert_by_date/snapshots/1412082842986/* ~/temp/packtcdma/alert_by_date/</strong></span>
<span class="strong"><strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/alertlist/snapshots/1412082842986/* ~/temp/packtcdma/alertlist/</strong></span>
<span class="strong"><strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/quote/snapshots/1412082842986/* ~/temp/packtcdma/quote/</strong></span>
<span class="strong"><strong>ubuntu:~$ sudo cp -p /var/lib/cassandra/data/packtcdma/watchlist/snapshots/1412082842986/* ~/temp/packtcdma/watchlist/</strong></span>
</pre></div></li><li class="listitem">Open cqlsh to connect to <code class="literal">ubtc01</code> and create a keyspace with the appropriate replication strategy in the production cluster:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubuntu:~$ cqlsh ubtc01</strong></span>
<span class="strong"><strong>cqlsh&gt; CREATE KEYSPACE packtcdma WITH replication = {'class': 'NetworkTopologyStrategy',  'NY1': '2'};</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">alert_by_date</code>, <code class="literal">alertlist</code>, <code class="literal">quote</code>, and <code class="literal">watchlist</code> tables:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; CREATE TABLE packtcdma.alert_by_date (</strong></span>
<span class="strong"><strong>  price_time timestamp,</strong></span>
<span class="strong"><strong>  symbol varchar,</strong></span>
<span class="strong"><strong>  signal_price float,</strong></span>
<span class="strong"><strong>  stock_name varchar,</strong></span>
<span class="strong"><strong>  PRIMARY KEY (price_time, symbol));</strong></span>
<span class="strong"><strong>cqlsh&gt; CREATE TABLE packtcdma.alertlist (</strong></span>
<span class="strong"><strong>  symbol varchar,</strong></span>
<span class="strong"><strong>  price_time timestamp,</strong></span>
<span class="strong"><strong>  signal_price float,</strong></span>
<span class="strong"><strong>  stock_name varchar,</strong></span>
<span class="strong"><strong>  PRIMARY KEY (symbol, price_time));</strong></span>
<span class="strong"><strong>cqlsh&gt; CREATE TABLE packtcdma.quote (</strong></span>
<span class="strong"><strong>  symbol varchar,</strong></span>
<span class="strong"><strong>  price_time timestamp,</strong></span>
<span class="strong"><strong>  close_price float,</strong></span>
<span class="strong"><strong>  high_price float,</strong></span>
<span class="strong"><strong>  low_price float,</strong></span>
<span class="strong"><strong>  open_price float,</strong></span>
<span class="strong"><strong>  stock_name varchar,</strong></span>
<span class="strong"><strong>  volume double,</strong></span>
<span class="strong"><strong>  PRIMARY KEY (symbol, price_time));</strong></span>
<span class="strong"><strong>cqlsh&gt; CREATE TABLE packtcdma.watchlist (</strong></span>
<span class="strong"><strong>  watch_list_code varchar,</strong></span>
<span class="strong"><strong>  symbol varchar,</strong></span>
<span class="strong"><strong>  PRIMARY KEY (watch_list_code, symbol));</strong></span>
</pre></div></li><li class="listitem">Load the <a id="id582" class="indexterm"/>SSTables back to the production cluster using the <code class="literal">sstableloader</code> utility:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ubuntu:~$ cd ~/temp</strong></span>
<span class="strong"><strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/alert_by_date</strong></span>
<span class="strong"><strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/alertlist</strong></span>
<span class="strong"><strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/quote</strong></span>
<span class="strong"><strong>ubuntu:~/temp$ sstableloader -d ubtc01 packtcdma/watchlist</strong></span>
</pre></div></li><li class="listitem">Check the legacy data in the production database on <code class="literal">ubtc02</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; select * from packtcdma.alert_by_date;</strong></span>
<span class="strong"><strong>cqlsh&gt; select * from packtcdma.alertlist;</strong></span>
<span class="strong"><strong>cqlsh&gt; select * from packtcdma.quote;</strong></span>
<span class="strong"><strong>cqlsh&gt; select * from packtcdma.watchlist;</strong></span>
</pre></div></li></ol></div><p>Although <a id="id583" class="indexterm"/>the previous steps look complicated, it is not difficult to understand what they want to achieve. It should be noted that we have set the replication factor per data center as <code class="literal">2</code> to provide data redundancy on both nodes, as shown in the <code class="literal">CREATE KEYSPACE</code> statement. The replication factor can be changed in future if needed.</p></div><div class="section" title="Deploying the Stock Screener Application"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec28"/>Deploying the Stock Screener Application</h3></div></div></div><p>As we <a id="id584" class="indexterm"/>have set up the production cluster and moved the legacy data into it, it is time to deploy the Stock Screener Application. The only thing needed to modify is the code to establish Cassandra connection to the production cluster. This is very easy to do with Python. The code in <code class="literal">chapter06_006.py</code> is modified to work with the production cluster as <code class="literal">chapter07_001.py</code>. A new test case named <code class="literal">testcase003()</code> is created to replace <code class="literal">testcase002()</code>. To save pages, the complete source code of <code class="literal">chapter07_001.py</code> is not shown here; only the <code class="literal">testcase003()</code> function is depicted as follows:</p><div class="informalexample"><pre class="programlisting"># -*- coding: utf-8 -*-
# program: chapter07_001.py

# other functions are not shown for brevity

def testcase003():
    ## create Cassandra instance with multiple nodes
    cluster = Cluster(['ubtc01', 'ubtc02'])
    
    ## establish Cassandra connection, using local default
    session = cluster.connect('packtcdma')
    
    start_date = datetime.datetime(2012, 6, 28)
    end_date = datetime.datetime(2013, 9, 28)
    
    ## load the watch list
    stocks_watched = load_watchlist(session, "WS01")
    
    for symbol in stocks_watched:
        ## retrieve data
        data = retrieve_data(session, symbol, start_date, end_date)
        
        ## compute 10-Day SMA
        data = sma(data, 10)
        
        ## generate the buy-and-hold signals
        alerts = signal_close_higher_than_sma10(data)
        
        ## save the alert list
        for index, r in alerts.iterrows():
            insert_alert(session, symbol, index, \
                         Decimal(r['close_price']), \
                         r['stock_name'])
    
    ## close Cassandra connection
    cluster.shutdown()

testcase003()</pre></div><p>The cluster <a id="id585" class="indexterm"/>connection code right at the beginning of the <code class="literal">testcase003()</code> function is passed with an array of the nodes to be connected (<code class="literal">ubtc01</code> and <code class="literal">ubtc02</code>). Here we adopted the default <code class="literal">RoundRobinPolicy</code> as the connection load balancing policy. It is used to decide how to distribute requests among all possible coordinator nodes in the cluster. There are many other options which are described in the driver API documentation.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note41"/>Note</h3><p>
<span class="strong"><strong>Cassandra Driver 2.1 Documentation</strong></span>
</p><p>For the complete <a id="id586" class="indexterm"/>API documentation of the Python Driver 2.1 for Apache Cassandra, you can refer to <a class="ulink" href="http://datastax.github.io/python-driver/api/index.html">http://datastax.github.io/python-driver/api/index.html</a>.</p></div></div></div></div></div></div>
<div class="section" title="Monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Monitoring</h1></div></div></div><p>As the<a id="id587" class="indexterm"/> application system goes live, we need to monitor its health day-by-day. Cassandra provides a number of tools for this purpose. We will introduce some of them with pragmatic recommendations. It is remarkable that each operating system also <a id="id588" class="indexterm"/>provides a bunch of tools and utilities for monitoring, for example, <code class="literal">top</code>, <code class="literal">df</code>, <code class="literal">du</code> on Linux and Task Manager on Windows. However, they are beyond the scope of this book.</p><div class="section" title="Nodetool"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec87"/>Nodetool</h2></div></div></div><p>The <a id="id589" class="indexterm"/>nodetool<a id="id590" class="indexterm"/> utility should not be new to us. It is a command-line interface used to monitor Cassandra and perform routine database operations. It includes the most important metrics for tables, server, and compaction statistics, and other useful commands for administration.</p><p>Here are the most commonly used <code class="literal">nodetool</code> options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">status</code>: This<a id="id591" class="indexterm"/> provides a concise summary of the cluster, such as the state, load, and IDs</li><li class="listitem" style="list-style-type: disc"><code class="literal">netstats</code>: This <a id="id592" class="indexterm"/>gives the network information for a node, focusing on read repair operations</li><li class="listitem" style="list-style-type: disc"><code class="literal">info</code>: This <a id="id593" class="indexterm"/>gives valuable node information including token, on disk load, uptime, Java heap memory usage, key cache, and row cache</li><li class="listitem" style="list-style-type: disc"><code class="literal">tpstats</code>: This <a id="id594" class="indexterm"/>provides statistics about the number of active, pending, and completed tasks for each stage of Cassandra operations by thread pool</li><li class="listitem" style="list-style-type: disc"><code class="literal">cfstats</code>: This <a id="id595" class="indexterm"/>gets the statistics about one or more tables, such as read-and-write counts and latencies, metrics about SSTable, memtable, bloom filter, and compaction.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note42"/>Note</h3><p>A detailed <a id="id596" class="indexterm"/>documentation of nodetool can be referred to at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html</a>.</p></div></div></div><div class="section" title="JMX and MBeans"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec88"/>JMX and MBeans</h2></div></div></div><p>Cassandra <a id="id597" class="indexterm"/>is <a id="id598" class="indexterm"/>written in the Java language and so<a id="id599" class="indexterm"/> it natively supports <span class="strong"><strong>Java Management Extensions</strong></span> (<span class="strong"><strong>JMX</strong></span>). We may use JConsole, a JMX-compliant tool, to<a id="id600" class="indexterm"/> monitor Cassandra.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>
<span class="strong"><strong>JConsole</strong></span>
</p><p>JConsole<a id="id601" class="indexterm"/> is included with Sun JDK 5.0 and higher versions. However, it consumes a significant amount of system resources. It is recommended that you run it on a remote machine rather than on the same host as a Cassandra node.</p></div></div><p>We can <a id="id602" class="indexterm"/>launch JConsole by typing <code class="literal">jconsole</code> in a terminal. Assuming that we want to monitor the local node, when the <span class="strong"><strong>New Connection</strong></span> dialog box pops up, we type <code class="literal">localhost:7199</code> (<code class="literal">7199</code> is the port number of JMX) in the <span class="strong"><strong>Remote Process</strong></span> textbox, as depicted in the following screenshot:</p><div class="mediaobject"><img src="graphics/8884OS_07_04.jpg" alt="JMX and MBeans"/></div><p>After having connected to the local Cassandra instance, we will see a well-organized GUI showing six separate tabs placed horizontally on the top, as seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/8884OS_07_05.jpg" alt="JMX and MBeans"/></div><p>The tabs of the <a id="id603" class="indexterm"/>GUI are explained as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Overview</strong></span>: This displays overview information about the JVM and monitored values</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Memory</strong></span>: This displays information about heap and non-heap memory usage and garbage collection metrics</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Threads</strong></span>: This displays information about thread use</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Classes</strong></span>: This displays information about class loading</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VM Summary</strong></span>: This displays information about the JVM</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>MBeans</strong></span>: This displays information about specific Cassandra metrics and operations</li></ul></div><p>Furthermore, Cassandra provides five MBeans<a id="id604" class="indexterm"/> for JConsole. They are briefly introduced as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.db</code>: This <a id="id605" class="indexterm"/>includes caching, table metrics, and compaction</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.internal</code>: These <a id="id606" class="indexterm"/>are internal server operations such as gossip and hinted handoff</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.metrics</code>: These<a id="id607" class="indexterm"/> are various metrics of the Cassandra instance such as cache and compaction</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.net</code>: This <a id="id608" class="indexterm"/>has Inter-node communication including FailureDetector, MessagingService and StreamingService</li><li class="listitem" style="list-style-type: disc"><code class="literal">org.apache.cassandra.request</code>: These<a id="id609" class="indexterm"/> include tasks related to read, write, and replication operations</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note44"/>Note</h3><p>
<span class="strong"><strong>MBeans</strong></span>
</p><p>An <span class="strong"><strong>Managed Bean</strong></span> (<span class="strong"><strong>MBean</strong></span>) is <a id="id610" class="indexterm"/>a Java object that represents a manageable resource such as an application, a service, a component, or a device running in the JVM. It can be used to collect statistics on concerns such as performance, resource usage, or problems, for getting and setting application configurations or properties, and notifying events like faults or state changes.</p></div></div></div><div class="section" title="The system log"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec89"/>The system log</h2></div></div></div><p>The most <a id="id611" class="indexterm"/>rudimentary,<a id="id612" class="indexterm"/> yet the most powerful, monitoring tool is Cassandra's system log. The default location of the system log is named <code class="literal">system.log</code> under <code class="literal">/var/log/cassandra/</code>. It is simply a text file and can be viewed or edited by any text editor. The following screenshot shows an extract of <code class="literal">system.log</code>:</p><div class="mediaobject"><img src="graphics/8884OS_07_06.jpg" alt="The system log"/></div><p>This piece <a id="id613" class="indexterm"/>of <a id="id614" class="indexterm"/>log looks long and weird. However, if you are a Java developer and you are familiar with the standard log library, Log4j, it is pretty straightforward. The beauty of Log4j is the provision of different log levels for us to control the granularity of the log statements to be recorded in <code class="literal">system.log</code>. As shown in the previous figure, the first word of each line is <code class="literal">INFO</code>, meaning that the log statement is a piece of information. Other log level choices include <code class="literal">FATAL</code>, <code class="literal">ERROR</code>, <code class="literal">WARN</code>, <code class="literal">DEBUG</code>, and <code class="literal">TRACE</code>, from the least verbose to the most verbose.</p><p>The system log is very valuable in troubleshooting problems as well. We may increase the log level to <code class="literal">DEBUG</code> or <code class="literal">TRACE</code> for troubleshooting. However, running a production Cassandra cluster in the <code class="literal">DEBUG</code> or <code class="literal">TRACE</code> mode will degrade its performance significantly. We must use them with great care.</p><p>We can change the standard log level in Cassandra by adjusting the <code class="literal">log4j.rootLogger</code> property in <code class="literal">log4j-server.properties</code> in the Cassandra configuration directory. The following screenshot shows the content of <code class="literal">log4j-server.properties</code> in ubtc02:</p><div class="mediaobject"><img src="graphics/8884OS_07_07.jpg" alt="The system log"/></div><p>It is important <a id="id615" class="indexterm"/>to mention that <code class="literal">system.log</code> and <code class="literal">log4j-server.properties</code> are only responsible for a single node. For a cluster of two nodes, we will<a id="id616" class="indexterm"/> have two <code class="literal">system.log</code> and two <code class="literal">log4j-server.properties</code> on the respective nodes.</p></div></div>
<div class="section" title="Performance tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Performance tuning</h1></div></div></div><p>Performance tuning<a id="id617" class="indexterm"/> is a large and complex topic that in itself can be a whole course. We can only scratch the surface of it in this short section. Similar to monitoring in the last section, operating system-specific performance tuning techniques are beyond the scope of this book.</p><div class="section" title="Java virtual machine"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec90"/>Java virtual machine</h2></div></div></div><p>Based on<a id="id618" class="indexterm"/> the information given by the monitoring tools and <a id="id619" class="indexterm"/>the system log, we can discover opportunities for performance tuning. The first things we usually watch are the Java heap memory and garbage collection. JVM's configuration settings<a id="id620" class="indexterm"/> are controlled in the environment settings file for Cassandra, <code class="literal">cassandra-env.sh</code>, located in <code class="literal">/etc/cassandra/</code>. An example is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/8884OS_07_08.jpg" alt="Java virtual machine"/></div><p>Basically, it already <a id="id621" class="indexterm"/>has the boilerplate options calculated to be optimized for the host system. It is also accompanied with explanation for us to tweak specific JVM parameters and the startup options of a Cassandra instance when we experience real issues; otherwise, these boilerplate options should not be altered.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>A detailed documentation on <a id="id622" class="indexterm"/>how to tune JVM for Cassandra can be found at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html</a>.</p></div></div></div><div class="section" title="Caching"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec91"/>Caching</h2></div></div></div><p>Another <a id="id623" class="indexterm"/>area <a id="id624" class="indexterm"/>we should pay attention to is caching. Cassandra includes integrated caching and distributes cache data around the cluster. For a cache specific to a table, we will focus on the partition key cache and the row cache.</p><div class="section" title="Partition key cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec29"/>Partition key cache</h3></div></div></div><p>The <a id="id625" class="indexterm"/>partition key cache, or <a id="id626" class="indexterm"/>key cache for short, is a cache of the partition index for a table. Using the key cache saves processor time and memory. However, enabling just the key cache makes the disk activity actually read the requested data rows.</p></div><div class="section" title="Row cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec30"/>Row cache</h3></div></div></div><p>The <a id="id627" class="indexterm"/>row cache<a id="id628" class="indexterm"/> is similar to a traditional cache. When a row is accessed, the entire row is pulled into memory, merging from multiple SSTables when required, and cached. This prevents Cassandra from retrieving that row using disk I/O again, which can tremendously improve read performance.</p><p>When both row cache and partition key cache are configured, the row cache returns results whenever possible. In the event of a row cache miss, the partition key cache might still provide a hit that makes the disk seek much more efficient.</p><p>However, there is one caveat. Cassandra caches all the rows of a partition when reading that partition. So if the partition is large or only a small portion of the partition is read every time, the row cache might not be beneficial. It is very easy to be misused and consequently the JVM will be exhausted, causing Cassandra to fail. That is why the row cache is disabled by default.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note46"/>Note</h3><p>We usually enable either the key or row cache for a table, not both at the same time.</p></div></div></div><div class="section" title="Monitoring cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Monitoring cache</h3></div></div></div><p>Either <a id="id629" class="indexterm"/>the <code class="literal">nodetool info</code> <a id="id630" class="indexterm"/>command or JMX MBeans can provide assistance in monitoring cache. We should make changes to cache options in small, incremental adjustments, and then monitor the effects of each change using the nodetool utility. The last two lines of output of the <code class="literal">nodetool info</code> command, as seen in the following figure, contain the <code class="literal">Row Cache</code> and <code class="literal">Key Cache</code> metrics of <code class="literal">ubtc02</code>:</p><div class="mediaobject"><img src="graphics/8884OS_07_09.jpg" alt="Monitoring cache"/></div><p>In the<a id="id631" class="indexterm"/> event <a id="id632" class="indexterm"/>of high memory consumption, we can consider tuning data caches.</p></div><div class="section" title="Enabling/disabling cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec32"/>Enabling/disabling cache</h3></div></div></div><p>We <a id="id633" class="indexterm"/>use <a id="id634" class="indexterm"/>the<a id="id635" class="indexterm"/> CQL to enable or disable caching by altering the cache property of a table. For instance, we use the <code class="literal">ALTER TABLE</code> statement to enable the row cache for <code class="literal">watchlist</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ALTER TABLE watchlist WITH caching=''ROWS_ONLY'';</strong></span>
</pre></div><p>Other available table<a id="id636" class="indexterm"/> caching options include <code class="literal">ALL</code>, <code class="literal">KEYS_ONLY</code> and <code class="literal">NONE</code>. They are quite self-explanatory and we do not go through each of them here.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>Further information <a id="id637" class="indexterm"/>about data caching can be found at <a class="ulink" href="http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html</a>.</p></div></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Summary</h1></div></div></div><p>This chapter highlights the most important aspects of deploying a Cassandra cluster into the production environment. Cassandra can be taught to understand the physical location of the nodes in the cluster in order to intelligently manage its availability, scalability and performance. We deployed the Stock Screener Application to the production environment, though the scale is small. It is also valuable for us to learn how to migrate legacy data from a non-production environment.</p><p>We then learned the basics of monitoring and performance tuning which are a must for a live running system. If you have experience in deploying other database and system, you may well appreciate the neatness and simplicity of Cassandra.</p><p>In the next chapter, we will have a look at the supplementary information pertinent to application design and development. We will also summarize of the essence of each chapter.</p></div></body></html>