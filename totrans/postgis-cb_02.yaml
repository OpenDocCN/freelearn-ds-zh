- en: Structures That Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Using geospatial views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using triggers to populate the geometry column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structuring spatial data with table inheritance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending inheritance – table partitioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing imports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing internal overlays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using polygon overlays for proportional census estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter focuses on ways to structure data using the functionality provided
    by the combination of PostgreSQL and PostGIS. These will be useful approaches
    for structuring and cleaning up imported data, converting tabular data into spatial
    data *on the fly* when it is entered, and maintaining relationships between tables
    and datasets using functionality endemic to the powerful combination of PostgreSQL
    and PostGIS. There are three categories of techniques with which we will leverage
    these functionalities: automatic population and modification of data using views
    and triggers, object orientation using PostgreSQL table inheritance, and using
    PostGIS functions (stored procedures) to reconstruct and normalize problematic
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic population of data is where the chapter begins. By leveraging PostgreSQL
    views and triggers, we can create ad hoc and flexible solutions to create connections
    between and within the tables. By extension, and for more formal or structured
    cases, PostgreSQL provides table inheritance and table partitioning, which allow
    for explicit hierarchical relationships between tables. This can be useful in
    cases where an object inheritance model enforces data relationships that either
    represent the data better, thereby resulting in greater efficiencies, or reduce
    the administrative overhead of maintaining and accessing the datasets over time.
    With PostGIS extending that functionality, the inheritance can apply not just
    to the commonly used table attributes, but to leveraging spatial relationships
    between tables, resulting in greater query efficiency with very large datasets.
    Finally, we will explore PostGIS SQL patterns that provide table normalization
    of data inputs, so datasets that come from flat filesystems or are not normalized
    can be converted to a form we would expect in a database.
  prefs: []
  type: TYPE_NORMAL
- en: Using geospatial views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Views in PostgreSQL allow the ad hoc representation of data and data relationships
    in alternate forms. In this recipe, we'll be using views to allow for the automatic
    creation of point data based on tabular inputs. We can imagine a case where the
    input stream of data is non-spatial, but includes longitude and latitude or some
    other coordinates. We would like to automatically show this data as points in
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create a view as a representation of spatial data pretty easily. The
    syntax for creating a view is similar to creating a table, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command line, our `SELECT` query manipulates the data for us.
    Let's start with a small dataset. In this case, we will start with some random
    points, which could be real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the table from which the view will be constructed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s populate this with the data for testing using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, to create the view, we will use the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our view is really a simple transformation of the existing data using PostGIS's
    `ST_MakePoint` function. The `ST_MakePoint` function takes the input of two numbers
    to create a PostGIS point, and in this case our view simply uses our *x* and *y*
    values to populate the data. Any time there is an update to the table to add a
    new record with *x* and *y* values, the view will populate a point, which is really
    useful for data that is constantly being updated.
  prefs: []
  type: TYPE_NORMAL
- en: There are two disadvantages to this approach. The first is that we have not
    declared our spatial reference system in the view, so any software consuming these
    points will not know the coordinate system we are using, that is, whether it is
    a geographic (latitude/longitude) or a planar coordinate system. We will address
    this problem shortly. The second problem is that many software systems accessing
    these points may not automatically detect and use the spatial information from
    the table. This problem is addressed in the *Using triggers to populate the geometry
    column* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The **spatial reference system identifier** (**SRID**) allows us to specify
    the coordinate system for a given dataset. The numbering system is a simple integer
    value to specify a given coordinate system. SRIDs are derived originally from
    the **European Petroleum Survey Group** (**EPSG**) and are now maintained by the
    Surveying and Positioning Committee of the International Association of **Oil
    and Gas Producers** (**OGP**). Useful tools for SRIDs are spatial reference ([http://spatialreference.org](http://spatialreference.org))
    and Prj2EPSG ([http://prj2epsg.org/search](http://prj2epsg.org/search)).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the first problem mentioned in the *How it works...* section, we
    can simply wrap our existing `ST_MakePoint` function in another function specifying
    the SRID as `ST_SetSRID`, as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using triggers to populate the geometry column* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using triggers to populate the geometry column
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we imagine that we have ever increasing data in our database,
    which needs spatial representation; however, in this case we want a hardcoded
    geometry column to be updated each time an insertion happens on the database,
    converting our *x* and *y* values to geometry as and when they are inserted into
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this approach is that the geometry is then registered in the
    `geometry_columns` view, and therefore this approach works reliably with more
    PostGIS client types than creating a new geospatial view. This also provides the
    advantage of allowing for a spatial index that can significantly speed up a variety
    of queries.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by creating another table of random points with `x`, `y`, and
    `z` values, as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need a geometry column to populate. By default, the geometry column
    will be populated with null values. We populate a geometry column using the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We now have a column called `geom` with an SRID of `3734`; that is, a point
    geometry type in two dimensions. Since we have `x`, `y`, and `z` data, we could,
    in principle, populate a 3D point table using a similar approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since all the geometry values are currently null, we will populate them using
    an `UPDATE` statement as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The query here is simple when broken down. We update the `xwhyzed1` table and
    set the `the_geom` column using `ST_MakePoint`, construct our point using the
    `x` and `y` columns, and wrap it in an `ST_SetSRID` function in order to apply
    the appropriate spatial reference information. So far, we have just set the table
    up. Now, we need to create a trigger in order to continue to populate this information
    once the table is in use. The first part of the trigger is a new populated geometry
    function using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In essence, we have created a function that does exactly what we did manually:
    update the table''s geometry column with the combination of `ST_SetSRID` and `ST_MakePoint`,
    but only to the new registers being inserted, and not to all the table.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we have a function created, we have not yet applied it as a trigger to
    the table. Let us do that here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that the general geometry column update has not taken place yet,
    then the original five registers still have their geometry column in `null`. Now,
    once the trigger has been activated, any inserts into our table should be populated
    with new geometry records. Let us do a test insert using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the rows to verify that the `geom` columns are updated with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Or use `pgAdmin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/228a3c05-cbc1-4e11-b229-dcf0370c8e88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After applying the general update, then all the registers will have a value
    on their `geom` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36e423c6-31fa-4611-9106-9608a6658827.png)'
  prefs: []
  type: TYPE_IMG
- en: Extending further...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've implemented an `insert` trigger. What if the value changes for
    a particular row? In that case, we will require a separate update trigger. We'll
    change our original function to test the `UPDATE` case, and we'll use `WHEN` in
    our trigger to constrain updates to the column being changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that the following function is written with the assumption that
    the user wants to always update the changing geometries based on the changing
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using geospatial views* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structuring spatial data with table inheritance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An unusual and useful property of the PostgreSQL database is that it allows
    for object inheritance models as they apply to tables. This means that we can
    have parent/child relationships between tables and leverage that to structure
    the data in meaningful ways. In our example, we will apply this to hydrology data.
    This data can be points, lines, polygons, or more complex structures, but they
    have one commonality: they are explicitly linked in a physical sense and inherently
    related; they are all about water. Water/hydrology is an excellent natural system
    to model this way, as our ways of modeling it spatially can be quite mixed depending
    on scales, details, the data collection process, and a host of other factors.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data we will be using is hydrology data that has been modified from engineering
    *blue lines* (see the following screenshot), that is, hydrologic data that is
    very detailed and is meant to be used at scales approaching 1:600\. The data in
    its original application aided, as breaklines, in detailed digital terrain modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dee607cd-b195-44a2-b68f-d00ab45c69bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While useful in itself, the data was further manipulated, separating the linear
    features from area features, with additional polygonization of the area features,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbba571c-8f5e-451b-93e2-382ea0d45cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the data was classified into basic waterway categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98c7b472-407a-4618-bde6-668416f084c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, a process was undertaken to generate centerlines for polygon features
    such as streams, which are effectively linear features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7a60213-43b3-4386-970e-da5f209e1949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we have three separate but related datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cuyahoga_hydro_polygon`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cuyahoga_hydro_polyline`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cuyahoga_river_centerlines`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let us look at the structure of the tabular data. Unzip the hydrology
    file from the book repository and go to that directory. The `ogrinfo` utility
    can help us with this, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e90c2a9b-ba18-42dd-9d26-677f877d92ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Executing this query on each of the shapefiles, we see the following fields
    that are common to all the shapefiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hyd_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`geom_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is by understanding our common fields that we can apply inheritance to completely
    structure our data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know our common fields, creating an inheritance model is easy.
    First, we will create a parent table with the fields common to all the tables,
    using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you are paying attention, you will note that we also added a `geometry` field
    as all of our shapefiles implicitly have this commonality. With inheritance, every
    record inserted in any of the child tables will also be saved in our parent table,
    only these records will be stored without the extra fields specified for the child
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To establish inheritance for a given table, we need to declare only the additional
    fields that the child table contains using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to load our data using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_hydro_polygon chp02.hydrology_polygon
    | psql -U me -d postgis_cookbook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_hydro_polyline chp02.hydrology_linestring
    | psql -U me -d postgis_cookbook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_river_centerlines
    chp02.hydrology_centerlines | psql -U me -d postgis_cookbook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we view our parent table, we will see all the records in all the child tables.
    The following is a screenshot of fields in `hydrology`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec505dd1-c9c9-430a-8fa1-0b2fa16302f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compare that to the fields available in `hydrology_linestring` that will reveal
    specific fields of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ac3acbf-428c-406c-a158-e1ab36fb935c.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PostgreSQL table inheritance allows us to enforce essentially hierarchical
    relationships between tables. In this case, we leverage inheritance to allow for
    commonality between related datasets. Now, if we want to query data from these
    tables, we can query directly from the parent table as follows, depending on whether
    we want a mix of geometries or just a targeted dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From any of the child tables, we could use the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to extend this concept in order to leverage and optimize storage
    and querying by using the `CHECK` constrains in conjunction with inheritance.
    For more info, see the *Extending inheritance – table partitioning* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Extending inheritance – table partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Table partitioning is an approach specific to PostgreSQL that extends inheritance
    to model tables that typically do not vary from each other in the available fields,
    but where the child tables represent logical partitioning of the data based on
    a variety of factors, be it time, value ranges, classifications, or in our case,
    spatial relationships. The advantages of partitioning include improved query performance
    due to smaller indexes and targeted scans of data, bulk loads, and deletes that
    bypass the costs of vacuuming. It can thus be used to put commonly used data on
    faster and more expensive storage, and the remaining data on slower and cheaper
    storage. In combination with PostGIS, we get the novel power of spatial partitioning,
    which is a really powerful feature for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could use many examples of large datasets that could benefit from partitioning.
    In our case, we will use a contour dataset. Contours are useful ways to represent
    terrain data, as they are well established and thus commonly interpreted. Contours
    can also be used to compress terrain data into linear representations, thus allowing
    it to be shown in conjunction with other data easily.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is, the storage of contour data can be quite expensive. Two-foot
    contours for a single US county can take 20 to 40 GB, and storing such data for
    a larger area such as a region or nation can become quite prohibitive from the
    standpoint of accessing the appropriate portion of the dataset in a performant
    way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in this case may be to prepare the data. If we had a monolithic
    contour table called `cuy_contours_2`, we could choose to clip the data to a series
    of rectangles that will serve as our table partitions; in this case, `chp02.contour_clip`,
    using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We are performing two tests here in our query. We are using `ST_Within`, which
    tests whether a given contour is entirely within our area of interest. If so,
    we perform an intersection; the resultant geometry should just be the geometry
    of the contour.
  prefs: []
  type: TYPE_NORMAL
- en: The `ST_Crosses` function checks whether the contour crosses the boundary of
    the geometry we are testing. This should capture all the geometries lying partially
    inside and partially outside our areas. These are the ones that we will truly
    intersect to get the resultant shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, it is easier and we don''t require this step. Our contour shapes
    are already individual shapefiles clipped to rectangular boundaries, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe831299-188c-4bc0-96e4-eb3047a474a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the data is already clipped into the chunks needed for our partitions,
    we can just continue to create the appropriate partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like with inheritance, we start by creating our parent table using the
    following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here again, we maintain our constraints, such as `PRIMARY KEY,` and specify
    the geometry type (`MultiLineStringZM`), not because these will propagate to the
    child tables, but for any client software accessing the parent table to anticipate
    such constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we may begin to create tables that inherit from our parent table. In the
    process, we will create a `CHECK` constraint specifying the limits of our associated
    geometry using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can complete the table structure for partitioning the contours with similar
    `CREATE TABLE` queries for our remaining tables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can load our contours shapefiles found in the `contours1` ZIP file
    into each of our child tables, using the following command, by replacing the filename.
    If we wanted to, we could even implement a trigger on the parent table, which
    would place each insert into its correct child table, though this might incur
    performance costs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `CHECK` constraint in combination with inheritance is all it takes to build
    a table partitioning. In this case, we're using a bounding box as our `CHECK`
    constraint and simply inheriting the columns from the parent table. Now that we
    have this in place, queries against the parent table will check our `CHECK` constraints
    first before employing a query.
  prefs: []
  type: TYPE_NORMAL
- en: This also allows us to place any of our lesser-used contour tables on cheaper
    and slower storage, thus allowing for cost-effective optimizations of large datasets.
    This structure is also beneficial for rapidly changing data, as updates can be
    applied to an entire area; the entire table for that area can be efficiently dropped
    and repopulated without traversing across the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more on table inheritance in general, particularly the flexibility associated
    with the usage of alternate columns in the child table, see the previous recipe,
    *Structuring spatial data with table inheritance*.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing imports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, data used in a spatial database is imported from other sources. As such,
    it may not be in a form that is useful for our current application. In such a
    case, it may be useful to write functions that will aid in transforming the data
    into a form that is more useful for our application. This is particularly the
    case when going from flat file formats, such as shapefiles, to relational databases
    such as PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: A shapefile is a de facto as well as a format specification for the storage
    of spatial data, and is probably the most common delivery format for spatial data.
    A shapefile, in spite of its name, is never just one file, but a collection of
    files. It consists of at least `*.shp` (which contains geometry), `*.shx` (an
    index file), and `*.dbf` (which contains the tabular information for the shapefile).
    It is a powerful and useful format, but as a flat file, it is inherently non-relational.
    Each geometry is associated in a one-to-one relationship with each row in a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many structures that might serve as a proxy for relational stores
    in a shapefile. We will explore one here: a single field with delimited text for
    multiple relations. This is a not-too-uncommon hack to encode multiple relationships
    into a flat file. The other common approach is to create multiple fields to store
    what in a relational arrangement would be a single field.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we will be working with is a trails dataset that has linear extents
    for a set of trails in a park system. The data is the typical data that comes
    from the GIS world; as a flat shapefile, there are no explicit relational constructs
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, unzip the `trails.zip` file and use the command line to go into it,
    then load the data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the linear data, we have some categories for the use type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e73e89c3-d887-45c7-85dd-14d2b2d8092a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to retain this information as well as the name. Unfortunately, the
    `label_name` field is a messy field with a variety of related names concatenated
    with an ampersand (`&`), as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/740e10f2-cda8-4acc-8e49-6b3b54802548.png)'
  prefs: []
  type: TYPE_IMG
- en: This is where the normalization of our table will begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we need to do is find all the fields that don't have ampersands
    and use those as our unique list of available trails. In our case, we can do this,
    as every trail has at least one segment that is uniquely named and not associated
    with another trail name. This approach will not work with all datasets, so be
    careful in understanding your data before applying this approach to that data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To select the fields ordered without ampersands, we use the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45c659e3-b010-4b13-9124-daaec6ca0ca6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we want to search for all the records that match any of these unique
    trail names. This will give us the list of records that will serve as relations.
    The first step in doing this search is to append the percent (`%`) signs to our
    unique list in order to build a string on which we can search using a `LIKE` query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll use this in the context of a `WITH` block to do the normalization
    itself. This will provide us with a table of unique IDs for each segment in our
    first column, along with the associated `label` column. For good measure, we will
    do this as a `CREATE TABLE` procedure, as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we view the first rows of the table created, `trails_names`, we have the
    following output with `pgAdmin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61a39308-ad2c-4f0a-8fd1-887f98d6b16e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have a table of the relations, we need a table of the geometries
    associated with `gid`. This, in comparison, is quite easy, as shown in the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we have generated a unique list of possible records in conjunction
    with a search for the associated records, in order to build table relationships.
    In one table, we have the geometry and a unique ID of each spatial record; in
    another table, we have the names associated with each of those unique IDs. Now
    we can explicitly leverage those relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to establish our unique IDs as primary keys, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use that `PRIMARY KEY` as a `FOREIGN KEY` in our `trails_names`
    table using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This step isn''t strictly necessary, but does enforce referential integrity
    for queries such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1196c509-656a-4907-a288-ca9b161ca303.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we had multiple fields we wanted to normalize, we could write `CREATE TABLE`
    queries for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to note that the approach framed in this recipe is not limited
    to cases where we have a delimited field. This approach can provide a relatively
    generic solution to the problem of normalizing flat files. For example, if we
    have a case where we have multiple fields to represent relational info, such as
    `label1`, `label2`, `label3`, or similar multiple attribute names for a single
    record, we can write a simple query to concatenate them together before feeding
    that info into our query.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing internal overlays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data from an external source can have issues in the table structure as well
    as in the topology, endemic to the geospatial data itself. Take, for example,
    the problem of data with overlapping polygons. If our dataset has polygons that
    overlap with internal overlays, then queries for area, perimeter, and other metrics
    may not produce predictable or consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few approaches that can solve the problem of polygon datasets with
    internal overlays. The general approach presented here was originally proposed
    by *Kevin Neufeld* of *Refractions Research*.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of writing our query, we will also produce a solution for converting
    polygons to linestrings.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, unzip the `use_area.zip` file and go into it using the command line;
    then, load the dataset using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the data is loaded into a table in the database, we can leverage PostGIS
    to flatten and get the union of the polygons, so that we have a normalized dataset.
    The first step in doing so using this approach will be to convert the *polygons*
    to *linestrings*. We can then link those *linestrings* and convert them back to
    *polygons*, representing the union of all the *polygon* inputs. We will perform
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert polygons to linestrings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert linestrings back to polygons
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the center points of the resultant polygons
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the resultant points to query tabular relationships
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To convert polygons to linestrings, we'll need to extract just the portions
    of the polygons we want using `ST_ExteriorRing`, convert those parts to points
    using `ST_DumpPoints`, and then connect those points back into lines like a connect-the-dots
    coloring book using `ST_MakeLine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking it down further, `ST_ExteriorRing (the_geom)` will grab just the outer
    boundary of our polygons. But `ST_ExteriorRing` returns polygons, so we need to
    take that output and create a line from it. The easiest way to do this is to convert
    it to points using `ST_DumpPoints` and then connect those points. By default,
    the `Dump` function returns an object called a `geometry_dump`, which is not just
    simple geometry, but the geometry in combination with an array of integers. The
    easiest way to return the geometry alone is the leverage object notation to extract
    just the geometry portion of `geometry_dump,` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Piecing the geometry back together with `ST_ExteriorRing` is done using the
    following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us a listing of points in order from the exterior rings of
    all the points from which we want to construct our lines using `ST_MakeLine`,
    as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the preceding approach is a process we may want to use in many other
    places, it might be prudent to create a function from this using the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the `polygon_to_line` function, we still need to force the
    linking of overlapping lines in our particular case. The `ST_Union` function will
    aid in this, as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s convert linestrings back to polygons, and for this we can polygonize
    the result using `ST_Polygonize`, as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ST_Polygonize` function will create a single multi polygon, so we need
    to explode this into multiple single polygon geometries if we are to do anything
    useful with it. While we are at it, we might as well do the following within a
    `CREATE TABLE` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be performing spatial queries against this geometry, so we should create
    an index in order to ensure our query performs well, as shown in the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to find the appropriate table information from the original geometry
    and apply that back to our resultant geometries, we will perform a point-in-polygon
    query. For that, we first need to calculate centroids on the resultant geometry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And as always, create a spatial index using the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The centroids then structure our point-in-polygon (`ST_Intersects`) relationship
    between the original tabular information and resultant polygons, using the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If we view the first rows of the table, we can see it links the identifier
    of points to their respective locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f23cff08-dac5-435e-a138-95f9e215cc74.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our essential approach here is to look at the underlying topology of the geometry
    and reconstruct a topology that is non-overlapping, and then use the centroids
    of that new geometry to construct a query that establishes the relationship to
    the original data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this stage, we can optionally establish a framework for referential integrity
    using a foreign key, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Using polygon overlays for proportional census estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PostgreSQL functions abound for the aggregation of tabular data, including `sum`,
    `count`, `min`, `max`, and so on. PostGIS as a framework does not explicitly have
    spatial equivalents of these, but this does not prevent us from building functions
    using the aggregate functions from PostgreSQL in concert with PostGIS's spatial
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will explore spatial summarization with the United States
    census data. The US census data, by nature, is aggregated data. This is done intentionally
    to protect the privacy of citizens. But when it comes to doing analyses with this
    data, the aggregate nature of the data can become problematic. There are some
    tricks to disaggregate data. Amongst the simplest of these is the use of a proportional
    sum, which we will do in this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem at hand is that a proposed trail has been drawn in order to provide
    services for the public. This example could apply to road construction or even
    finding sites for commercial properties for the purpose of provisioning services.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, unzip the `trail_census.zip` file, then perform a quick data load using
    the following commands from the unzipped folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands will produce the following outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e1a9e32-06fd-496d-9623-aa47bb59958a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we view the proposed trail in our favorite desktop GIS, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ce64b3-b125-4afe-b247-6d99e788dabd.png)'
  prefs: []
  type: TYPE_IMG
- en: In our case, we want to know the population within 1 mile of the trail, assuming
    that persons living within 1 mile of the trail are the ones most likely to use
    it, and thus most likely to be served by it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out the population near this proposed trail, we overlay census block
    group population density information. Illustrated in the next screenshot is a
    1-mile buffer around the proposed trail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0554efef-2029-4dc8-a325-a1a3f4a1b898.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the things we might note about this census data is the wide range of
    census densities and census block group sizes. An approach to calculating the
    population would be to simply select all census blocks that intersect our area,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3452784-b8b6-4916-a4e4-996f34b1f12f.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a simple procedure that gives us an estimate of 130 to 288 people living
    within 1 mile of the trail, but looking at the shape of the selection, we can
    see that we are overestimating the population by taking the complete blocks in
    our estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we just used the block groups whose centroids lay within 1 mile
    of our proposed trail alignment, we would underestimate the population.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we will make some useful assumptions. Block groups are designed to
    be moderately homogeneous within the block group population distribution. Assuming
    that this holds true for our data, we can assume that for a given block group,
    if 50% of the block group is within our target area, we can attribute half of
    the population of that block group to our estimate. Apply this to all our block
    groups, sum them, and we have a refined estimate that is likely to be better than
    pure intersects or centroid queries. Thus, we employ a proportional sum.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the problem of a proportional sum is a generic problem, it could apply to
    many problems. We will write the underlying proportioning as a function. A function
    takes inputs and returns a value. In our case, we want our proportioning function
    to take two geometries, that is, the geometry of our buffered trail and block
    groups as well as the value we want proportioned, and we want it to return the
    proportioned value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now, for the purpose of our calculation, for any given intersection of buffered
    area and block group, we want to find the proportion that the intersection is
    over the overall block group. Then this value should be multiplied by the value
    we want to scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SQL, the function looks like the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding query in its full form looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we have written the query as a function, the query uses the `SELECT`
    statement to loop through all available records and give us a proportioned population.
    Astute readers will note that we have not yet done any work on summarization;
    we have only worked on the proportionality portion of the problem. We can do the
    summarization upon calling the function using PostgreSQL''s built-in aggregate
    functions. What is neat about this approach is that we need not just apply a sum,
    but we could also calculate other aggregates such as min or max. In the following
    example, we will just apply a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The value returned is quite different (a population of 96,081), which is more
    likely to be accurate.
  prefs: []
  type: TYPE_NORMAL
