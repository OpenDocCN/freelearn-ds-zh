<html><head></head><body><div><div><h1 class="header-title">Getting Started with PyCUDA</h1>
                
            
            
                
<p>In the last chapter, we set up our programming environment. Now, with our drivers and compilers firmly in place, we will begin the actual GPU programming! We will start by learning how to use PyCUDA for some basic and fundamental operations. We will first see how to query our GPU—that is, we will start by writing a small Python program that will tell us what the characteristics of our GPU are, such as the core count, architecture, and memory. We will then spend some time getting acquainted with how to transfer memory between Python and the GPU with PyCUDA's <kbd>gpuarray</kbd> class and how to use this class for basic computations. The remainder of this chapter will be spent showing how to write some basic functions (which we will refer to as <strong>CUDA Kernels</strong>) that we can directly launch onto the GPU.</p>
<p>The learning outcomes for this chapter are as follows:</p>
<ul>
<li>Determining GPU characteristics, such as memory capacity or core count, using PyCUDA</li>
<li>Understanding the difference between host (CPU) and device (GPU) memory and how to use PyCUDA's <kbd>gpuarray</kbd> class to transfer data between the host and device</li>
<li>How to do basic calculations using only <kbd>gpuarray</kbd> objects</li>
<li>How to perform basic element-wise operations on the GPU with the PyCUDA <kbd>ElementwiseKernel</kbd> function</li>
<li>Understanding the functional programming concept of reduce/scan operations and how to make a basic reduction or scan CUDA kernel</li>
</ul>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward) installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with the PyCUDA module is also required.</p>
<p>This chapter's code is also available on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.<a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA"/></p>
<p>For more information about the prerequisites, check the <em>Preface</em> of this book; for the software and hardware requirements, check the <kbd>README</kbd> section in <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.<a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA"/></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Querying your GPU</h1>
                
            
            
                
<p>Before we begin to program our GPU, we should really know something about its technical capacities and limits. We can determine this by doing what is known as a <strong>GPU query</strong>. A GPU query is a very basic operation that will tell us the specific technical details of our GPU, such as available GPU memory and core count. NVIDIA includes a command-line example written in pure CUDA-C called <kbd>deviceQuery</kbd> in the <kbd>samples</kbd> directory (for both Windows and Linux) that we can run to perform this operation. Let's take a look at the output that is produced on the author's Windows 10 laptop (which is a Microsoft Surface Book 2 with a GTX 1050 GPU):</p>
<p class="mce-root"/>
<div><img src="img/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png" width="1811" height="1626"/></div>
<p>Let's look at some of the essentials of all of the technical information displayed here. First, we see that there is only one GPU installed, Device 0—it is possible that a host computer has multiple GPUs and makes use of them, so CUDA will designate each <em>GPU device</em> an individual number. There are some cases where we may have to be specific about the device number, so it is always good to know. We can also see the specific type of device that we have (here, GTX 1050), and which CUDA version we are using. There are two more things we will take note of for now: the total number of cores (here, 640), and the total amount of global memory on the device (in this case, 2,048 megabytes, that is, 2 gigabytes). </p>
<p class="mce-root"/>
<p>While you can see many other technical details from <kbd>deviceQuery</kbd>, the core count and amount of memory are usually the first two things your eyes should zero in on the first time you run this on a new GPU, since they can give you the most immediate idea of the capacity of your new device.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Querying your GPU with PyCUDA</h1>
                
            
            
                
<p>Now, finally, we will begin our foray into the world of GPU programming by writing our own version of <kbd>deviceQuery</kbd> in Python. Here, we will primarily concern ourselves with only the amount of available memory on the device, the compute capability, the number of multiprocessors, and the total number of CUDA cores.</p>
<p>We will begin by initializing CUDA as follows:</p>
<pre class="mce-root">import pycuda.driver as drv<br/>drv.init()</pre>
<p>Note that we will always have to initialize PyCUDA with <kbd>pycuda.driver.init()</kbd> or by importing the PyCUDA <kbd>autoinit</kbd> submodule with <kbd>import pycuda.autoinit</kbd>!</p>
<p>We can now immediately check how many GPU devices we have on our host computer with this line:</p>
<pre>print 'Detected {} CUDA Capable device(s)'.format(drv.Device.count())</pre>
<p>Let's type this into IPython and see what happens:</p>
<div><img src="img/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png" width="1337" height="233"/></div>
<p>Great! So far, I have verified that my laptop does indeed have one GPU in it. Now, let's extract some more interesting information about this GPU (and any other GPU on the system) by adding a few more lines of code to iterate over each device that can be individually accessed with <kbd>pycuda.driver.Device</kbd> (indexed by number). The name of the device (for example, GeForce GTX 1050) is given by the <kbd>name</kbd> function. We then get the <strong>compute capability</strong> of the device with the <kbd>compute_capability</kbd> function and total amount of device memory with the <kbd>total_memory</kbd> function. </p>
<div><strong>Compute capability</strong> can be thought of as a <em>version number</em> for each NVIDIA GPU architecture; this will give us some important information about the device that we can't otherwise query, as we will see in a minute.</div>
<p>Here's how we will write it:</p>
<pre>for i in range(drv.Device.count()):<br/>    <br/>     gpu_device = drv.Device(i)<br/>     print 'Device {}: {}'.format( i, gpu_device.name() )<br/>     compute_capability = float( '%d.%d' % gpu_device.compute_capability() )<br/>     print '\t Compute Capability: {}'.format(compute_capability)<br/>     print '\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2))</pre>
<p>Now, we are ready to look at some of the remaining attributes of our GPU, which PyCUDA yields to us in the form of a Python dictionary type. We will use the following lines to convert this into a dictionary that is indexed by strings indicating attributes:</p>
<pre>    device_attributes_tuples = gpu_device.get_attributes().iteritems()<br/>     device_attributes = {}<br/>    <br/>     for k, v in device_attributes_tuples:<br/>         device_attributes[str(k)] = v</pre>
<p>We can now determine the number of <em>multiprocessors</em> on our device with the following:</p>
<pre>    num_mp = device_attributes['MULTIPROCESSOR_COUNT']</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>A GPU divides its individual cores up into larger units known as <strong>Streaming</strong> <strong>Multiprocessors (SMs)</strong>; a GPU device will have several SMs, which will each individually have a particular number of CUDA cores, depending on the compute capability of the device. To be clear: the number of cores per multiprocessor is not indicated directly by the GPU—this is given to us implicitly by the compute capability. We will have to look up some technical documents from NVIDIA to determine the number of cores per multiprocessor (see <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities</a>), and then create a lookup table to give us the number of cores per multiprocessor. We do so as such, using the <kbd>compute_capability</kbd> variable to look up the number of cores:</p>
<pre>    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 6.2 : 128}[compute_capability]</pre>
<p>We can now finally determine the total number of cores on our device by multiplying these two numbers:</p>
<pre>    print '\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp)</pre>
<p>We now can finish up our program by iterating over the remaining keys in our dictionary and printing the corresponding values:</p>
<pre>    device_attributes.pop('MULTIPROCESSOR_COUNT')<br/>    <br/>     for k in device_attributes.keys():<br/>         print '\t {}: {}'.format(k, device_attributes[k])</pre>
<p>So, now we finally completed our first true GPU program of the text! (Also available at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py</a>). Now, we can run it as follows: </p>
<p class="mce-root"/>
<div><img src="img/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png" width="1356" height="1040"/></div>
<p>We can now have a little pride that we can indeed write a program to query our GPU! Now, let's actually begin to learn to <em>use</em> our GPU, rather than just observe it.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using PyCUDA's gpuarray class</h1>
                
            
            
                
<p>Much like how NumPy's <kbd>array</kbd> class is the cornerstone of numerical programming within the NumPy environment, PyCUDA's <kbd>gpuarray</kbd> class plays an analogously prominent role within GPU programming in Python. This has all of the features you know and love from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing, array unraveling, and overloaded operators for point-wise computations (for example, <kbd>+</kbd>, <kbd>-</kbd>, <kbd>*</kbd>, <kbd>/</kbd>, and <kbd>**</kbd>).</p>
<p class="mce-root"/>
<p><kbd>gpuarray</kbd> is really an indispensable tool for any budding GPU programmer. We will spend this section going over this particular data structure and gaining a strong grasp of it before we move on.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Transferring data to and from the GPU with gpuarray</h1>
                
            
            
                
<p class="mce-root">As we note from writing our prior <kbd>deviceQuery</kbd> program in Python, a GPU has its own memory apart from the host computer's memory, which is known as <strong>device memory</strong>. (Sometimes this is known more specifically as <strong>global device memory</strong><em>,</em> to differentiate this from the additional cache memory, shared memory, and register memory that is also on the GPU.) For the most part, we treat (global) device memory on the GPU as we do dynamically allocated heap memory in C (with the <kbd>malloc</kbd> and <kbd>free</kbd> functions) or C++ (as with the <kbd>new</kbd> and <kbd>delete</kbd> operators); in CUDA C, this is complicated further with the additional task of transferring data back and forth between the CPU to the GPU (with commands such as <kbd>cudaMemcpyHostToDevice</kbd> and <kbd>cudaMemcpyDeviceToHost</kbd>), all while keeping track of multiple pointers in both the CPU and GPU space and performing proper memory allocations (<kbd>cudaMalloc</kbd>) and deallocations (<kbd>cudaFree</kbd>).</p>
<p>Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation, and data transfers with the <kbd>gpuarray</kbd> class. As stated, this class acts similarly to NumPy arrays, using vector/ matrix/tensor shape structure information for the data. <kbd>gpuarray</kbd> objects even perform automatic cleanup based on the lifetime, so we do not have to worry about <em>freeing</em> any GPU memory stored in a <kbd>gpuarray</kbd> object when we are done with it. </p>
<p>How exactly do we use this to transfer data from the host to the GPU? First, we must contain our host data in some form of NumPy array (let's call it <kbd>host_data</kbd>), and then use the <kbd>gpuarray.to_gpu(host_data)</kbd> command to transfer this over to the GPU and create a new GPU array. </p>
<p class="mce-root"/>
<p>Let's now perform a simple computation within the GPU (pointwise multiplication by a constant on the GPU), and then retrieve the GPU data into a new with the <kbd>gpuarray.get</kbd> function. Let's load up IPython and see how this works (note that here we will initialize PyCUDA with <kbd>import pycuda.autoinit</kbd>):</p>
<div><img src="img/14eef3e5-273f-45c9-b42f-99d07628f9d8.png" style="" width="1014" height="657"/></div>
<p>One thing to note is that we specifically denoted that the array on the host had its type specifically set to a NumPy <kbd>float32</kbd> type with the <kbd>dtype</kbd> option when we set up our NumPy array; this corresponds directly with the float type in C/C++. Generally speaking, it's a good idea to specifically set data types with NumPy when we are sending data to the GPU. The reason for this is twofold: first, since we are using a GPU for increasing the performance of our application, we don't want any unnecessary overhead of using an unnecessary type that will possibly take up more computational time or memory, and second, since we will soon be writing portions of code in inline CUDA C, we will have to be very specific with types or our code won't work correctly, keeping in mind that C is a statically-typed language.</p>
<p class="mce-root"/>
<p>Remember to specifically set data types for NumPy arrays that will be transferred to the GPU. This can be done with the <kbd>dtype</kbd> option in the constructor of the <kbd>numpy.array</kbd> class.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Basic pointwise arithmetic operations with gpuarray</h1>
                
            
            
                
<p>In the last example, we saw that we can use the (overloaded) Python multiplication operator (<kbd>*</kbd> ) to multiply each element in a <kbd>gpuarray</kbd> object by a scalar value (here it was 2); note that a pointwise operation is intrinsically parallelizable, and so when we use this operation on a <kbd>gpuarray</kbd> object PyCUDA is able to offload each multiplication operation onto a single thread, rather than computing each multiplication in serial, one after the other (in fairness, some versions of NumPy can use the advanced SSE instructions found in modern x86 chips for these computations, so in some cases the performance will be comparable to a GPU). To be clear: these pointwise operations performed on the GPU are in parallel since the computation of one element is not dependent on the computation of any other element. </p>
<p>To get a feel for how the operators work, I would suggest that the reader load up IPython and create a few <kbd>gpuarray</kbd> objects on the GPU, and then play around with these operations for a few minutes to see that these operators do work similarly to arrays in NumPy. Here is some inspiration:</p>
<div><img src="img/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png" style="" width="916" height="1743"/></div>
<p>Now, we can see that <kbd>gpuarray</kbd> objects act predictably and are in accordance with how NumPy arrays act. (Notice that we will have to pull the output off the GPU with the <kbd>get</kbd> function!) Let's now do some comparison between CPU and GPU computation time to see if and when there is any advantage to doing these operations on the GPU.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">A speed test</h1>
                
            
            
                
<p>Let's write up a little program (<kbd>time_calc0.py</kbd>) that will do a speed comparison test between a scalar multiplication on the CPU and then the same operation on the GPU. We will then use NumPy's <kbd>allclose</kbd> function to compare the two output values. We will generate an array of 50 million random 32-bit floating point values (this will amount to roughly 48 megabytes of data, so this should be entirely feasible with several gigabytes of memory on any somewhat modern host and GPU device), and then we will time how long it takes to scalar multiply the array by two on both devices. Finally, we will compare the output values to ensure that they are equal. Here's how it's done:</p>
<pre>import numpy as np<br/>import pycuda.autoinit<br/>from pycuda import gpuarray<br/>from time import time<br/>host_data = np.float32( np.random.random(50000000) )<br/><br/>t1 = time()<br/>host_data_2x =  host_data * np.float32(2)<br/>t2 = time()<br/><br/>print 'total time to compute on CPU: %f' % (t2 - t1)<br/>device_data = gpuarray.to_gpu(host_data)<br/><br/>t1 = time()<br/>device_data_2x =  device_data * np.float32( 2 )<br/>t2 = time()<br/><br/>from_device = device_data_2x.get()<br/>print 'total time to compute on GPU: %f' % (t2 - t1)<br/><br/>print 'Is the host computation the same as the GPU computation? : {}'.format(np.allclose(from_device, host_data_2x) )</pre>
<p>(You can find the <kbd>time_calc0.py</kbd> file on the repository provided to you earlier.)</p>
<p>Now, let's load up IPython and run this a few times to get an idea of the general speed of these, and see if there is any variance. (Here, this is being run on a 2017-era Microsoft Surface Book 2 with a Kaby Lake i7 processor and a GTX 1050 GPU.):</p>
<div><img src="img/a278a2f1-e099-4907-805f-708f2884a7c3.png" style="" width="1078" height="866"/></div>
<p>We first notice that the CPU computation time is about the same for each computation (roughly 0.08 seconds). Yet, we notice that the GPU computation time is far slower than the CPU computation the first time we run this (1.09 seconds), and it becomes much faster in the subsequent run, which remains roughly constant in every following run (in the range of 7 or 9 milliseconds). If you exit IPython, and then run the program again, the same thing will occur. What is the reason for this phenomenon? Well, let's do some investigative work using IPython's built-in <kbd>prun</kbd> profiler. (This works similarly to the <kbd>cProfiler</kbd> module that was featured in <a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml">Chapter 1</a>, <em>Why GPU Programming?</em>.)</p>
<p>First, let's load our program as text within IPython with the following lines, which we can then run with our profiler via Python's <kbd>exec</kbd> command:</p>
<pre>with open('time_calc0.py','r') as f:<br/>     time_calc_code = f.read()</pre>
<p>We now type <kbd>%prun -s cumulative exec(time_calc_code)</kbd> into our IPython console (with the leading <kbd>%</kbd>) and see what operations are taking the most time:</p>
<div><img src="img/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png" width="1444" height="1050"/></div>
<p>Now, there are a number of suspicious calls to a Python module file, <kbd>compiler.py</kbd>; these take roughly one second total, a little less than the time it takes to do the GPU computation here. Now let's run this again and see if there are any differences:</p>
<div><img src="img/da5995b8-f05d-45d7-950c-f921d79b3886.png" width="1439" height="794"/></div>
<p>Notice that this time, there are no calls to <kbd>compiler.py</kbd>. Why is this? By the nature of the PyCUDA library, GPU code is often compiled and linked with NVIDIA's <kbd>nvcc</kbd> compiler the first time it is run in a given Python session; it is then cached and, if the code is called again, then it doesn't have to be recompiled. This may include even <em>simple</em> operations such as this scalar multiply! (We will see eventually see that this can be ameliorated by using the pre-compiled code in, <a href="5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml">Chapter 10</a>, <em>Working with Compiled GPU Code</em>, or by using NVIDIA's own linear algebra libraries with the Scikit-CUDA module, which we will see in <a href="55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml">Chapter 7</a>, <em>Using the CUDA Libraries with Scikit-CUDA</em>).</p>
<p>In PyCUDA, GPU code is often compiled at runtime with the NVIDIA <kbd>nvcc</kbd> compiler and then subsequently called from PyCUDA. This can lead to an unexpected slowdown, usually the first time a program or GPU operation is run in a given Python session.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using PyCUDA's ElementWiseKernel for performing pointwise computations</h1>
                
            
            
                
<p>We will now see how to program our own point-wise (or equivalently, <em>element-wise</em>) operations directly onto our GPU with the help of PyCUDA's <kbd>ElementWiseKernel</kbd> function. This is where our prior knowledge of C/C++ programming will become useful—we'll have to write a little bit of <em>inline code</em> in CUDA C, which is compiled externally by NVIDIA's <kbd>nvcc</kbd> compiler and then launched at runtime by our code via PyCUDA.</p>
<p>We use the term <strong>kernel</strong> quite a bit in this text; by <em>kernel</em>, we always mean a function that is launched directly onto the GPU by CUDA. We will use several functions from PyCUDA that generate templates and design patterns for different types of kernels, easing our transition into GPU programming.</p>
<p>Let's dive right in; we're going to start by explicitly rewriting the code to multiply each element of a <kbd>gpuarray</kbd> object by 2 in CUDA-C; we will use the <kbd>ElementwiseKernel</kbd> function from PyCUDA to generate our code. You should try typing the following code directly into an IPython console. (The less adventurous can just download this from this text's Git repository, which has the filename <kbd>simple_element_kernel_example0.py</kbd>): </p>
<pre>import numpy as np<br/>import pycuda.autoinit<br/>from pycuda import gpuarray<br/>from time import time<br/>from pycuda.elementwise import ElementwiseKernel<br/>host_data = np.float32( np.random.random(50000000) )<br/>gpu_2x_ker = ElementwiseKernel(<br/>"float *in, float *out",<br/>"out[i] = 2*in[i];",<br/>"gpu_2x_ker")</pre>
<p>Let's take a look at how this is set up; this is, of course, several lines of inline C. We first set the input and output variables in the first line ( <kbd>"float *in, float *out"</kbd> ), which will generally be in the form of C pointers to allocated memory on the GPU. In the second line, we define our element-wise operation with <kbd>"out[i] = 2*in[i];"</kbd>, which will multiply each point in <kbd>in</kbd> by two and place this in the corresponding index of <kbd>out</kbd>.</p>
<p class="mce-root"/>
<p>Note that PyCUDA automatically sets up the integer index <kbd>i</kbd> for us. When we use <kbd>i</kbd> as our index, <kbd>ElementwiseKernel</kbd> will automatically parallelize our calculation over <kbd>i</kbd> among the many cores in our GPU. Finally, we give our piece of code its internal CUDA C kernel name ( <kbd>"gpu_2x_ker"</kbd> ). Since this refers to CUDA C's namespace and not Python's, it's fine (and also convenient) to give this the same name as in Python.</p>
<p>Now, let's do a speed comparison:</p>
<pre>def speedcomparison():<br/>    t1 = time()<br/>    host_data_2x =  host_data * np.float32(2)<br/>    t2 = time()<br/>    print 'total time to compute on CPU: %f' % (t2 - t1)<br/>    device_data = gpuarray.to_gpu(host_data)<br/>    # allocate memory for output<br/>    device_data_2x = gpuarray.empty_like(device_data)<br/>    t1 = time()<br/>    gpu_2x_ker(device_data, device_data_2x)<br/>    t2 = time()<br/>    from_device = device_data_2x.get()<br/>    print 'total time to compute on GPU: %f' % (t2 - t1)<br/>    print 'Is the host computation the same as the GPU computation? : {}'.format(np.allclose(from_device, host_data_2x) )<br/><br/>if __name__ == '__main__':<br/>    speedcomparison()</pre>
<p>Now, let's run this program:</p>
<div><img src="img/02db7f9f-e682-41fb-af4c-2833d054a746.png" style="" width="1194" height="181"/></div>
<p>Whoa! That doesn't look good. Let's run the <kbd>speedcomparison()</kbd> function a few times from IPython:</p>
<div><img src="img/9514e819-e7cd-42c3-b1af-d5ea832c6864.png" style="" width="1083" height="945"/></div>
<p>As we can see, the speed increases dramatically after the first time we use a given GPU function. Again, as with the prior example, this is because PyCUDA compiles our inline CUDA C code the first time a given GPU kernel function is called using the <kbd>nvcc</kbd> compiler. After the code is compiled, then it is cached and re-used for the remainder of a given Python session.</p>
<p class="mce-root"/>
<p>Now, let's cover something else important before we move on, which is very subtle. The little kernel function we defined operates on C float pointers; this means that we will have to allocate some empty memory on the GPU that is pointed to by the <kbd>out</kbd> variable. Take a look at this portion of code again from the <kbd>speedcomparison()</kbd> function:</p>
<pre>device_data = gpuarray.to_gpu(host_data)<br/># allocate memory for output<br/>device_data_2x = gpuarray.empty_like(device_data)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As we did before, we send a NumPy array over to the GPU (<kbd>host_data</kbd>) via the <kbd>gpuarray.to_gpu</kbd> function, which automatically allocates data onto the GPU and copies it over from the CPU space. We will plug this into the <kbd>in</kbd> part of our kernel function. In the next line, we allocate empty memory on the GPU with the <kbd>gpuarray.empty_like</kbd> function. This acts as a plain <kbd>malloc</kbd> in C, allocating an array of the same size and data type as <kbd>device_data</kbd>, but without copying anything. We can now use this for the <kbd>out</kbd> part of our kernel function. We now look at the next line in <kbd>speedcomparison()</kbd> to see how to launch our kernel function onto the GPU (ignoring the lines we use for timing):</p>
<pre>gpu_2x_ker(device_data, device_data_2x)</pre>
<p>Again, the variables we set correspond directly to the first line we defined with <kbd>ElementwiseKernel</kbd> (here being, <kbd>"float *in, float *out"</kbd>).</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Mandelbrot revisited</h1>
                
            
            
                
<p class="mce-root">Let's again look at the problem of generating the Mandelbrot set from <a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml">Chapter 1</a>,<a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml"/> <em>Why GPU Programming?</em>. The original code is available under the <kbd>1</kbd> folder in the repository, with the filename <kbd>mandelbrot0.py</kbd>, which you should take another look at before we continue. We saw that there were two main components of this program: the first being the generation of the Mandelbrot set, and the second concerning dumping the Mandelbrot set into a PNG file. In the first chapter, we realized that we could parallelize only the generation of the Mandelbrot set, and considering that this takes the bulk of the time for the program to do, this would be a good candidate for an algorithm to offload this onto a GPU. Let's figure out how to do this. (We will refrain from re-iterating over the definition of the Mandelbrot set, so if you need a deeper review, please re-read the <em>Mandelbrot</em> <em>revisited</em> section of <a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml">Chapter 1</a>, <em>Why GPU Programming?</em>)</p>
<p>First, let's make a new Python function based on <kbd>simple_mandelbrot</kbd> from the original program. We'll call it <kbd>gpu_mandelbrot</kbd>, and this will take in the same exact input as before:</p>
<pre>def gpu_mandelbrot(width, height, real_low, real_high, imag_low, imag_high, max_iters, upper_bound):</pre>
<p>We will proceed a little differently from here. We will start by building a complex lattice that consists of each point in the complex plane that we will analyze. </p>
<p>Here, we'll use some tricks with the NumPy matrix type to easily generate the lattice, and then typecast the result from a NumPy <kbd>matrix</kbd> type to a two-dimensional NumPy <kbd>array</kbd> (since PyCUDA can only handle NumPy <kbd>array</kbd> types, not <kbd>matrix</kbd> types). Notice how we are very carefully setting our NumPy types:</p>
<pre>    real_vals = np.matrix(np.linspace(real_low, real_high, width), dtype=np.complex64)<br/>    imag_vals = np.matrix(np.linspace( imag_high, imag_low, height), dtype=np.complex64) * 1j<br/>    mandelbrot_lattice = np.array(real_vals + imag_vals.transpose(), dtype=np.complex64)  </pre>
<p>So, we now have a two-dimensional complex array that represents the lattice from which we will generate our Mandelbrot set; as we will see, we can operate on this very easily within the GPU. Let's now transfer our lattice to the GPU and allocate an array that we will use to represent our Mandelbrot set:</p>
<pre>    # copy complex lattice to the GPU<br/>    mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)    <br/>    # allocate an empty array on the GPU<br/>    mandelbrot_graph_gpu = gpuarray.empty(shape=mandelbrot_lattice.shape, dtype=np.float32)</pre>
<p>To reiterate—the <kbd>gpuarray.to_array</kbd> function only can operate on NumPy <kbd>array</kbd> types, so we were sure to have type-cast this beforehand before we sent it to the GPU. Next, we have to allocate some memory on the GPU with the <kbd>gpuarray.empty</kbd> function, specifying the size/shape of the array and the type. Again, you can think of this as acting similarly to <kbd>malloc</kbd> in C; remember that we won't have to deallocate or <kbd>free</kbd> this memory later, due to the <kbd>gpuarray</kbd> object destructor taking care of memory clean-up automatically when the end of the scope is reached.</p>
<p>When you allocate memory on the GPU with the PyCUDA functions <kbd>gpuarray.empty</kbd> or <kbd>gpuarray.empty_like</kbd>, you do not have to deallocate this memory later due to the destructor of the <kbd>gpuarray </kbd>object managing all memory clean up.</p>
<p class="mce-root"/>
<p class="mceNonEditable">We're now ready to launch the kernel; the only change we have to make is to change the</p>
<p class="mce-root"/>
<p>We haven't written our kernel function yet to generate the Mandelbrot set, but let's just write how we want the rest of this function to go:</p>
<pre>    mandel_ker( mandelbrot_lattice_gpu, mandelbrot_graph_gpu, np.int32(max_iters), np.float32(upper_bound))<br/>              <br/>    mandelbrot_graph = mandelbrot_graph_gpu.get()<br/>    <br/>    return mandelbrot_graph</pre>
<p>So this is how we want our new kernel to act—the first input will be the complex lattice of points (NumPy <kbd>complex64</kbd> type) we generated, the second will be a pointer to a two-dimensional floating point array (NumPy <kbd>float32</kbd> type) that will indicate which elements are members of the Mandelbrot set, the third will be an integer indicating the maximum number of iterations for each point, and the final input will be the upper bound for each point used for determining membership in the Mandelbrot class. Notice that we are <em>very</em> careful in typecasting everything that goes into the GPU!</p>
<p>The next line retrieves the Mandelbrot set we generated from the GPU back into CPU space, and the end value is returned. (Notice that the input and output of <kbd>gpu_mandelbrot</kbd> is exactly the same as that of <kbd>simple_mandelbrot</kbd>).</p>
<p>Let's now look at how to properly define our GPU kernel. First, let's add the appropriate <kbd>include</kbd> statements to the header:</p>
<pre>import pycuda.autoinit<br/>from pycuda import gpuarray<br/>from pycuda.elementwise import ElementwiseKernel</pre>
<p>We are now ready to write our GPU kernel! We'll show it here and then go over this line-by-line:</p>
<pre>mandel_ker = ElementwiseKernel(<br/>"pycuda::complex&lt;float&gt; *lattice, float *mandelbrot_graph, int max_iters, float upper_bound",<br/>"""<br/>mandelbrot_graph[i] = 1;<br/>pycuda::complex&lt;float&gt; c = lattice[i]; <br/>pycuda::complex&lt;float&gt; z(0,0);<br/>for (int j = 0; j &lt; max_iters; j++)<br/>    {  <br/>     z = z*z + c;     <br/>     if(abs(z) &gt; upper_bound)<br/>         {<br/>          mandelbrot_graph[i] = 0;<br/>          break;<br/>         }<br/>    }         <br/>""",<br/>"mandel_ker")</pre>
<p>First, we set our input with the first string passed to <kbd>ElementwiseKernel</kbd>. We have to realize that when we are working in CUDA-C, particular C datatypes will correspond directly to particular Python NumPy datatypes. Again, note that when arrays are passed into a CUDA kernel, they are seen as C pointers by CUDA. Here, a CUDA C <kbd>int</kbd> type corresponds exactly to a NumPy <kbd>int32</kbd> type, while a CUDA C <kbd>float</kbd> type corresponds to a NumPy <kbd>float32</kbd> type. An internal PyCUDA class template is then used for complex types—here PyCUDA <kbd>::complex&lt;float&gt;</kbd> corresponds to Numpy <kbd>complex64</kbd>.</p>
<p>Let's look at the content of the second string, which is deliminated with three quotes (<kbd>"""</kbd>). This allows us to use multiple lines within the string; we will use this when we write larger inline CUDA kernels in Python. </p>
<p>While the arrays we have passed in are two-dimensional arrays in Python, CUDA will only see these as being one-dimensional and indexed by <kbd>i</kbd>. Again, <kbd>ElementwiseKernel</kbd> indexes <kbd>i</kbd> across multiple cores and threads for us automatically. We initialize each point in the output to one with <kbd>mandelbrot_graph[i] = 1;</kbd>, as <kbd>i</kbd> will be indexed over every single element of our Mandelbrot set; we're going to assume that every point will be a member unless proven otherwise. (Again, the Mandelbrot set is over two dimensions, real and complex, but <kbd>ElementwiseKernel</kbd> will automatically translate everything into a one-dimensional set. When we interact with the data again in Python, the two-dimensional structure of the Mandelbrot set will be preserved.)</p>
<p>We set up our <kbd>c</kbd> value as in Python to the appropriate lattice point with <kbd>pycuda::complex&lt;float&gt; c = lattice[i];</kbd> and initialize our <kbd>z</kbd> value to <kbd>0</kbd> with <kbd>pycuda::complex&lt;float&gt; z(0,0);</kbd> (the first zero corresponds to the real part, while the second corresponds to the imaginary part). We then perform a loop over a new iterator, <kbd>j</kbd>, with <kbd>for(int j = 0; j &lt; max_iters; j++)</kbd>. (Note that this algorithm will not be parallelized over <kbd>j</kbd> or any other index—only <kbd>i</kbd>! This <kbd>for</kbd> loop will run serially over <kbd>j</kbd>—but the entire piece of code will be parallelized across <kbd>i</kbd>.)</p>
<p>We then set the new value of <kbd><em>z</em></kbd> with <kbd>z = z*z + c;</kbd> as per the Mandelbrot algorithm. If the absolute value of this element exceeds the upper bound ( <kbd>if(abs(z) &gt; upper_bound)</kbd> ), we set this point to 0 ( <kbd>mandelbrot_graph[i] = 0;</kbd> ) and break out of the loop with the <kbd>break</kbd> keyword. </p>
<p>In the final string passed into <kbd>ElementwiseKernel</kbd> we give the kernel its internal CUDA C name, here <kbd>"mandel_ker"</kbd>.</p>
<p>We're now ready to launch the kernel; the only change we have to make is to change the reference from <kbd>simple_mandelbrot</kbd> in the main function to <kbd>gpu_mandelbrot</kbd>, and we're ready to go. Let's launch this from IPython:</p>
<div><img src="img/f00d5080-4975-4023-9f14-397a8e007ac4.png" style="" width="1108" height="111"/></div>
<p>Let's check the dumped image to make sure this is correct:</p>
<div><img src="img/6fa6851a-bcce-46d0-a63d-8023766da21a.png" style="" width="437" height="421"/></div>
<p>This is certainly the same Mandelbrot image that is produced in the first chapter, so we have successfully implemented this onto a GPU! Let's now look at the speed increase we're getting: in the first chapter, it took us 14.61 seconds to produce this graph; here, it only took 0.894 seconds. Keep in mind that PyCUDA also has to compile and link our CUDA C code at runtime, and the time it takes to make the memory transfers to and from the GPU. Still, even with all of that extra overhead, it is a very worthwhile speed increase! (You can view the code for our GPU Mandelbrot with the file named <kbd>gpu_mandelbrot0.py</kbd> in the Git repository.)</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">A brief foray into functional programming</h1>
                
            
            
                
<p>Before we continue, let's briefly do a review of two functions available in Python for <strong>functional programming</strong><em>—</em><kbd>map</kbd> and <kbd>reduce</kbd>. These are both considered to be <em>functional</em> because they both act on <em>functions</em> for their operation. We find these interesting because these both correspond to common design patterns in programming, so we can swap out different functions in the input to get a multitude of different (and useful) operations.</p>
<p>Let's first recall the <kbd>lambda</kbd> keyword in Python. This allows us to define an <strong>anonymous function</strong>—in most cases, these can be thought of as a <kbd>throwaway</kbd> function that we may only wish to use once, or functions that are able to be defined on a single line. Let's open up IPython right now and define a little function that squares a number as such—<kbd>pow2 = lambda x : x**2</kbd>. Let's test it out on a few numbers:</p>
<div><img src="img/f7154a51-4486-4292-9ed0-89415e526394.png" style="" width="513" height="374"/></div>
<p>Let's recall that <kbd>map</kbd> acts on two input values: a function and a <kbd>list</kbd> of objects that the given function can act on. <kbd>map</kbd> outputs a list of the function's output for each element in the original list. Let's now define our squaring operation as an anonymous function which we input into map, and a list of the last few numbers we checked with the following—<kbd>map(lambda x : x**2, [2,3,4])</kbd>:</p>
<div><img src="img/c41f370e-9cba-40ba-a5df-e84857044437.png" style="" width="633" height="76"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>We see that <kbd>map</kbd> acts as <kbd>ElementwiseKernel</kbd>! This is actually a standard design pattern in functional programming. Now, let's look at <kbd>reduce</kbd>; rather than taking in a list and outputting a directly corresponding list, reduce takes in a list, performs a recursive binary operation on it, and outputs a singleton. Let's get a notion of this design pattern by typing <kbd>reduce(lambda x, y : x + y, [1,2,3,4])</kbd>. When we type this in IPython, we will see that this will output a single number, 10, which is indeed the sum of <em>1+2+3+4</em>. You can try replacing the summation above with multiplication, and seeing that this indeed works for recursively multiplying a long list of numbers together. Generally speaking, we use reduce operations with <em>associative binary operations</em>; this means that, no matter the order we perform our operation between sequential elements of the list, will always invariably give the same result, provided that the list is kept in order. (This is not to be confused with the <em>commutative property</em>.)</p>
<p>We will now see how PyCUDA handles programming patterns akin to <kbd>reduce</kbd>—with <strong>parallel scan</strong> and <strong>reduction kernels</strong>.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Parallel scan and reduction kernel basics</h1>
                
            
            
                
<p>Let's look at a basic function in PyCUDA that reproduces the functionality of reduce—<kbd>InclusiveScanKernel</kbd>. (You can find the code under the <kbd>simple_scankernal0.py</kbd> filename.) Let's execute a basic example that sums a small list of numbers on the GPU:</p>
<pre>import numpy as np<br/>import pycuda.autoinit<br/>from pycuda import gpuarray<br/>from pycuda.scan import InclusiveScanKernel<br/>seq = np.array([1,2,3,4],dtype=np.int32)<br/>seq_gpu = gpuarray.to_gpu(seq)<br/>sum_gpu = InclusiveScanKernel(np.int32, "a+b")<br/>print sum_gpu(seq_gpu).get()<br/>print np.cumsum(seq)</pre>
<p>We construct our kernel by first specifying the input/output type (here, NumPy <kbd>int32</kbd>) and in the string, <kbd>"a+b"</kbd>. Here, <kbd>InclusiveScanKernel</kbd> sets up elements named <kbd>a</kbd> and <kbd>b</kbd> in the GPU space automatically, so you can think of this string input as being analogous to <kbd>lambda a,b: a + b</kbd> in Python. We can really put any (associative) binary operation here, provided we remember to write it in C.</p>
<p class="mce-root"/>
<p>When we run <kbd>sum_gpu</kbd>, we see that we will get an array of the same size as the input array. Each element in the array represents the value for each step in the calculation (the NumPy <kbd>cumsum</kbd> function gives the same output, as we can see). The last element will be the final output that we are seeking, which corresponds to the output of reduce:</p>
<div><img src="img/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png" style="" width="563" height="105"/></div>
<p>Let's try something a little more challenging; let's find the maximum value in a <kbd>float32</kbd> array:</p>
<pre>import numpy as np<br/>import pycuda.autoinit<br/>from pycuda import gpuarray<br/>from pycuda.scan import InclusiveScanKernel<br/>seq = np.array([1,100,-3,-10000, 4, 10000, 66, 14, 21],dtype=np.int32)<br/>seq_gpu = gpuarray.to_gpu(seq)<br/>max_gpu = InclusiveScanKernel(np.int32, "a &gt; b ? a : b")<br/>print max_gpu(seq_gpu).get()[-1]<br/>print np.max(seq)</pre>
<p>(You can find the complete code in the file named <kbd>simple_scankernal1.py</kbd>.)</p>
<p>Here, the main change we made is to replace the <kbd>a + b</kbd> string with <kbd>a &gt; b ? a : b</kbd>. (In Python, this would be rendered within a <kbd>reduce</kbd> statement as <kbd>lambda a, b:  max(a,b)</kbd>). Here, we are using a trick to give the max among <kbd>a</kbd> and <kbd>b</kbd> with the C language's <kbd>?</kbd> operator. We finally display the last value of the resulting element in the output array, which will be exactly the last element (which we can always retrieve with the <kbd>[-1]</kbd> index in Python).</p>
<p>Now, let's finally look one more PyCUDA function for generating GPU kernels—<kbd>ReductionKernel</kbd>. Effectively, <kbd>ReductionKernel</kbd> acts like a <kbd>ElementwiseKernel</kbd> function followed by a parallel scan kernel. What algorithm is a good candidate for implementing with a <kbd>ReductionKernel</kbd>? The first that tends to come to mind is the dot product from linear algebra. Let's remember computing the dot product of two vectors has two steps:</p>
<ol>
<li>Multiply the vectors pointwise</li>
<li>Sum the resulting pointwise multiples</li>
</ol>
<p class="mce-root"/>
<p>These two steps are also called <em>multiply and accumulate</em>. Let's set up a kernel to do this computation now:</p>
<pre>dot_prod = ReductionKernel(np.float32, neutral="0", reduce_expr="a+b", map_expr="vec1[i]*vec2[i]", arguments="float *vec1, float *vec2")<br/></pre>
<p>First, note the datatype we use for our kernel (a <kbd>float32</kbd>). We then set up the input arguments to our CUDA C kernel with <kbd>arguments</kbd>, (here two float arrays representing each vector designated with <kbd>float *</kbd>) and set the pointwise calculation with <kbd>map_expr</kbd>, here it is pointwise multiplication. As with <kbd>ElementwiseKernel</kbd>, this is indexed over <kbd>i</kbd>. We set up <kbd>reduce_expr</kbd> the same as with <kbd>InclusiveScanKernel</kbd>. This will take the resulting output from the element-wise operation and perform a reduce-type operation on the array. Finally, we set the <em>neutral element</em> with neutral. This is an element that will act as an identity for <kbd>reduce_expr</kbd>; here, we set <kbd>neutral=0</kbd>, because <kbd>0</kbd> is always the identity under addition (under multiplication, one is the identity). We'll see why exactly we have to set this up when we cover parallel prefix in greater depth later in this book.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>We first saw how to query our GPU from PyCUDA, and with this re-create the CUDA <kbd>deviceQuery</kbd> program in Python. We then learned how to transfer NumPy arrays to and from the GPU's memory with the PyCUDA <kbd>gpuarray</kbd> class and its <kbd>to_gpu</kbd> and <kbd>get</kbd> functions. We got a feel for using <kbd>gpuarray</kbd> objects by observing how to use them to do basic calculations on the GPU, and we learned to do a little investigative work using IPython's <kbd>prun</kbd> profiler. We saw there is sometimes some arbitrary slowdown when running GPU functions from PyCUDA for the first time in a session, due to PyCUDA launching NVIDIA's <kbd>nvcc</kbd> compiler to compile inline CUDA C code. We then saw how to use the <kbd>ElementwiseKernel</kbd> function to compile and launch element-wise operations, which are automatically parallelized onto the GPU from Python. We did a brief review of functional programming in Python (in particular the <kbd>map</kbd> and <kbd>reduce</kbd> functions), and finally, we covered how to do some basic reduce/scan-type computations on the GPU using the <kbd>InclusiveScanKernel</kbd> and <kbd>ReductionKernel</kbd> functions.</p>
<p>Now that we have the absolute basics down about writing and launching kernel functions, we should realize that PyCUDA has covered the vast amount of the overhead in writing a kernel for us with its templates. We will spend the next chapter learning about the principles of CUDA kernel execution, and how CUDA arranges concurrent threads in a kernel into abstract <strong>grids</strong> and <strong>blocks</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>
<p>In <kbd>simple_element_kernel_example0.py</kbd>, we don't consider the memory transfers to and from the GPU in measuring the time for the GPU computation. Try measuring the time that the <kbd>gpuarray</kbd> functions, <kbd>to_gpu</kbd> and <kbd>get</kbd>, take with the Python time command. Would you say it's worth offloading this particular function onto the GPU, with the memory transfer times in consideration?</p>
</li>
<li>
<p>In <a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml" target="_blank">Chapter 1</a>, <em>Why GPU Programming?</em>, we had a discussion of Amdahl's Law, which gives us some idea of the gains we can potentially get by offloading portions of a program onto a GPU. Name two issues that we have seen in this chapter that Amdahl's law does not take into consideration.</p>
</li>
<li>
<p>Modify <kbd>gpu_mandel0.py</kbd> to use smaller and smaller lattices of complex numbers, and compare this to the same lattices CPU version of the program. Can we choose a small enough lattice such that the CPU version is actually faster than the GPU version?</p>
</li>
<li>
<p>Create a kernel with <kbd>ReductionKernel</kbd> that takes two <kbd>complex64</kbd> arrays on the GPU of the same length and returns the absolute largest element among both arrays.</p>
</li>
<li>
<p>What happens if a <kbd>gpuarray</kbd> object reaches end-of-scope in Python?</p>
</li>
<li>
<p>Why do you think we need to define <kbd>neutral</kbd> when we use <kbd>ReductionKernel</kbd>?</p>
</li>
<li>
<p>If in <kbd>ReductionKernel</kbd> we set <kbd>reduce_expr ="a &gt; b ? a : b"</kbd>, and we are operating on int32 types, then what should we set "<kbd>neutral</kbd>" to?</p>
</li>
</ol>


            

            
        
    </div></div></body></html>