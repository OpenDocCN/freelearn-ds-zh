- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning for Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll consider **machine learning** (**ML**) models typically
    used on relational data and their applications within network science. While many
    network-specific tools provide good insights into network structure and prediction
    of spread across a network, ML tools allow us to leverage additional information
    about individuals in the network to construct a more complete view of relationships,
    spreading processes, and key outcomes related to the network or its individuals.
    We’ll consider friendship networks and metadata associated with individuals and
    their connections to other individuals to explore ML on networks.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll first return to network construction based on shared activities and traits
    of individuals, move on to clustering based on both network and metadata features,
    and finally predict individual and friendship network outcomes based on networks
    and their metadata. You’ll learn how to combine network metrics with metadata
    and how to build several types of ML models using network data, upon which we
    will build in the remaining chapters of this book. Let’s dive into some friendship
    networks and their metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to friendship networks and friendship relational datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML on networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SDL on networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for the practical examples presented in this chapter can be found
    here: [https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python](https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to friendship networks and friendship relational datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll consider a friendship network based on student behavior
    factors to form a network. We’ll then apply **unsupervised learning** (**UL**)
    methods, namely clustering, to group individuals into friendship groups to compare
    performance before and after adding extra network structural information.
  prefs: []
  type: TYPE_NORMAL
- en: Friendship network introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider a group of classmates in a small school with enrollment based
    on age and geography, as is common in the United States. Classmates may participate
    in the same extracurricular activities, such as sports teams, the school paper,
    or a concert band. They may also study together, share meals, or get together
    to hang out on weekends. Some may form a core group of friends who take some of
    the same classes, participate in the same extracurriculars, study together, and
    hang out together outside of school-related activities. Strong social ties such
    as these often form an integral source of social support and lasting social relationships.
    These tend to be very important to individual life decisions and outcomes, particularly
    in adolescence and early adulthood, where peers play an important role in psychosocial
    development.
  prefs: []
  type: TYPE_NORMAL
- en: Other groups of friends may only study together or play on the same team, with
    few other shared interests or interactions. Weak social ties such as these also
    play an important role in society, connecting individuals with a wide range of
    resources across a community and exposing young people to a wider variety of viewpoints
    and new ideas. Social change often comes from weak ties across diverse communities,
    such as playing on the same sports teams, sharing classes in school, and participating
    in religious activities. While weak social ties often don’t provide strong social
    support, they serve a bridging function within networks and can introduce individuals
    to others who will become strong social ties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our first friendship network, we’ll consider both weak and strong social
    ties. Strong social ties mainly occur within a group of seven friends who mostly
    play on the same team, share some classes, study together, and play sports before
    school and at weekends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – An illustration of a group of boys playing basketball before
    school](img/B21087_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – An illustration of a group of boys playing basketball before school
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.1* shows three of the strong-social-tie boys playing basketball
    before school. We’d expect ideas, behaviors, and communicable diseases to spread
    quickly through this part of the network, as this group spends most of its time
    together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, weak social ties within the network consist of occasional interactions
    that might include core courses or one shared interest that brings individuals
    together for short periods of time, such that they recognize each other and might
    know something about fellow students but probably don’t know much about other
    students’ interests, home life, or aspirations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – An illustration of students in the same classroom for a course
    who may not interact outside of the classroom](img/B21087_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – An illustration of students in the same classroom for a course
    who may not interact outside of the classroom
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.2* shows students in the same classroom who may not interact outside
    of that single class. Weak social ties such as these expose students to different
    ideas, different interests, seasonal flu, and more but have less influence on
    an individual than on the group of individuals as a whole.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll infer groups of students likely to share strong social
    ties through UL algorithms on both network metrics and metadata related to student
    demographics. We’ll also analyze social network risk on randomly generated networks
    to understand different epidemic risks for different types of networks through
    **supervised learning** (**SL**) with GNNs. Let’s explore our initial dataset
    a bit before diving into some analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Friendship demographic and school factor dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll mainly work with a dataset containing information about
    a group of 25 students who are connected by many different lifestyle factors:
    team membership, casual workouts, weekend sports activities, game attendance,
    and homework study group membership. Demographic and socioeconomic factors, as
    well as class assignments, also connect these students by registration in four
    elective courses, gender, neighborhood of residence, and prior attendance at one
    of two local junior high schools.'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was derived from Farrelly’s secondary school diary over the course
    of a month in her freshman year. Farrelly herself is individual *#7*. To create
    a weighted network, we’ll sum up connections across factors between pairs of students.
    This will give us an approximation of which students are most connected to each
    other. We’ll first explore clustering to discern friendship groups.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can cluster this network based on metadata alone before we
    move into clustering on both metadata and network metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ML on networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have explored friendship data a bit, let’s see how clustering algorithm
    performance varies depending on whether or not we include structural information
    about the network. We’ll start by considering just student factors.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering based on student factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our first attempt at clustering, we’ll focus on the dataset itself, which
    contains metadata regarding student demographics and social activities. One of
    the simplest clustering algorithms is *k-means clustering*, which partitions data
    iteratively to minimize within-cluster variance and maximize between-cluster variance.
    This means that students clustered together have more in common with students
    in that same cluster than with students in other clusters. K-means clustering
    is a simple algorithm that works well in most cases. However, one needs to specify
    the number of expected clusters, which is typically not known ahead of time. We’ll
    use a cluster size of `3` and assess model fit; in addition, we’ll restart the
    algorithm five times to ensure that we have an optimal three-cluster solution
    regardless of algorithm start point and random error.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are on a Windows machine, you may get a warning that does not impact
    results; some of the packages on `scikit-learn` are not updated with the new Windows
    operating systems in mind. New releases of operating systems and updates to package
    dependencies tend to trigger these warnings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive into the k-means clustering code with `Script 9.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The clustering results suggest that the three-cluster solution is a good fit.
    One cluster group (*#0*) includes individuals `1`-`7` and individual `10`; this
    group mostly does homework together, attends games, works out together on weekends,
    and plays on the same team. Cluster *#0* is characterized by a tight-knit group
    of friends who share many of the same activities and are near each other most
    of the week. We’d be concerned about an epidemic starting and spreading with this
    group. Likely, they share the same protective behaviors, such as healthy eating,
    regular physical activity, and social engagement. However, an infectious disease
    or risk behavior that might lead to physical injury (trying a dangerous take on
    a sports move, taking dares…) is a concern, as the behavior is likely to spread
    through the entire group of friends.
  prefs: []
  type: TYPE_NORMAL
- en: Another group (*#1*) includes individuals `8`-`9`, `12`-`14`, `16`, `19`, and
    `23`-`25`; these individuals usually share *class 2*, don’t work out or play sports
    together outside of school, don’t do homework together, and don’t share many other
    classes. Cluster *#1* is characterized by a lack of involvement and engagement
    with others in our sample. This group is low risk for both protective behavior
    and risk behavior spreading as they don’t have strong social ties to others in
    our sample. Likely, they wouldn’t be influenced or influence others with behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The last group (*#2*) includes individuals `11`, `15`, `17`-`18`, and `20`-`22`;
    this group is heterogeneous and includes teammates who don’t have much else in
    common, individuals who share a few classes, and isolated individuals with few
    connections to others. In general, this group is low risk for epidemic or behavior
    spread like cluster *#1*; however, they are more active within the sample and
    may be influenced somewhat by teammates or those with whom they share multiple
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering based on student factors and network metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s create a network based on thresholded Pearson correlations, which
    represents the similarity of activities/classes across individuals by adding to
    `Script 9.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this addition to `Script 9.1` yields a plot of the friendship network,
    as shown in *Figure 9**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A network plot of the thresholded friendship dataset](img/B21087_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A network plot of the thresholded friendship dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.3* shows two separate groups, with one very small group consisting
    of two individuals and a much larger group with sparse and dense connectivity
    among individuals in the group. We’d expect the degree and PageRank centralities
    to vary quite a bit among individuals, given the connectivity patterns of our
    friendship dataset. Let’s add to `Script 9.1` and append our feature matrix to
    rerun our k-means analysis, including both demographic factors and two scaled
    centrality metrics, to see how our clustering changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may find a warning about copying objects; this does not impact the analysis
    or object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can see some changes in our clustering results compared to our initial k-means
    model. In cluster *#0*, individual `19` is added (a teammate who does homework
    with the initial *#0* cluster and attends the game). Our initial cluster *#1*
    shows individuals `8`-`9`, `12`, `16`, and `23`-`25`; individuals `13`, `14`,
    and `19` are no longer assigned to this cluster but other individuals remain.
    In the remaining cluster, individuals `13` and `14` join our initial cluster,
    both of whom seem to have more connectivity than initial cluster *#1*, fitting
    better with cluster *#2* based on centrality metrics. It seems that adding network
    connectivity metrics improves k-means clustering results, as individuals who may
    not share every activity but show similar group connections are reassigned to
    groups that more closely fit their positions within the social network.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how we can use a semi-supervised clustering algorithm that we
    first encountered in [*Chapter 5*](B21087_05.xhtml#_idTextAnchor066)—spectral
    clustering—to obtain a semi-supervised solution to our friendship network clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering on the friendship network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in [*Chapter 5*](B21087_05.xhtml#_idTextAnchor066), spectral clustering
    offers a clustering option to partition either an adjacency matrix or a distance
    matrix; this can be done as a UL or `Script 9.1` to run an unsupervised spectral
    clustering with three clusters and five initializations (similar to our k-means
    runs) on our friendship dataset to compare with our k-means results by adding
    to `Script 9.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Again, you may encounter a Windows warning from scikit-learn or a warning about
    the graph not being fully connected (assessed via Laplacian, which results in
    a different approach to the clustering than would be run for a fully connected
    network). Neither of these warnings will impact the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These results differ significantly compared to the k-means solutions we obtained
    in the previous subsection. Given that both k-means models consider specific activities
    and course schedules rather than just a correlation summary, this difference makes
    sense. The spectral clustering solution focuses solely on network connectivity
    rather than the factors included in the friendship dataset or a combination of
    connectivity and factors. In this case, the k-means solutions make more sense
    given our data—particularly the second k-means solution, which includes network
    metrics and the original factors.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of unsupervised versus semi-supervised clustering algorithms is
    highly specific to the task at hand. For very large networks, k-means algorithms
    have solutions that scale well, and adding network connectivity metrics that scale
    well may improve k-means solutions without sacrificing efficiency. For problems
    that involve a pure network connectivity solution, spectral clustering may be
    preferable, particularly if the factors used to construct the network were not
    collected or are unknown for a third-party network. However, spectral clustering
    can also take partially labeled data as input, allowing for SSL that can guide
    the learning process given what is known already about the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how UL and SSL algorithms can be used on network datasets,
    let’s turn our attention to SL algorithms, focusing on an exciting new type of
    **deep learning** (**DL**) algorithm specifically designed to take network datasets
    as their input.
  prefs: []
  type: TYPE_NORMAL
- en: DL on networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll consider a new type of DL model called GNNs, which process
    and operate on networks by embedding vertex, edge, or global properties of the
    network to learn outcomes related to individual networks, vertex properties within
    a network, or edge properties within a network. Essentially, the DL architecture
    evolves the topology of these embeddings to find key topological features in the
    input data that are predictive of the outcome. This can be done in a fully supervised
    or semi-supervised fashion. In this example, we’ll focus on SSL, where only some
    of the labels are known; however, by providing all labels as input, this can be
    changed to an SL setting.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the technical details of GNNs, let’s explore their use cases
    in more depth. Classifying networks themselves often yields important insight
    into problems such as image features or type, molecular compound toxicity or potential
    use as a pharmaceutical agent, or potential for epidemic spread within a country
    of interest given travel routes and population hubs.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, data such as molecules or images is transformed into network structure
    prior to the network embedding step of GNNs. Within the context of molecular compounds,
    atoms that share a covalent bound, for instance, are represented as vertices connected
    by an edge. Each compound, then, results in a unique network based on the molecular
    structure of that compound. For proteins, amino acids can serve as vertices, with
    connections existing between amino acids sharing a bond (such as a cysteine bridge
    resulting from a disulfide bond).
  prefs: []
  type: TYPE_NORMAL
- en: When screening potential compounds for use in pharmaceutical development, we
    often want to predict if the compound might have toxic effects. Using known databases
    of toxic compounds and compounds with no toxic effects, we can develop a GNN to
    predict the toxic effects of new compounds in development based on the molecular
    structures of the new compounds, given what we know about molecules that are known
    to be or not to be toxic. This allows for quick screening of potential new drugs
    for toxicity prior to animal or human trials.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs are also able to learn vertex labels given an input network, which is the
    focus of this chapter. For instance, within a crime or terrorism network, we may
    wish to identify potential leadership within the network given some knowledge
    of leaders and non-leaders from collected intelligence data. Incomplete information
    is common within intelligence data, and learning from what is known can be valuable
    in identifying key players in the network who are not known and who may be difficult
    to identify from informants or undercover agents. Since vertex prediction involves
    a network that has been constructed, we typically skip to the embedding steps
    of the GNN rather than wrangle the data. However, it might be necessary to add
    vertex labels to the graph to denote known information about leadership structure
    in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Edge learning with GNNs mirrors vertex learning, typically through the use of
    an existing network with complete or incomplete information about edge properties
    (such as communication frequency or importance across members of a terrorist network
    that might involve coordinating a terrorist attack or recruiting new members in
    a geographic region). We embed the edges rather than the vertices in this case
    before proceeding with the GNN training.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know a bit about problems we can tackle with GNNs, let’s learn more
    about the architecture and mathematical operations used to build a GNN.
  prefs: []
  type: TYPE_NORMAL
- en: GNN introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GNN construction involves a few key steps. In the prior subsection, we mentioned
    data transformation as a potential first step. GNNs require a network or tensor
    of networks as input to the embedding step of the algorithm, so data must contain
    network-structured data and some outcome label associated with the networks themselves
    or edges/vertices in the network of interest. Some data engineering may be required
    to wrangle image(s), molecule(s), or other data sources into network structures.
    In the prior subsection, we overviewed how molecule or protein data can be transformed
    into a network structure. Many common types of data have standard transformation
    methods to transform them into network data; for example, in prior chapters, we’ve
    transformed spatial and time series data into network structures that could be
    used as input for a GNN.
  prefs: []
  type: TYPE_NORMAL
- en: Once our data exists in a network structure with a set of labels for networks,
    edges, or vertices, we’re ready to embed the relevant structures at a network,
    edge, or vertex level. Embeddings aim to find a low-dimensional representation
    of relevant network geometry at the level of embedding (network, edge, or vertex).
    They can also include other relevant information, such as other attributes of
    networks, edges, or vertices. Sometimes, it’s advantageous to create these embeddings
    manually to include both relevant network structure and attribute information.
    For instance, in our friendship network, we have data on many activities in which
    individuals participate; we may wish to create an embedding that captures not
    only the network centrality metrics but also the activity participation for individuals
    represented as a network vertex. In our k-means example, including both types
    of information (network structure and collected activity data) improved k-means
    performance in finding groups we hypothesized to exist.
  prefs: []
  type: TYPE_NORMAL
- en: Many GNN packages in Python, such as PyTorch (which we will use later in the
    section), have functions that summarize network properties at the network, edge,
    and vertex level to create an automatic embedding at a specified dimensionality.
    How we embed data prior to GNN training greatly impacts results, so this step
    is important to consider when building a GNN. Even with package functions such
    as the PyTorch one that we’ll use, specifying a dimensionality impacts algorithm
    performance. We don’t want too low of a dimensionality (missing key features relevant
    to the outcome of interest), but we also don’t want too high of a dimensionality
    (which might include a lot of noise). In practice, this parameter is often optimized
    through grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the embedding, we can define the outcomes as target labels. We
    may need to employ one-hot encoding to transform text labels into a sequence of
    binary outcomes. Just as other DL algorithms can handle multiclass classification
    problems, continuous outcomes, or other types of distributions, GNNs can fit many
    different outcomes of interest. This flexibility makes them ideal for modeling
    outcomes across network classification/regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DL architecture itself is not unique. Readers who are familiar with **convolutional
    neural networks** (**CNNs**) will recognize many of the components and backfitting
    algorithms we’ll discuss, as they are identical within the context of GNNs. We
    start with an input layer with a dimension equal to the embedding dimension, and
    we end with an output layer with a dimension equal to the number of classes of
    our outcome (for classification problems, which we’ll consider in this chapter).
    When only the input and output layers exist, **neural networks** (**NNs**) approximate
    linear regression, with a learned map between input matrices and output vectors.
    However, between these layers, we typically include hidden layers that further
    process features between the input and output layers, as shown in *Figure 9**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – A summary of the GNN life cycle, including data engineering
    and DL architecture steps](img/B21087_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – A summary of the GNN life cycle, including data engineering and
    DL architecture steps
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers refine the topological maps between the input and output layers,
    often pooling topological features found during the training process to feed into
    the next hidden layer. For small networks and small samples of networks, the number
    of hidden layers should be small to maintain the stability of the solution and
    obtain good performance. For larger networks or sets of networks, more hidden
    layers can be added to improve performance without encountering instability of
    solutions and fit.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers typically employ a non-linear mapping function, called an *activation
    function*, between the input layer and output layer connected to that specific
    hidden layer. In practice, only a few activation functions are common, including
    the *ReLU function*, which returns 0 for negative or zero input values and the
    input value for positive input values.
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolution layers*, also commonly used in hidden layers, apply a filter function
    (typically a kernel) to the input layer to transform it through the defined kernel
    function. Typically, a convolution layer will reduce the dimensionality of the
    matrix or tensor, so zero padding to maintain dimensionality may be used to avoid
    the problem of transforming layers in a way that limits the number of layers possible
    given the dimensionality of the output. For small datasets, such as the one we
    consider, this is not necessarily a problem, as shallow networks tend to be the
    only stable GNNs that we can train given the limited data size.'
  prefs: []
  type: TYPE_NORMAL
- en: The theory of building effective architectures is beyond the scope of this book,
    and readers interested in DL who do not have a background can obtain this knowledge
    by reading through the references provided at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an architecture is defined (either through expert guessing or, again,
    grid search to optimize architecture), we must fit the parameters connecting nodes
    in each layer and across layers (called *weights*). There are many options to
    do this, and it’s possible to define custom fitting algorithms. However, we’ll
    focus on the two most common options within the PyTorch package used in our example:
    **Adam optimizers** and **stochastic gradient** **descent** (**SGD**).'
  prefs: []
  type: TYPE_NORMAL
- en: '*SGD* fits weights between nodes within and across layers by exploring the
    gradient function defined on the NN in much the way that gradient boosting fits
    a linear regression model. A *learning rate* is defined to control the exploration
    of the gradient function. A steeper learning rate fits a model more quickly but
    may not find global minima or maxima. One caveat of SGD is that the algorithm
    can get stuck in local optima, resulting in lower accuracies of results than what
    is possible given the input data, mapping functions between layers, and the outcome
    data. It also tends to be slower, requiring more algorithm iterations and potentially
    more processing power to even fit the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adam optimizers* allow for flexible learning rates for different weights between
    nodes, leading to faster model fits and avoiding local optima by allowing the
    learning rate to adjust to the local gradient landscape. Adam also allows for
    decay rates, further customizing local learning of weights. Many Adam optimizers
    have evolved since the initial Adam optimizer was developed, and it’s likely more
    will be developed for GNNs and other DL architectures. One drawback is that Adam
    optimizers tend to be memory-intensive. When training large GNNs, it may be preferable
    to use SGD to avoid memory issues during training.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it’s difficult to know which optimizer is best for fitting the
    weights of the defined architecture, and grid search, again, is typically employed
    for industry GNN models to optimize this choice. Once a fitting algorithm is selected,
    a predefined (again, usually optimized by grid search) number of iterations is
    run, or the algorithm runs until meeting a stopping criterion. Adam optimizers
    tend to converge more quickly than SGD optimizers, but performance can vary depending
    on the data and architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand a bit about the building blocks of GNNs, let’s explore
    an example using an open source sports network consisting of students assigned
    to two different teachers (our outcome of interest).
  prefs: []
  type: TYPE_NORMAL
- en: Example GNN classifying the Karate Network dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our example, we’ll predict vertex-level attributes in a common open source
    network: Zachary’s `Karate Network` dataset. This dataset consists of 34 individuals
    connected by 78 edges in a karate training network who ended up splitting between
    an administrator and one of the instructors when a conflict between the administrator
    and instructor occurred. One of the primary tasks for vertex classification and
    learning problems on this network is to predict which individuals ended up siding
    with which person in the conflict (the administrator or the instructor). We will
    predict vertex labels through a semi-supervised GNN model approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll first install the needed packages and import our dataset. If you don’t
    have the necessary packages installed, please install them on your machine prior
    to running our code. We’ve provided this step as an option in `Script 9.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To embed our vertex data, we’ll use PyTorch’s default embedding algorithm with
    a dimensionality of `6`. Anything that is in the `4`-`6` dimension range should
    work reasonably well, given the size of our network. Let’s add to `Script 9.2`
    to embed our vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This piece of the script should output embedding vectors for each vertex in
    our network. Now that we have our vertices embedded, we can create our labels.
    Given that we wish to demonstrate a semi-supervised approach, we’ll feed our network
    information on six vertices (`1`, `3`, `5`, `12`, `15`, and `32`). You can play
    around with this part of the script to see how fewer or more vertices impact the
    performance and stability of our chosen architecture. Let’s add the label information
    by adding to `Script 9.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll need to build our GNN architecture and define training parameters.
    Many of the papers and tutorials on GNNs using this dataset employ a very shallow
    network architecture and Adam optimizers. For the sake of comparison and demonstration
    of other options for building GNNs, we’ll use two hidden layers instead of one
    (including convolution layers coupled with ReLU functions), employ small layers
    (eight and six nodes, respectively for hidden layers), an SGD fitting algorithm
    (with a learning rate of `0.01` and a momentum driving the algorithm of `0.8`,
    which is close to the default value), and `990` iterations. Many examples that
    exist online use Adam optimizers and a single hidden layer with more nodes than
    our architecture, allowing for fewer training iterations. However, for larger
    network vertex-label prediction problems, a more complex architecture is likely
    to perform better, so we will show a way to include more hidden layers and a way
    to use a different fitting algorithm than Adam. Let’s define our architecture
    and fit our weights by adding to `Script 9.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the loss function (logistic regression link function here) that
    decreases across iterations as your output. Typical accuracies from GNN architectures
    fall into the 95%-100% range. Because this dataset is small and our architecture
    is large, your output accuracies may vary quite a bit between runs of the algorithm.
    This has to do with random sampling within the fitting steps of the algorithm
    and the coarseness of the underlying gradient landscape. Let’s add to `Script
    9.2` to find our accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our run of the algorithm gives an accuracy of ~97% in this run of the algorithm.
    That is on par with the performance of other GNN architectures. However, don’t
    be surprised if your accuracy is significantly lower in one or more runs of the
    script, as we don’t have a large enough sample size to fit this type of architecture.
    Changing the embedding dimension, architecture, and training algorithm parameters
    will impact accuracy, and interested readers are encouraged to revise `Script
    9.2` as a way to see how different choices impact accuracy and stability of fit.
  prefs: []
  type: TYPE_NORMAL
- en: In general, GNN classifiers work much better and show better stability on larger
    networks and with more labels fed into semi-supervised usage. The Zachary Karate
    Network dataset is small enough that other methods are recommended to classify
    the network. However, learning labels on a huge social network (such as those
    found on social media platforms) or a large geographic network (such as a United
    States city network with connections defined by roads connecting cities larger
    than 50,000 people) would result in a more stable GNN solution, and it would be
    possible to create a very deep architecture. However, to fit these large models,
    we often need a cloud computing platform, as the large datasets and large number
    of iterations can be difficult on a laptop.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs have shown great promise in network-based classification problems in many
    different fields, and it is likely that they will continue to evolve and solve
    pressing problems with large networks and collections of networks. However, cloud
    computing solutions are often needed, and this requires expertise working with
    data and Python notebook solutions on the cloud computing platform used to store
    data and fit the GNN.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we considered several use cases of ML algorithms on network
    datasets. This included UL on a friendship network through fitting k-means and
    spectral clustering. We considered k-means clustering on both the original dataset
    of activities in which individuals participated and the original dataset, with
    added network metrics to improve clustering accuracy. We then turned to SL and
    SSL on networks and collections of networks through a type of DL algorithm called
    GNNs. We accurately predicted the labels of individuals in Zachary’s Karate Network
    dataset through a shallow GNN and compared results with other existing solutions
    to this network classification problem. In [*Chapter 10*](B21087_10.xhtml#_idTextAnchor128),
    we'll mine educational data for causal relationships using network tools related
    to conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Acharya, D. B., & Zhang, H. (2021). *Weighted Graph Nodes Clustering via Gumbel
    Softmax*. arXiv preprint arXiv:2102.10775.
  prefs: []
  type: TYPE_NORMAL
- en: Bongini, P., Bianchini, M., & Scarselli, F. (2021). Molecular generative graph
    neural networks for drug discovery. *Neurocomputing,* *450, 242-252.*
  prefs: []
  type: TYPE_NORMAL
- en: Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., & Yin, D. (2019, May).
    Graph neural networks for social recommendation. *In The World Wide Web Conference
    (**pp. 417-426).*
  prefs: []
  type: TYPE_NORMAL
- en: 'Hartigan, J. A., & Wong, M. A. (1979). Algorithm AS 136: A k-means clustering
    algorithm. *Journal of the Royal Statistical Society. Series C (Applied Statistics),*
    *28(1), 100-108.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imambi, S., Prakash, K. B., & Kanagachidambaresan, G. R. (2021). PyTorch. *Programming
    with TensorFlow: Solution for Edge Computing* *Applications, 87-104.*'
  prefs: []
  type: TYPE_NORMAL
- en: Kumar, V. (2020). *An Investigation Into Graph Neural Networks (Doctoral dissertation,
    Trinity College* *Dublin, Ireland).*
  prefs: []
  type: TYPE_NORMAL
- en: 'Labonne, M. (2023). *Hands-On Graph Neural Networks Using Python: Practical
    techniques and architectures for building powerful graph and deep learning apps
    with PyTorch. Packt* *Publishing Ltd.*'
  prefs: []
  type: TYPE_NORMAL
- en: Liang, F., Qian, C., Yu, W., Griffith, D., & Golmie, N. (2022). Survey of graph
    neural networks and applications. *Wireless Communications and Mobile* *Computing,
    2022.*
  prefs: []
  type: TYPE_NORMAL
- en: Mantzaris, A. V., Chiodini, D., & Ricketson, K. (2021). Utilizing the simple
    graph convolutional neural network as a model for simulating influence spread
    in networks. *Computational Social Networks,* *8, 1-17.*
  prefs: []
  type: TYPE_NORMAL
- en: Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., & Fang, B. (2021). STGSN—A Spatial–Temporal
    Graph Neural Network framework for time-evolving social networks. *Knowledge-Based
    Systems,* *214, 106746.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Ng, A., Jordan, M., & Weiss, Y. (2001). On spectral clustering: Analysis and
    an algorithm. *Advances in neural information processing* *systems, 14.*'
  prefs: []
  type: TYPE_NORMAL
- en: Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008).
    The graph neural network model. *IEEE transactions on neural networks,* *20(1),
    61-80.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Wieder, O., Kohlbacher, S., Kuenemann, M., Garon, A., Ducrot, P., Seidel, T.,
    & Langer, T. (2020). A compact review of molecular property prediction with graph
    neural networks. *Drug Discovery Today: Technologies,* *37, 1-12.*'
  prefs: []
  type: TYPE_NORMAL
- en: Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive
    survey on graph neural networks. *IEEE transactions on neural networks and learning
    systems,* *32(1), 4-24.*
  prefs: []
  type: TYPE_NORMAL
- en: Zachary, W. W. (1977). An information flow model for conflict and fission in
    small groups. *Journal of Anthropological Research,* *33(4), 452-473.*
  prefs: []
  type: TYPE_NORMAL
- en: Zhang, L., Xu, J., Pan, X., Ye, J., Wang, W., Liu, Y., & Wei, Q. (2023). Visual
    analytics of route recommendation for tourist evacuation based on graph neural
    network. *Scientific Reports,* *13(1), 17240.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... & Sun, M. (2020).
    *Graph neural networks: A review of methods and applications. AI open,* *1, 57-81.*'
  prefs: []
  type: TYPE_NORMAL
