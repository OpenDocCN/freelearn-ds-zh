- en: Kernels, Threads, Blocks, and Grids
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核、线程、块和网格
- en: In this chapter, we'll see how to write effective **CUDA kernels***.* In GPU
    programming, a**kernel** (which we interchangeably use with terms such as *CUDA
    kernel* or *kernel function*) is a parallel function that can be launched directly
    from the **host** (the CPU) onto the **device** (the GPU), while a **device function**
    is a function that can only be called from a kernel function or another device
    function. (Generally speaking, device functions look and act like normal serial
    C/C++ functions, only they are running on the GPU and are called in parallel from
    kernels.)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何编写有效的 **CUDA 内核**。在 GPU 编程中，**内核**（我们交替使用术语如 **CUDA 内核** 或 **内核函数**）是一个可以直接从
    **主机**（CPU）启动到 **设备**（GPU）的并行函数，而 **设备函数** 是只能从内核函数或另一个设备函数中调用的函数。（一般来说，设备函数看起来和表现就像正常的串行
    C/C++ 函数，只是它们在 GPU 上运行，并且从内核中并行调用。）
- en: We'll then get an understanding of how CUDA uses the notion of **threads**,
    **blocks**, and **grids** to abstract away some of the underlying technical details
    of the GPU (such as cores, warps, and streaming multiprocessors, which we'll cover
    later in this book), and how we can use these notions to ease the cognitive overhead
    in parallel programming. We'll learn about thread synchronization (both block-level
    and grid-level), and intra-thread communication in CUDA using both **global**
    and **shared** **memory**. Finally, we'll delve into the technical details of
    how to implement our own parallel prefix type algorithms on the GPU (that is,
    the scan/reduce type functions we covered in the last chapter), which allow us
    to put all of the principles we'll learn in this chapter into practice.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将了解 CUDA 如何使用 **线程**、**块** 和 **网格** 的概念来抽象掉 GPU 的某些底层技术细节（例如核心、战程和流式多处理器，我们将在本书的后续章节中介绍），以及我们如何使用这些概念来减轻并行编程的认知负担。我们将学习关于线程同步（包括块级和网格级）以及
    CUDA 中使用 **全局** 和 **共享** **内存** 的线程间通信。最后，我们将深入了解如何在 GPU 上实现我们自己的并行前缀类型算法的技术细节（即我们在上一章中介绍过的扫描/归约类型函数），这将使我们能够将本章学到的所有原理付诸实践。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Understanding the difference between a kernel and a device function
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解内核和设备函数之间的区别
- en: How to compile and launch a kernel in PyCUDA and use a device function within
    a kernel
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 PyCUDA 中编译和启动内核，并在内核中使用设备函数
- en: Effectively using threads, blocks, and grids in the context of launching a kernel
    and how to use `threadIdx` and `blockIdx` within a kernel
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启动内核的上下文中有效地使用线程、块和网格，以及如何在内核中使用 `threadIdx` 和 `blockIdx`
- en: How and why to synchronize threads within a kernel, using both `__syncthreads()`
    for synchronizing all threads among a single block and the host to synchronize
    all threads among an entire grid of blocks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何以及为什么在内核中使用线程同步，使用 `__syncthreads()` 同步单个块中的所有线程，以及主机同步整个块网格中的所有线程
- en: How to use device global and shared memory for intra-thread communication
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用设备全局和共享内存进行线程间通信
- en: How to use all of our newly acquired knowledge about kernels to properly implement
    a GPU version of the parallel prefix sum
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用我们新学到的关于内核的所有知识来正确实现并行前缀和的 GPU 版本
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章，需要一个配备现代 NVIDIA GPU（2016 年及以后）的 Linux 或 Windows 10 PC，并安装所有必要的 GPU 驱动程序和
    CUDA 工具包（9.0 及以后版本）。还需要一个合适的 Python 2.7 安装（例如 Anaconda Python 2.7），并带有 PyCUDA
    模块。
- en: 'This chapter''s code is also available on GitHub at:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
- en: For more information about the prerequisites, check the *Preface* of this book;
    for the software and hardware requirements, check the `README` section in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于先决条件的更多信息，请参阅本书的**前言**；关于软件和硬件要求，请查阅 [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)
    中的 `README` 部分。
- en: Kernels
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核
- en: As in the last chapter, we'll be learning how to write CUDA kernel functions
    as inline CUDA C in our Python code and launch them onto our GPU using PyCUDA.
    In the last chapter, we used templates provided by PyCUDA to write kernels that
    fall into particular design patterns; in contrast, we'll now see how to write
    our own kernels from the ground up, so that we can write a versatile variety of
    kernels that may not fall into any particular design pattern covered by PyCUDA,
    and so that we may get a more fine-tuned control over our kernels. Of course,
    these gains will come at the expense of greater complexity in programming; we'll
    especially have to get an understanding of **threads**, **blocks**, and **grids**
    and their role in kernels, as well as how to **synchronize** the threads in which
    our kernel is executing, as well as understand how to exchange data among threads.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一章所做的那样，我们将学习如何在Python代码中编写CUDA内核函数作为内联CUDA C，并使用PyCUDA将它们启动到我们的GPU上。在上一章中，我们使用了PyCUDA提供的模板来编写符合特定设计模式的内核；相比之下，我们现在将看到如何从头开始编写我们自己的内核，这样我们就可以编写各种灵活的内核，这些内核可能不符合PyCUDA覆盖的任何特定设计模式，并且我们可以对内核有更精细的控制。当然，这些收益将伴随着编程复杂性的增加；我们特别需要理解**线程**、**块**和**网格**及其在内核中的作用，以及如何**同步**执行内核的线程，以及了解如何在线程之间交换数据。
- en: Let's start simple and try to re-create some of the element-wise operations
    we saw in the last chapter, but this time without using the `ElementwiseKernel`
    function; we'll now be using the `SourceModule` function. This is a very powerful
    function in PyCUDA that allows us to build a kernel from scratch, so as usual
    it's best to start simple.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始，尝试重新创建我们在上一章中看到的一些元素级操作，但这次我们不使用`ElementwiseKernel`函数；我们现在将使用`SourceModule`函数。这是PyCUDA中的一个非常强大的函数，它允许我们从零开始构建内核，所以通常最好从简单开始。
- en: The PyCUDA SourceModule function
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA的SourceModule函数
- en: We'll use the `SourceModule` function from PyCUDA to compile raw inline CUDA
    C code into usable kernels that we can launch from Python. We should note that
    `SourceModule` actually compiles code into a **CUDA module**, this is like a Python
    module or Windows DLL, only it contains a collection of compiled CUDA code. This
    means we'll have to "pull out" a reference to the kernel we want to use with PyCUDA's
    `get_function`, before we can actually launch it. Let's start with a basic example
    of how to use a CUDA kernel with `SourceModule`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyCUDA的`SourceModule`函数来编译原始的内联CUDA C代码，将其编译成可用的内核，这样我们就可以从Python中启动它们。我们应该注意，`SourceModule`实际上是将代码编译成一个**CUDA模块**，这就像一个Python模块或Windows
    DLL，只是它包含了一组编译后的CUDA代码。这意味着在使用PyCUDA的`get_function`获取我们想要使用的内核的引用之前，我们不得不“提取”出来。让我们从一个基本的例子开始，看看如何使用`SourceModule`来使用CUDA内核。
- en: 'As before, we''ll start with making one of the most simple kernel functions
    possible—one that multiplies a vector by a scalar. We''ll start with the imports:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将从一个最简单的内核函数开始——一个将向量乘以标量的函数。我们首先进行导入：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can immediately dive into writing our kernel:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以立即开始编写我们的内核：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, let''s stop and contrast this with how it was done in `ElementwiseKernel`.
    First, when we declare a kernel function in CUDA C proper, we precede it with
    the `__global__` keyword. This will distinguish the function as a kernel to the
    compiler. We''ll always just declare this as a `void` function, because we''ll
    always get our output values by passing a pointer to some empty chunk of memory
    that we pass in as a parameter. We can declare the parameters as we would with
    any standard C function: first we have `outvec`, which will be our output scaled
    vector, which is of course a floating-point array pointer. Next, we have `scalar`,
    which is represented with a mere `float`; notice that this is not a pointer! If
    we wish to pass simple singleton input values to our kernel, we can always do
    so without using pointers. Finally, we have our input vector, `vec`, which is
    of course another floating-point array pointer.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们停下来，将其与在`ElementwiseKernel`中是如何做的进行对比。首先，当我们声明CUDA C中的内核函数时，我们在它前面加上`__global__`关键字。这将使编译器将这个函数识别为内核。我们总是只声明为一个`void`函数，因为我们总是通过传递一个指向我们作为参数传递的某个空内存块的指针来获取我们的输出值。我们可以像声明任何标准C函数的参数一样声明参数：首先我们有`outvec`，它将是我们输出的缩放向量，当然是一个浮点数组指针。接下来是`scalar`，它用一个简单的`float`表示；注意，这不是一个指针！如果我们希望向内核传递简单的单例输入值，我们总是可以这样做而不使用指针。最后，我们有我们的输入向量`vec`，当然也是一个浮点数组指针。
- en: Singleton input parameters to a kernel function can be passed in directly from
    the host without using pointers or allocated device memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内核函数的单例输入参数可以直接从主机传递，而无需使用指针或分配设备内存。
- en: 'Let''s peer into the kernel before we continue with testing it. We recall that
    `ElementwiseKernel` automatically parallelized over multiple GPU threads by a
    value, `i`, which was set for us by PyCUDA; the identification of each individual
    thread is given by the `threadIdx` value, which we retrieve as follows: `int i
    = threadIdx.x;`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续测试内核之前，让我们先看看内核。我们回忆一下，`ElementwiseKernel`通过PyCUDA设置的一个值`i`自动并行化多个GPU线程；每个单独线程的标识由`threadIdx`值给出，我们通过以下方式检索它：`int
    i = threadIdx.x;`。
- en: '`threadIdx` is used to tell each individual thread its identity. This is usually
    used to determine an index for what values should be processed on the input and
    output data arrays. (This can also be used for assigning particular threads different
    tasks than others with standard C control flow statements such as `if` or `switch`.)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`threadIdx`用于告诉每个单独的线程其身份。这通常用于确定输入和输出数据数组中应该处理哪些值的索引。（这也可以用于使用标准的C控制流语句，如`if`或`switch`，为特定线程分配不同于其他线程的任务。）'
- en: 'Now, we are ready to perform our scalar multiplication in parallel as before:
    `outvec[i] = scalar*vec[i];`.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备像以前一样并行执行标量乘法：`outvec[i] = scalar*vec[i];`。
- en: 'Now, let''s test this code: we first must *pull out* a reference to our compiled
    kernel function from the CUDA module we just compiled with `SourceModule`. We
    can get this kernel reference with Python''s `get_function` as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试这段代码：我们首先必须从我们刚刚用`SourceModule`编译的CUDA模块中*提取*编译后的内核函数的引用。我们可以使用Python的`get_function`获取这个内核引用，如下所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we have to put some data on the GPU to actually test our kernel. Let''s
    set up a floating-point array of 512 random values, and then copy these into an
    array in the GPU''s global memory using the `gpuarray.to_gpu` function. (We''re
    going to multiply this random vector by a scalar both on the GPU and CPU, and
    see if the output matches.) We''ll also allocate a chunk of empty memory to the
    GPU''s global memory using the `gpuarray.empty_like` function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须在GPU上放置一些数据来实际测试我们的内核。让我们设置一个包含512个随机值的浮点数组，然后使用`gpuarray.to_gpu`函数将这些值复制到GPU的全局内存中的数组。
    （我们将在这个GPU和CPU上对这个随机向量乘以一个标量，看看输出是否匹配。）我们还将使用`gpuarray.empty_like`函数在GPU的全局内存中分配一块空内存：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now prepared to launch our kernel. We''ll set the scalar value as `2`.
    (Again, since the scalar is a singleton, we don''t have to copy this value to
    the GPU—we should be careful that we typecast it properly, however.) Here we''ll
    have to specifically set the number of threads to `512` with the `block` and `grid`
    parameters. We are now ready to launch:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备启动内核。我们将标量值设置为`2`。（同样，由于标量是单例，我们不需要将其值复制到GPU上——但我们应该小心地正确类型转换它。）在这里，我们必须使用`block`和`grid`参数将线程数特别设置为`512`。我们现在准备启动：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now check whether the output matches with the expected output by using
    the `get` function in our `gpuarray` output object and comparing this to the correct
    output with NumPy''s `allclose` function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`gpuarray`输出对象中的`get`函数来检查输出是否与预期输出匹配，并与NumPy的`allclose`函数比较正确输出：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (The code to this example is available as the `simple_scalar_multiply_kernel.py`
    file, under `4` in the repository.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例的代码作为`simple_scalar_multiply_kernel.py`文件，位于存储库的`4`目录下。）
- en: Now we are starting to remove the training wheels of the PyCUDA kernel templates
    we learned in the previous chapter—we can now directly write a kernel in pure
    CUDA C and launch it to use a specific number of threads on our GPU. However,
    we'll have to learn a bit more about how CUDA structures threads into collections
    of abstract units known as **blocks** and **grids** before we can continue with
    kernels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始移除我们在上一章学习的PyCUDA内核模板的训练轮——我们现在可以直接用纯CUDA C编写内核并启动它，以在GPU上使用特定数量的线程。然而，在我们继续内核之前，我们还得学习更多关于CUDA如何将线程组织成称为**块**和**网格**的抽象单元集合的知识。
- en: Threads, blocks, and grids
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程、块和网格
- en: So far in this book, we have been taking the term **thread** for granted. Let's
    step back for a moment and see exactly what this means—a thread is a sequence
    of instructions that is executed on a single core of the GPU—*cores* and *threads*
    should not be thought of as synonymous! In fact, it is possible to launch kernels
    that use many more threads than there are cores on the GPU. This is because, similar
    to how an Intel chip may only have four cores and yet be running hundreds of processes
    and thousands of threads within Linux or Windows, the operating system's scheduler
    can switch between these tasks rapidly, giving the appearance that they are running
    simultaneously. The GPU handles threads in a similar way, allowing for seamless
    computation over tens of thousands of threads.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们一直将“**线程**”这个术语视为理所当然。让我们退后一步，看看这究竟意味着什么——线程是一系列在GPU单个核心上执行的指令——**核心**和**线程**不应被视为同义词！实际上，可以启动使用比GPU上核心更多的线程的内核。这是因为，类似于英特尔芯片可能只有四个核心，但在Linux或Windows中却可以运行数百个进程和数千个线程，操作系统的调度器可以在这些任务之间快速切换，给人一种它们同时运行的感觉。GPU以类似的方式处理线程，允许在数万个线程上无缝计算。
- en: Multiple threads are executed on the GPU in abstract units known as **blocks**.
    You should recall how we got the thread ID from `threadIdx.x` in our scalar multiplication
    kernel; there is an `x` at the end because there is also `threadIdx.y` and `threadIdx.z`.
    This is because you can index blocks over three dimensions, rather than just one
    dimension. Why do we do this? Let's recall the example regarding the computation
    of the Mandelbrot set from [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?* and [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA*. This is calculated point-by-point over a two-dimensional
    plane. It may therefore make more sense for us to index the threads over two dimensions
    for algorithms like this. Similarly, it may make sense to use three dimensions
    in some cases—in a physics simulation, we may have to calculate the positions
    of moving particles within a 3D grid.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，多个线程在称为**块**的抽象单元中执行。你应该记得我们是如何从标量乘法内核中的`threadIdx.x`获取线程ID的；结尾有一个`x`，因为还有`threadIdx.y`和`threadIdx.z`。这是因为你可以对三个维度进行索引，而不仅仅是单个维度。我们为什么要这样做呢？让我们回顾一下关于计算曼德布罗特集的例子，来自[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)，*为什么进行GPU编程？*和[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)，*使用PyCUDA入门*。这是在二维平面上逐点计算的。因此，对于像这样的算法，我们可能更倾向于在两个维度上索引线程。同样，在某些情况下，使用三个维度可能是有意义的——在物理模拟中，我们可能需要在3D网格中计算移动粒子的位置。
- en: Blocks are further executed in abstract batches known as **grids**, which are
    best thought of as *blocks of blocks.* As with threads in a block, we can index
    each block in the grid in up to three dimensions with the constant values that
    are given by `blockIdx.x` , `blockIdx.y`, and `blockIdx.z`. Let's look at an example
    to help us make sense of these concepts; we'll only use two dimensions here for
    simplicity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 块进一步在称为**网格**的抽象批次中执行，最好将其视为**块的块**。与块中的线程一样，我们可以使用由`blockIdx.x`、`blockIdx.y`和`blockIdx.z`给出的常量值在网格中最多三个维度上索引每个块。让我们通过一个例子来帮助我们理解这些概念；为了简单起见，这里我们只使用两个维度。
- en: Conway's game of life
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 康威的生命游戏
- en: '*The Game of Life* (often called *LIFE* for short) is a cellular automata simulation
    that was invented by the British mathematician John Conway back in 1970\. This
    sounds complex, but it''s really quite simple—LIFE is a zero-player *game* that
    consists of a two-dimensional binary lattice of *cells* that are either considered
    *live* or *dead*. The lattice is iteratively updated by the following set of rules:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*生命游戏*（通常简称为*LIFE*）是由英国数学家约翰·康威在1970年发明的一种细胞自动机模拟。这听起来很复杂，但实际上非常简单——LIFE是一个零玩家*游戏*，由一个二维二进制格子的*细胞*组成，这些细胞要么被认为是*活*的，要么是*死的*。该格子通过以下规则迭代更新：'
- en: Any live cell with fewer than two live neighbors dies
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何拥有少于两个活邻居的活细胞都会死亡
- en: Any live cell with two or three neighbors lives
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何拥有两个或三个邻居的活细胞都会存活
- en: Any live cell with more than three neighbors dies
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何拥有超过三个邻居的活细胞都会死亡
- en: Any dead cell with exactly three neighbors comes to life
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何拥有恰好三个邻居的死亡细胞都会复活
- en: These four simple rules give rise to a complex simulation with interesting mathematical
    properties that is also aesthetically quite pleasing to watch when animated. However,
    with a large number of cells in the lattice, it can run quite slowly, and usually
    results in *choppy* animation when programmed in pure serial Python. However,
    this is parallelizable, as it is clear that each cell in the lattice can be managed
    by a single CUDA thread.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个简单的规则产生了一个具有有趣数学性质且在动画中看起来相当美观的复杂模拟。然而，在格子中有大量单元时，它可能运行得相当慢，并且通常在纯Python序列编程中会导致*不流畅*的动画。然而，这是可并行的，因为很明显，格子中的每个单元都可以由一个CUDA线程管理。
- en: We'll now implement LIFE as a CUDA kernel and animate it as using the `matplotlib.animation`
    module. This will be interesting to us right now because namely we'll be able
    to apply our new knowledge of blocks and grids here.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现LIFE作为一个CUDA内核，并使用`matplotlib.animation`模块来动画化它。这对我们来说现在很有趣，因为我们将能够在这里应用我们对块和网格的新知识。
- en: 'We''ll start by including the appropriate modules as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先按照以下方式包含适当的模块：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s dive into writing our kernel via `SourceModule` . We''re going
    to start by using the C language''s `#define` directive to set up some constants
    and macros that we''ll use throughout our kernel. Let''s look at the first two
    we''ll set up, `_X` and `_Y`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过`SourceModule`深入编写我们的内核。我们将首先使用C语言的`#define`指令来设置一些常量和宏，这些我们将贯穿整个内核使用。让我们看看我们将设置的第一个，`_X`
    和 `_Y`：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's first remember how `#define` works here—it will literally replace any
    text of `_X` or `_Y` with the defined values (in the parentheses here) at compilation
    time—that is, it creates macros for us. (As a matter of personal style, I usually
    precede all of my C macros with an underscore.)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先记住`#define`在这里是如何工作的——它将在编译时将任何`_X`或`_Y`的文本文字替换为定义的值（在这里的括号中），即它为我们创建宏。（作为一个个人风格问题，我通常在我的所有C宏前加一个下划线。）
- en: In C and C++, `#define` is used for creating **macros**. This means that `#define`
    doesn't create any function or set up a proper constant variables—it just allows
    us to write things shorthand in our code by swapping text out right before compilation
    time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在C和C++中，`#define`用于创建**宏**。这意味着`#define`不会创建任何函数或设置适当的常量变量——它只是允许我们在编译前通过替换文本来简化我们的代码。
- en: 'Now, let''s talk about what `_X` and `_Y` mean specifically—these will be the
    Cartesian *x* and *y* values of a single CUDA thread''s cell on the two-dimensional
    lattice we are using for LIFE. We''ll launch the kernel over a two-dimensional
    grid consisting of two-dimensional blocks that will correspond to the entire cell
    lattice. We''ll have to use both thread and block constants to find the Cartesian
    point on the lattice. Let''s look at some diagrams to make the point. A thread
    residing in a two-dimensional CUDA block can be visualized as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们具体谈谈 `_X` 和 `_Y` 的含义——这些将是我们在用于LIFE的二维格子上单个CUDA线程单元的笛卡尔 *x* 和 *y* 值。我们将在一个由二维块组成的二维网格上启动内核，这些块将对应整个单元格。我们将必须使用线程和块常量来找到格子上的笛卡尔点。让我们看看一些图表来阐明这一点。一个位于二维CUDA块中的线程可以如下可视化：
- en: '![](img/b18aaa4b-b830-44f1-9371-80c57b0ab285.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b18aaa4b-b830-44f1-9371-80c57b0ab285.png)'
- en: At this point, you may be wondering why we don't launch our kernel over a single
    block, so we can just set `_X` as `threadIdx.x` and `_Y` as `threadIdx.y` and
    be done with it. This is due to a limitation on block size imposed on us by CUDA—currently,
    only blocks consisting of at most 1,024 threads are supported. This means that
    we can only make our cell lattice of dimensions 32 x 32 at most, which would make
    for a rather boring simulation that might be better done on a CPU, so we'll have
    to launch multiple blocks over a grid. (The dimensions of our current block will
    be given by `blockDim.x` and `blockDim.y`, which will help us determine the objective
    *x* and *y* coordinates, as we'll see.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道为什么我们不只在单个块上启动内核，这样我们就可以直接将`_X`设置为`threadIdx.x`，将`_Y`设置为`threadIdx.y`，然后完成。这是由于CUDA对我们施加的块大小限制——目前，仅支持最多由1,024个线程组成的块。这意味着我们最多只能制作32
    x 32维度的单元格，这将导致一个相当无聊的模拟，可能更适合在CPU上完成，因此我们将在网格上启动多个块。（我们当前块的大小将由`blockDim.x`和`blockDim.y`给出，这将帮助我们确定目标
    *x* 和 *y* 坐标，正如我们将看到的。）
- en: 'Similarly, as before, we can determine which block we are in within a two-dimensional
    grid with `blockIdx.x` and `blockIdx.y`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，像之前一样，我们可以使用`blockIdx.x`和`blockIdx.y`确定我们在二维网格中的哪个块：
- en: '![](img/45159b32-9f17-4a39-a50f-0e4bfb47b1f2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45159b32-9f17-4a39-a50f-0e4bfb47b1f2.png)'
- en: 'After we think of the math a little bit, it should be clear that `_X` should
    be defined as `(threadIdx.x + blockIdx.x * blockDim.x)` and `_Y` should be defined
    as `( threadIdx.y + blockIdx.y * blockDim.y )`. (The parentheses are added so
    as not to interfere with the order of operations when the macros are inserted
    in the code.) Now, let''s continue defining the remaining macros:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对数学进行一些思考之后，应该很明显，`_X`应该定义为`(threadIdx.x + blockIdx.x * blockDim.x)`，而`_Y`应该定义为`(threadIdx.y
    + blockIdx.y * blockDim.y)`。（添加括号是为了在宏插入代码时不会干扰运算顺序。）现在，让我们继续定义剩余的宏：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `_WIDTH` and `_HEIGHT` macros will give us the width and height of our cell
    lattice, respectively, which should be clear from the diagrams. Let's discuss
    the `_XM` and `_YM` macros. In our implementation of LIFE, we'll have the endpoints
    "wrap around" to the other side of the lattice—for example, we'll consider the
    *x*-value of `-1` to be `_WIDTH - 1`, and a *y*-value of `-1` to be `_HEIGHT -
    1`, and we'll likewise consider an *x*-value of `_WIDTH` to be `0` and a *y*-value
    of `_HEIGHT` to be `0`. Why do we need this? When we calculate the number of living
    neighbors of a given cell, we might be at some edge and the neighbors might be
    external points—defining these macros to modulate our points will cover this for
    us automatically. Notice that we have to add the width or height before we use
    C's modulus operator—this is because, unlike Python, the modulus operator in C
    can return negative values for integers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`_WIDTH`和`_HEIGHT`宏将分别给出我们细胞晶格的宽度和高度，这应该从图中很清楚。让我们讨论`_XM`和`_YM`宏。在我们的LIFE实现中，我们将端点“环绕”到晶格的另一侧——例如，我们将*x*-值为`-1`视为`_WIDTH
    - 1`，将*y*-值为`-1`视为`_HEIGHT - 1`，同样，我们将*x*-值为`_WIDTH`视为`0`，将*y*-值为`_HEIGHT`视为`0`。为什么我们需要这样做？当我们计算给定细胞的活邻居数量时，我们可能处于某个边缘，邻居可能是外部点——定义这些宏来调制我们的点将自动为我们解决这个问题。请注意，在使用C的取模运算符之前，我们必须添加宽度或高度——这是因为与Python不同，C中的取模运算符对于整数可以返回负值。'
- en: 'We now have one final macro to define. We recall that PyCUDA passes two-dimensional
    arrays into CUDA C as one-dimensional pointers; two-dimensional arrays are passed
    in **row-wise** from Python into one dimensional C pointers. This means that we''ll
    have to translate a given Cartesian (*x*,*y*) point for a given cell on the lattice
    into a one dimensional point within the pointer corresponding to the lattice.
    Here, we can do so as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个最终的宏需要定义。我们回忆一下，PyCUDA将二维数组作为一维指针传递给CUDA C；二维数组从Python中以行向量的方式传递到一维C指针。这意味着我们需要将晶格上给定细胞的笛卡尔(*x*,
    *y*)点转换为指针对应的一维点。在这里，我们可以这样做：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since our cell lattice is stored row-wise, we have to multiply the *y*-value
    by the width to offset to the point corresponding to the appropriate row. We can
    now finally begin with our implementation of LIFE. Let''s start with the most
    important part of LIFE—counting the number of living neighbors a given cell has.
    We''ll implement this using a CUDA **device function**, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的细胞晶格是按行存储的，我们必须将*y*-值乘以宽度来偏移到对应行的点。现在，我们最终可以开始实现LIFE了。让我们从LIFE最重要的部分开始——计算给定细胞有多少活邻居。我们将使用CUDA
    **设备函数**来实现这一点，如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A device function is a C function written in serial, which is called by an individual
    CUDA thread in kernel. That is to say, this little function will be called in
    parallel by multiple threads from our kernel. We'll represent our cell lattice
    as a collection of 32-bit integers (1 will represent a living cell and 0 will
    represent a dead one), so this will work for our purposes; we just have to add
    the values of the neighbors around our current cell.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 设备函数是一个串行编写的C函数，由内核中的单个CUDA线程调用。也就是说，这个小程序将在我们的内核中由多个线程并行调用。我们将我们的细胞晶格表示为32位整数的集合（1表示活细胞，0表示死细胞），因此这适用于我们的目的；我们只需添加我们当前细胞周围邻居的值。
- en: A CUDA **device function** is a serial C function that is called by an individual
    CUDA thread from within a kernel. While these functions are serial in themselves,
    they can be run in parallel by multiple GPU threads. Device functions cannot by
    themselves by launched by a host computer onto a GPU, only kernels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA **设备函数** 是一个串行C函数，它由内核中的单个CUDA线程调用。虽然这些函数本身是串行的，但它们可以通过多个GPU线程并行运行。设备函数不能由主机计算机直接在GPU上启动，只能通过内核。
- en: 'We are now prepared to write our kernel implementation of LIFE. Actually, we''ve
    done most of the hard work already—we check the number of neighbors of the current
    thread''s cell, check whether the current cell is living or dead, and then use
    the appropriate switch-case statements to determine its status for the next iteration
    according to the rules of LIFE. We''ll use two integer pointer arrays for this
    kernel—one will be in reference to the last iteration as input (`lattice`) and
    the other in reference to the iteration that we''ll calculate as output (`lattice_out`):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备编写LIFE的内核实现。实际上，我们已经完成了大部分艰苦的工作——我们检查当前线程单元的邻居数量，检查当前单元是活着还是死亡，然后使用适当的switch-case语句根据LIFE的规则确定其在下一次迭代中的状态。我们将为此内核使用两个整数指针数组：一个将引用上一次迭代的输入（`lattice`），另一个将引用我们将计算的输出迭代（`lattice_out`）：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We remember to close off the inline CUDA C segment with the triple-parentheses,
    and then get a reference to our CUDA C kernel with `get_function`. Since the kernel
    will only update the lattice once, we''ll set up a short function in Python that
    will cover for all of the overhead of updating the lattice for the animation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记得使用三重括号关闭内联CUDA C段，然后使用`get_function`获取我们的CUDA C内核的引用。由于内核只会更新一次晶格，我们将在Python中设置一个简短的功能，以覆盖更新晶格动画的所有开销：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `frameNum` parameter is just a value that is required by Matplotlib's animation
    module for update functions that we can ignore, while `img` will be the representative
    image of our cell lattice that is required by the module that will be iteratively
    displayed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`frameNum`参数只是Matplotlib动画模块更新函数所需的一个值，我们可以忽略它，而`img`将是我们的单元晶格的代表性图像，这是模块迭代显示所必需的。'
- en: Let's focus on the other three remaining parameters—`newLattice_gpu` and `lattice_gpu`
    will be PyCUDA arrays that we'll keep persistent, as we want to avoid re-allocating
    chunks of memory on the GPU when we can. `lattice_gpu` will be the current generation
    of the cell array that will correspond to the `lattice` parameter in the kernel,
    while `newLattice_gpu` will be the next generation of the lattice. `N` will indicate
    the the height and width of the lattice (in other words, we'll be working with
    an *N x N* lattice).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注剩下的三个参数—`newLattice_gpu`和`lattice_gpu`将是我们将保持持久的PyCUDA数组，因为我们希望在可能的情况下避免在GPU上重新分配内存块。`lattice_gpu`将是单元阵列的当前一代，它将对应于内核中的`lattice`参数，而`newLattice_gpu`将是晶格的下一代。`N`将表示晶格的高度和宽度（换句话说，我们将处理一个*N
    x N*的晶格）。
- en: 'We launch the kernel with the appropriate parameters and set the block and
    grid sizes as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用适当的参数启动内核，并设置块和网格大小如下：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We'll set the block sizes as 32 x 32 with `(32, 32, 1)`; since we are only using
    two dimensions for our cell lattice, we can just set the *z*-dimension as one.
    Remember that blocks are limited to 1,024 threads—*32 x 32 = 1024*, so this will
    work. (Keep in mind that there is nothing special here about 32 x 32; we could
    use values such as 16 x 64 or 10 x 10 if we wanted to, as long as the total number
    of threads does not exceed 1,024.)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置块大小为32 x 32，使用`(32, 32, 1)`；由于我们只使用两个维度来构建我们的单元晶格，因此可以将*z*-维度设置为1。请记住，块的大小限制为1,024个线程—*32
    x 32 = 1024*，所以这将是可行的。（请注意，这里32 x 32并没有什么特殊之处；如果我们想的话，可以使用16 x 64或10 x 10这样的值，只要线程总数不超过1,024即可。）
- en: The number of threads in a CUDA block is limited to a maximum of 1,024.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA块中的线程数限制为最多1,024个。
- en: We now look at grid value—here, since we are working with dimensions of 32,
    it should be clear that *N* (in this case) should be divisible by 32\. That means
    that in this case, we are limited to lattices such as 64 x 64, 96 x 96, 128 x
    128, and 1024 x 1024\. Again, if we want to use lattices of a different size,
    then we'll have to alter the dimensions of the blocks. (If this doesn't make sense,
    then please look at the previous diagrams and review how we defined the width
    and height macros in our kernel.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看网格值——在这里，由于我们正在处理32的维度，应该很明显，*N*（在这种情况下）应该是32的倍数。这意味着在这种情况下，我们限制在64 x
    64、96 x 96、128 x 128和1024 x 1024这样的晶格。再次强调，如果我们想使用不同大小的晶格，那么我们必须改变块的大小。（如果这还不清楚，请查看之前的图表并回顾我们在内核中定义的宽度和高度宏。）
- en: 'We can now set up the image data for our animation after grabbing the latest
    generated lattice from the GPU''s memory with the `get()` function. We finally
    copy the new lattice data into the current data using the PyCUDA slice operator,
    `[:]`, which will copy over the previously allocated memory on the GPU so that
    we don''t have to re-allocate:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`get()`函数从GPU内存中获取最新生成的晶格后，设置我们的动画图像数据。我们最终使用PyCUDA切片操作符`[:]`将新的晶格数据复制到当前数据中，这将复制之前在GPU上分配的内存，这样我们就不需要重新分配：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s set up a lattice of size 256 x 256\. We now will set up an initial state
    for our lattice using the choice function from the `numpy.random` module. We''ll
    populate a *N* x *N* graph of integers randomly with ones and zeros; generally,
    if around 25% of the points are ones and the rest zeros, we can generate some
    interesting lattice animations, so we''ll go with that:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个256 x 256大小的晶格。我们现在将使用`numpy.random`模块中的选择函数为我们的晶格设置一个初始状态。我们将使用随机数填充一个*N*
    x *N*的整数图，一和零；一般来说，如果大约25%的点是一，其余的是零，我们可以生成一些有趣的晶格动画，所以我们就这样做：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can set up the lattices on the GPU with the appropriate `gpuarray`
    functions and set up the Matplotlib animation accordingly, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用适当的`gpuarray`函数在GPU上设置晶格，并相应地设置Matplotlib动画，如下所示：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now run our program and enjoy the show (the code is also available as
    the `conway_gpu.py` file under the `4` directory in the GitHub repository):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行我们的程序并享受表演（代码也作为`conway_gpu.py`文件在GitHub仓库的`4`目录下提供）：
- en: '![](img/bb012845-31d4-4511-a697-9eef0e2772b2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bb012845-31d4-4511-a697-9eef0e2772b2.png)'
- en: Thread synchronization and intercommunication
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程同步和交互
- en: We'll now discuss two important concepts in GPU programming—**thread synchronization**
    and **thread intercommunication**. Sometimes, we need to ensure that every single
    thread has reached the same exact line in the code before we continue with any
    further computation; we call this thread synchronization. Synchronization works
    hand-in-hand with thread intercommunication, that is, different threads passing
    and reading input from each other; in this case, we'll usually want to make sure
    that all of the threads are aligned at the same step in computation before any
    data is passed around. We'll start here by learning about the CUDA `__syncthreads`
    device function, which is used for synchronizing a single block in a kernel.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论GPU编程中的两个重要概念——**线程同步**和**线程交互**。有时，在继续任何进一步的计算之前，我们需要确保每个线程都达到了代码中的同一确切行；我们称之为线程同步。同步与线程交互协同工作，即不同的线程相互传递和读取输入；在这种情况下，我们通常想要确保在传递任何数据之前，所有线程都在计算中的同一步骤上对齐。我们将从这里开始，了解CUDA
    `__syncthreads`设备函数，它用于同步内核中的单个块。
- en: Using the __syncthreads() device function
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`__syncthreads()`设备函数
- en: In our prior example of Conway's *Game of Life*, our kernel only updated the
    lattice once for every time it was launched by the host. There are no issues with
    synchronizing all of the threads among the launched kernel in this case, since
    we only had to work with the lattice's previous iteration that was readily available.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的康威生命游戏示例中，我们的内核在每次由主机启动时只更新晶格一次。在这种情况下，同步所有已启动内核中的线程没有问题，因为我们只需要处理晶格的先前迭代，这是现成的。
- en: Now let's suppose that we want to do something slightly different—we want to
    re-write our kernel so that it performs a certain number of iterations on a given
    cell lattice without being re-launched over and over by the host. This may initially
    seem trivial—a naive solution would be to just put an integer parameter to indicate
    the number of iterations and a `for` loop in the inline `conway_ker` kernel, make
    some additional trivial changes, and be done with it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设我们想要做一些稍微不同的事情——我们想要重新编写我们的内核，使其在给定的细胞晶格上执行一定数量的迭代，而不需要主机反复重新启动。这最初可能看起来很微不足道——一个简单的解决方案可能就是添加一个整数参数来指示迭代次数，并在内联`conway_ker`内核中的`for`循环中，做一些额外的简单更改，然后完成。
- en: However, this raises the issue of **race conditions**; this is the issue of
    multiple threads reading and writing to the same memory address and the problems
    that may arise from that. Our old `conway_ker` kernel avoids this issue by using
    two arrays of memory, one that is strictly read from, and one that is strictly
    written to for each iteration. Furthermore, since the kernel only performs a single
    iteration, we are effectively using the host for the synchronization of the threads.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这引发了**竞态条件**的问题；这是多个线程读取和写入相同内存地址以及由此可能产生的问题。我们的旧 `conway_ker` 内核通过使用两个内存数组来避免这个问题，一个数组严格用于读取，另一个数组严格用于每个迭代的写入。此外，由于内核只执行单个迭代，我们实际上是在使用主机来同步线程。
- en: We want to do multiple iterations of LIFE on the GPU that are fully synchronized;
    we also will want to use a single array of memory for the lattice. We can avoid
    race conditions by using a CUDA device function called `__syncthreads()`. This
    function is a **block level synchronization barrier**—this means that every thread
    that is executing within a block will stop when it reaches a `__syncthreads()`
    instance and wait until each and every other thread within the same block reaches
    that same invocation of `__syncthreads()` before the the threads continue to execute
    the subsequent lines of code.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在 GPU 上进行多个完全同步的 LIFE 迭代；我们还将希望使用单个内存数组来表示格。我们可以通过使用名为 `__syncthreads()`
    的 CUDA 设备函数来避免竞态条件。这个函数是一个**块级同步屏障**——这意味着当线程遇到 `__syncthreads()` 实例时，它将停止执行，并等待同一块内的每个其他线程都达到相同的
    `__syncthreads()` 调用，然后线程才会继续执行后续的代码行。
- en: '`__syncthreads()` can only synchronize threads within a single CUDA block,
    not all threads within a CUDA grid!'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`__syncthreads()` 只能在单个 CUDA 块内同步线程，而不能在 CUDA 网格内的所有线程！'
- en: 'Let''s now create our new kernel; this will be a modification of the prior
    LIFE kernel that will perform a certain number of iterations and then stop. This
    means we''ll not represent this as an animation, just as a static image, so we''ll
    load the appropriate Python modules in the beginning. (This code is also available
    in the `conway_gpu_syncthreads.py` file, in the GitHub repository):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的新内核；这将是对先前 LIFE 内核的修改，它将执行一定数量的迭代然后停止。这意味着我们不会将其表示为动画，而是一个静态图像，所以我们在开始时将加载适当的
    Python 模块。（此代码也位于 GitHub 仓库中的 `conway_gpu_syncthreads.py` 文件中）：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s again set up our kernel that will compute LIFE:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次设置我们的内核，该内核将计算 LIFE：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Of course, our CUDA C code will go here, which will be largely the same as
    before. We''ll have to only make some changes to our kernel. Of course, we can
    preserve the device function, `nbrs`. In our declaration, we''ll use only one
    array to represent the cell lattice. We can do this since we''ll be using proper
    thread synchronization. We''ll also have to indicate the number of iterations
    with an integer. We set the parameters as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的 CUDA C 代码将在这里，这将与之前大致相同。我们只需对我们的内核做一些修改。当然，我们可以保留设备函数 `nbrs`。在我们的声明中，我们将只使用一个数组来表示细胞格。我们可以这样做，因为我们将会使用适当的线程同步。我们还需要用一个整数来指示迭代次数。我们设置参数如下：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We''ll continue similarly as before, only iterating with a `for` loop:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续像之前一样进行，只是使用 `for` 循环进行迭代：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s recall that previously, we directly set the new cell lattice value directly
    within the array. Here, we''ll hold the value in the `cell_value` variable until
    all of the threads in the block are synchronized. We proceed similarly as before,
    blocking execution with `__syncthreads` until all of the new cell values are determined
    for the current iteration, and only then setting the values within the lattice
    array:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下，之前我们直接在数组中设置新的细胞格值。这里，我们将保留该值在 `cell_value` 变量中，直到块内的所有线程都同步。我们继续像之前一样进行，使用
    `__syncthreads` 阻塞执行，直到当前迭代的所有新细胞值都确定，然后才在格子数组中设置值：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We''ll now launch the kernel as before and display the output, iterating over
    the lattice 1,000,000 times. Note that we are using only a single block in our
    grid, which is of a size of 32 x 32, due to the limit of 1,024 threads per block.
    (Again, it should be emphasized that `__syncthreads` only works over all threads
    in a block, rather than over all threads in a grid, which is why we are limiting
    ourselves to a single block here):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将像之前一样启动内核并显示输出，对晶格进行1,000,000次迭代。请注意，由于每个块中线程数量的限制为1,024，我们只使用我们的网格中的一个块，其大小为32
    x 32。（再次强调，`__syncthreads`仅在块中的所有线程上工作，而不是在整个网格上的所有线程上，这就是为什么我们在这里限制自己只使用一个块的原因）：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we run the program, we''ll get the desired output as follows (this is
    what a random LIFE lattice will converge to after one million iterations!):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行程序时，我们将得到以下所需的输出（这是随机LIFE晶格在经过一百万次迭代后收敛的结果！）：
- en: '![](img/38be0537-84a4-447c-a25b-0f60c15726b1.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/38be0537-84a4-447c-a25b-0f60c15726b1.png)'
- en: Using shared memory
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用共享内存
- en: We can see from the prior example that the threads in the kernel can intercommunicate
    using arrays within the GPU's global memory; while it is possible to use global
    memory for most operations, we can speed things up by using **shared memory**.
    This is a type of memory meant specifically for intercommunication of threads
    within a single CUDA block; the advantage of using this over global memory is
    that it is much faster for pure inter-thread communication. In contrast to global
    memory, though, memory stored in shared memory cannot directly be accessed by
    the host—shared memory must be copied back into global memory by the kernel itself
    first.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的例子中我们可以看到，内核中的线程可以通过GPU全局内存中的数组进行相互通信；虽然对于大多数操作可以使用全局内存，但我们可以通过使用**共享内存**来加快速度。这是一种专门用于单个CUDA块内线程间通信的内存类型；使用它的优点是它对于纯线程间通信要快得多。然而，与全局内存相比，存储在共享内存中的内存不能直接被主机访问——共享内存必须首先由内核本身复制回全局内存。
- en: 'Let''s first step back for a moment before we continue and think about what
    we mean. Let''s look at some of the variables that are declared in our iterative
    LIFE kernel that we just saw. Let''s first look at `x` and `y`, two integers that
    hold the Cartesian coordinates of a particular thread''s cell. Remember that we
    are setting their values with the `_X` and `_Y` macros. (Compiler optimizations
    notwithstanding, we want to store these values in variables to reduce computation
    because directly using `_X` and `_Y` will recompute the `x` and `y` values every
    time these macros are referenced in our code):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们先退一步，思考一下我们的意思。让我们看看在我们刚才看到的迭代LIFE内核中声明的变量。让我们首先看看`x`和`y`，这两个整数保存了特定线程单元格的笛卡尔坐标。记住，我们使用`_X`和`_Y`宏设置它们的值。（尽管编译器优化除外，但我们希望将这些值存储在变量中以减少计算，因为直接使用`_X`和`_Y`将在我们的代码中每次引用这些宏时重新计算`x`和`y`的值）：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We note that, for every single thread, there will be a unique Cartesian point
    in the lattice that will correspond to `x` and `y`. Similarly, we use a variable,
    `n`, which is declared with `int n = nbrs(x, y, lattice);`, to indicate the number
    of living neighbors around a particular cell. This is because, when we normally
    declare variables in CUDA, they are by default local to each individual thread.
    Note that, even if we declare an array within a thread such as `int a[10];`, there
    will be an array of size 10 that is local to each thread.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，对于每个单独的线程，晶格中都有一个唯一的笛卡尔点，它将对应于`x`和`y`。同样，我们使用一个变量`n`，它声明为`int n = nbrs(x,
    y, lattice);`，来指示特定单元格周围的活细胞数量。这是因为，当我们通常在CUDA中声明变量时，它们默认是每个单独线程的局部变量。请注意，即使我们在线程内部声明一个数组，如`int
    a[10];`，也将在每个线程中有一个大小为10的局部数组。
- en: Local thread arrays (for example, a declaration of `int a[10];` within the kernel)
    and pointers to global GPU memory (for example, a value passed as a kernel parameter
    of the form `int * b`) may look and act similarly, but are very different. For
    every thread in the kernel, there will be a separate `a` array that the other
    threads cannot read, yet there is a single `b` that will hold the same values
    and be equally accessible for all of the threads.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 局部线程数组（例如，内核中`int a[10];`的声明）和指向全局GPU内存的指针（例如，以`int * b`的形式作为内核参数传递的值）看起来和表现可能相似，但实际上非常不同。对于内核中的每个线程，将有一个单独的`a`数组，其他线程无法读取，但有一个单一的`b`将持有相同的值，并且对所有线程都是同等可访问的。
- en: We are prepared to use shared memory. This allows us to declare variables and
    arrays that are shared among the threads within a single CUDA block. This memory
    is much faster than using global memory pointers (as we have been using till now),
    as well as reduces the overhead of allocating memory in the case of pointers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备使用共享内存。这允许我们在单个CUDA块内的线程之间声明共享变量和数组。这种内存比使用全局内存指针（如我们至今所使用的）要快得多，同时减少了分配内存的开销。
- en: 'Let''s say we want a shared integer array of size 10\. We declare it as follows—`__shared__
    int a[10]` . Note that we don''t have to limit ourselves to arrays; we can make
    shared singleton variables as follows: `__shared__ int x`.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要一个大小为10的共享整数数组。我们声明如下——`__shared__ int a[10]`。请注意，我们不必局限于数组；我们可以创建共享单例变量，如下所示：`__shared__
    int x`。
- en: 'Let''s rewrite a few lines of iterative version of LIFE that we saw in the
    last sub-section to make use of shared memory. First, let''s just rename the input
    pointer to `p_lattice`, so we can instead use this variable name on our shared
    array, and lazily preserve all of the references to " lattice" in our code. Since
    we''ll be sticking with a 32 x 32 cell lattice here, we set up the new shared
    `lattice` array as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写上一小节中看到的LIFE迭代版本的几行代码，以利用共享内存。首先，让我们将输入指针重命名为`p_lattice`，这样我们就可以在我们的共享数组上使用这个变量名，并在我们的代码中懒加载所有对“lattice”的引用。由于我们将坚持使用32
    x 32单元格的晶格，我们按照以下方式设置新的共享`lattice`数组：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We''ll now have to copy all values from the global memory `p_lattice` array
    into `lattice`. We''ll index our shared array exactly in the same way, so we can
    just use our old `_INDEX` macro here. Note that we make sure to put `__syncthreads()`
    after we copy, to ensure that all of the memory accesses to lattice are entirely
    completed before we proceed with the LIFE algorithm:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将所有值从全局内存`p_lattice`数组复制到`lattice`。我们将以完全相同的方式索引我们的共享数组，因此我们只需在这里使用我们旧的`_INDEX`宏。请注意，我们在复制后确保放置`__syncthreads()`，以确保在继续LIFE算法之前，所有对晶格的内存访问都已完全完成：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The rest of the kernel is exactly as before, only we have to copy from the
    shared lattice back into the GPU array. We do so as follows and then close off
    the inline code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 内核的其余部分与之前完全相同，只是我们需要将共享晶格复制回GPU数组。我们这样做，然后关闭内联代码：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can now run this as before, with the same exact test code. (This example
    can be seen in `conway_gpu_syncthreads_shared.py` in the GitHub repository.)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以像以前一样运行它，使用相同的测试代码。（这个例子可以在GitHub仓库中的`conway_gpu_syncthreads_shared.py`中看到。）
- en: The parallel prefix algorithm
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行前缀算法
- en: We'll now be using our new knowledge of CUDA kernels to implement the **parallel
    prefix algorithm**, also known as the **scan design pattern**. We have already
    seen simple examples of this in the form of PyCUDA's `InclusiveScanKernel` and
    `ReductionKernel` functions in the previous chapter. We'll now look into this
    idea in a little more detail.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们对CUDA内核的新知识来实现**并行前缀算法**，也称为**扫描设计模式**。我们已经在上一章中看到了这种算法的简单示例，即PyCUDA的`InclusiveScanKernel`和`ReductionKernel`函数。现在我们将更详细地探讨这个想法。
- en: The central motivation of this design pattern is that we have a binary operator
    ![](img/9388a619-6713-4ea7-93d8-a85fc2fd8094.png) , that is to say a function
    that acts on two input values and gives one output value (such as—+, ![](img/362dcb88-5323-4213-8ea4-03a9785d4984.png),
    ![](img/2abe6459-7144-4bdf-9569-b0a70726e422.png) (maximum), ![](img/380e1f66-b930-42d9-b7d0-a565b74b858f.png)
    (minimum)), and collection of elements, ![](img/d10897a0-55d1-4a8f-9862-fd43a6f729ea.png),
    and from these we wish to compute ![](img/d1dace09-f460-4cee-abb6-81691be4dcf6.png)
    efficiently. Furthermore, we make the assumption that our binary operator ![](img/2c1163a9-8a0f-480d-be93-f5cbaa606034.png)
    is **associative**—this means that, for any three elements, *x*, *y*, and *z*,
    we always have:![](img/7f9f94dd-6751-4a0e-abcc-32333a71812d.png) .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计模式的核心动机是我们有一个二元运算符 ![二元运算符](img/9388a619-6713-4ea7-93d8-a85fc2fd8094.png)，也就是说一个作用于两个输入值并给出一个输出值的函数（例如—+,
    ![二元运算符](img/362dcb88-5323-4213-8ea4-03a9785d4984.png), ![二元运算符](img/2abe6459-7144-4bdf-9569-b0a70726e422.png)（最大值），![二元运算符](img/380e1f66-b930-42d9-b7d0-a565b74b858f.png)（最小值）），以及元素集合
    ![二元运算符](img/d10897a0-55d1-4a8f-9862-fd43a6f729ea.png)，并且从这些中我们希望高效地计算 ![二元运算符](img/d1dace09-f460-4cee-abb6-81691be4dcf6.png)。此外，我们假设我们的二元运算符
    ![二元运算符](img/2c1163a9-8a0f-480d-be93-f5cbaa606034.png) 是**结合律**的——这意味着，对于任何三个元素，*x*，*y*
    和 *z*，我们总是有：![二元运算符](img/7f9f94dd-6751-4a0e-abcc-32333a71812d.png)。
- en: We wish to retain the partial results, that is the *n - 1* sub-computations—![](img/cadd1c5c-4dfa-45f8-bca5-e3e810c6187b.png).
    The aim of the parallel prefix algorithm is to produce this collection of *n*
    sums efficiently. It normally takes *O(n)* time to produce these *n* sums in a
    serial operation, and we wish to reduce the time complexity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望保留部分结果，即 *n - 1* 个子计算——![图片](img/cadd1c5c-4dfa-45f8-bca5-e3e810c6187b.png)。并行前缀算法的目的是有效地生成这个包含
    *n* 个和的集合。在串行操作中生成这些 *n* 个和通常需要 *O(n)* 的时间，而我们希望降低时间复杂度。
- en: When the terms "parallel prefix" or "scan" are used, it usually means an algorithm
    that produces all of these *n* results, while "reduce"/"reduction" usually means
    an algorithm that only yields the single final result, ![](img/864d06dd-ccd8-48c3-8a0a-fd437cd6436e.png).
    (This is the case with PyCUDA.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用“并行前缀”或“扫描”这些术语时，通常指的是一种生成所有这些 *n* 个结果的算法，而“归约”/“归约”通常指的是只产生单个最终结果的算法，![图片](img/864d06dd-ccd8-48c3-8a0a-fd437cd6436e.png)。
    (这是 PyCUDA 的情况。)
- en: There are actually several variations of the parallel prefix algorithm, and
    we'll first start with the simplest (and oldest) version first, which is called
    the naive parallel prefix algorithm.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，并行前缀算法有几种变体，我们将首先从最简单（也是最古老）的版本开始，这被称为朴素并行前缀算法。
- en: The naive parallel prefix algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素并行前缀算法
- en: 'The **naive parallel prefix algorithm** is the original version of this algorithm;
    this algorithm is "naive" because it makes an assumption that given *n* input
    elements, ![](img/2a2cf114-8fcf-41ec-83de-66803d5c7345.png), with the further
    assumption that *n* is *dyadic* (that is, ![](img/f23d2bc1-c7c4-44fc-a947-ac7571697516.png)
    for some positive integer, *k*), and we can run the algorithm in parallel over
    *n* processors (or *n* threads). Obviously, this will impose strong limits on
    the cardinality *n* of sets that we may process. However, given these conditions
    are satisfied, we have a nice result in that its computational time complexity
    is only *O(log n)*. We can see this from the pseudocode of the algorithm. Here,
    we''ll indicate the input values with ![](img/79ff2d6e-25d9-478d-8a3c-5fe79faa77cf.png)
    and the output values as ![](img/be8218cd-e2fd-4453-87fd-50f3cf71308c.png):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素并行前缀算法**是这个算法的原始版本；这个算法被称为“朴素”，因为它假设给定 *n* 个输入元素，![图片](img/2a2cf114-8fcf-41ec-83de-66803d5c7345.png)，并且进一步假设
    *n* 是二进制的（即，![图片](img/f23d2bc1-c7c4-44fc-a947-ac7571697516.png) 对于某个正整数 *k*），并且我们可以在
    *n* 个处理器（或 *n* 个线程）上并行运行算法。显然，这将对我们可能处理的集合的基数 *n* 施加严格的限制。然而，在这些条件得到满足的情况下，我们有一个很好的结果，即其计算时间复杂度仅为
    *O(log n)*。我们可以从算法的伪代码中看到这一点。在这里，我们将用 ![图片](img/79ff2d6e-25d9-478d-8a3c-5fe79faa77cf.png)
    来表示输入值，用 ![图片](img/be8218cd-e2fd-4453-87fd-50f3cf71308c.png) 来表示输出值：'
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, we can clearly see that this will take *O(log n)* asymptotic time, as the
    outer loop is parallelized over the `parfor` and the inner loop takes *log[2](n)*.
    It should be easy to see after a few minutes of thought that the *y[i]* values
    will yield our desired output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以清楚地看到这将需要 *O(log n)* 的渐近时间，因为外层循环在 `parfor` 上并行化，而内层循环需要 *log[2](n)*。经过几分钟的思考后，应该很容易看出
    *y[i]* 的值将产生我们期望的输出。
- en: Now let's begin our implementation; here, our binary operator will simply be
    addition. Since this example is illustrative, this kernel will be strictly over
    1,024 threads.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始我们的实现；在这里，我们的二进制运算符将是简单的加法。由于这个例子是说明性的，这个内核将严格地使用 1,024 个线程。
- en: 'Let''s just set up the header and dive right into writing our kernel:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置好标题，然后直接进入编写内核：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So, let''s look at what we have: we represent our input elements as a GPU array
    of doubles, that is `double *vec`, and represent the output values with `double
    *out`. We declare a shared memory `sum_buf` array that we''ll use for the calculation
    of our output. Now, let''s look at the implementation of the algorithm itself:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看我们有什么：我们用 GPU 数组 `double *vec` 来表示输入元素，用 `double *out` 来表示输出值。我们声明一个共享内存
    `sum_buf` 数组，我们将用它来计算我们的输出。现在，让我们看看算法本身的实现：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Of course, there is no `parfor,` which is implicit over the `tid` variable,
    which indicates the thread number. We are also able to omit the use of *log[2]*
    and *2^i* by starting with a variable that is initialized to 1, and then iteratively
    multiplying by 2 every iteration of i. (Note that if we want to be even more technical,
    we can do this with the bitwise shift operators .) We bound the iterations of
    `i` by 10, since *2^(10) = 1024*. Now we''ll close off our new kernel as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有 `parfor`，它是隐式地在 `tid` 变量上进行的，这表示线程号。我们还可以通过从初始化为 1 的变量开始，并在每次迭代 i 中迭代乘以
    2 来省略使用 *log[2]* 和 *2^i*。注意，如果我们想更加技术化，我们可以使用位运算符来完成这个操作。）我们将 `i` 的迭代次数限制为 10，因为
    *2^(10) = 1024*。现在我们将按照以下方式关闭我们的新内核：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s now look at the test code following the kernel:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看内核之后的测试代码：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We're only going to concern ourselves with the final sum in the output, which
    we retrieve with `outvec_gpu[-1].get()`, recalling that the "-1" index gives the
    last member of an array in Python. This will be the sum of every element in `vec`;
    the partial sums are in the prior values of `outvec_gpu`. (This example can be
    seen in the `naive_prefix.py` file in the GitHub repository.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心输出中的最终求和，我们通过 `outvec_gpu[-1].get()` 来检索它，回忆一下，在 Python 中 "-1" 索引给出数组的最后一个成员。这将
    `vec` 中每个元素的求和；部分和在 `outvec_gpu` 的先前值中。（这个例子可以在 GitHub 仓库中的 `naive_prefix.py`
    文件中看到。）
- en: By its nature, the parallel prefix algorithm has to run over *n* threads, corresponding
    to a size-n array, where *n* is dyadic (again, this means that *n* is some power
    of 2). However, we can extend this algorithm to an arbitrary non-dyadic size assuming
    that our operator has a **identity element** (or equivalently, **neutral element**)—that
    is to say, that there is some value *e* so that for any *x* value, we have—![](img/9a41546d-6056-4ca9-bfdd-0e9ec13ae195.png).
    In the case that our operator is + , the identity element is 0; in the case that
    it is ![](img/04590efc-4d27-477f-a7bb-96da2f050011.png), it is 1; all we do then
    is just pad the elements ![](img/f1e4ffaf-26a6-4f45-b9f4-20340837e38c.png) with
    a series of *e* values so that we have the a dyadic cardinality of the new set
    ![](img/8c22f00e-f896-44d2-99f2-7d5ecc3b35eb.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，并行前缀算法必须运行在 *n* 个线程上，对应于大小为 *n* 的数组，其中 *n* 是二进制的（再次强调，这意味着 *n* 是 2 的某个幂）。然而，如果我们假设我们的算子有一个**恒等元素**（或者说，**中性元素**），即对于任何
    *x* 值，都有—![](img/9a41546d-6056-4ca9-bfdd-0e9ec13ae195.png)。在这种情况下，如果我们的算子是 +，则恒等元素是
    0；如果它是 ![](img/04590efc-4d27-477f-a7bb-96da2f050011.png)，则恒等元素是 1；我们只是用一系列 *e*
    值填充元素 ![](img/f1e4ffaf-26a6-4f45-b9f4-20340837e38c.png)，以便我们有一个新的集合的二进制基数 ![](img/8c22f00e-f896-44d2-99f2-7d5ecc3b35eb.png)。
- en: Inclusive versus exclusive prefix
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包含与排除前缀
- en: Let's stop for a moment and make a very subtle, but very important distinction.
    So far, we have been concerned with taking inputs of the form ![](img/cb300a9a-6dff-4df9-b962-91cb62ccd143.png)
    , and as output producing an array of sums of the form ![](img/244e529c-37f3-46ba-a492-b34765482abb.png).
    Prefix algorithms that produce output as such are called **inclusive**; in the
    case of an **inclusive prefix algorithm**, the corresponding element at each index
    is included in the summation in the same index of the output array. This is in
    contrast to prefix algorithms that are **exclusive**. An **exclusive prefix algorithm**
    differs in that it similarly takes *n* input values of the form ![](img/1bba2db0-de4c-42d3-a676-d482b67584c2.png)
    and produces the length-*n* output array ![](img/c789f125-403d-4264-b4f4-a57d73c11977.png).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，做一个非常微妙但非常重要的区分。到目前为止，我们一直关注的是接受形式为 ![](img/cb300a9a-6dff-4df9-b962-91cb62ccd143.png)
    的输入，并输出形式为 ![](img/244e529c-37f3-46ba-a492-b34765482abb.png) 的数组。产生这种输出的前缀算法被称为**包含的**；在**包含前缀算法**的情况下，输出数组中每个索引对应的元素都包含在相同索引的求和操作中。这与**排除前缀算法**形成对比。**排除前缀算法**的不同之处在于，它同样接受形式为
    ![](img/1bba2db0-de4c-42d3-a676-d482b67584c2.png) 的 *n* 个输入值，并产生长度为 *n* 的输出数组
    ![](img/c789f125-403d-4264-b4f4-a57d73c11977.png)。
- en: This is important because some efficient variations of the prefix algorithm
    are exclusive by their nature. We'll see an example of one in the next sub-section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为前缀算法的一些高效变体本质上都是**排他的**。我们将在下一小节中看到一个例子。
- en: Note that the exclusive algorithm yields nearly the same output as the inclusive
    algorithm, only it is right-shifted and omits the final value. We can therefore
    trivially obtain the equivalent output from either algorithm, provided we keep
    a copy of ![](img/179c86ba-3a68-4b15-98bf-24d9e930e963.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，独占算法产生的输出几乎与包含算法相同，只是它向右移动并省略了最终值。因此，只要我们保留 ![](img/179c86ba-3a68-4b15-98bf-24d9e930e963.png)
    的副本，我们就可以从任一算法中轻易地获得等效的输出。
- en: A work-efficient parallel prefix algorithm
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效并行前缀算法
- en: Before we continue with our new algorithm, we'll look at the naive algorithm
    from two perspectives. In an ideal case, the computational time complexity is
    *O(log n)*, but this is only when we have a sufficient number of processors for
    our data set; when the cardinality (number of elements) of our dataset, *n*, is
    much larger than the number of processors, we have that this becomes an *O(n log
    n)* time algorithm.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续介绍新的算法之前，我们将从两个角度来审视朴素算法。在理想情况下，计算时间复杂度为 *O(log n)*，但这仅在我们有足够数量的处理器来处理我们的数据集时成立；当我们的数据集的基数（元素数量）*n*远大于处理器数量时，这将成为一个
    *O(n log n)* 的时间算法。
- en: Let's define a new concept with relation to our binary operator ![](img/b89f2449-e123-451e-a958-6a782b932821.png)—the
    **work** performed by a parallel algorithm here is the number of invocations of
    this operator across all threads for the duration of the execution. Similarly,
    the **span** is the number of invocations a thread makes in the duration of execution
    of the kernel; while the **span** of the whole algorithm is the same as the longest
    span among each individual thread, which will tell us the total execution time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个与我们的二元运算符 ![](img/b89f2449-e123-451e-a958-6a782b932821.png) 相关的新概念——这里的**工作**是由并行算法在执行期间所有线程对该运算符的调用次数。同样，**跨度**是线程在内核执行期间进行的调用次数；而整个算法的**跨度**与每个单独线程中最长的跨度相同，这将告诉我们总的执行时间。
- en: We seek to specifically reduce the amount of work performed by the algorithm
    across all threads, rather than focus merely span. In the case of the naive prefix,
    the additional work that is required costs a more time when the number of available
    processors falls short; this extra work will just spill over into the limited
    number of processors available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求具体减少算法在所有线程上执行的工作量，而不仅仅是关注跨度。在朴素前缀的情况下，当可用的处理器数量不足时，所需额外的工作会花费更多的时间；这额外的工作将溢出到有限的处理器数量中。
- en: We'll present a new algorithm that is **work efficient**, and hence more suitable
    for a limited number of processors. This consists of two separate two distinct
    parts—the **up-sweep (or reduce) phase** and the **down-sweep phase**. We should
    also note the algorithm we'll see is an exclusive prefix algorithm.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一个新的算法，它是**高效工作的**，因此更适合有限数量的处理器。这由两个独立的部分组成——**上推阶段（或归约）**和**下推阶段**。我们还应该注意，我们将看到的算法是一个独占前缀算法。
- en: The **up-sweep phase** is similar to a single reduce operation to produce the
    value that is given by the reduce algorithm, that is ![](img/f380e618-5246-4853-b1eb-976537a28ab6.png)
    ; in this case we retain the partial sums (![](img/b739e65c-64e0-4a07-ae84-740401252763.png))
    that are required the achieve the end result. The down-sweep phase will then operate
    on these partial sums and give us the final result. Let's look at some pseudocode,
    starting with the up-sweep phase. (The next subsection will then dive into the
    implementation from the pseudocode immediately.)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**上推阶段**类似于一个单一的归约操作，以产生由归约算法给出的值，即 ![](img/f380e618-5246-4853-b1eb-976537a28ab6.png)
    ；在这种情况下，我们保留所需的局部和(![](img/b739e65c-64e0-4a07-ae84-740401252763.png))以实现最终结果。然后，下推阶段将操作这些局部和，并给出最终结果。让我们看看一些伪代码，从上推阶段开始。（接下来的小节将立即从伪代码中深入到实现。）'
- en: Work-efficient parallel prefix (up-sweep phase)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效并行前缀（上推阶段）
- en: 'This is the pseudocode for the up-sweep. (Notice the `parfor` over the `j`
    variable, which means that this block of code can be parallelized over threads
    indexed by `j`):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '这是上推阶段的伪代码。（注意对 `j` 变量的 `parfor`，这意味着此代码块可以并行化到由 `j` 索引的线程上）： '
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Work-efficient parallel prefix (down-sweep phase)
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效并行前缀（下推阶段）
- en: 'Now let''s continue with the down-sweep, which will operate on the output of
    the up-sweep:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下推阶段，它将操作上推阶段的输出：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Work-efficient parallel prefix — implementation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效并行前缀 — 实现
- en: As a capstone for this chapter, we'll write an implementation of this algorithm
    that can operate on arrays of arbitrarily large size over 1,024\. This will mean
    that this will operate over grids as well as blocks; that being such, we'll have
    to use the host for synchronization; furthermore, this will require that we implement
    two separate kernels for up-sweep and down-sweep phases that will act as the `parfor`
    loops in both phases, as well as Python functions that will act as the outer `for`
    loop for the up- and down-sweeps.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的总结，我们将编写一个算法的实现，它可以操作超过1,024的任意大小的数组。这意味着这将操作网格以及块；因此，我们将不得不使用主机进行同步；此外，这需要我们实现两个单独的内核，用于up-sweep和down-sweep阶段，它们将作为两个阶段中的`parfor`循环，以及Python函数，将作为up-和down-sweep的外部`for`循环。
- en: 'Let''s begin with an up-sweep kernel. Since we''ll be iteratively re-launching
    this kernel from the host, we''ll also need a parameter that indicates current
    iteration (`k`). We''ll use two arrays for the computation to avoid race conditions—`x`
    (for the current iteration) and `x_old` (for the prior iteration). We declare
    the kernel as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从up-sweep内核开始。由于我们将从主机迭代重新启动此内核，我们还需要一个参数来指示当前迭代（`k`）。我们将使用两个数组进行计算以避免竞态条件——`x`（用于当前迭代）和`x_old`（用于前一个迭代）。我们如下声明内核：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let''s set the `tid` variable, which will be the current thread''s identification
    among *all* threads in *all* *blocks* in the grid. We use the same trick as in
    our original grid-level implementation of Conway''s *Game of Life* that we saw
    earlier:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置`tid`变量，它将在网格中所有块的所有线程中标识当前线程。我们使用与我们在前面看到的原始网格级实现康威的*生命游戏*相同的技巧：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We''ll now use C bit-wise shift operators to generate 2^k and 2^(k+1) directly
    from `k`. We now set `j` to be `tid` times `_2k1`—this will enable us to remove
    the "if `j` is divisible by 2^(k+1)", as in the pseudocode, enabling us to only
    launch as many threads as we''ll need:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用C位运算符直接从`k`生成2^k和2^(k+1)。我们现在将`j`设置为`tid`乘以`_2k1`——这将使我们能够移除伪代码中的"if
    `j` is divisible by 2^(k+1)"，从而只启动我们需要的线程数：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can easily generate dyadic (power-of-2) integers in CUDA C with the left
    bit-wise shift operator (`<<`). Recall that the integer 1 (that is 2⁰) is represented
    as 0001, 2 (2¹) is represented as 0010, 4 (2² ) is represented as 0100, and so
    on. We can therefore compute 2^k with the `1 << k` operation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用CUDA C中的左位运算符（`<<`）轻松生成二进制（2的幂）整数。回想一下，整数1（即2⁰）表示为0001，2（2¹）表示为0010，4（2²）表示为0100，依此类推。因此，我们可以通过`1
    << k`操作来计算2^k。
- en: 'We can now run the up-sweep phase with a single line, noting that `j` is indeed
    divisible by 2^(k+1) by its construction:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以用一行代码运行up-sweep阶段，注意到`j`确实可以被2^(k+1)整除，这是其构造决定的：
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We're done writing our kernel! But this is not a full implementation of the
    up-sweep, of course. We have to do the rest in Python. Let's get our kernel and
    begin the implementation. This mostly speaks for itself as it follows the pseudocode
    exactly; we should recall that we are updating `x_old_gpu` by copying from `x_gpu`
    using `[:]`, which will preserve the memory allocation and merely copy the new
    data over rather than re-allocate. Also note how we set our block and grid sizes
    depending on how many threads we have to launch—we try to keep our block sizes
    as multiples of size 32 (which is our rule-of-thumb in this text, we go into the
    details why we use 32 specifically in [Chapter 11](e853faad-3ee4-4df7-9cdb-98f74e435527.xhtml),
    *Performance Optimization in CUDA*). We should put `from __future__ import division`
    at the beginning of our file, since we'll use Python 3-style division in calculating
    our block and kernel sizes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了内核的编写！但当然，这并不是up-sweep的完整实现。我们还需要用Python来完成剩余的部分。让我们获取我们的内核并开始实现。这部分主要自说自话，因为它完全遵循伪代码；我们应该记住，我们是通过使用`[:]`从`x_gpu`复制来更新`x_old_gpu`的，这将保留内存分配，并且只是复制新数据而不是重新分配。此外，请注意我们如何根据要启动的线程数量设置我们的块和网格大小——我们试图保持块大小为32的倍数（这是我们在本文中的经验法则，我们将在[第11章](e853faad-3ee4-4df7-9cdb-98f74e435527.xhtml)，*CUDA性能优化*中具体说明为什么我们特别使用32）。我们应该在文件开头放置`from
    __future__ import division`，因为我们将在计算块和内核大小时使用Python 3风格的除法。
- en: 'One issue to mention is that we are assuming that `x` is of dyadic length 32
    or greater—this can be modified trivially if you wish to have this operate on
    arrays of other sizes by padding our arrays with zeros, however:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个问题需要提及，我们假设`x`是长度为32或更大的二进制长度——如果你希望这个操作在大小不同的数组上运行，可以简单地通过用零填充我们的数组来修改这一点：
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we''ll embark on writing the down-sweep. Again, let''s start with the kernel,
    which will have the functionality of the inner `parfor` loop of the pseudocode.
    It follows similarly as before—again, we''ll use two arrays, so using a `temp`
    variable as in the pseudocode is unnecessary here, and again we use bit-shift
    operators to obtain the values of 2^k and 2^(k+1). We calculate `j` similarly
    to before:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开始编写下降扫描部分。同样，我们先从核心开始，这个核心将具有伪代码中内层`parfor`循环的功能。它遵循之前的模式——再次，我们将使用两个数组，因此在这里使用伪代码中的`temp`变量是不必要的，并且再次我们使用位移操作符来获取2^k和2^(k+1)的值。我们计算`j`的方式与之前相似：
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We now can write our Python function that will iteratively launch the kernel,
    which corresponds to the outer `for` loop of the down-sweep phase. This is similar
    to the Python function for the up-sweep phase. One important distinction from
    looking at the pseudocode is that we have to iterate from the largest value in
    the outer `for` loop to the smallest; we can just use Python''s `reversed` function
    to do this. Now we can implement the down-sweep phase:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写我们的Python函数，该函数将迭代启动内核，这对应于下降扫描阶段的外层`for`循环。这与上升扫描阶段的Python函数类似。从伪代码中观察的一个重要区别是我们必须从外层`for`循环中的最大值迭代到最小值；我们可以直接使用Python的`reversed`函数来完成这个操作。现在我们可以实现下降扫描阶段：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Having implemented both the up-sweep and down-sweep phases, our last task is
    trivial to complete:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了上升扫描和下降扫描阶段之后，我们最后的任务就变得非常简单了：
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We have now fully implemented a host-synchronized version of the work-efficient
    parallel prefix algorithm! (This implementation is available in the `work-efficient_prefix.py`
    file in the repository, along with some test code.)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完全实现了工作高效并行前缀算法的主机同步版本！（这个实现可以在存储库中的`work-efficient_prefix.py`文件中找到，还有一些测试代码。）
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started with an implementation of Conway's *Game of Life*, which gave us
    an idea of how the many threads of a CUDA kernel are organized in a block-grid
    tensor-type structure. We then delved into block-level synchronization by way
    of the CUDA function, `__syncthreads()`, as well as block-level thread intercommunication
    by using shared memory; we also saw that single blocks have a limited number of
    threads that we can operate over, so we'll have to be careful in using these features
    when we create kernels that will use more than one block across a larger grid.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从康威的《生命游戏》实现开始，这让我们对CUDA内核中许多线程在块-网格张量结构中的组织方式有了了解。然后我们通过CUDA函数`__syncthreads()`深入研究了块级同步，以及通过使用共享内存进行块级线程间通信；我们还看到单个块可以操作的线程数量有限，因此当我们创建将在更大网格中使用多个块的内核时，我们必须小心使用这些功能。
- en: We gave an overview of the theory of parallel prefix algorithms, and we ended
    by implementing a naive parallel prefix algorithm as a single kernel that could
    operate on arrays limited by a size of 1,024 (which was synchronized with `___syncthreads`
    and performed both the `for` and `parfor` loops internally), and with a work-efficient
    parallel prefix algorithm that was implemented across two kernels and three Python
    functions could operate on arrays of arbitrary size, with the kernels acting as
    the inner `parfor` loops of the algorithm, and with the Python functions effectively
    operating as the outer `for` loops and synchronizing the kernel launches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了并行前缀算法的理论，并以实现一个简单的并行前缀算法作为结束，该算法作为一个单独的内核可以操作大小受限为1,024的数组（它与`___syncthreads`同步，并在内部执行了`for`和`parfor`循环），以及一个工作高效的并行前缀算法，它通过两个内核和三个Python函数实现，可以操作任意大小的数组，其中内核作为算法的内层`parfor`循环，Python函数则有效地作为外层`for`循环并同步内核的启动。
- en: Questions
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Change the random vector in `simple_scalar_multiply_kernel.py` so that it is
    of a length of 10,000, and modify the `i` index in the definition of the kernel
    so that it can be used over multiple blocks in the form of a grid. See if you
    can now launch this kernel over 10,000 threads by setting block and grid parameters
    to something like `block=(100,1,1)` and `grid=(100,1,1)`.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`simple_scalar_multiply_kernel.py`中的随机向量修改为长度为10,000，并修改内核定义中的`i`索引，使其可以以网格的形式在多个块中使用。看看你能否现在通过将块和网格参数设置为类似`block=(100,1,1)`和`grid=(100,1,1)`的方式，在10,000个线程上启动这个内核。
- en: In the previous question, we launched a kernel that makes use of 10,000 threads
    simultaneously; as of 2018, there is no NVIDIA GPU with more than 5,000 cores.
    Why does this still work and give the expected results?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个问题中，我们启动了一个同时使用10,000个线程的核心；截至2018年，没有任何NVIDIA GPU拥有超过5,000个核心。为什么这仍然有效并给出预期的结果？
- en: The naive parallel prefix algorithm has time complexity O(*log n*) given that
    we have *n* or more processors for a dataset of size *n*. Suppose that we use
    a naive parallel prefix algorithm on a GTX 1050 GPU with 640 cores. What does
    the asymptotic time complexity become in the case that `n >> 640`?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个大小为n的数据集，并且我们有n个或更多的处理器，原始并行前缀算法的时间复杂度为O(*log n*)。假设我们在具有640个核心的GTX 1050
    GPU上使用原始并行前缀算法。当`n >> 640`时，渐近时间复杂度会变成什么？
- en: Modify `naive_prefix.py` to operate on arrays of arbitrary size (possibly non-dyadic),
    only bounded by 1,024.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`naive_prefix.py`以操作任意大小的数组（可能是非二进制的），但受限于1,024。
- en: The `__syncthreads()` CUDA device function only synchronizes threads across
    a single block. How can we synchronize across all threads in all blocks across
    a grid?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__syncthreads()` CUDA设备函数仅同步单个块内的线程。我们如何在整个网格的所有块的所有线程之间进行同步？'
- en: You can convince yourself that the second prefix sum algorithm really is more
    work-efficient than the naive prefix sum algorithm with this exercise. Suppose
    that we have a dataset of size 32\. What is the exact number of "addition" operations
    required by the first and second algorithm in this case?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过这个练习说服自己，第二个前缀和算法确实比原始前缀和算法更高效。假设我们有一个大小为32的数据集。在这种情况下，第一个和第二个算法所需的“加法”操作的确切数量是多少？
- en: In the implementation of the work-efficient parallel prefix we use a Python
    function to iterate our kernels and synchronize the results. Why can't we just
    put a `for` loop inside the kernels with careful use of `__syncthreads()` instead?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实现工作高效的并行前缀时，我们使用Python函数来迭代我们的内核并同步结果。为什么我们不能在内核内简单地放置一个`for`循环，并谨慎地使用`__syncthreads()`呢？
- en: Why does it make more sense to implement the naive parallel prefix within a
    single kernel that handles its own synchronization within CUDA C, than it makes
    more sense to implement the work-efficient parallel prefix using both kernels
    and Python functions and have the host handle the synchronization?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在单个内核中实现原始并行前缀，该内核在CUDA C中处理自己的同步，比使用两个内核和Python函数实现工作高效的并行前缀并让主机处理同步更有意义？
