<html><head></head><body>
<div id="_idContainer156" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-96"><a id="_idTextAnchor097" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-97" class="calibre5"><a id="_idTextAnchor098" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Deploying and Managing ML Models with Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">The seamless deployment and effective management of models have become pivotal components of developing data science with Snowpark. </span><span class="kobospan" id="kobo.3.2">The previous chapter covered how to prepare the data and train the model. </span><span class="kobospan" id="kobo.3.3">This chapter delves into the intricacies of leveraging Snowpark to deploy and manage </span><strong class="bold"><span class="kobospan" id="kobo.4.1">machine learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">ML</span></strong><span class="kobospan" id="kobo.7.1">) models efficiently, from deployment to integration with feature stores and model registries, exploring the essentials of streamlining ML models </span><span><span class="kobospan" id="kobo.8.1">in Snowpark.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.9.1">In this chapter, we’re going to cover the following </span><span><span class="kobospan" id="kobo.10.1">main topics:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.11.1">Deploying ML models </span><span><span class="kobospan" id="kobo.12.1">in Snowpark</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.13.1">Managing Snowpark </span><span><span class="kobospan" id="kobo.14.1">model data</span></span></li>
</ul>
<h1 id="_idParaDest-98" class="calibre5"><a id="_idTextAnchor099" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.15.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.16.1">Please refer to the </span><em class="italic"><span class="kobospan" id="kobo.17.1">Technical requirements</span></em><span class="kobospan" id="kobo.18.1"> section in the previous chapter for </span><span><span class="kobospan" id="kobo.19.1">environment setup.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.20.1">Supporting materials are available </span><span><span class="kobospan" id="kobo.21.1">at </span></span><a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.22.1">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</span></span></a><span><span class="kobospan" id="kobo.23.1">.</span></span></p>
<h1 id="_idParaDest-99" class="calibre5"><a id="_idTextAnchor100" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.24.1">Deploying ML models in Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.25.1">In </span><a id="_idIndexMarker336" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.26.1">the preceding</span><a id="_idIndexMarker337" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.27.1"> chapter, we learned about how to develop ML models. </span><span class="kobospan" id="kobo.27.2">Now that the models are ready, we must deploy them into Snowpark. </span><span class="kobospan" id="kobo.27.3">To make it easier for developers to deploy the models, the Snowpark ML library consists of functions that encompass the introduction of a new development interface </span><a id="_idIndexMarker338" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.28.1">and additional functionalities aimed at securely facilitating the deployment of both features and models. </span><span class="kobospan" id="kobo.28.2">Snowpark MLOps seamlessly complements</span><a id="_idIndexMarker339" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.29.1"> the Snowpark ML Development API by offering advanced model management capabilities and integrated deployment functionalities within the Snowflake ecosystem. </span><span class="kobospan" id="kobo.29.2">In the following subsections, we will explore the model registry and deploy the model for inference to </span><span><span class="kobospan" id="kobo.30.1">obtain predictions.</span></span></p>
<h2 id="_idParaDest-100" class="calibre7"><a id="_idTextAnchor101" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.31.1">Snowpark ML model registry</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.32.1">A </span><strong class="bold"><span class="kobospan" id="kobo.33.1">model registry</span></strong><span class="kobospan" id="kobo.34.1"> is a</span><a id="_idIndexMarker340" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.35.1"> centralized repository that enables model developers to organize, share, and publish ML models efficiently. </span><span class="kobospan" id="kobo.35.2">It streamlines collaboration </span><a id="_idIndexMarker341" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.36.1">among teams and stakeholders, facilitating the collaborative management of the lifecycle of all models within an organization. </span><span class="kobospan" id="kobo.36.2">Organizing models is crucial for tracking various versions, quickly identifying the latest, and gaining insights into each model’s hyperparameters. </span><span class="kobospan" id="kobo.36.3">A well-structured model registry enhances reproducibility and compelling comparison of results. </span><span class="kobospan" id="kobo.36.4">It also allows tracking and analyzing model accuracy metrics, empowering informed decisions and continuous improvement. </span><span class="kobospan" id="kobo.36.5">The following diagram shows the deployment of a model into a </span><span><span class="kobospan" id="kobo.37.1">model registry:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer133">
<span class="kobospan" id="kobo.38.1"><img alt="Figure 6.1 – Deploying a model into a model registry" src="image/B19923_06_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.39.1">Figure 6.1 – Deploying a model into a model registry</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.40.1">The </span><a id="_idIndexMarker342" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.41.1">model registry is a Python API that manages models within the Snowflake environment, offering scalable, secure deployment and management capabilities for models within Snowflake. </span><span class="kobospan" id="kobo.41.2">The Snowpark model registry is built upon a native Snowflake model entity, incorporating built-in versioning support for more streamlined management </span><span><span class="kobospan" id="kobo.42.1">of models.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.43.1">Preparing the model</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.44.1">To </span><a id="_idIndexMarker343" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.45.1">illustrate the model registration process, we’ll efficiently craft a streamlined XGBoost model using minimal parameters, leveraging grid search on the </span><em class="italic"><span class="kobospan" id="kobo.46.1">Bike Sharing</span></em><span class="kobospan" id="kobo.47.1"> dataset. </span><span class="kobospan" id="kobo.47.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.48.1">BSD_TRAINING</span></strong><span class="kobospan" id="kobo.49.1"> table prepared in the previous chapter is the foundational dataset for constructing our </span><span><span class="kobospan" id="kobo.50.1">XGBoost model.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.51.1">Here, we are making a feature list and finding label and </span><span><span class="kobospan" id="kobo.52.1">output columns:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.53.1">
FEATURE_LIST = [ "HOLIDAY", "WORKINGDAY", "HUMIDITY", "TEMP", "ATEMP", 
    "WINDSPEED", "SEASON", "WEATHER"]
LABEL_COLUMNS = ['COUNT']
OUTPUT_COLUMNS = ['PREDICTED_COUNT']
df = session.table("BSD_TRAINING")
df = df.drop("DATETIME","DATE")
df.show(2)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.54.1">This</span><a id="_idIndexMarker344" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.55.1"> will print out the </span><span><span class="kobospan" id="kobo.56.1">following DataFrame:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer134">
<span class="kobospan" id="kobo.57.1"><img alt="Figure 6.2 – Model DataFrame" src="image/B19923_06_2.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.58.1">Figure 6.2 – Model DataFrame</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.59.1">For the sake of simplicity, we will focus on optimizing two parameters </span><span><span class="kobospan" id="kobo.60.1">within XGBoost:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.61.1">
from snowflake.ml.modeling.model_selection import GridSearchCV
from snowflake.ml.modeling.xgboost import XGBRegressor
param_grid = {
    "max_depth":[3, 4, 5, 6, 7, 8],
    "min_child_weight":[1, 2, 3, 4],
}
grid_search = GridSearchCV(
    estimator=XGBRegressor(),
    param_grid=param_grid,
    n_jobs = -1,
    scoring="neg_root_mean_squared_error",
    input_cols=FEATURE_LIST,
    label_cols=LABEL_COLUMNS,
    output_cols=OUTPUT_COLUMNS
)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.62.1">This code</span><a id="_idIndexMarker345" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.63.1"> employs Snowflake’s ML module to perform a grid search for hyperparameter tuning on a gradient boosting regressor. </span><span class="kobospan" id="kobo.63.2">It explores combinations of </span><strong class="source-inline"><span class="kobospan" id="kobo.64.1">max_depth</span></strong><span class="kobospan" id="kobo.65.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.66.1">min_child_weight</span></strong><span class="kobospan" id="kobo.67.1"> within specified ranges, aiming to optimize the model based on the input and label </span><span><span class="kobospan" id="kobo.68.1">columns provided.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.69.1">The subsequent logical progression involves partitioning the dataset into training and </span><span><span class="kobospan" id="kobo.70.1">testing sets:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.71.1">
train_df, test_df = df.random_split(weights=[0.7, 0.3], seed=0)
grid_search.fit(train_df)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.72.1">This division is essential to facilitate model fitting, allowing us to train the model on the designated </span><span><span class="kobospan" id="kobo.73.1">training dataset.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.74.1">Extracting the optimum parameter</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.75.1">Having</span><a id="_idIndexMarker346" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.76.1"> successfully trained our dataset using the XGBoost model, the next imperative is identifying optimal parameter values defined through grid search. </span><span class="kobospan" id="kobo.76.2">Remarkably similar to the process in the </span><strong class="source-inline"><span class="kobospan" id="kobo.77.1">scikit-learn</span></strong><span class="kobospan" id="kobo.78.1"> package, Snowpark ML offers a comparable methodology. </span><span class="kobospan" id="kobo.78.2">The ensuing code mirrors the steps in extracting these optimal parameters and subsequently visualizing them, demystifying the process for </span><span><span class="kobospan" id="kobo.79.1">seamless comprehension:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.80.1">
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
gs_results = grid_search.to_sklearn().cv_results_
max_depth_val = []
min_child_weight_val = []
for param_dict in gs_results["params"]:
    max_depth_val.append(param_dict["max_depth"])
    min_child_weight_val.append(param_dict["min_child_weight"])
mape_val = gs_results["mean_test_score"]*-1</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.81.1">The </span><a id="_idIndexMarker347" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.82.1">preceding code uses </span><strong class="source-inline"><span class="kobospan" id="kobo.83.1">pandas</span></strong><span class="kobospan" id="kobo.84.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.85.1">seaborn</span></strong><span class="kobospan" id="kobo.86.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.87.1">matplotlib</span></strong><span class="kobospan" id="kobo.88.1"> to analyze and visualize grid search results from a Snowpark ML model. </span><span class="kobospan" id="kobo.88.2">It extracts the parameters, such as </span><strong class="source-inline"><span class="kobospan" id="kobo.89.1">max_depth</span></strong><span class="kobospan" id="kobo.90.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.91.1">min_child_weight</span></strong><span class="kobospan" id="kobo.92.1">, along with the corresponding </span><strong class="bold"><span class="kobospan" id="kobo.93.1">mean absolute percentage error</span></strong><span class="kobospan" id="kobo.94.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.95.1">MAPE</span></strong><span class="kobospan" id="kobo.96.1">) values for evaluation. </span><span class="kobospan" id="kobo.96.2">The following code showcases </span><span><span class="kobospan" id="kobo.97.1">the values:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.98.1">
gs_results_df = pd.DataFrame(data={
    "max_depth":max_depth_val,
    "min_child_weight":min_child_weight_val,
    "mape":mape_val})
sns.relplot(data=gs_results_df, x="min_child_weight",
    y="mape", hue="max_depth", kind="line")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.99.1">The preceding code creates a </span><strong class="source-inline"><span class="kobospan" id="kobo.100.1">pandas</span></strong><span class="kobospan" id="kobo.101.1"> DataFrame named </span><strong class="source-inline"><span class="kobospan" id="kobo.102.1">gs_results_df</span></strong><span class="kobospan" id="kobo.103.1"> from listed </span><strong class="source-inline"><span class="kobospan" id="kobo.104.1">max_depth</span></strong><span class="kobospan" id="kobo.105.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.106.1">min_child_weight</span></strong><span class="kobospan" id="kobo.107.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.108.1">mape</span></strong><span class="kobospan" id="kobo.109.1"> values. </span><span class="kobospan" id="kobo.109.2">It then utilizes </span><strong class="source-inline"><span class="kobospan" id="kobo.110.1">seaborn</span></strong><span class="kobospan" id="kobo.111.1"> to generate a line plot, visualizing the relationship between learning rates, MAPE scores, and different numbers of estimators. </span><span class="kobospan" id="kobo.111.2">Finally, the </span><strong class="source-inline"><span class="kobospan" id="kobo.112.1">matplotlib</span></strong> <strong class="source-inline"><span class="kobospan" id="kobo.113.1">plt.show()</span></strong><span class="kobospan" id="kobo.114.1"> command displays the </span><span><span class="kobospan" id="kobo.115.1">following plot:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer135">
<span class="kobospan" id="kobo.116.1"><img alt="Figure 6.3 – Data plot" src="image/B19923_06_3.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.117.1">Figure 6.3 – Data plot</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.118.1">Upon</span><a id="_idIndexMarker348" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.119.1"> careful observation of the previous plot, it becomes evident that a </span><strong class="source-inline"><span class="kobospan" id="kobo.120.1">max_depth</span></strong><span class="kobospan" id="kobo.121.1"> value of </span><strong class="source-inline"><span class="kobospan" id="kobo.122.1">8</span></strong><span class="kobospan" id="kobo.123.1"> paired with a </span><strong class="source-inline"><span class="kobospan" id="kobo.124.1">min_child_weight</span></strong><span class="kobospan" id="kobo.125.1"> learning rate of </span><strong class="source-inline"><span class="kobospan" id="kobo.126.1">2</span></strong><span class="kobospan" id="kobo.127.1"> yields the optimal results. </span><span class="kobospan" id="kobo.127.2">It’s noteworthy that, akin to </span><strong class="source-inline"><span class="kobospan" id="kobo.128.1">scikit-learn</span></strong><span class="kobospan" id="kobo.129.1">, Snowpark ML offers streamlined methods for extracting these optimal parameters, simplifying the process for enhanced </span><span><span class="kobospan" id="kobo.130.1">user convenience:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.131.1">
grid_search.to_sklearn().best_estimator_</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.132.1">The code transforms the Snowpark ML grid search results into a format compatible with </span><strong class="source-inline"><span class="kobospan" id="kobo.133.1">scikit-learn</span></strong><span class="kobospan" id="kobo.134.1"> and then retrieves the best estimator, representing the model with </span><span><span class="kobospan" id="kobo.135.1">optimal hyperparameters:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer136">
<span class="kobospan" id="kobo.136.1"><img alt="Figure 6.4 – Snowpark ML grid search result" src="image/B19923_06_4.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.137.1">Figure 6.4 – Snowpark ML grid search result</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.138.1">In the </span><a id="_idIndexMarker349" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.139.1">next section, we will use the Snowpark model registry to log </span><span><span class="kobospan" id="kobo.140.1">the model.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.141.1">Logging the optimal model</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.142.1">With </span><a id="_idIndexMarker350" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.143.1">our optimal model in hand, the pivotal phase of the model registry unfolds. </span><span class="kobospan" id="kobo.143.2">Much akin to the previously created model, we can extend this process to encompass multiple models, registering each through the model registry. </span><span class="kobospan" id="kobo.143.3">In this case, we will be registering only our optimal model. </span><span class="kobospan" id="kobo.143.4">We’ll delve into a step-by-step exploration of how models can be registered and </span><span><span class="kobospan" id="kobo.144.1">seamlessly deployed.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.145.1">Note on the Model Registry and Feature Store</span></p>
<p class="callout"><span class="kobospan" id="kobo.146.1">While we write this chapter, both the Model Registry and Feature Store are in private preview. </span><span class="kobospan" id="kobo.146.2">Once they are open to all, the API methods might be slightly different from what we see in </span><span><span class="kobospan" id="kobo.147.1">this book.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.148.1">Next, we need to create a registry to log </span><span><span class="kobospan" id="kobo.149.1">our model:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.150.1">
from snowflake.ml.registry import model_registry
registry = model_registry.ModelRegistry(session=session,
    database_name="SNOWPARK_DEFINITIVE_GUIDE",
    schema_name="MY_SCHEMA", create_if_not_exists=True)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.151.1">The preceding code snippet imports the Snowflake ML model registry and initializes a </span><strong class="source-inline"><span class="kobospan" id="kobo.152.1">ModelRegistry</span></strong><span class="kobospan" id="kobo.153.1"> instance with session information, specified database, and schema names. </span><span class="kobospan" id="kobo.153.2">If not existing, it creates a registry in the </span><strong class="source-inline"><span class="kobospan" id="kobo.154.1">SNOWPARK_DEFINITIVE_GUIDE</span></strong><span class="kobospan" id="kobo.155.1"> database and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.156.1">MY_SCHEMA</span></strong></span><span><span class="kobospan" id="kobo.157.1"> schema.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.158.1">The </span><a id="_idIndexMarker351" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.159.1">following code prepares the essential details to log a model in the </span><span><span class="kobospan" id="kobo.160.1">model registry:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.161.1">
optimal_model = grid_search.to_sklearn().best_estimator_
optimal_max_depth = \
    grid_search.to_sklearn().best_estimator_.max_depth
optimal_min_child_weight = \
    grid_search.to_sklearn().best_estimator_.min_child_weight
optimal_mape = gs_results_df.loc[
    (gs_results_df['max_depth']==optimal_max_depth) &amp;
    (gs_results_df['min_child_weight']== \
        optimal_min_child_weight), 'mape'].values[0]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.162.1">It extracts the optimal model, as determined by the grid search, and retrieves specific hyperparameters such as </span><strong class="source-inline"><span class="kobospan" id="kobo.163.1">max_depth</span></strong><span class="kobospan" id="kobo.164.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.165.1">min_child_weight</span></strong><span class="kobospan" id="kobo.166.1">, and optimal </span><span><span class="kobospan" id="kobo.167.1">parameter values.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.168.1">Having completed all necessary steps for model registration, the preceding code seamlessly integrates the gathered information to register our optimal XGBoost model officially in the </span><span><span class="kobospan" id="kobo.169.1">model registry:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.170.1">
model_name = "bike_model_xg_boost"
model_version = 1
X = train_df.select(FEATURE_LIST).limit(100)
registry.log_model( model_name=model_name,
                    model_version=model_version,
                    model=optimal_model,
                    sample_input_data=X,
                    options={"embed_local_ml_library": True, \
                             "relax": True})
registry.set_metric(model_name=model_name,
                    model_version=model_version,
                    metric_name="mean_abs_pct_err",
                    metric_value=optimal_mape)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.171.1">The </span><a id="_idIndexMarker352" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.172.1">code assigns a name (</span><strong class="source-inline"><span class="kobospan" id="kobo.173.1">bike_model_xg_boost</span></strong><span class="kobospan" id="kobo.174.1">) and version (</span><strong class="source-inline"><span class="kobospan" id="kobo.175.1">1</span></strong><span class="kobospan" id="kobo.176.1">) to the model and logs it into the registry with associated details, including the sample input data and specific options. </span><span class="kobospan" id="kobo.176.2">Additionally, it sets a custom metric, MAPE (</span><strong class="source-inline"><span class="kobospan" id="kobo.177.1">mean_abs_pct_err</span></strong><span class="kobospan" id="kobo.178.1">), for the registered model with its corresponding value (</span><strong class="source-inline"><span class="kobospan" id="kobo.179.1">optimal_mape</span></strong><span class="kobospan" id="kobo.180.1">). </span><span class="kobospan" id="kobo.180.2">To verify successful registration, execute the </span><span><span class="kobospan" id="kobo.181.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.182.1">
registry.list_models().to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.183.1">This will confirm whether our XGBoost model and the gradient boost model (only the XGBoost model are steps shown here to avoid unnecessary repetition) are appropriately listed in the </span><span><span class="kobospan" id="kobo.184.1">model registry:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer137">
<span class="kobospan" id="kobo.185.1"><img alt="Figure 6.5 – Model registered in the model registry" src="image/B19923_06_5.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.186.1">Figure 6.5 – Model registered in the model registry</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.187.1">In the iterative journey of experimentation with diverse models and varied parameter configurations, we</span><a id="_idIndexMarker353" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.188.1"> diligently register each model within the model registry through a structured methodology that ensures that each model, fine-tuned and optimized, is stored efficiently for future use. </span><span class="kobospan" id="kobo.188.2">In the next section, we will deploy the model from the registry using Snowpark MLOps and predict </span><span><span class="kobospan" id="kobo.189.1">its results.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.190.1">Model deployment</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.191.1">In the </span><a id="_idIndexMarker354" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1">preceding sections, we navigated the intricate landscape of deploying models through complex </span><strong class="bold"><span class="kobospan" id="kobo.193.1">user-defined functions</span></strong><span class="kobospan" id="kobo.194.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.195.1">UDFs</span></strong><span class="kobospan" id="kobo.196.1">) or stored procedures. </span><span class="kobospan" id="kobo.196.2">However, the new Snowpark model registry simplifies the cumbersome process. </span><span class="kobospan" id="kobo.196.3">It enhances the maintainability of models by providing a streamlined and standardized framework for handling predictive models in a production setting. </span><span class="kobospan" id="kobo.196.4">This shift in methodology optimizes operational efficiency and aligns seamlessly with contemporary practices in the dynamic field of data science. </span><span class="kobospan" id="kobo.196.5">A standard model deployment would follow this </span><span><span class="kobospan" id="kobo.197.1">naming convention:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.198.1">
model_deployment_name = model_name + f"{model_version}" + "_UDF"
registry.deploy(model_name=model_name,
                model_version=model_version,
                deployment_name=model_deployment_name,
                target_method="predict",
                permanent=True,
                options={"relax_version": True})</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.199.1">The preceding snippet generates a unique name for deploying the model as a UDF. </span><span class="kobospan" id="kobo.199.2">It then deploys the specified model version using the generated deployment name for the </span><strong class="source-inline"><span class="kobospan" id="kobo.200.1">predict</span></strong><span class="kobospan" id="kobo.201.1"> target method, ensuring permanence in the deployment. </span><span class="kobospan" id="kobo.201.2">Additionally, it includes an option to relax version constraints during deployment. </span><span class="kobospan" id="kobo.201.3">Just as we’ve showcased the catalog of registered models, an equivalent insight into deployed models can be obtained using the following line </span><span><span class="kobospan" id="kobo.202.1">of code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.203.1">
registry.list_deployments(model_name, model_version).to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.204.1">This functionality</span><a id="_idIndexMarker355" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.205.1"> provides a comprehensive view of models transitioning from registration to deployment within </span><span><span class="kobospan" id="kobo.206.1">the system:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer138">
<span class="kobospan" id="kobo.207.1"><img alt="Figure 6.6 – Bike model deployment" src="image/B19923_06_6.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.208.1">Figure 6.6 – Bike model deployment</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.209.1">Now, let’s leverage our deployed model to infer predictions for the test data and assess the accuracy of our predictions against </span><span><span class="kobospan" id="kobo.210.1">actual outcomes:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.211.1">
model_ref = model_registry.ModelReference(
    registry=registry,
    model_name=model_name,
    model_version=model_version)
result_sdf = model_ref.predict(
    deployment_name=model_deployment_name,
    data=test_df)
result_sdf.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.212.1">The code initiates a </span><strong class="source-inline"><span class="kobospan" id="kobo.213.1">ModelReference</span></strong><span class="kobospan" id="kobo.214.1"> object, linking to a specific model within the registry by referencing its name and version. </span><span class="kobospan" id="kobo.214.2">Subsequently, it leverages this reference to predict the provided test data using the specified deployment, resulting in a Snowpark DataFrame (</span><strong class="source-inline"><span class="kobospan" id="kobo.215.1">result_sdf</span></strong><span class="kobospan" id="kobo.216.1">). </span><span class="kobospan" id="kobo.216.2">Finally, it displays the expected results through the </span><strong class="source-inline"><span class="kobospan" id="kobo.217.1">show()</span></strong><span class="kobospan" id="kobo.218.1"> method, as shown in the </span><span><span class="kobospan" id="kobo.219.1">following screenshot:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer139">
<span class="kobospan" id="kobo.220.1"><img alt="Figure 6.7 – Model result DataFrame" src="image/B19923_06_7.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.221.1">Figure 6.7 – Model result DataFrame</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.222.1">Having </span><a id="_idIndexMarker356" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.223.1">observed a comprehensive cycle encompassing model development, registration, and deployment, it’s noteworthy that this process is replicable for any model-building endeavor through the model registry. </span><span class="kobospan" id="kobo.223.2">In the subsequent section, we will elucidate several beneficial methods inherent in the model registry, elevating its usability and augmenting the overall modeling experience. </span><span class="kobospan" id="kobo.223.3">Now that we have deployed the model, we will look at other model </span><span><span class="kobospan" id="kobo.224.1">registry methods.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.225.1">Model registry methods</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.226.1">Beyond the</span><a id="_idIndexMarker357" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.227.1"> functionality outlined for model deployment, the model registry extends its utility with several beneficial methods designed for effective model maintenance and housekeeping activities. </span><span class="kobospan" id="kobo.227.2">In this section, we will explore a selection of these methods to enhance our understanding of their practical applications. </span><span class="kobospan" id="kobo.227.3">We will start with </span><span><span class="kobospan" id="kobo.228.1">model metrics.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.229.1">Model metrics</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.230.1">Linking metrics</span><a id="_idIndexMarker358" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.231.1"> to your model version is a pivotal feature within the model registry. </span><span class="kobospan" id="kobo.231.2">This functionality serves as a fundamental aspect, providing a systematic means to gauge the performance of each model version distinctly. </span><span class="kobospan" id="kobo.231.3">By associating metrics, users gain valuable insights into the efficacy of different iterations, facilitating informed decision-making based on the quantitative evaluation of model performance across various versions. </span><span class="kobospan" id="kobo.231.4">It also helps in automating the pipeline, thereby retraining if model metrics drop below the threshold value. </span><span class="kobospan" id="kobo.231.5">This deliberate metrics integration enriches the comprehensive model management capabilities and establishes a structured framework for ongoing model evaluation </span><span><span class="kobospan" id="kobo.232.1">and refinement:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.233.1">
registry.set_metric(model_name=model_name,
                    model_version=model_version,
                    metric_name="mean_abs_pct_err",
                    metric_value=optimal_mape)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.234.1">The </span><a id="_idIndexMarker359" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.235.1">preceding line sets a custom metric, </span><strong class="source-inline"><span class="kobospan" id="kobo.236.1">mean_abs_pct_err</span></strong><span class="kobospan" id="kobo.237.1">, for a specific model version in the model registry, assigning the calculated MAPE value to quantify the model’s performance. </span><span class="kobospan" id="kobo.237.2">It enhances the model registry’s ability to track and evaluate the effectiveness of different </span><span><span class="kobospan" id="kobo.238.1">model versions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.239.1">
registry.get_metric_value(model_name=model_name,
                          model_version=model_version,
                          metric_name="mean_abs_pct_err")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.240.1">This will print the </span><span><span class="kobospan" id="kobo.241.1">following output:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer140">
<span class="kobospan" id="kobo.242.1"><img alt="Figure 6.8 – MAPE value of mean_abs_pct_err" src="image/B19923_06_8.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.243.1">Figure 6.8 – MAPE value of mean_abs_pct_err</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">In addition to setting, we can retrieve the value of a specific custom metric, </span><strong class="source-inline"><span class="kobospan" id="kobo.245.1">mean_abs_pct_err</span></strong><span class="kobospan" id="kobo.246.1">, associated with a particular model version from the model registry. </span><span class="kobospan" id="kobo.246.2">It allows users to access and analyze quantitative performance metrics for practical model evaluation and comparison across </span><span><span class="kobospan" id="kobo.247.1">different versions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.248.1">
registry.get_metrics(model_name=model_name, 
    model_version=model_version)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.249.1">Much like retrieving a specific metric for a deployed model, an analogous approach allows us to access a comprehensive list of all associated metrics for a given deployed model. </span><span class="kobospan" id="kobo.249.2">This facilitates a holistic understanding of the model’s performance, providing a detailed overview of various metrics related to its evaluation and contributing to a thorough analysis of </span><span><span class="kobospan" id="kobo.250.1">its effectiveness:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer141">
<span class="kobospan" id="kobo.251.1"><img alt="Figure 6.9 – Metrics of mean_abs_pct_err" src="image/B19923_06_9.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.252.1">Figure 6.9 – Metrics of mean_abs_pct_err</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.253.1">We can find </span><a id="_idIndexMarker360" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.254.1">the value of metrics from the model in the registry. </span><span class="kobospan" id="kobo.254.2">In the next section, we will cover model tags </span><span><span class="kobospan" id="kobo.255.1">and descriptions.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.256.1">Model tags and descriptions</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.257.1">Setting a </span><a id="_idIndexMarker361" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.258.1">tag name and description for a deployed model is crucial for effective experiment tracking and documentation. </span><span class="kobospan" id="kobo.258.2">Tags and descriptions provide context and insights into the model’s purpose, configuration, and notable characteristics. </span><span class="kobospan" id="kobo.258.3">This aids in maintaining a structured and informative record, enhancing reproducibility, and facilitating a more comprehensive analysis of </span><span><span class="kobospan" id="kobo.259.1">experiment outcomes:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.260.1">
registry.set_tag(model_name=model_name,
                 model_version=model_version,
                 tag_name="usage",
                 tag_value="experiment")
registry.list_models().to_pandas()[["NAME", "TAGS"]]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.261.1">The provided code first sets a tag named </span><strong class="source-inline"><span class="kobospan" id="kobo.262.1">stage</span></strong><span class="kobospan" id="kobo.263.1"> with the </span><strong class="source-inline"><span class="kobospan" id="kobo.264.1">experiment_1</span></strong><span class="kobospan" id="kobo.265.1"> value for a specific model version in the model registry. </span><span class="kobospan" id="kobo.265.2">This tagging is a contextual marker for the model’s purpose or usage. </span><span class="kobospan" id="kobo.265.3">The subsequent line retrieves and displays, in a tabular format, the names of all models along with their associated tags, showcasing the tagged information for </span><span><span class="kobospan" id="kobo.266.1">each model:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer142">
<span class="kobospan" id="kobo.267.1"><img alt="Figure 6.10 – Model tags" src="image/B19923_06_10.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.268.1">Figure 6.10 – Model tags</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.269.1">Another noteworthy aspect is the flexibility to modify and remove tags as necessary, allowing</span><a id="_idIndexMarker362" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.270.1"> for a dynamic adjustment of our experiment design. </span><span class="kobospan" id="kobo.270.2">This capability empowers users to iteratively refine contextual information associated with a model, providing meaningful and evolving tags. </span><span class="kobospan" id="kobo.270.3">The ability to alter and remove tags enhances experiment design flexibility. </span><span class="kobospan" id="kobo.270.4">It ensures that the documentation and context surrounding models can adapt to changing insights and requirements throughout the </span><span><span class="kobospan" id="kobo.271.1">experimentation lifecycle:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.272.1">
registry.remove_tag(model_name=model_name,
                    model_version=model_version,
                    tag_name="usage")
registry.list_models().to_pandas()[["NAME", "TAGS"]]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.273.1">The provided code initiates the removal of a specific tag, named </span><strong class="source-inline"><span class="kobospan" id="kobo.274.1">usage</span></strong><span class="kobospan" id="kobo.275.1">, from a particular model version within the model registry. </span><span class="kobospan" id="kobo.275.2">Following this operation, the subsequent line retrieves and displays, in a tabular format, the names of all models along with their associated tags. </span><span class="kobospan" id="kobo.275.3">This showcases the updated information after removing the specified tag, providing a comprehensive view of models and their altered </span><span><span class="kobospan" id="kobo.276.1">tag configurations:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer143">
<span class="kobospan" id="kobo.277.1"><img alt="Figure 6.11 – Model tags removed" src="image/B19923_06_11.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.278.1">Figure 6.11 – Model tags removed</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.279.1">We can also provide descriptive information for a deployed model, offering valuable context and aiding future references. </span><span class="kobospan" id="kobo.279.2">The ability to furnish a meaningful description enhances the comprehensibility of the model’s purpose, configuration, or other pertinent details. </span><span class="kobospan" id="kobo.279.3">The ensuing code block, which is self-explanatory and mirrors the process of setting tags, enables the assignment of a descriptive narrative to a deployed model, ensuring that vital information is encapsulated for reference in subsequent analyses </span><span><span class="kobospan" id="kobo.280.1">or experiments:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.281.1">
registry.set_model_description(model_name=model_name,
    model_version=model_version,
    description="this is a test model")
print(registry.get_model_description(model_name=model_name,
    model_version=model_version))</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.282.1">The</span><a id="_idIndexMarker363" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.283.1"> model description is set and can be retrieved to display on </span><span><span class="kobospan" id="kobo.284.1">the screen:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer144">
<span class="kobospan" id="kobo.285.1"><img alt="Figure 6.12 – Model description" src="image/B19923_06_12.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.286.1">Figure 6.12 – Model description</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.287.1">Now that we have the model tags and description set, we will examine how to access the </span><span><span class="kobospan" id="kobo.288.1">registry history.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.289.1">Registry history</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.290.1">Accessing</span><a id="_idIndexMarker364" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.291.1"> the registry history is an invaluable capability, offering a chronological account of model versions, associated metrics, tags, and descriptions. </span><span class="kobospan" id="kobo.291.2">This historical perspective enhances transparency in model development and empowers data scientists to make informed decisions, track performance trends, and iterate on model improvements with precision. </span><span class="kobospan" id="kobo.291.3">The ML registry, coupled with its history-tracking feature, thus emerges as a pivotal asset in the data science arsenal, fostering a structured and efficient approach to model development, deployment, and </span><span><span class="kobospan" id="kobo.292.1">ongoing refinement:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.293.1">
registry.get_history().to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.294.1">The code retrieves and converts the entire history of the model registry into a </span><strong class="source-inline"><span class="kobospan" id="kobo.295.1">pandas</span></strong><span class="kobospan" id="kobo.296.1"> DataFrame, presenting a comprehensive tabular view of all recorded events </span><span><span class="kobospan" id="kobo.297.1">and changes:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer145">
<span class="kobospan" id="kobo.298.1"><img alt="Figure 6.13 – Registry history" src="image/B19923_06_13.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.299.1">Figure 6.13 – Registry history</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.300.1">Narrowing down the search in the registry history is a common practice, and it can be achieved</span><a id="_idIndexMarker365" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.301.1"> by specifying a model name and version. </span><span class="kobospan" id="kobo.301.2">This targeted filtering allows for more focused exploration, aligning with typical preferences when navigating the model </span><span><span class="kobospan" id="kobo.302.1">registry history:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.303.1">
registry.get_model_history(model_name=model_name,
    model_version=model_version).to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.304.1">This code fetches and converts the specific history of a particular model version, identified by its name and version, into a </span><span><strong class="source-inline"><span class="kobospan" id="kobo.305.1">pandas</span></strong></span><span><span class="kobospan" id="kobo.306.1"> DataFrame:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer146">
<span class="kobospan" id="kobo.307.1"><img alt="Figure 6.14 – Registry history filter" src="image/B19923_06_14.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.308.1">Figure 6.14 – Registry history filter</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.309.1">The resulting DataFrame offers a detailed chronological record of all events and changes associated with that specific model version within the registry. </span><span class="kobospan" id="kobo.309.2">In the next section, we will learn about operations on the </span><span><span class="kobospan" id="kobo.310.1">model registry.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.311.1">Model registry operations</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.312.1">In the</span><a id="_idIndexMarker366" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.313.1"> contemporary landscape of ML, the lifecycle of models is continually contracting, leading to shorter durations for deployed models. </span><span class="kobospan" id="kobo.313.2">Concurrently, experiments with varying parameters generate many models, and their subsequent deployments are registered. </span><span class="kobospan" id="kobo.313.3">This proliferation necessitates a thoughtful approach to model management, including periodic cleanup processes to maintain a streamlined and efficient </span><span><span class="kobospan" id="kobo.314.1">model registry:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.315.1">
registry.delete_deployment(model_name=model_name,
    model_version=model_version,
    deployment_name=model_deployment_name)
    registry.list_deployments(model_name, model_version).to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.316.1">The preceding code deletes a specific deployment instance identified by the model’s name, version, and deployment name from the model registry, ensuring efficient cleanup and management of </span><span><span class="kobospan" id="kobo.317.1">deployed models:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer147">
<span class="kobospan" id="kobo.318.1"><img alt="Figure 6.15 – Deleting a specific deployment" src="image/B19923_06_15.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.319.1">Figure 6.15 – Deleting a specific deployment</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.320.1">It serves</span><a id="_idIndexMarker367" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.321.1"> as a method to remove obsolete or undesired deployments. </span><span class="kobospan" id="kobo.321.2">We can also delete a whole model from the registry by using the </span><span><span class="kobospan" id="kobo.322.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.323.1">
registry.delete_model(model_name=model_name,
    model_version=model_version)
registry.list_models().to_pandas()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.324.1">Similar to deleting a deployment, this code will delete a model from the </span><span><span class="kobospan" id="kobo.325.1">model registry:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer148">
<span class="kobospan" id="kobo.326.1"><img alt="Figure 6.16 – Deleting a model" src="image/B19923_06_16.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.327.1">Figure 6.16 – Deleting a model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.328.1">We can see that the entire model has been deleted from the registry. </span><span class="kobospan" id="kobo.328.2">In the next section, we will look at the benefits of a </span><span><span class="kobospan" id="kobo.329.1">model registry.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.330.1">Benefits of the model registry in the model lifecycle</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.331.1">The</span><a id="_idIndexMarker368" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.332.1"> Snowpark model registry streamlines the management of ML models throughout their lifecycle. </span><span class="kobospan" id="kobo.332.2">Let’s delve into how the model registry in Snowpark can assist in various stages of the ML </span><span><span class="kobospan" id="kobo.333.1">model lifecycle:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.334.1">Model development</span></strong><span class="kobospan" id="kobo.335.1">: During the development phase, data scientists can use Snowpark to build, train, and validate ML models directly within Snowflake. </span><span class="kobospan" id="kobo.335.2">The model registry provides a centralized location to store and version control these models, making it easier to track changes, compare performance, and collaborate with </span><span><span class="kobospan" id="kobo.336.1">team members.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.337.1">Model deployment</span></strong><span class="kobospan" id="kobo.338.1">: Once a model is trained and validated, it needs to be deployed into production environments for inference. </span><span class="kobospan" id="kobo.338.2">The model registry facilitates seamless deployment by providing a standardized interface to deploy models</span><a id="_idIndexMarker369" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.339.1"> across different environments. </span><span class="kobospan" id="kobo.339.2">This ensures consistency and reliability in model </span><span><span class="kobospan" id="kobo.340.1">deployment processes.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.341.1">Model monitoring</span></strong><span class="kobospan" id="kobo.342.1">: Monitoring the performance of deployed models is crucial for detecting drift and ensuring continued accuracy over time. </span><span class="kobospan" id="kobo.342.2">The model registry can integrate with monitoring tools to track model performance metrics, such as accuracy, precision, recall, and F1-score, enabling proactive maintenance </span><span><span class="kobospan" id="kobo.343.1">and optimization.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.344.1">Model governance</span></strong><span class="kobospan" id="kobo.345.1">: Ensuring compliance with regulatory requirements and organizational policies is essential for responsible AI deployment. </span><span class="kobospan" id="kobo.345.2">The model registry supports governance by providing capabilities for access control, audit logging, and versioning. </span><span class="kobospan" id="kobo.345.3">This helps organizations maintain visibility and control over the entire </span><span><span class="kobospan" id="kobo.346.1">model lifecycle.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.347.1">Model retraining and updating</span></strong><span class="kobospan" id="kobo.348.1">: ML models need to be periodically retrained and updated to adapt to changing data distributions and business requirements. </span><span class="kobospan" id="kobo.348.2">The model registry simplifies this process by enabling data scientists to seamlessly retrain models using updated data and algorithms while preserving the lineage and history of </span><span><span class="kobospan" id="kobo.349.1">model versions.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.350.1">Model retirement</span></strong><span class="kobospan" id="kobo.351.1">: As models become obsolete or are replaced by newer versions, they need to be retired gracefully. </span><span class="kobospan" id="kobo.351.2">The model registry facilitates the retirement process by archiving outdated models, documenting reasons for retirement, and ensuring that relevant stakeholders are notified </span><span><span class="kobospan" id="kobo.352.1">of changes.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.353.1">The model registry offers an organized framework for model management and provides functionalities for efficient housekeeping, including setting and tracking metrics, tags, and descriptions. </span><span class="kobospan" id="kobo.353.2">The registry’s history-tracking capabilities have emerged as a valuable feature, allowing users to gain insights into the evolution of models over time. </span><span class="kobospan" id="kobo.353.3">Tags and descriptions offer context and facilitate experiment tracking for accessing and filtering the registry history, enabling a comprehensive view of model-related activities. </span><span class="kobospan" id="kobo.353.4">Overall, the model</span><a id="_idIndexMarker370" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.354.1"> registry emerges as a powerful addition to Snowpark ML, centralizing model management, facilitating experimentation, and ensuring a streamlined and organized approach to model development </span><span><span class="kobospan" id="kobo.355.1">and deployment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.356.1">Overall, the model registry in Snowpark plays a pivotal role in streamlining the ML model lifecycle, from development and deployment to monitoring, governance, retraining, and retirement. </span><span class="kobospan" id="kobo.356.2">By providing a centralized platform for managing models, it helps organizations maximize the value of their ML investments while minimizing operational overhead </span><span><span class="kobospan" id="kobo.357.1">and risks.</span></span></p>
<h1 id="_idParaDest-101" class="calibre5"><a id="_idTextAnchor102" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.358.1">Managing Snowpark model data</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.359.1">In the previous </span><a id="_idIndexMarker371" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.360.1">section, we covered the deployment of ML models using the model registry. </span><span class="kobospan" id="kobo.360.2">This section will look at managing Snowpark Models using feature stores. </span><span class="kobospan" id="kobo.360.3">Snowpark ML Feature Store simplifies the feature engineering process and is integral to ML, significantly influencing model performance based on the quality of features employed. </span><span class="kobospan" id="kobo.360.4">This chapter will help us learn about using feature stores and managing </span><span><span class="kobospan" id="kobo.361.1">Snowpark models.</span></span></p>
<h2 id="_idParaDest-102" class="calibre7"><a id="_idTextAnchor103" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.362.1">Snowpark Feature Store</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.363.1">The Snowpark Feature Store</span><a id="_idIndexMarker372" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.364.1"> is an integrated </span><a id="_idIndexMarker373" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.365.1">solution for data scientists and ML engineers. </span><span class="kobospan" id="kobo.365.2">It facilitates the creation, storage, management, and serving of ML features for model training and inference and is accessible through the Snowpark ML library. </span><span class="kobospan" id="kobo.365.3">The feature store defines, manages, and retrieves features, supported by a managed infrastructure for feature metadata management and continuous feature processing. </span><span class="kobospan" id="kobo.365.4">Its primary function is to make these features readily available for reuse in the ongoing development of future ML models. </span><span class="kobospan" id="kobo.365.5">Feature stores play a pivotal role in operationalizing data input, tracking, and governance within the realm of feature engineering </span><span><span class="kobospan" id="kobo.366.1">for ML:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer149">
<span class="kobospan" id="kobo.367.1"><img alt="Figure 6.17 – Feature Store" src="image/B19923_06_17.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.368.1">Figure 6.17 – Feature Store</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">By </span><a id="_idIndexMarker374" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.370.1">leveraging the Snowpark Feature Store, which</span><a id="_idIndexMarker375" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.371.1"> is designed to simplify and enhance this process by offering increased efficiency for data scientists and ML practitioners. </span><span class="kobospan" id="kobo.371.2">ML teams can uphold a singular and updated source of truth for model training, versioning, and inference features. </span><span class="kobospan" id="kobo.371.3">We will use the </span><em class="italic"><span class="kobospan" id="kobo.372.1">Bike Sharing</span></em><span class="kobospan" id="kobo.373.1"> dataset and the ML model developed in the previous section to showcase how the Feature Store enhances the model development and </span><span><span class="kobospan" id="kobo.374.1">deployment cycle.</span></span></p>
<h2 id="_idParaDest-103" class="calibre7"><a id="_idTextAnchor104" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.375.1">Benefits of Feature Store</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.376.1">Utilizing </span><a id="_idIndexMarker376" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.377.1">feature stores provides several benefits for ML initiatives. </span><span class="kobospan" id="kobo.377.2">Firstly, they enable feature reuse by saving developed features, allowing them to be quickly accessed and repurposed for new ML models, thereby saving time and effort. </span><span class="kobospan" id="kobo.377.3">Secondly, they ensure feature consistency by providing a centralized registry for all ML features, maintaining consistent definitions and documentation across teams. </span><span class="kobospan" id="kobo.377.4">Thirdly, feature stores help maintain peak model performance by centralizing feature pipelines, ensuring consistency between training and inference, and continuously monitoring data pipelines for </span><span><span class="kobospan" id="kobo.378.1">any discrepancies.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.379.1">Furthermore, feature stores enhance security and data governance by providing detailed information about each ML model’s training data and deployment data, facilitating iteration and debugging. </span><span class="kobospan" id="kobo.379.2">Integrating feature stores with cloud data warehouses enhances data security, ensuring the protection of both models and training data. </span><span class="kobospan" id="kobo.379.3">Lastly, feature stores foster collaboration between teams by offering a centralized platform for the development, storage, modification, and sharing of ML features, promoting</span><a id="_idIndexMarker377" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.380.1"> cross-team collaboration and idea-sharing for multiple </span><span><span class="kobospan" id="kobo.381.1">business applications.</span></span></p>
<h2 id="_idParaDest-104" class="calibre7"><a id="_idTextAnchor105" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.382.1">Feature stores versus data warehouses</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.383.1">Delving into the distinction between feature stores and data warehouses sheds light on their collaborative role in enhancing value within </span><span><span class="kobospan" id="kobo.384.1">ML projects.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.385.1">Similarities – shared traits and functions</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.386.1">Both</span><a id="_idIndexMarker378" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.387.1"> feature stores and data warehouses exhibit parallels in their operational methodologies. </span><span class="kobospan" id="kobo.387.2">They rely on </span><strong class="bold"><span class="kobospan" id="kobo.388.1">Extract, Transform, Load</span></strong><span class="kobospan" id="kobo.389.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.390.1">ETL</span></strong><span class="kobospan" id="kobo.391.1">) pipelines to facilitate data management and accessibility. </span><span class="kobospan" id="kobo.391.2">Additionally, they serve as repositories endowed with metadata, fostering seamless data sharing and utilization across </span><span><span class="kobospan" id="kobo.392.1">organizational teams.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.393.1">End users – tailored utility</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.394.1">A notable </span><a id="_idIndexMarker379" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.395.1">deviation lies in their primary user base. </span><span class="kobospan" id="kobo.395.2">Data warehouses traditionally cater to analysts entrenched in the generation of comprehensive business reports, delving into historical data for strategic insights. </span><span class="kobospan" id="kobo.395.3">Conversely, feature stores cater specifically to data scientists immersed in the development of predictive ML models. </span><span class="kobospan" id="kobo.395.4">While the latter may draw from data warehouses for supplementary insights, their core function revolves around leveraging feature stores for streamlined model development </span><span><span class="kobospan" id="kobo.396.1">and inference.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.397.1">Data types – structural variances</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.398.1">Structurally, data</span><a id="_idIndexMarker380" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.399.1"> warehouses house domain-specific data within relational databases characterized by well-defined schemas. </span><span class="kobospan" id="kobo.399.2">This structured format facilitates streamlined querying and retrieval of pertinent information, ideal for analytical endeavors. </span><span class="kobospan" id="kobo.399.3">Conversely, feature stores house a distinct array of feature values crucial for ML model training. </span><span class="kobospan" id="kobo.399.4">These values encompass both quantitative and categorical variables, enriching the model development process with </span><span><span class="kobospan" id="kobo.400.1">granular insights.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.401.1">ETL pipelines – divergent trajectories</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.402.1">The</span><a id="_idIndexMarker381" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.403.1"> operational dynamics of ETL pipelines further accentuate the disparity between feature stores and data warehouses. </span><span class="kobospan" id="kobo.403.2">ETL processes within data warehouses predominantly focus on data cleansing and transformation, ensuring data accuracy and coherence within the defined schema. </span><span class="kobospan" id="kobo.403.3">In contrast, feature store pipelines embark on a more intricate journey, encompassing data extraction, transformation, and feature engineering. </span><span class="kobospan" id="kobo.403.4">The transformations within feature stores often entail sophisticated computations and aggregations to distill intricate insights vital for model training and inference, underscoring their pivotal role in the </span><span><span class="kobospan" id="kobo.404.1">ML lifecycle.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.405.1">Now that we’ve grasped the essence of feature stores, comprehending their significance and differentiation from data warehouses, let’s delve deeper into the various components comprising a </span><span><span class="kobospan" id="kobo.406.1">feature store.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.407.1">In the subsequent section, we’ll embark on the creation of a rudimentary feature store tailored to the </span><em class="italic"><span class="kobospan" id="kobo.408.1">Bike Sharing</span></em><span class="kobospan" id="kobo.409.1"> dataset, focusing solely on weather-related features. </span><span class="kobospan" id="kobo.409.2">The process entails </span><span><span class="kobospan" id="kobo.410.1">the following:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.411.1">Feature </span><span><span class="kobospan" id="kobo.412.1">store creation</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.413.1">Feature </span><span><span class="kobospan" id="kobo.414.1">entity creation</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.415.1">Selecting and transforming </span><span><span class="kobospan" id="kobo.416.1">weather features</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.417.1">Creating a </span><span><span class="kobospan" id="kobo.418.1">feature view</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.419.1">Generating datasets enriched with the </span><span><span class="kobospan" id="kobo.420.1">feature view</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.421.1">Constructing an ML model empowered by the </span><span><span class="kobospan" id="kobo.422.1">enriched dataset</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.423.1">Facilitating predictions based on the </span><span><span class="kobospan" id="kobo.424.1">trained model</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.425.1">Let’s discuss each of them </span><span><span class="kobospan" id="kobo.426.1">in detail.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.427.1">Creating a feature store</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.428.1">Initiating</span><a id="_idIndexMarker382" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.429.1"> work with the Snowflake Feature Store involves establishing a new feature store or connecting to an existing one. </span><span class="kobospan" id="kobo.429.2">This is accomplished by furnishing specific details to the </span><strong class="source-inline"><span class="kobospan" id="kobo.430.1">FeatureStore</span></strong><span class="kobospan" id="kobo.431.1"> constructor, including a Snowpark session, database name, feature store name, and default warehouse name. </span><span class="kobospan" id="kobo.431.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.432.1">creation_mode</span></strong><span class="kobospan" id="kobo.433.1"> parameter is crucial in determining whether a new feature store should be created if it does not exist. </span><span class="kobospan" id="kobo.433.2">To implement this functionality, we’ll use the </span><span><span class="kobospan" id="kobo.434.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.435.1">
from snowflake.ml.feature_store import (
    FeatureStore, FeatureView, Entity, CreationMode)
fs = FeatureStore(
    session=session,
    database="SNOWPARK_DEFINITIVE_GUIDE",
    name="BIKE_SHARING_FEATURES",
    default_warehouse="COMPUTE_WH",
    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,
)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.436.1">This will open a session to the feature store and allow it to be accessed in the Snowpark session. </span><span class="kobospan" id="kobo.436.2">The next step will be to set up a feature entity on this </span><span><span class="kobospan" id="kobo.437.1">feature store.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.438.1">Creating feature entities</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.439.1">Entities </span><a id="_idIndexMarker383" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.440.1">are fundamental elements linked with features and feature views, providing the cornerstone for feature lookups by defining join keys. </span><span class="kobospan" id="kobo.440.2">Users can generate novel entities and formally register them within the feature store, thereby fostering connections and relationships between various features. </span><span class="kobospan" id="kobo.440.3">This code creates an entity named </span><strong class="source-inline"><span class="kobospan" id="kobo.441.1">WEATHER</span></strong><span class="kobospan" id="kobo.442.1"> with an </span><strong class="source-inline"><span class="kobospan" id="kobo.443.1">ID</span></strong><span class="kobospan" id="kobo.444.1"> join key, registers it in the</span><a id="_idIndexMarker384" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.445.1"> feature store (</span><strong class="source-inline"><span class="kobospan" id="kobo.446.1">fs</span></strong><span class="kobospan" id="kobo.447.1">), and then displays a list of entities in the </span><span><span class="kobospan" id="kobo.448.1">feature store:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.449.1">
entity = Entity(name="ENTITY_WEATHER", join_keys=["ID"])
fs.register_entity(entity)
fs.list_entities().show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.450.1">This generates the </span><span><span class="kobospan" id="kobo.451.1">following output:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer150">
<span class="kobospan" id="kobo.452.1"><img alt="Figure 6.18 – Feature entity" src="image/B19923_06_18.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.453.1">Figure 6.18 – Feature entity</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.454.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.455.1">ENTITY_WEATHER</span></strong><span class="kobospan" id="kobo.456.1"> entity has been created with the ID as the join key. </span><span class="kobospan" id="kobo.456.2">The next step is to set up </span><span><span class="kobospan" id="kobo.457.1">feature views.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.458.1">Creating feature views</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.459.1">Within </span><a id="_idIndexMarker385" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.460.1">a feature store, feature views act as comprehensive pipelines, systematically transforming raw data into interconnected features at regular intervals. </span><span class="kobospan" id="kobo.460.2">These feature views are materialized from designated source tables, ensuring incremental and efficient updates as fresh data is introduced. </span><span class="kobospan" id="kobo.460.3">In our previous chapter, we explored a dataset that comprised various weather-related features. </span><span class="kobospan" id="kobo.460.4">To preprocess this data effectively, we employed a </span><span><span class="kobospan" id="kobo.461.1">Snowpark pipeline.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.462.1">Through this pipeline, we performed transformations on the </span><strong class="source-inline"><span class="kobospan" id="kobo.463.1">SEASON</span></strong><span class="kobospan" id="kobo.464.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.465.1">WEATHER</span></strong><span class="kobospan" id="kobo.466.1"> columns using one-hot encoding techniques. </span><span class="kobospan" id="kobo.466.2">Additionally, we normalized the </span><strong class="source-inline"><span class="kobospan" id="kobo.467.1">TEMP</span></strong><span class="kobospan" id="kobo.468.1"> column to ensure consistency and facilitate model training. </span><span class="kobospan" id="kobo.468.2">Given that we thoroughly discussed each step of this pipeline in our previous chapter, we’ll be revisiting it briefly, focusing more on a high-level overview rather than delving into </span><span><span class="kobospan" id="kobo.469.1">detailed explanations:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.470.1">
import snowflake.ml.modeling.preprocessing as snowml
from snowflake.ml.modeling.pipeline import Pipeline
from snowflake.snowpark.types import IntegerType
# CREATING ID COLUMN
from snowflake.snowpark.functions \
    import monotonically_increasing_id
df = df.withColumn("ID", monotonically_increasing_id())
df = df.drop("DATETIME","DATE")
CATEGORICAL_COLUMNS = ["SEASON","WEATHER"]
CATEGORICAL_COLUMNS_OHE = ["SEASON_OE","WEATHER_OE"]
MIN_MAX_COLUMNS = ["TEMP"]
import numpy as np
categories = {
    "SEASON": np.array([1,2,3,4]),
    "WEATHER": np.array([1,2,3,4]),
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.471.1">This code</span><a id="_idIndexMarker386" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.472.1"> block utilizes Snowflake’s ML capabilities for data preprocessing. </span><span class="kobospan" id="kobo.472.2">It imports necessary modules such as preprocessing functions and the </span><strong class="source-inline"><span class="kobospan" id="kobo.473.1">Pipeline</span></strong><span class="kobospan" id="kobo.474.1"> class. </span><span class="kobospan" id="kobo.474.2">The code creates a new </span><strong class="source-inline"><span class="kobospan" id="kobo.475.1">ID</span></strong><span class="kobospan" id="kobo.476.1"> column with unique identifiers for each row and drops unnecessary columns. </span><span class="kobospan" id="kobo.476.2">It defines lists of categorical columns and their transformed versions after one-hot encoding, along with columns to be normalized. </span><span class="kobospan" id="kobo.476.3">Additionally, it specifies categories for each categorical column, likely for encoding purposes, facilitating effective ML </span><span><span class="kobospan" id="kobo.477.1">model processing:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.478.1">
preprocessing_pipeline = Pipeline(
    steps=[
        (
            "OE",
            snowml.OrdinalEncoder(
                input_cols=CATEGORICAL_COLUMNS,
                output_cols=CATEGORICAL_COLUMNS_OHE,
                categories=categories
            )
        ),
        (
            "MMS",
            snowml.MinMaxScaler(
                clip=True,
                input_cols=MIN_MAX_COLUMNS,
                output_cols=MIN_MAX_COLUMNS,
            )
        )
    ]
)
transformed_df = preprocessing_pipeline.fit(df).transform(df)
transformed_df.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.479.1">In the first step, an ordinal encoder (</span><strong class="source-inline"><span class="kobospan" id="kobo.480.1">OE</span></strong><span class="kobospan" id="kobo.481.1">) is applied to transform categorical columns specified in the </span><strong class="source-inline"><span class="kobospan" id="kobo.482.1">CATEGORICAL_COLUMNS</span></strong><span class="kobospan" id="kobo.483.1"> list into their one-hot encoded versions, as defined by the </span><strong class="source-inline"><span class="kobospan" id="kobo.484.1">CATEGORICAL_COLUMNS_OHE</span></strong><span class="kobospan" id="kobo.485.1"> list. </span><span class="kobospan" id="kobo.485.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.486.1">categories</span></strong><span class="kobospan" id="kobo.487.1"> parameter specifies the categories for each categorical column, likely used for </span><span><span class="kobospan" id="kobo.488.1">encoding purposes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.489.1">In the second step, a min-max scaler (</span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">MMS</span></strong><span class="kobospan" id="kobo.491.1">) is used to normalize columns specified in the </span><strong class="source-inline"><span class="kobospan" id="kobo.492.1">MIN_MAX_COLUMNS</span></strong><span class="kobospan" id="kobo.493.1"> list. </span><span class="kobospan" id="kobo.493.2">This scaler ensures that values in these columns are scaled to a specific range, typically between </span><strong class="source-inline"><span class="kobospan" id="kobo.494.1">0</span></strong><span class="kobospan" id="kobo.495.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.496.1">1</span></strong><span class="kobospan" id="kobo.497.1">, while preserving their </span><span><span class="kobospan" id="kobo.498.1">relative proportions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.499.1">The </span><a id="_idIndexMarker387" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.500.1">preprocessing pipeline is then applied to the </span><strong class="source-inline"><span class="kobospan" id="kobo.501.1">df</span></strong><span class="kobospan" id="kobo.502.1"> DataFrame using the fit-transform paradigm, where the pipeline is first fit to the data to learn parameters (for example, category mappings for ordinal encoding), and then applied to transform the DataFrame. </span><span class="kobospan" id="kobo.502.2">The transformed DataFrame is then displayed using the </span><strong class="source-inline"><span class="kobospan" id="kobo.503.1">show()</span></strong><span class="kobospan" id="kobo.504.1"> method. </span><span class="kobospan" id="kobo.504.2">Overall, this code prepares the DataFrame for further analysis or model training by preprocessing its columns using the specified pipeline. </span><span class="kobospan" id="kobo.504.3">The resultant DataFrame is </span><span><span class="kobospan" id="kobo.505.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer151">
<span class="kobospan" id="kobo.506.1"><img alt="Figure 6.19 – Transformed DataFrame" src="image/B19923_06_19.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.507.1">Figure 6.19 – Transformed DataFrame</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.508.1">Throughout the model-building process, various models are constructed using subsets of features, such as weather features and time-related features. </span><span class="kobospan" id="kobo.508.2">Additionally, models are developed using combined data to ascertain superior performance. </span><span class="kobospan" id="kobo.508.3">To expedite the model-building process and reduce data engineering overheads, we’ll organize weather-related features into a dedicated feature view. </span><span class="kobospan" id="kobo.508.4">Subsequently, we’ll leverage this feature view to generate datasets and construct an XGBoost model in the </span><span><span class="kobospan" id="kobo.509.1">ensuing section:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.510.1">
feature_df = transformed_df.select(["SEASON_OE",
    "WEATHER_OE", "TEMP", "ATEMP", "HUMIDITY",
    "WINDSPEED", "ID"])
fv = FeatureView(
    name="WEATHER_FEATURES",
    entities=[entity],
    feature_df=feature_df,
    desc="weather features"
)
fv = fs.register_feature_view(
    feature_view=fv,
    version="V1",
    block=True
)
fs.read_feature_view(fv).show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.511.1">The </span><a id="_idIndexMarker388" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.512.1">code selects specific columns from the DataFrame to create a feature DataFrame (</span><strong class="source-inline"><span class="kobospan" id="kobo.513.1">feature_df</span></strong><span class="kobospan" id="kobo.514.1">). </span><span class="kobospan" id="kobo.514.2">Then, it constructs a feature view named </span><strong class="source-inline"><span class="kobospan" id="kobo.515.1">WEATHER_FEATURES</span></strong><span class="kobospan" id="kobo.516.1"> associated with the previously defined entity and registers it in the feature store (</span><strong class="source-inline"><span class="kobospan" id="kobo.517.1">fs</span></strong><span class="kobospan" id="kobo.518.1">) with version </span><strong class="source-inline"><span class="kobospan" id="kobo.519.1">V1</span></strong><span class="kobospan" id="kobo.520.1">. </span><span class="kobospan" id="kobo.520.2">The resulting DataFrame is </span><span><span class="kobospan" id="kobo.521.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer152">
<span class="kobospan" id="kobo.522.1"><img alt="Figure 6.20 – Feature DataFrame" src="image/B19923_06_20.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.523.1">Figure 6.20 – Feature DataFrame</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.524.1">Once developed, features can be systematically stored in the feature store, fostering their availability for reuse or seamless sharing among various ML models and teams. </span><span class="kobospan" id="kobo.524.2">This functionality </span><a id="_idIndexMarker389" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.525.1">significantly accelerates the creation of new ML models, eliminating the redundancy of building each feature from scratch. </span><span class="kobospan" id="kobo.525.2">In the same way, we can create another feature view as rental features by combining </span><span><span class="kobospan" id="kobo.526.1">similar features.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.527.1">Preparing the dataset</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.528.1">Once our </span><a id="_idIndexMarker390" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.529.1">feature pipelines are meticulously configured and ready, we can initiate their deployment to generate training data. </span><span class="kobospan" id="kobo.529.2">Subsequently, these feature pipelines become instrumental in facilitating model prediction, marking the seamless transition from feature engineering to the practical application of </span><span><span class="kobospan" id="kobo.530.1">ML models:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.531.1">
#GENERATING TRAINING DATA
spine_df = session.table("BSD_TRAINING")
spine_df = spine_df.withColumn("ID",
    monotonically_increasing_id())
spine_df = spine_df.select("ID", "COUNT")
spine_df.show()
train_data = fs.generate_dataset(
    spine_df=spine_df,
    features=[
        fv.slice([
            "HUMIDITY","SEASON_OE","TEMP",
            "WEATHER_OE","WINDSPEED"
        ])
    ],
    materialized_table=None,
    spine_timestamp_col=None,
    spine_label_cols=["COUNT"],
    save_mode="merge",
    exclude_columns=['ID']
)
train_data.df.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.532.1">Creating training</span><a id="_idIndexMarker391" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.533.1"> data becomes straightforward as materialized feature views inherently encompass crucial metadata such as join keys and timestamps</span><a id="_idIndexMarker392" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.534.1"> for </span><strong class="bold"><span class="kobospan" id="kobo.535.1">point-in-time</span></strong><span class="kobospan" id="kobo.536.1"> (</span><span><strong class="bold"><span class="kobospan" id="kobo.537.1">PIT</span></strong></span><span><span class="kobospan" id="kobo.538.1">) lookup:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer153">
<span class="kobospan" id="kobo.539.1"><img alt="Figure 6.21 – Training data" src="image/B19923_06_21.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.540.1">Figure 6.21 – Training data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.541.1">The process primarily involves supplying spine data—termed so because it serves as the foundational structure enriched by feature joins. </span><span class="kobospan" id="kobo.541.2">In our case, spine data encompasses the feature to be predicted—</span><strong class="source-inline"><span class="kobospan" id="kobo.542.1">COUNT</span></strong><span class="kobospan" id="kobo.543.1">—along with the join key column ID. </span><span class="kobospan" id="kobo.543.2">Moreover, the flexibility to generate datasets with subsets of features within the feature view is available through slicing. </span><span class="kobospan" id="kobo.543.3">Now that we have the training data ready, we will use it to train the model and predict the data output using the </span><span><span class="kobospan" id="kobo.544.1">feature store.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.545.1">The </span><a id="_idIndexMarker393" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.546.1">preparation of all data—both for training and operational use—requires meticulous handling through feature pipelines. </span><span class="kobospan" id="kobo.546.2">These pipelines, resembling traditional data pipelines, aggregate, validate, and transform data output in a format suitable for input into the ML model. </span><span class="kobospan" id="kobo.546.3">Properly orchestrated feature pipelines ensure that data is refined before being fed into the model, maintaining the integrity and relevance of features derived from the </span><span><span class="kobospan" id="kobo.547.1">training process.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.548.1">Model training</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.549.1">We</span><a id="_idIndexMarker394" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.550.1"> covered the model-building process extensively in the previous chapter, so in this section, we will focus on building it using the training dataset generated from feature views from the feature store. </span><span class="kobospan" id="kobo.550.2">We are using a similar method outlined in the previous chapter in training a gradient boost model but just using </span><span><span class="kobospan" id="kobo.551.1">feature views:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.552.1">
from snowflake.ml.modeling.model_selection import GridSearchCV
from snowflake.ml.modeling.ensemble \
    import GradientBoostingRegressor
FEATURE_LIST = ["TEMP", "WINDSPEED", "SEASON_OE", "WEATHER_OE"]
LABEL_COLUMNS = ['COUNT']
OUTPUT_COLUMNS = ['PREDICTED_COUNT']
param_grid = {
    "n_estimators":[100, 200, 300, 400, 500],
    "learning_rate":[0.1, 0.2, 0.3, 0.4, 0.5],
}
grid_search = GridSearchCV(
    estimator=GradientBoostingRegressor(),
    param_grid=param_grid,
    n_jobs = -1,
    scoring="neg_root_mean_squared_error",
    input_cols=FEATURE_LIST,
    label_cols=LABEL_COLUMNS,
    output_cols=OUTPUT_COLUMNS
)
train_df = train_data.df.drop(["ID"])
grid_search.fit(train_df)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.553.1">The </span><a id="_idIndexMarker395" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.554.1">elegance of Snowpark surfaces in this simplicity, as no significant modifications are needed to train a model using feature views seamlessly. </span><span class="kobospan" id="kobo.554.2">We will create testing data to test the model for accuracy using the </span><span><span class="kobospan" id="kobo.555.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.556.1">
test_df = spine_df.limit(3).select("ID")
enriched_df = fs.retrieve_feature_values(
    test_df, train_data.load_features())
enriched_df = enriched_df.drop('ID')
enriched_df.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.557.1">This creates a test DataFrame (</span><strong class="source-inline"><span class="kobospan" id="kobo.558.1">test_df</span></strong><span class="kobospan" id="kobo.559.1">) by selecting the </span><strong class="source-inline"><span class="kobospan" id="kobo.560.1">ID</span></strong><span class="kobospan" id="kobo.561.1"> column from the first three rows of </span><strong class="source-inline"><span class="kobospan" id="kobo.562.1">spine_df</span></strong><span class="kobospan" id="kobo.563.1">. </span><span class="kobospan" id="kobo.563.2">Then, it retrieves and displays feature values for the test data frame using </span><a id="_idIndexMarker396" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.564.1">the feature store and training data generated from </span><span><span class="kobospan" id="kobo.565.1">feature views:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer154">
<span class="kobospan" id="kobo.566.1"><img alt="Figure 6.22 – Testing data" src="image/B19923_06_22.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.567.1">Figure 6.22 – Testing data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.568.1">Now that the testing data is ready, we can predict the model using this data to get </span><span><span class="kobospan" id="kobo.569.1">the results.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.570.1">Model prediction</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.571.1">In this</span><a id="_idIndexMarker397" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.572.1"> section, we will use the testing data generated from the feature store to make </span><span><span class="kobospan" id="kobo.573.1">a prediction:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.574.1">
pred = grid_search.predict(enriched_df.to_pandas())
pred.head()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.575.1">The prediction displays the results with the predicted count value, showing the number of customers using shared bikes at the </span><span><span class="kobospan" id="kobo.576.1">given hour:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer155">
<span class="kobospan" id="kobo.577.1"><img alt="Figure 6.23 – Model prediction" src="image/B19923_06_23.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.578.1">Figure 6.23 – Model prediction</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.579.1">This shows how easy and improved it is to use a feature store to build a Snowpark ML model. </span><span class="kobospan" id="kobo.579.2">In the next section, we will highlight some benefits of using </span><span><span class="kobospan" id="kobo.580.1">feature stores.</span></span></p>
<h2 id="_idParaDest-105" class="calibre7"><a id="_idTextAnchor106" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.581.1">When to utilize versus when to avoid feature stores</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.582.1">Feature stores </span><a id="_idIndexMarker398" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.583.1">are particularly advantageous in ML processes when there’s a need for efficient feature management and reuse across multiple models or teams. </span><span class="kobospan" id="kobo.583.2">They shine in</span><a id="_idIndexMarker399" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.584.1"> the </span><span><span class="kobospan" id="kobo.585.1">following scenarios:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.586.1">Feature reuse</span></strong><span class="kobospan" id="kobo.587.1">: Features need to be reused or shared between different ML models or teams, reducing redundant efforts in </span><span><span class="kobospan" id="kobo.588.1">feature engineering</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.589.1">Consistency and governance</span></strong><span class="kobospan" id="kobo.590.1">: Ensuring consistent definitions, documentation, and governance of features across diverse ML projects or teams </span><span><span class="kobospan" id="kobo.591.1">is critical</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.592.1">Model performance</span></strong><span class="kobospan" id="kobo.593.1">: Maintaining peak model performance by ensuring consistency between feature definitions in training and inference pipelines, thus avoiding performance degradation due </span><span><span class="kobospan" id="kobo.594.1">to discrepancies</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.595.1">Data collaboration</span></strong><span class="kobospan" id="kobo.596.1">: Fostering collaboration between different teams or stakeholders involved in ML projects by offering a centralized platform for feature development, storage, modification, </span><span><span class="kobospan" id="kobo.597.1">and sharing</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.598.1">Scalability</span></strong><span class="kobospan" id="kobo.599.1">: Handling large volumes of features and data efficiently, especially in environments where data is continuously evolving or </span><span><span class="kobospan" id="kobo.600.1">being updated</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.601.1">However, feature stores may not be necessary in the </span><span><span class="kobospan" id="kobo.602.1">following scenarios:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.603.1">Simple models</span></strong><span class="kobospan" id="kobo.604.1">: For simple models with few features and minimal complexity, the overhead of setting up and maintaining a feature store may outweigh </span><span><span class="kobospan" id="kobo.605.1">its benefits</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.606.1">Static data</span></strong><span class="kobospan" id="kobo.607.1">: In cases where the data is relatively static and doesn’t require frequent updates or feature engineering, the need for a feature store may </span><span><span class="kobospan" id="kobo.608.1">be limited</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.609.1">Limited collaboration</span></strong><span class="kobospan" id="kobo.610.1">: When ML projects involve a small, tightly-knit team working on isolated tasks without the need for extensive collaboration or feature sharing, the use of a feature store may </span><span><span class="kobospan" id="kobo.611.1">be unnecessary</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.612.1">Resource constraints</span></strong><span class="kobospan" id="kobo.613.1">: Organizations with limited resources or infrastructure may find it challenging to implement and maintain a feature </span><span><span class="kobospan" id="kobo.614.1">store effectively</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.615.1">In summary, while</span><a id="_idIndexMarker400" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.616.1"> feature stores offer numerous benefits for efficient feature management in ML projects, their adoption should be carefully considered based on the specific needs and constraints of each project </span><span><span class="kobospan" id="kobo.617.1">or organization.</span></span></p>
<h1 id="_idParaDest-106" class="calibre5"><a id="_idTextAnchor107" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.618.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.619.1">This chapter discussed the model registry and the importance of meaningful tags and descriptions, offering context and facilitating experiment tracking. </span><span class="kobospan" id="kobo.619.2">We also highlighted different methods of operating with the model registry. </span><span class="kobospan" id="kobo.619.3">We navigated through the capabilities of the Snowflake Feature Store within the Snowpark ML ecosystem and how to utilize it for managing </span><span><span class="kobospan" id="kobo.620.1">Snowflake models.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.621.1">In the next chapter, we will learn about developing native applications using the </span><span><span class="kobospan" id="kobo.622.1">Snowpark framework.</span></span></p>
</div>
</body></html>