- en: Chapter 2. Exploratory Data Analysis and Visualization in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analytic pipelines are not built from raw data in a single step. Rather, development
    is an iterative process that involves understanding the data in greater detail
    and systematically refining both model and inputs to solve a problem. A key part
    of this cycle is interactive data analysis and visualization, which can provide
    initial ideas for features in our predictive modeling or clues as to why an application
    is not behaving as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spreadsheet programs are one kind of interactive tool for this sort of exploration:
    they allow the user to import tabular information, pivot and summarize data, and
    generate charts. However, what if the data in question is too large for such a
    spreadsheet application? What if the data is not tabular, or is not displayed
    effectively as a line or bar chart? In the former case, we could simply obtain
    a more powerful computer, but the latter is more problematic. Simply put, many
    traditional data visualization tools are not well suited to complex data types
    such as text or images. Additionally, spreadsheet programs often assume data is
    in a finalized form, whereas in practice we will often need to clean up the raw
    data before analysis. We might also want to calculate more complex statistics
    than simple averages or sums. Finally, using the same programming tools to clean
    up and visualize our data as well as generate the model itself and test its performance
    allows a more streamlined development process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we introduce interactive Python (IPython) notebook applications
    (Pérez, Fernando, and Brian E. Granger. *IPython: a system for interactive scientific
    computing*. *Computing in Science & Engineering* 9.3 (2007): 21-29). The notebooks
    form a data preparation, exploration, and modeling environment that runs inside
    a web browser. The commands typed in the input cells of an IPython notebook are
    translated and executed as they are received: this kind of interactive programming
    is helpful for data exploration, where we may refine our efforts and successively
    develop more detailed analyses. Recording our work in these Notebooks will help
    to both backtrack during debugging and serve as a record of insights that can
    be easily shared with colleagues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading raw data into an IPython notebook, cleaning it, and manipulating it
    using the Pandas library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using IPython to process numerical, categorical, geospatial, or time-series
    data, and perform basic statistical analyses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic exploratory analyses: summary statistics (mean, variance, median), distributions
    (histogram and kernel density), and auto-correlation (time-series).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to distributed data processing with Spark RDDs and DataFrames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring categorical and numerical data in IPython
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start our explorations in IPython by loading a text file into a DataFrame,
    calculating some summary statistics, and visualizing distributions. For this exercise
    we'll use a set of movie ratings and metadata from the Internet Movie Database
    ([http://www.imdb.com/](http://www.imdb.com/)) to investigate what factors might
    correlate with high ratings for films on this website. Such information might
    be helpful, for example, in developing a recommendation system based on this kind
    of user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Installing IPython notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To follow along with the examples, you should have a Windows, Linux, or Mac
    OSX operating system installed on your computer and access to the Internet. There
    are a number of options available to install IPython: since each of these resources
    includes installation guides, we provide a summary of the available sources and
    direct the reader to the relevant documentation for more in-depth instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For most users, a pre-bundled Python environment such as Anaconda (Continuum
    Analytics) or Canopy (Enthought) provides an out-of-the-box distribution with
    IPython and all the libraries we will use in these exercises: these products are
    self-contained, and thus you should not have to worry about conflicting versions
    or dependency management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more ambitious users, you can install a python distribution of your choice,
    followed by individual installation of the required libraries using package managers
    such as `pip` or `easy_install`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notebook interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve installed IPython, open the command prompt (terminal) on your
    computer and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that depending upon where you installed the program, the `jupyter` command
    may require the binary file that launches `jupyter` to be on your system path.
    You should see a series of commands like the following in your terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The notebook interface](img/B04881_chapter02_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This starts the **kernel**, the python interpreter that computes the result
    of commands entered into the notebook. If you want to stop the notebook, type
    *Ctrl* + *C*, and enter **yes**, and the kernel will shut down.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the kernel starts, your default web browser should also open, giving you
    a homepage that looks like this:![The notebook interface](img/B04881_chapter02_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Files** tab (see above) will show you all of the files in the directory
    where you started the IPython process. Clicking **Running** will give you a list
    of all running notebooks – there are none when you start:![The notebook interface](img/B04881_chapter02_30.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the Clusters panel gives a list of external clusters, should we decide
    to parallelize our calculations by submitting commands to be processed on more
    than one machine. We won't worry about this for now, but it will come in useful
    later when we begin to train predictive models, a task that may often be accelerated
    by distributing the work among many computers or processors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Returning to the **Files** tab, you will notice two options in the top right-hand
    corner. One is to **Upload** a file: while we are running IPython locally, it
    could just as easily be running on a remote server, with the analyst accessing
    the notebook through a browser. In this case, to interact with files stored on
    our own machine, we can use this button to open a prompt and selected the desired
    files to upload to the server, where we could then analyze them in the notebook.
    The **New** tab lets you create a new folder, text file, a Python terminal running
    in the browser, or a notebook.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For now, let''s open the sample notebook for this chapter by double clicking
    on **B04881_chapter02_code01.ipynb**. This opens the notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The notebook interface](img/B04881_chapter02_03.jpg)The notebook consists
    of a series of cells, areas of text where we can type python code, execute it,
    and see the results of the commands. The python code in each cell can be executed
    by clicking the ![The notebook interface](img/B04881_chapter02_55.jpg) button
    on the toolbar, and a new cell can be inserted below the current one by clicking
    ![The notebook interface](img/B04881_chapter02_56.jpg).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While the import statements in the first cell probably look familiar from your
    experience of using python in the command line or in a script, the `%matplotlib`
    inline command is not actually python: it is a markup instruction to the notebook
    that `matplotlib` images are to be displayed inline the browser. We enter this
    command at the beginning of the notebook so that all of our later plots use this
    setting. To run the import statements, click the ![The notebook interface](img/B04881_chapter02_57.jpg)
    button or press *Ctrl* + *Enter*. The `ln[1]` on the cell may briefly change to
    `[*]` as the command executes. There will be no output in this case, as all we
    did was import library dependencies. Now that our environment is ready, we can
    start examining some data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading and inspecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, we will import the data in `movies.csv` into a DataFrame object using
    the Pandas library (McKinney, Wes. *Python for data analysis: Data wrangling with
    Pandas*, NumPy, and IPython. O''Reilly Media, Inc., 2012). This DataFrame resembles
    traditional spreadsheet software and allows powerful extensions such as custom
    transformations and aggregations. These may be combined with numerical methods,
    such as those available in NumPy, for more advanced statistical analysis of the
    data. Let us continue our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If this were a new notebook, to add new cells we would go to the toolbar, click
    **Insert** and **Insert Cell Below**, or use the ![Loading and inspecting data](img/B04881_chapter02_57.jpg).
    button. However, in this example all the cells are already generated, therefore
    we run the following command in the second cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We've now created a DataFrame object using the Pandas library, `imdb_ratings`,
    and can begin analyzing the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s start by peeking at the beginning and end of the data using `head()`
    and `tail()`. Notice that by default this command returns the first five rows
    of data, but we can supply an integer argument to the command to specify the number
    of lines to return. Also, by default the first line of the file is assumed to
    contain the column names, which in this case is correct. Typing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We can similarly look at the last 15 lines of the data by typing:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Loading and inspecting data](img/B04881_chapter02_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Looking at individual rows gives us a sense of what kind of data the file contains:
    we can also look at summaries for all rows in each column using the command `describe()`,
    which returns the number of records, mean value, and other aggregate statistics.
    Try typing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Column names and their datatypes can be accessed using the properties `columns`
    and `dtypes`. Typing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Gives us the names of the columns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_58.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'If we issue the command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the datatypes of the columns have been automatically inferred
    when we first loaded the file:![Loading and inspecting data](img/B04881_chapter02_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we want to access the data in individual columns, we can do so using either
    `{DataFrame_name}.{column_name}` or `{DataFrame_name}[''column_name'']` (similar
    to a python dictionary). For example, typing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: or
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Without much work, we can already use these simple commands to ask a number
    of diagnostic questions about the data. Do the summary statistics we generated
    using `describe()` make sense (for example, the max rating should be 10, while
    the minimum is 1)? Is the data correctly parsed into the columns we expect?
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the first five rows of data we visualized using the `head()`
    command, this initial inspection also reveals some formatting issues we might
    want to consider. In the **budget** column, several entries have the value `NaN`,
    representing missing values. If we were going to try to predict movie ratings
    based on features including **budget**, we might need to come up with a rule to
    fill in these missing values, or encode them in a way that is correctly represented
    to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Basic manipulations – grouping, filtering, mapping, and pivoting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have looked at the basic features of the Pandas DataFrame, let
    us start applying some transformations and calculations to this data beyond the
    simple statistics we obtained through `describe()`. For example, if we wanted
    to calculate how many films belong to each release year, we can use following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the result is by default sorted by the count of records in each
    year (with the most films in this dataset released in 2002). What if we wanted
    to sort by the release year? The `sort_index()` command orders the result by its
    index (the year to which the count belongs). The index is similar to the axis
    of a plot, with values representing the point at each axis tick. Using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the DataFrame to begin asking analytical questions about the
    data, logically slicing and sub-selecting as we might in a database query. For
    example, let us select the subset of films released after 1999 with an R rating
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can group the data by any column(s) and calculate aggregated
    statistics using the `groupby` command and pass an array of calculations to perform
    as an argument to `aggregate`. Let us use the mean and standard deviation functions
    from NumPy to find the average and variation in ratings for films released in
    a given year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, sometimes the questions we want to ask require us to reshape or transform
    the raw data we are given. This will happen frequently in later chapters, when
    we develop features for predictive models. Pandas provide many tools for performing
    this kind of transformation. For example, while it would also be interesting to
    aggregate the data based on genre, we notice that in this dataset each genre is
    represented as a single column, with 1 or 0 indicating whether a film belongs
    to a given genre. It would be more useful for us to have a single column indicating
    which genre the film belongs to for use in aggregation operations. We can make
    such a column using the command `idxmax()` with the argument 1 to represent the
    maximum argument across columns (0 would represent the max index along rows),
    which returns the column with the greatest value out of those selected. Typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Gives the following result when we examine this new genre column using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We may also perhaps like to plot the data with colors representing a particular
    genre. To generate a color code for each genre, we can use a custom mapping function
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify the output by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also transpose the table and perform statistical calculations using
    the `pivot_table` command, which can perform aggregate calculations on groupings
    of rows and columns as in a spreadsheet. For example, to calculate the average
    rating per genre per year we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have performed some exploratory calculations, let us look at some
    visualizations of this information.
  prefs: []
  type: TYPE_NORMAL
- en: Charting with Matplotlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the practical features of IPython notebooks is the ability to plot data
    inline with our analyses. For example, if we wanted to visualize the distribution
    of film lengths we could use the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Charting with Matplotlib](img/B04881_chapter02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this is not really a very attractive image. To make a more aesthetically
    pleasing plot, we can change the default style using the `style.use()` command.
    Let us change the style to `ggplot`, which is used in the `ggplot` graphical library
    (Wickham, Hadley. *ggplot: An Implementation of the Grammar of Graphics*. R package
    version 0.4\. 0 (2006)). Typing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Gives a much more attractive graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see preceding, the default plot is a line chart. The line chart
    plots each datapoint (movie runtime) as a line, ordered from left to right by
    their row number in the DataFrame. To make a density plot of films by their genre,
    we can plot using the `groupby` command with the argument `type=kde`. **KDE**
    is an abbreviation for **Kernel Density Estimate** (Rosenblatt, Murray. Remarks
    on some nonparametric estimates of a density function. *The Annals of Mathematical
    Statistics 27.3 (1956): 832-837*; Parzen, Emanuel. On estimation of a probability
    density function and mode. The annals of mathematical statistics 33.3 (1962):
    1065-1076), meaning that for each point (film runtime) we estimate the density
    (proportion of the population with that runtime) with the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where `f(x)` is an estimate of the probability density, n is the number of
    records in our dataset, `h` is a bandwidth parameter, and `K` is a kernel function.
    As an example, if `K` were the Gaussian kernel given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where σ is the standard deviation and μ is the mean of the normal distribution,
    then the KDE represents the average density of all other datapoints in a normally
    distributed ''window'' around a given point x. The width of this window is given
    by *h*. Thus, the KDE allows us to plot a smoothed representation of a histogram
    by plotting not the absolute count at a given point, but a continuous probability
    estimate at the point. To this KDE plot, let us also add annotations for the axes,
    and limit the maximum runtime to 2 hrs using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We see, unsurprisingly, that many animated films are short, while others categories
    average around 90 minutes in length. We can also plot similar density curves to
    examine the distribution of ratings between genres using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Interestingly, documentaries have on average the highest rating, while action
    films have the lowest. We could also visualize this same information using a boxplot
    using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the boxplot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the notebook to start to make this sort of plotting automated
    for a dataset. For example, we often would like to look at the marginal plot of
    each variable (its single-dimensional distribution) compared to all others in
    order to find correlations between columns in our dataset. We can do this using
    the built-in `scatter_matrix` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will allow us to plot the pairwise distribution of all the variables we
    have selected, giving us an overview of potential correlations between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This single plot actually gives a lot of information. For example, it shows
    that in general higher budget films have higher ratings, and films made in the
    1920s have higher average rating than those before. Using this sort of scatter
    matrix, we can look for correlations that might guide the development of a predictive
    model, such as a predictor of ratings given other movie features. All we need
    to do is give this function a subset of columns in the DataFrame to plot (since
    we want to exclude non-numerical data which cannot be visualized in this way),
    and we can replicate this analysis for any new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to visualize these distributions in more detail? As an example,
    lets break the correlation between length and rating by genre using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this command, we create a 3x2 grid to hold plots for our six genres. We then
    iterate over the data groups by genre, and if we have reached the third row we
    reset and move to the second column. We then plot the data, using the `genre_color`
    column we generated previously, along with the index (the genre group) to label
    the plot. We scale the size of each point (representing an individual film) by
    the number of votes it received. The resulting scatterplots show the relationship
    between length and genre, with the size of the point giving sense of how much
    confidence we should place in the value of the point.
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have looked at some basic analysis using categorical data and numerical
    data, let's continue with a special case of numerical data – time series.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the `imdb` data contained movie release years, fundamentally the objects
    of interest were the individual films and the ratings, not a linked series of
    events over time that might be correlated with one another. This latter type of
    data – a time series – raises a different set of questions. Are datapoints correlated
    with one another? If so, over what timeframe are they correlated? How noisy is
    the signal? Pandas DataFrames have many built-in tools for time series analysis,
    which we will examine in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and converting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our previous example, we were able to use the data more or less in the form
    in which it was supplied. However, there is not always a guarantee that this will
    be the case. In our second example, we''ll look at a time series of oil prices
    in the US by year over the last century (Makridakis, Spyros, Steven C. Wheelwright,
    and Rob J. Hyndman. *Forecasting methods and applications*, John Wiley & Sons.
    Inc, New York(1998). We''ll start again by loading this data into the notebook,
    and inspecting it visually using `tail()` by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning and converting](img/B04881_chapter02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last row is unexpected, since it does not look like a year at all. In fact,
    it is a footer comment in the spreadsheet. As it is not actually part of the data,
    we will need to remove it from the dataset, which we can do with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This will remove from the dataset and rows in which the second column is NaN
    (not a correctly formatted number). We can verify that we have cleaned up the
    dataset by using the tail command again
  prefs: []
  type: TYPE_NORMAL
- en: 'The second aspect of this data that we would like to clean up is the format.
    If we look at the format of the columns using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'we see that the year is not by default interpreted as a Python date type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning and converting](img/B04881_chapter_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We would like the **Year** column to be a Python date. type Pandas provides
    the built-in capability to perform this conversion using the `convert_object()`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At the same time, we can rename the column with prices something a little less
    verbose using the `rename` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then verify that the output from using the head() command shows these
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning and converting](img/B04881_chapter02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now have the data in a format in which we can start running some diagnostics
    on this time series.
  prefs: []
  type: TYPE_NORMAL
- en: Time series diagnostics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can plot this data using the `matplotlib` commands covered in the previous
    section using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the time series plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series diagnostics](img/B04881_chapter02_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a number of natural questions we might ask of this data. Are the
    fluctuations in oil prices per year completely random, or do year-by-year measurements
    correlate with one another? There seem to be some cycles in the data, but it is
    difficult to quantify the degree of this correlation. A visual tool we can use
    to help diagnose this feature is a `lag_plot, which is available in Pandas using
    the following commands`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Time series diagnostics](img/B04881_chapter02_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A lag plot simply plots a yearly oil price (x-axis) versus the oil price in
    the year immediately following it (y-axis). If there is no correlation, we would
    expect a circular cloud. The linear pattern here shows that there is some structure
    in the data, which fits with the fact that year-by-year prices go up or down.
    How strong is this correlation compared to expectation? We can use an autocorrelation
    plot to answer this question, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the following autocorrelation plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series diagnostics](img/B04881_chapter02_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this plot, the correlation between points at different lags (difference in
    years) is plotted along with a 95% confidence interval (solid) and 99% confidence
    interval (dashed) line for the expected range of correlation on random data. Based
    on this visualization, there appears to be exceptional correlation for lags of
    <10 years, which fits with the approximate duration of the peak price periods
    in the first plot of this data above.
  prefs: []
  type: TYPE_NORMAL
- en: Joining signals and correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, let us look at an example of comparing the oil price time series to
    another dataset, the number of car crash fatalities in the US for the given years
    (*List of Motor Vehicle Deaths in U.S. by Year*. Wikipedia. Wikimedia Foundation.
    Web. 02 May 2016\. [https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year](https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We might hypothesize, for instance, that as the price of oil increases, on
    average consumers will drive less, leading to future car crashes. Again, we will
    need to convert the dataset time to date format, after first converting it from
    a number to a string, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the first few lines with the `head()` command confirms that we have
    successfully formatted the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can join this data to the oil prices statistics and compare the two trends
    over time. Notice that we need to rescale the crash data by dividing by 1000 so
    that it can be easily viewed on the same axis in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use `merge()` to join the data, specifying the column to use to match
    rows in each dataset through the `on` variable, and plot the result using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How correlated are these two signals? We can again use an `auto_correlation`
    plot to explore this question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So it appears that the correlation is outside the expected fluctuation at 20
    years or less, a longer range of correlation than appears in the oil prices alone.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Working with large datasets**'
  prefs: []
  type: TYPE_NORMAL
- en: The examples we give in this section are of modest size. In real-world applications,
    we may deal with datasets that will not fit on our computer, or require analyses
    that are so computationally intensive that they must be split across multiple
    machines to run in a reasonable timeframe. For these use cases, it may not be
    possible to use IPython Notebook in the form we have illustrated using Pandas
    DataFrames. A number of alternative applications are available for processing
    data at this scale, including PySpark, ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)),
    H20 ([http://www.h2o.ai/](http://www.h2o.ai/)), and XGBoost ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)).
    We can also use many of these tools through a notebook, and thus achieve interactive
    manipulation and modeling for extremely large data volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Working with geospatial data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our last case study, let us explore the analysis of geospatial data using
    an extension to the Pandas library, GeoPandas. You will need to have GeoPandas
    installed in your IPython environment to follow this example. If it is not already
    installed, you can add it using `easy_install` or pip.
  prefs: []
  type: TYPE_NORMAL
- en: Loading geospatial data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to our other dependencies, we will import the `GeoPandas` library
    using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We load dataset for this example, the coordinates of countries in Africa ("Africa."
    Maplibrary.org. Web. 02 May 2016\. [http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/](http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/))
    which are contained in a shape (`.shp`) file as before into a **GeoDataFrame**,
    an extension of the Pandas DataFrame, using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining the first few lines using `head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading geospatial data](img/B04881_chapter02_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the data consists of identifier columns, along with a geometry
    object representing the shape of the country. The `GeoDataFrame` also has a `plot()`
    function, to which we can pass a `column` argument that gives the field to use
    for generating the color of each polygon using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading geospatial data](img/B04881_chapter02_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, right now this color code is based on the country name, so does not
    offer much insight about the map. Instead, let us try to color each country based
    on its population using information about the population density of each country
    (*Population by Country – Thematic Map – World*. *Population by Country – Thematic
    Map-World*. Web. 02 May 2016, [http://www.indexmundi.com/map/?v=21](http://www.indexmundi.com/map/?v=21)).
    First we read in the population using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that here we have applied the `sep=''\t''` argument to `read_csv()`, as
    the columns in this file are not comma separated like the other examples thus
    far. Now we can join this data to the geographical coordinates using merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike the example with oil prices and crash fatalities above, here the columns
    we wish to use to join the data has a different name in each dataset, so we must
    use the `left_on` and `right_on` arguments to specify the desired column in each
    table. We can then plot the map with colors derived from the population data using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the new map as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading geospatial data](img/B04881_chapter02_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we can clearly see the most populous countries (Ethiopia, Democratic Republic
    of Congo, and Egypt) highlighted in white.
  prefs: []
  type: TYPE_NORMAL
- en: Working in the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous examples, we have assumed you are running the IPython notebook
    locally on your computer through your web browser. As mentioned, it is also possible
    for the application to run on an external server, with the user uploading files
    through the interface to interact with remotely. One convenient form of such external
    services are cloud platforms such as **Amazon Web Services** (**AWS**), Google
    Compute Cloud, and Microsoft Azure. Besides offering a hosting platform to run
    applications like the notebook, these services also offer storage for data sets
    much larger than what we would be able to store in our personal computers. By
    running our notebook in the cloud, we can more easily interact with these distributed
    storage systems using a shared infrastructure for data access and manipulation
    that also enforces desirable security and data governance. Lastly, cheap computing
    resources available via these cloud services may also allow us to scale the sorts
    of computation we describe in later chapters, adding extra servers to handle commands
    entered in the notebook on the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we've mainly focused on datasets that can fit on a single machine. For
    larger datasets, we may need to access them through distributed file systems such
    as Amazon S3 or HDFS. For this purpose, we can utilize the open-source distributed
    computing framework PySpark ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)).
    PySpark is a distributed computing framework that uses the abstraction of **Resilient
    Distributed Datasets** (**RDDs**) for parallel collections of objects, which allows
    us to programmatically access a dataset as if it fits on a single machine. In
    later chapters we will demonstrate how to build predictive models in PySpark,
    but for this introduction we focus on data manipulation functions in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the SparkContext
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in any spark application is the generation of the SparkContext.
    The SparkContext contains any job-specific configurations (such as memory settings
    or the number of worker tasks), and allows us to connect to a Spark cluster by
    specifying the master. We start the SparkContext with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument gives the URL for our Spark master, the machine which coordinates
    execution of Spark jobs and distributes tasks to the worker machines in a cluster.
    All Spark jobs consist of two kinds of task: the **Driver** (which issues commands
    and collects information about the progress of the job), and **Executors** (which
    execute operations on the RDD). These could be created on the same machine (as
    is the case in our example), or on different machines, allowing a dataset that
    will not fit in memory on a single machine to be analyzed using parallel computation
    across several computers. In this case we will run locally, so give the argument
    for the master as `localhost`, but otherwise this could be the URL of a remote
    machine in our cluster. The second argument is just the name we give to our application,
    which we specify with a uniquely generated id using the `uuid` library. If this
    command is successful, you should see in your terminal where you are running the
    notebook a stack trace such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the SparkContext](img/B04881_chapter02_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can open the SparkUI using the address `http://localhost:4040`, which looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the SparkContext](img/B04881_chapter02_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see our job name in the top-right hand corner, and we can use this
    page to track the progress of our jobs once we begin running them. The SparkContext
    is now ready to receive commands, and we can see the progress of any operations
    we execute in our notebook in the `ui`. If you want to stop the SparkContext,
    we can simply use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if we are running locally we can only start one SparkContext on `localhost`
    at a time, so if we want to make changes to the context we will need to stop and
    restart it. Once we have created the base SparkContext, we can instantiate other
    contexts objects that contain parameters and functionality for particular kinds
    of datasets. For this example, we will use a SqlContext, which allows us to operate
    on DataFrames and use SQL logic to query a dataset. We generate the SqlContext
    using the SparkContext as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Creating an RDD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate our first RDD, let us load the movies dataset again, and turn it
    into a list of tuples using all columns but the index and the row number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The `itertuples()` command returns each row of a pandas DataFrame as a tuple,
    which we then slice by turning it into a list and taking the indices `2` and greater
    (representing all columns but the index of the row, which is automatically inserted
    by Pandas, and the row number, which was one of the original columns in the file
    ). To convert this local collection, we call `sc.parallelize`, which converts
    a collection into an RDD. We can examine how many partitions exist in this distributed
    collection using the function `getNumPartitions()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we just created this dataset locally, it only has one partition. We can
    change the number of partitions in an RDD, which can change the load of work done
    on each subset of data, using the `repartition()` (to increase the number of partitions)
    and `coalesce()` (to decrease) functions. You can verify that the following commands
    change the number of partitions in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to examine a small sample of data from the RDD we can use the `take()`
    function. The following command will return five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that there is no activity on the Spark UI until you enter commands
    that require a result to be printed to the notebook, such as `getNumPartitions()`
    or `take()`. This is because Spark follows a model of lazy execution, only returning
    results when they are required for a downstream operation and otherwise waiting
    for such an operation. Besides those mentioned, other operations that will force
    execution are writes to disk and `collect()` (described below).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to load our data using the PySpark DataFrames API (similar to Pandas
    DataFrames) instead of an RDD (which does not have many of the utility functions
    for DataFrame manipulation we illustrated above), we will need a file in **JavaScript
    Object Notation** (**JSON**) format. We can generate this file using the following
    command, which maps the elements of each row into a dictionary and casts it to
    JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: If you examine the output directory, you will notice that we have actually saved
    a directory with the name `movies.json` containing individual files (as many as
    there are partitions in our RDD). This is the same way in which data is stored
    in the **Hadoop distributed file system** (**HDFS**) in directories.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have just scratched the surface of everything we can do with an
    RDD. We can perform other actions such as filtering, grouping RDDs by a key, projecting
    subsets of each row, ordering data within groups, joining to other RDDs, and many
    other operations. The full range of available transformations and operations is
    documented at [http://spark.apache.org/docs/latest/api/python/pyspark.html](http://spark.apache.org/docs/latest/api/python/pyspark.html).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our file in the JSON format, we can load it as a Spark DataFrame
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: If we intend to perform many operations on this data, we can cache it (persist
    it in temporary storage), allowing us to operate on the data Spark's own internal
    storage format, which is optimized for repeated access. We cache the dataset using
    the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`SqlContext` also allows us to declare a table alias for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then query this data as if it were a table in a relational database
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the Pandas DataFrames, we can aggregate them by particular columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also access individual columns using similar syntax to Pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to bring all data to a single machine rather than operating on dataset
    partitions which may be spread across several computers, we can call the `collect()`
    command. Use this command with caution: for large datasets it will cause all of
    the partitions of the data to be combined and sent to the Drive, which could potentially
    overload the memory of the Driver. The `collect()` command will return an array
    of row objects, for which we can use `get()` to access individual elements (columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Not all operations we are interested in performing on our data may be available
    in the DataFrame API, so if necessary we can convert the DataFrame into an RDD
    of rows using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even convert a PySpark DataFrame into Pandas DataFrame using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In later chapters, we will cover setting up applications and building models
    in Spark, but you should now be able to perform many of the same basic data manipulations
    you used in Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now examined many of the tasks needed to start building analytical applications.
    Using the IPython notebook, we have covered how to load data in a file into a
    DataFrame in Pandas, rename columns in the dataset, filter unwanted rows, convert
    column data types, and create new columns. In addition, we have joined data from
    different sources and performed some basic statistical analyses using aggregations
    and pivots. We have visualized the data using histograms, scatter plots, and density
    plots as well as autocorrelation and log plots for time series. We also visualized
    geospatial data, using coordinate files to overlay data on maps. In addition,
    we processed the movies dataset using PySpark, creating both an RDD and a PySpark
    DataFrame, and performed some basic operations on these datatypes.
  prefs: []
  type: TYPE_NORMAL
- en: We will build on these tools in future sections, manipulating the raw input
    to develop features for building predictive analytics pipelines. We will later
    utilize similar tools to visualize and understand the features and performance
    of the predictive models we develop, as well as reporting the insights that they
    may deliver.
  prefs: []
  type: TYPE_NORMAL
