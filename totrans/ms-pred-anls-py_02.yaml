- en: Chapter 2. Exploratory Data Analysis and Visualization in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：Python中的探索性数据分析与可视化
- en: Analytic pipelines are not built from raw data in a single step. Rather, development
    is an iterative process that involves understanding the data in greater detail
    and systematically refining both model and inputs to solve a problem. A key part
    of this cycle is interactive data analysis and visualization, which can provide
    initial ideas for features in our predictive modeling or clues as to why an application
    is not behaving as expected.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分析管道不是一步从原始数据中构建的。相反，开发是一个迭代过程，涉及更详细地了解数据，并系统地细化模型和输入以解决问题。这个周期的一个关键部分是交互式数据分析和可视化，这可以为我们的预测建模提供初步的想法，或者为为什么应用程序没有按预期行为提供线索。
- en: 'Spreadsheet programs are one kind of interactive tool for this sort of exploration:
    they allow the user to import tabular information, pivot and summarize data, and
    generate charts. However, what if the data in question is too large for such a
    spreadsheet application? What if the data is not tabular, or is not displayed
    effectively as a line or bar chart? In the former case, we could simply obtain
    a more powerful computer, but the latter is more problematic. Simply put, many
    traditional data visualization tools are not well suited to complex data types
    such as text or images. Additionally, spreadsheet programs often assume data is
    in a finalized form, whereas in practice we will often need to clean up the raw
    data before analysis. We might also want to calculate more complex statistics
    than simple averages or sums. Finally, using the same programming tools to clean
    up and visualize our data as well as generate the model itself and test its performance
    allows a more streamlined development process.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格程序是此类探索的一种交互式工具：它们允许用户导入表格信息，旋转和汇总数据，并生成图表。然而，如果数据太大而无法使用此类电子表格应用程序怎么办？如果数据不是表格形式，或者无法有效地以线形或条形图的形式显示呢？在前一种情况下，我们可能只需获得一台更强大的计算机，但后一种情况则更为复杂。简而言之，许多传统的数据可视化工具并不适合复杂的数据类型，如文本或图像。此外，电子表格程序通常假设数据是最终形式，而实际上我们在分析之前通常需要清理原始数据。我们可能还希望计算比简单平均值或总和更复杂的统计数据。最后，使用相同的编程工具来清理和可视化我们的数据，以及生成模型本身并测试其性能，可以使得开发过程更加流畅。
- en: 'In this chapter we introduce interactive Python (IPython) notebook applications
    (Pérez, Fernando, and Brian E. Granger. *IPython: a system for interactive scientific
    computing*. *Computing in Science & Engineering* 9.3 (2007): 21-29). The notebooks
    form a data preparation, exploration, and modeling environment that runs inside
    a web browser. The commands typed in the input cells of an IPython notebook are
    translated and executed as they are received: this kind of interactive programming
    is helpful for data exploration, where we may refine our efforts and successively
    develop more detailed analyses. Recording our work in these Notebooks will help
    to both backtrack during debugging and serve as a record of insights that can
    be easily shared with colleagues.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了交互式Python（IPython）笔记本应用程序（Pérez, Fernando，和Brian E. Granger. *IPython：一个交互式科学计算系统*。《科学计算中的计算》9.3（2007）：21-29）。这些笔记本形成了一个数据准备、探索和建模环境，它运行在网页浏览器中。在IPython笔记本的输入单元中键入的命令在接收时被翻译并执行：这种交互式编程对于数据探索很有帮助，因为我们可能需要改进我们的努力，并逐步开发更详细的分析。在这些笔记本中记录我们的工作将有助于在调试期间回溯，并作为可以轻松与同事分享的见解的记录。
- en: 'In this chapter we will discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Reading raw data into an IPython notebook, cleaning it, and manipulating it
    using the Pandas library.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始数据读取到IPython笔记本中，使用Pandas库对其进行清理和处理。
- en: Using IPython to process numerical, categorical, geospatial, or time-series
    data, and perform basic statistical analyses.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用IPython处理数值、分类、地理空间或时间序列数据，并执行基本统计分析。
- en: 'Basic exploratory analyses: summary statistics (mean, variance, median), distributions
    (histogram and kernel density), and auto-correlation (time-series).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本探索性分析：汇总统计（均值、方差、中位数）、分布（直方图和核密度）、以及自相关（时间序列）。
- en: An introduction to distributed data processing with Spark RDDs and DataFrames.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark RDDs和DataFrames的分布式数据处理简介。
- en: Exploring categorical and numerical data in IPython
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在IPython中探索分类和数值数据
- en: We will start our explorations in IPython by loading a text file into a DataFrame,
    calculating some summary statistics, and visualizing distributions. For this exercise
    we'll use a set of movie ratings and metadata from the Internet Movie Database
    ([http://www.imdb.com/](http://www.imdb.com/)) to investigate what factors might
    correlate with high ratings for films on this website. Such information might
    be helpful, for example, in developing a recommendation system based on this kind
    of user feedback.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将文本文件加载到DataFrame中，计算一些汇总统计量，并可视化分布来开始我们的IPython探索。为此，我们将使用来自互联网电影数据库（[http://www.imdb.com/](http://www.imdb.com/)）的一组电影评分和元数据来调查哪些因素可能与该网站上电影的评分相关。此类信息可能有助于，例如，开发基于此类用户反馈的推荐系统。
- en: Installing IPython notebook
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装IPython笔记本
- en: 'To follow along with the examples, you should have a Windows, Linux, or Mac
    OSX operating system installed on your computer and access to the Internet. There
    are a number of options available to install IPython: since each of these resources
    includes installation guides, we provide a summary of the available sources and
    direct the reader to the relevant documentation for more in-depth instructions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随示例，您应该在计算机上安装Windows、Linux或Mac OSX操作系统，并能够访问互联网。有几种安装IPython的选项：由于每个资源都包括安装指南，我们提供了可用的来源摘要，并将读者指引到相关文档以获取更深入的说明。
- en: 'For most users, a pre-bundled Python environment such as Anaconda (Continuum
    Analytics) or Canopy (Enthought) provides an out-of-the-box distribution with
    IPython and all the libraries we will use in these exercises: these products are
    self-contained, and thus you should not have to worry about conflicting versions
    or dependency management.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大多数用户，Anaconda（Continuum Analytics）或Canopy（Enthought）之类的预捆绑Python环境提供了一个包含IPython和我们将在这项练习中使用的所有库的即用型发行版：这些产品是自包含的，因此您不需要担心版本冲突或依赖关系管理。
- en: For more ambitious users, you can install a python distribution of your choice,
    followed by individual installation of the required libraries using package managers
    such as `pip` or `easy_install`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更有雄心的用户，您可以选择安装Python发行版，然后使用`pip`或`easy_install`之类的包管理器安装所需的库。
- en: The notebook interface
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 笔记本界面
- en: 'Let''s get started with the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤开始：
- en: 'Once you''ve installed IPython, open the command prompt (terminal) on your
    computer and type:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您安装了IPython，打开计算机上的命令提示符（终端）并输入：
- en: '[PRE0]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that depending upon where you installed the program, the `jupyter` command
    may require the binary file that launches `jupyter` to be on your system path.
    You should see a series of commands like the following in your terminal:'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，根据您安装程序的位置，`jupyter`命令可能需要将启动`jupyter`的二进制文件放在您的系统路径中。您应该在终端中看到一系列如下命令：
- en: '![The notebook interface](img/B04881_chapter02_01.jpg)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![笔记本界面](img/B04881_chapter02_01.jpg)'
- en: This starts the **kernel**, the python interpreter that computes the result
    of commands entered into the notebook. If you want to stop the notebook, type
    *Ctrl* + *C*, and enter **yes**, and the kernel will shut down.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将启动**内核**，即计算笔记本中输入的命令结果的Python解释器。如果您想停止笔记本，请按*Ctrl* + *C*，然后输入**yes**，内核将关闭。
- en: When the kernel starts, your default web browser should also open, giving you
    a homepage that looks like this:![The notebook interface](img/B04881_chapter02_02.jpg)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当内核启动时，您的默认网络浏览器也应该打开，显示一个主页，如下所示：![笔记本界面](img/B04881_chapter02_02.jpg)
- en: The **Files** tab (see above) will show you all of the files in the directory
    where you started the IPython process. Clicking **Running** will give you a list
    of all running notebooks – there are none when you start:![The notebook interface](img/B04881_chapter02_30.jpg)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文件**标签（见上图）将显示您启动IPython进程的目录中的所有文件。点击**运行**将显示所有正在运行的笔记本列表——开始时没有：![笔记本界面](img/B04881_chapter02_30.jpg)'
- en: Finally, the Clusters panel gives a list of external clusters, should we decide
    to parallelize our calculations by submitting commands to be processed on more
    than one machine. We won't worry about this for now, but it will come in useful
    later when we begin to train predictive models, a task that may often be accelerated
    by distributing the work among many computers or processors.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，集群面板会列出外部集群，如果我们决定通过将命令提交到多个机器上处理来并行化我们的计算，那么我们会考虑这一点。现在我们不必担心这个问题，但当我们开始训练预测模型时，它将变得非常有用，这项任务通常可以通过在多台计算机或处理器之间分配工作来加速。
- en: 'Returning to the **Files** tab, you will notice two options in the top right-hand
    corner. One is to **Upload** a file: while we are running IPython locally, it
    could just as easily be running on a remote server, with the analyst accessing
    the notebook through a browser. In this case, to interact with files stored on
    our own machine, we can use this button to open a prompt and selected the desired
    files to upload to the server, where we could then analyze them in the notebook.
    The **New** tab lets you create a new folder, text file, a Python terminal running
    in the browser, or a notebook.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For now, let''s open the sample notebook for this chapter by double clicking
    on **B04881_chapter02_code01.ipynb**. This opens the notebook:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The notebook interface](img/B04881_chapter02_03.jpg)The notebook consists
    of a series of cells, areas of text where we can type python code, execute it,
    and see the results of the commands. The python code in each cell can be executed
    by clicking the ![The notebook interface](img/B04881_chapter02_55.jpg) button
    on the toolbar, and a new cell can be inserted below the current one by clicking
    ![The notebook interface](img/B04881_chapter02_56.jpg).'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While the import statements in the first cell probably look familiar from your
    experience of using python in the command line or in a script, the `%matplotlib`
    inline command is not actually python: it is a markup instruction to the notebook
    that `matplotlib` images are to be displayed inline the browser. We enter this
    command at the beginning of the notebook so that all of our later plots use this
    setting. To run the import statements, click the ![The notebook interface](img/B04881_chapter02_57.jpg)
    button or press *Ctrl* + *Enter*. The `ln[1]` on the cell may briefly change to
    `[*]` as the command executes. There will be no output in this case, as all we
    did was import library dependencies. Now that our environment is ready, we can
    start examining some data.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading and inspecting data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, we will import the data in `movies.csv` into a DataFrame object using
    the Pandas library (McKinney, Wes. *Python for data analysis: Data wrangling with
    Pandas*, NumPy, and IPython. O''Reilly Media, Inc., 2012). This DataFrame resembles
    traditional spreadsheet software and allows powerful extensions such as custom
    transformations and aggregations. These may be combined with numerical methods,
    such as those available in NumPy, for more advanced statistical analysis of the
    data. Let us continue our analysis:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'If this were a new notebook, to add new cells we would go to the toolbar, click
    **Insert** and **Insert Cell Below**, or use the ![Loading and inspecting data](img/B04881_chapter02_57.jpg).
    button. However, in this example all the cells are already generated, therefore
    we run the following command in the second cell:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We've now created a DataFrame object using the Pandas library, `imdb_ratings`,
    and can begin analyzing the data.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s start by peeking at the beginning and end of the data using `head()`
    and `tail()`. Notice that by default this command returns the first five rows
    of data, but we can supply an integer argument to the command to specify the number
    of lines to return. Also, by default the first line of the file is assumed to
    contain the column names, which in this case is correct. Typing:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Gives the following output:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_04.jpg)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We can similarly look at the last 15 lines of the data by typing:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Loading and inspecting data](img/B04881_chapter02_05.jpg)'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Looking at individual rows gives us a sense of what kind of data the file contains:
    we can also look at summaries for all rows in each column using the command `describe()`,
    which returns the number of records, mean value, and other aggregate statistics.
    Try typing:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives the following output:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_06.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Column names and their datatypes can be accessed using the properties `columns`
    and `dtypes`. Typing:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Gives us the names of the columns:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_58.jpg)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'If we issue the command:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, the datatypes of the columns have been automatically inferred
    when we first loaded the file:![Loading and inspecting data](img/B04881_chapter02_07.jpg)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we want to access the data in individual columns, we can do so using either
    `{DataFrame_name}.{column_name}` or `{DataFrame_name}[''column_name'']` (similar
    to a python dictionary). For example, typing:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: or
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Gives the following output:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Loading and inspecting data](img/B04881_chapter02_08.jpg)'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Without much work, we can already use these simple commands to ask a number
    of diagnostic questions about the data. Do the summary statistics we generated
    using `describe()` make sense (for example, the max rating should be 10, while
    the minimum is 1)? Is the data correctly parsed into the columns we expect?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the first five rows of data we visualized using the `head()`
    command, this initial inspection also reveals some formatting issues we might
    want to consider. In the **budget** column, several entries have the value `NaN`,
    representing missing values. If we were going to try to predict movie ratings
    based on features including **budget**, we might need to come up with a rule to
    fill in these missing values, or encode them in a way that is correctly represented
    to the algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Basic manipulations – grouping, filtering, mapping, and pivoting
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have looked at the basic features of the Pandas DataFrame, let
    us start applying some transformations and calculations to this data beyond the
    simple statistics we obtained through `describe()`. For example, if we wanted
    to calculate how many films belong to each release year, we can use following
    command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Which gives the output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_09.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the result is by default sorted by the count of records in each
    year (with the most films in this dataset released in 2002). What if we wanted
    to sort by the release year? The `sort_index()` command orders the result by its
    index (the year to which the count belongs). The index is similar to the axis
    of a plot, with values representing the point at each axis tick. Using the command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认情况下，结果是按每年记录数排序的（在这个数据集中，2002 年上映的电影最多）。如果我们想按发行年份排序怎么办？`sort_index()`
    命令按其索引（属于的年份）对结果进行排序。索引类似于图表的轴，其值表示每个轴刻度处的点。使用以下命令：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Gives the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 给出以下输出：
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_31.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![基本操作 – 分组、过滤、映射和转置](img/B04881_chapter02_31.jpg)'
- en: 'We can also use the DataFrame to begin asking analytical questions about the
    data, logically slicing and sub-selecting as we might in a database query. For
    example, let us select the subset of films released after 1999 with an R rating
    using the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 DataFrame 来开始对数据进行分析性提问，就像在数据库查询中那样进行逻辑切片和子选择。例如，让我们使用以下命令选择 1999 年之后上映并具有
    R 评分的电影子集：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives the following output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_11.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![基本操作 – 分组、过滤、映射和转置](img/B04881_chapter02_11.jpg)'
- en: 'Similarly, we can group the data by any column(s) and calculate aggregated
    statistics using the `groupby` command and pass an array of calculations to perform
    as an argument to `aggregate`. Let us use the mean and standard deviation functions
    from NumPy to find the average and variation in ratings for films released in
    a given year:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以根据任何列（s）对数据进行分组，并使用 `groupby` 命令计算聚合统计信息，将执行计算的数组作为参数传递给 `aggregate`。让我们使用
    NumPy 中的平均值和标准差函数来找到特定年份上映电影的平均评分和变化：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了：
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_32.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![基本操作 – 分组、过滤、映射和转置](img/B04881_chapter02_32.jpg)'
- en: 'However, sometimes the questions we want to ask require us to reshape or transform
    the raw data we are given. This will happen frequently in later chapters, when
    we develop features for predictive models. Pandas provide many tools for performing
    this kind of transformation. For example, while it would also be interesting to
    aggregate the data based on genre, we notice that in this dataset each genre is
    represented as a single column, with 1 or 0 indicating whether a film belongs
    to a given genre. It would be more useful for us to have a single column indicating
    which genre the film belongs to for use in aggregation operations. We can make
    such a column using the command `idxmax()` with the argument 1 to represent the
    maximum argument across columns (0 would represent the max index along rows),
    which returns the column with the greatest value out of those selected. Typing:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时我们想要提出的问题需要我们重塑或转换我们给出的原始数据。这在后面的章节中会经常发生，当我们为预测模型开发特征时。Pandas 提供了许多执行这种转换的工具。例如，虽然根据类型对数据进行聚合也可能很有趣，但我们注意到在这个数据集中，每个类型都由一个单独的列表示，其中
    1 或 0 表示一部电影是否属于某个特定类型。对我们来说，有一个单独的列来指示电影属于哪个类型，以便在聚合操作中使用会更有用。我们可以使用带有参数 1 的
    `idxmax()` 命令来创建这样的列，以表示跨列的最大值（0 将表示沿行的最大索引），它返回所选列中值最大的列。键入：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Gives the following result when we examine this new genre column using:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用以下内容检查这个新的类型列时，给出以下结果：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_33.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![基本操作 – 分组、过滤、映射和转置](img/B04881_chapter02_33.jpg)'
- en: 'We may also perhaps like to plot the data with colors representing a particular
    genre. To generate a color code for each genre, we can use a custom mapping function
    with the following commands:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也许还希望用代表特定类型的颜色来绘制数据。为了为每个类型生成一个颜色代码，我们可以使用以下命令的自定义映射函数：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can verify the output by typing:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过键入以下内容来验证输出：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Which gives:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_34.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![基本操作 – 分组、过滤、映射和转置](img/B04881_chapter02_34.jpg)'
- en: 'We can also transpose the table and perform statistical calculations using
    the `pivot_table` command, which can perform aggregate calculations on groupings
    of rows and columns as in a spreadsheet. For example, to calculate the average
    rating per genre per year we can use the following command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Which gives the output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic manipulations – grouping, filtering, mapping, and pivoting](img/B04881_chapter02_35.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Now that we have performed some exploratory calculations, let us look at some
    visualizations of this information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Charting with Matplotlib
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the practical features of IPython notebooks is the ability to plot data
    inline with our analyses. For example, if we wanted to visualize the distribution
    of film lengths we could use the command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Charting with Matplotlib](img/B04881_chapter02_16.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'However, this is not really a very attractive image. To make a more aesthetically
    pleasing plot, we can change the default style using the `style.use()` command.
    Let us change the style to `ggplot`, which is used in the `ggplot` graphical library
    (Wickham, Hadley. *ggplot: An Implementation of the Grammar of Graphics*. R package
    version 0.4\. 0 (2006)). Typing the following commands:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Gives a much more attractive graphic:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_36.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'As you can see preceding, the default plot is a line chart. The line chart
    plots each datapoint (movie runtime) as a line, ordered from left to right by
    their row number in the DataFrame. To make a density plot of films by their genre,
    we can plot using the `groupby` command with the argument `type=kde`. **KDE**
    is an abbreviation for **Kernel Density Estimate** (Rosenblatt, Murray. Remarks
    on some nonparametric estimates of a density function. *The Annals of Mathematical
    Statistics 27.3 (1956): 832-837*; Parzen, Emanuel. On estimation of a probability
    density function and mode. The annals of mathematical statistics 33.3 (1962):
    1065-1076), meaning that for each point (film runtime) we estimate the density
    (proportion of the population with that runtime) with the equation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_60.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Where `f(x)` is an estimate of the probability density, n is the number of
    records in our dataset, `h` is a bandwidth parameter, and `K` is a kernel function.
    As an example, if `K` were the Gaussian kernel given by:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_59.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'where σ is the standard deviation and μ is the mean of the normal distribution,
    then the KDE represents the average density of all other datapoints in a normally
    distributed ''window'' around a given point x. The width of this window is given
    by *h*. Thus, the KDE allows us to plot a smoothed representation of a histogram
    by plotting not the absolute count at a given point, but a continuous probability
    estimate at the point. To this KDE plot, let us also add annotations for the axes,
    and limit the maximum runtime to 2 hrs using the following commands:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Which gives the following plot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_39.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'We see, unsurprisingly, that many animated films are short, while others categories
    average around 90 minutes in length. We can also plot similar density curves to
    examine the distribution of ratings between genres using the following commands:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which gives the following plot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_40.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Interestingly, documentaries have on average the highest rating, while action
    films have the lowest. We could also visualize this same information using a boxplot
    using the following commands:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This gives the boxplot as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_41.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the notebook to start to make this sort of plotting automated
    for a dataset. For example, we often would like to look at the marginal plot of
    each variable (its single-dimensional distribution) compared to all others in
    order to find correlations between columns in our dataset. We can do this using
    the built-in `scatter_matrix` function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will allow us to plot the pairwise distribution of all the variables we
    have selected, giving us an overview of potential correlations between them:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_42.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: This single plot actually gives a lot of information. For example, it shows
    that in general higher budget films have higher ratings, and films made in the
    1920s have higher average rating than those before. Using this sort of scatter
    matrix, we can look for correlations that might guide the development of a predictive
    model, such as a predictor of ratings given other movie features. All we need
    to do is give this function a subset of columns in the DataFrame to plot (since
    we want to exclude non-numerical data which cannot be visualized in this way),
    and we can replicate this analysis for any new dataset.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to visualize these distributions in more detail? As an example,
    lets break the correlation between length and rating by genre using the following
    commands:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this command, we create a 3x2 grid to hold plots for our six genres. We then
    iterate over the data groups by genre, and if we have reached the third row we
    reset and move to the second column. We then plot the data, using the `genre_color`
    column we generated previously, along with the index (the genre group) to label
    the plot. We scale the size of each point (representing an individual film) by
    the number of votes it received. The resulting scatterplots show the relationship
    between length and genre, with the size of the point giving sense of how much
    confidence we should place in the value of the point.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting with Matplotlib](img/B04881_chapter02_43.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Now that we have looked at some basic analysis using categorical data and numerical
    data, let's continue with a special case of numerical data – time series.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the `imdb` data contained movie release years, fundamentally the objects
    of interest were the individual films and the ratings, not a linked series of
    events over time that might be correlated with one another. This latter type of
    data – a time series – raises a different set of questions. Are datapoints correlated
    with one another? If so, over what timeframe are they correlated? How noisy is
    the signal? Pandas DataFrames have many built-in tools for time series analysis,
    which we will examine in the next section.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `imdb` 数据包含了电影发行年份，但本质上我们感兴趣的是单个电影和评分，而不是随时间推移可能相互关联的一系列事件。这种后一种类型的数据——时间序列——提出了不同的问题。数据点是否相互关联？如果是，它们在什么时间段内相关？信号有多嘈杂？Pandas
    DataFrames 有许多内置的时间序列分析工具，我们将在下一节中探讨。
- en: Cleaning and converting
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗和转换
- en: 'In our previous example, we were able to use the data more or less in the form
    in which it was supplied. However, there is not always a guarantee that this will
    be the case. In our second example, we''ll look at a time series of oil prices
    in the US by year over the last century (Makridakis, Spyros, Steven C. Wheelwright,
    and Rob J. Hyndman. *Forecasting methods and applications*, John Wiley & Sons.
    Inc, New York(1998). We''ll start again by loading this data into the notebook,
    and inspecting it visually using `tail()` by typing:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，我们能够以提供的数据形式使用这些数据。然而，并不总是有保证这种情况会发生。在我们的第二个例子中，我们将查看过去一个世纪美国按年份的石油价格时间序列（Makridakis,
    Spyros, Steven C. Wheelwright, 和 Rob J. Hyndman. 《预测方法和应用》，John Wiley & Sons.
    Inc, 纽约(1998)。我们将再次通过将此数据加载到笔记本中，并使用 `tail()` 通过输入来检查它：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Which gives the output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Cleaning and converting](img/B04881_chapter02_10.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![清洗和转换](img/B04881_chapter02_10.jpg)'
- en: 'The last row is unexpected, since it does not look like a year at all. In fact,
    it is a footer comment in the spreadsheet. As it is not actually part of the data,
    we will need to remove it from the dataset, which we can do with the following
    commands:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行是意外的，因为它看起来根本不像一个年份。实际上，它是电子表格中的一个页脚注释。由于它实际上不是数据的一部分，我们需要将其从数据集中删除，这可以通过以下命令完成：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will remove from the dataset and rows in which the second column is NaN
    (not a correctly formatted number). We can verify that we have cleaned up the
    dataset by using the tail command again
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从数据集中删除第二列是 NaN（不是正确格式的数字）的行。我们可以通过再次使用 tail 命令来验证我们已经清理了数据集。
- en: 'The second aspect of this data that we would like to clean up is the format.
    If we look at the format of the columns using:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望清理数据的第二个方面是格式。如果我们查看列的格式，使用：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'we see that the year is not by default interpreted as a Python date type:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到年份默认不是解释为 Python 日期类型：
- en: '![Cleaning and converting](img/B04881_chapter_02_25.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![清洗和转换](img/B04881_chapter_02_25.jpg)'
- en: 'We would like the **Year** column to be a Python date. type Pandas provides
    the built-in capability to perform this conversion using the `convert_object()`
    command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望 **年份** 列是 Python 日期类型。Pandas 提供了使用 `convert_object()` 命令执行此转换的内置功能：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'At the same time, we can rename the column with prices something a little less
    verbose using the `rename` command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们可以使用 `rename` 命令将价格列重命名为更简洁的名称：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can then verify that the output from using the head() command shows these
    changes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过使用 head() 命令来验证输出显示了这些变化：
- en: '![Cleaning and converting](img/B04881_chapter02_26.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![清洗和转换](img/B04881_chapter02_26.jpg)'
- en: We now have the data in a format in which we can start running some diagnostics
    on this time series.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了可以开始对这个时间序列进行一些诊断的数据格式。
- en: Time series diagnostics
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列诊断
- en: 'We can plot this data using the `matplotlib` commands covered in the previous
    section using the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用上一节中介绍的 `matplotlib` 命令来绘制这些数据，如下所示：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This produces the time series plot as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下时间序列图：
- en: '![Time series diagnostics](img/B04881_chapter02_44.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![时间序列诊断](img/B04881_chapter02_44.jpg)'
- en: 'There are a number of natural questions we might ask of this data. Are the
    fluctuations in oil prices per year completely random, or do year-by-year measurements
    correlate with one another? There seem to be some cycles in the data, but it is
    difficult to quantify the degree of this correlation. A visual tool we can use
    to help diagnose this feature is a `lag_plot, which is available in Pandas using
    the following commands`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Time series diagnostics](img/B04881_chapter02_45.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'A lag plot simply plots a yearly oil price (x-axis) versus the oil price in
    the year immediately following it (y-axis). If there is no correlation, we would
    expect a circular cloud. The linear pattern here shows that there is some structure
    in the data, which fits with the fact that year-by-year prices go up or down.
    How strong is this correlation compared to expectation? We can use an autocorrelation
    plot to answer this question, using the following commands:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Which gives the following autocorrelation plot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series diagnostics](img/B04881_chapter02_46.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: In this plot, the correlation between points at different lags (difference in
    years) is plotted along with a 95% confidence interval (solid) and 99% confidence
    interval (dashed) line for the expected range of correlation on random data. Based
    on this visualization, there appears to be exceptional correlation for lags of
    <10 years, which fits with the approximate duration of the peak price periods
    in the first plot of this data above.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Joining signals and correlation
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, let us look at an example of comparing the oil price time series to
    another dataset, the number of car crash fatalities in the US for the given years
    (*List of Motor Vehicle Deaths in U.S. by Year*. Wikipedia. Wikimedia Foundation.
    Web. 02 May 2016\. [https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year](https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'We might hypothesize, for instance, that as the price of oil increases, on
    average consumers will drive less, leading to future car crashes. Again, we will
    need to convert the dataset time to date format, after first converting it from
    a number to a string, using the following commands:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Checking the first few lines with the `head()` command confirms that we have
    successfully formatted the data:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_47.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'We can join this data to the oil prices statistics and compare the two trends
    over time. Notice that we need to rescale the crash data by dividing by 1000 so
    that it can be easily viewed on the same axis in the following command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then use `merge()` to join the data, specifying the column to use to match
    rows in each dataset through the `on` variable, and plot the result using:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot is shown below:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_48.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'How correlated are these two signals? We can again use an `auto_correlation`
    plot to explore this question:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Which gives:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining signals and correlation](img/B04881_chapter02_49.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: So it appears that the correlation is outside the expected fluctuation at 20
    years or less, a longer range of correlation than appears in the oil prices alone.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Working with large datasets**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The examples we give in this section are of modest size. In real-world applications,
    we may deal with datasets that will not fit on our computer, or require analyses
    that are so computationally intensive that they must be split across multiple
    machines to run in a reasonable timeframe. For these use cases, it may not be
    possible to use IPython Notebook in the form we have illustrated using Pandas
    DataFrames. A number of alternative applications are available for processing
    data at this scale, including PySpark, ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)),
    H20 ([http://www.h2o.ai/](http://www.h2o.ai/)), and XGBoost ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)).
    We can also use many of these tools through a notebook, and thus achieve interactive
    manipulation and modeling for extremely large data volumes.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Working with geospatial data
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our last case study, let us explore the analysis of geospatial data using
    an extension to the Pandas library, GeoPandas. You will need to have GeoPandas
    installed in your IPython environment to follow this example. If it is not already
    installed, you can add it using `easy_install` or pip.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Loading geospatial data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to our other dependencies, we will import the `GeoPandas` library
    using the command:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We load dataset for this example, the coordinates of countries in Africa ("Africa."
    Maplibrary.org. Web. 02 May 2016\. [http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/](http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/))
    which are contained in a shape (`.shp`) file as before into a **GeoDataFrame**,
    an extension of the Pandas DataFrame, using:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Examining the first few lines using `head()`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading geospatial data](img/B04881_chapter02_50.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the data consists of identifier columns, along with a geometry
    object representing the shape of the country. The `GeoDataFrame` also has a `plot()`
    function, to which we can pass a `column` argument that gives the field to use
    for generating the color of each polygon using:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Which gives the following visualization:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading geospatial data](img/B04881_chapter02_51.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'However, right now this color code is based on the country name, so does not
    offer much insight about the map. Instead, let us try to color each country based
    on its population using information about the population density of each country
    (*Population by Country – Thematic Map – World*. *Population by Country – Thematic
    Map-World*. Web. 02 May 2016, [http://www.indexmundi.com/map/?v=21](http://www.indexmundi.com/map/?v=21)).
    First we read in the population using:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Note that here we have applied the `sep=''\t''` argument to `read_csv()`, as
    the columns in this file are not comma separated like the other examples thus
    far. Now we can join this data to the geographical coordinates using merge:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里我们已将`sep='\t'`参数应用于`read_csv()`，因为该文件中的列不是像迄今为止的其他示例那样以逗号分隔。现在我们可以使用合并操作将此数据与地理坐标连接起来：
- en: '[PRE41]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Unlike the example with oil prices and crash fatalities above, here the columns
    we wish to use to join the data has a different name in each dataset, so we must
    use the `left_on` and `right_on` arguments to specify the desired column in each
    table. We can then plot the map with colors derived from the population data using:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与上面提到的石油价格和事故死亡率的例子不同，这里我们希望用来连接数据的列在每个数据集中都有不同的名称，因此我们必须使用`left_on`和`right_on`参数来指定每个表中所需的列。然后我们可以使用以下方法使用来自人口数据的颜色绘制地图：
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Which gives the new map as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了新的地图如下：
- en: '![Loading geospatial data](img/B04881_chapter02_52.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![加载地理空间数据](img/B04881_chapter02_52.jpg)'
- en: Now we can clearly see the most populous countries (Ethiopia, Democratic Republic
    of Congo, and Egypt) highlighted in white.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以清楚地看到人口最多的国家（埃塞俄比亚、刚果民主共和国和埃及）以白色突出显示。
- en: Working in the cloud
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在云端工作
- en: In the previous examples, we have assumed you are running the IPython notebook
    locally on your computer through your web browser. As mentioned, it is also possible
    for the application to run on an external server, with the user uploading files
    through the interface to interact with remotely. One convenient form of such external
    services are cloud platforms such as **Amazon Web Services** (**AWS**), Google
    Compute Cloud, and Microsoft Azure. Besides offering a hosting platform to run
    applications like the notebook, these services also offer storage for data sets
    much larger than what we would be able to store in our personal computers. By
    running our notebook in the cloud, we can more easily interact with these distributed
    storage systems using a shared infrastructure for data access and manipulation
    that also enforces desirable security and data governance. Lastly, cheap computing
    resources available via these cloud services may also allow us to scale the sorts
    of computation we describe in later chapters, adding extra servers to handle commands
    entered in the notebook on the backend.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们假设您正在通过您的网络浏览器在本地计算机上运行IPython笔记本。如前所述，应用程序也可以在远程服务器上运行，用户可以通过界面上传文件以远程交互。这种外部服务的一种方便形式是云平台，如**Amazon
    Web Services**（**AWS**）、Google Compute Cloud和Microsoft Azure。除了提供托管平台以运行笔记本等应用程序外，这些服务还提供存储，可以存储比我们个人电脑能存储的更大的数据集。通过在云端运行我们的笔记本，我们可以更轻松地使用共享的数据访问和处理基础设施与这些分布式存储系统进行交互，同时也强制执行所需的安全性和数据治理。最后，通过这些云服务提供的廉价计算资源也可能使我们能够扩展我们在后面章节中描述的计算类型，通过添加额外的服务器来处理笔记本后端输入的命令。
- en: Introduction to PySpark
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark简介
- en: So far we've mainly focused on datasets that can fit on a single machine. For
    larger datasets, we may need to access them through distributed file systems such
    as Amazon S3 or HDFS. For this purpose, we can utilize the open-source distributed
    computing framework PySpark ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)).
    PySpark is a distributed computing framework that uses the abstraction of **Resilient
    Distributed Datasets** (**RDDs**) for parallel collections of objects, which allows
    us to programmatically access a dataset as if it fits on a single machine. In
    later chapters we will demonstrate how to build predictive models in PySpark,
    but for this introduction we focus on data manipulation functions in PySpark.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注可以适应单个机器的数据库集。对于更大的数据集，我们可能需要通过分布式文件系统如Amazon S3或HDFS来访问它们。为此，我们可以利用开源分布式计算框架PySpark
    ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/))。PySpark是一个分布式计算框架，它使用**弹性分布式数据集**（**RDDs**）的抽象来处理对象的并行集合，这使得我们可以像它适合单个机器一样程序化地访问数据集。在后面的章节中，我们将演示如何在PySpark中构建预测模型，但在此介绍中，我们关注PySpark中的数据处理函数。
- en: Creating the SparkContext
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建SparkContext
- en: 'The first step in any spark application is the generation of the SparkContext.
    The SparkContext contains any job-specific configurations (such as memory settings
    or the number of worker tasks), and allows us to connect to a Spark cluster by
    specifying the master. We start the SparkContext with the following command:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 Spark 应用程序的第一步是生成 SparkContext。SparkContext 包含任何特定作业的配置（例如内存设置或工作任务的数目），并允许我们通过指定主节点来连接到
    Spark 集群。我们使用以下命令启动 SparkContext：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The first argument gives the URL for our Spark master, the machine which coordinates
    execution of Spark jobs and distributes tasks to the worker machines in a cluster.
    All Spark jobs consist of two kinds of task: the **Driver** (which issues commands
    and collects information about the progress of the job), and **Executors** (which
    execute operations on the RDD). These could be created on the same machine (as
    is the case in our example), or on different machines, allowing a dataset that
    will not fit in memory on a single machine to be analyzed using parallel computation
    across several computers. In this case we will run locally, so give the argument
    for the master as `localhost`, but otherwise this could be the URL of a remote
    machine in our cluster. The second argument is just the name we give to our application,
    which we specify with a uniquely generated id using the `uuid` library. If this
    command is successful, you should see in your terminal where you are running the
    notebook a stack trace such as the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数给出了我们的 Spark 主节点的 URL，即协调 Spark 作业执行并将任务分配给集群中工作机器的机器。所有 Spark 作业都包含两种任务：**驱动器**（负责发布命令并收集作业进度的信息）和**执行器**（在
    RDD 上执行操作）。这些任务可以创建在同一台机器上（如我们的示例所示），也可以在不同的机器上，这样就可以使用多台计算机的并行计算来分析无法在单台机器内存中容纳的数据集。在这种情况下，我们将本地运行，因此将主节点的参数指定为
    `localhost`，但否则这可以是集群中远程机器的 URL。第二个参数只是我们给应用程序起的名字，我们使用 `uuid` 库生成的唯一 ID 来指定它。如果此命令成功，你应该在你的终端中看到运行笔记本的位置出现以下堆栈跟踪：
- en: '![Creating the SparkContext](img/B04881_chapter02_53.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![创建 SparkContext](img/B04881_chapter02_53.jpg)'
- en: 'We can open the SparkUI using the address `http://localhost:4040`, which looks
    like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用地址 `http://localhost:4040` 打开 SparkUI，它看起来如下所示：
- en: '![Creating the SparkContext](img/B04881_chapter02_54.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![创建 SparkContext](img/B04881_chapter02_54.jpg)'
- en: 'You can see our job name in the top-right hand corner, and we can use this
    page to track the progress of our jobs once we begin running them. The SparkContext
    is now ready to receive commands, and we can see the progress of any operations
    we execute in our notebook in the `ui`. If you want to stop the SparkContext,
    we can simply use the following command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在右上角看到我们的作业名称，一旦开始运行它们，我们可以使用这个页面来跟踪作业的进度。现在 SparkContext 已经准备好接收命令，并且我们可以在
    `ui` 中看到我们在笔记本中执行的任何操作的进度。如果你想停止 SparkContext，我们可以简单地使用以下命令：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Note that if we are running locally we can only start one SparkContext on `localhost`
    at a time, so if we want to make changes to the context we will need to stop and
    restart it. Once we have created the base SparkContext, we can instantiate other
    contexts objects that contain parameters and functionality for particular kinds
    of datasets. For this example, we will use a SqlContext, which allows us to operate
    on DataFrames and use SQL logic to query a dataset. We generate the SqlContext
    using the SparkContext as an argument:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们本地运行，我们一次只能在一个 `localhost` 上启动一个 SparkContext，所以如果我们想更改上下文，我们需要停止并重新启动它。一旦我们创建了基本的
    SparkContext，我们就可以实例化其他上下文对象，这些对象包含特定类型数据集的参数和功能。对于这个例子，我们将使用 SqlContext，它允许我们对
    DataFrame 进行操作并使用 SQL 逻辑查询数据集。我们使用 SparkContext 作为参数来生成 SqlContext：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Creating an RDD
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 RDD
- en: 'To generate our first RDD, let us load the movies dataset again, and turn it
    into a list of tuples using all columns but the index and the row number:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成我们的第一个 RDD，让我们再次加载电影数据集，并使用除了索引和行号之外的所有列将其转换为元组列表：
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The `itertuples()` command returns each row of a pandas DataFrame as a tuple,
    which we then slice by turning it into a list and taking the indices `2` and greater
    (representing all columns but the index of the row, which is automatically inserted
    by Pandas, and the row number, which was one of the original columns in the file
    ). To convert this local collection, we call `sc.parallelize`, which converts
    a collection into an RDD. We can examine how many partitions exist in this distributed
    collection using the function `getNumPartitions()`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`itertuples()` 命令将 pandas DataFrame 的每一行返回为一个元组，然后我们通过将其转换为列表并取索引 `2` 及以上（代表所有列，但不是行的索引，这是
    Pandas 自动插入的，以及行号，这是文件中的原始列之一）来切片。为了将此本地集合转换为 RDD，我们调用 `sc.parallelize`，它将集合转换为
    RDD。我们可以使用 `getNumPartitions()` 函数检查这个分布式集合中有多少个分区：'
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Since we just created this dataset locally, it only has one partition. We can
    change the number of partitions in an RDD, which can change the load of work done
    on each subset of data, using the `repartition()` (to increase the number of partitions)
    and `coalesce()` (to decrease) functions. You can verify that the following commands
    change the number of partitions in our example:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚在本地创建了此数据集，它只有一个分区。我们可以使用 `repartition()`（增加分区数量）和 `coalesce()`（减少）函数来更改
    RDD 中的分区数量，这可以改变对数据每个子集所做的负载。你可以验证以下命令更改了我们示例中的分区数量：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If we want to examine a small sample of data from the RDD we can use the `take()`
    function. The following command will return five rows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想检查 RDD 中的小部分数据，可以使用 `take()` 函数。以下命令将返回五行：
- en: '[PRE49]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: You may notice that there is no activity on the Spark UI until you enter commands
    that require a result to be printed to the notebook, such as `getNumPartitions()`
    or `take()`. This is because Spark follows a model of lazy execution, only returning
    results when they are required for a downstream operation and otherwise waiting
    for such an operation. Besides those mentioned, other operations that will force
    execution are writes to disk and `collect()` (described below).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在输入需要将结果打印到笔记本的命令之前，Spark UI 上没有任何活动，例如 `getNumPartitions()` 或 `take()`。这是因为
    Spark 遵循惰性执行模型，只有在需要用于下游操作时才返回结果，否则等待这样的操作。除了提到的那些之外，其他将强制执行的操作包括写入磁盘和 `collect()`（下面将描述）。
- en: 'In order to load our data using the PySpark DataFrames API (similar to Pandas
    DataFrames) instead of an RDD (which does not have many of the utility functions
    for DataFrame manipulation we illustrated above), we will need a file in **JavaScript
    Object Notation** (**JSON**) format. We can generate this file using the following
    command, which maps the elements of each row into a dictionary and casts it to
    JSON:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 PySpark DataFrames API（类似于 Pandas DataFrames）来加载数据，而不是使用 RDD（它没有我们上面展示的
    DataFrame 操作的许多实用函数），我们需要一个 **JavaScript 对象表示法**（**JSON**）格式的文件。我们可以使用以下命令生成此文件，该命令将每行的元素映射到一个字典，并将其转换为
    JSON：
- en: '[PRE50]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If you examine the output directory, you will notice that we have actually saved
    a directory with the name `movies.json` containing individual files (as many as
    there are partitions in our RDD). This is the same way in which data is stored
    in the **Hadoop distributed file system** (**HDFS**) in directories.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查输出目录，你会注意到我们实际上保存了一个名为 `movies.json` 的目录，其中包含单个文件（与我们的 RDD 中的分区数量一样多）。这是数据在
    **Hadoop 分布式文件系统**（**HDFS**）中存储在目录中的相同方式。
- en: Note that we have just scratched the surface of everything we can do with an
    RDD. We can perform other actions such as filtering, grouping RDDs by a key, projecting
    subsets of each row, ordering data within groups, joining to other RDDs, and many
    other operations. The full range of available transformations and operations is
    documented at [http://spark.apache.org/docs/latest/api/python/pyspark.html](http://spark.apache.org/docs/latest/api/python/pyspark.html).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们刚刚只是触及了 RDD 可以执行的所有操作的一小部分。我们可以执行其他操作，如过滤、按键对 RDD 进行分组、投影每行的子集、在组内排序数据、与其他
    RDD 进行连接，以及许多其他操作。所有可用的转换和操作的完整范围在 [http://spark.apache.org/docs/latest/api/python/pyspark.html](http://spark.apache.org/docs/latest/api/python/pyspark.html)
    中有文档说明。
- en: Creating a Spark DataFrame
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Spark DataFrame
- en: 'Now that we have our file in the JSON format, we can load it as a Spark DataFrame
    using:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 JSON 格式的文件，我们可以使用以下方式将其加载为 Spark DataFrame：
- en: '[PRE51]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: If we intend to perform many operations on this data, we can cache it (persist
    it in temporary storage), allowing us to operate on the data Spark's own internal
    storage format, which is optimized for repeated access. We cache the dataset using
    the following command.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打算对这份数据执行许多操作，我们可以将其缓存（在临时存储中持久化），这样我们就可以在 Spark 自己的内部存储格式上操作数据，该格式针对重复访问进行了优化。我们可以使用以下命令缓存数据集。
- en: '[PRE52]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`SqlContext` also allows us to declare a table alias for the dataset:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`SqlContext` 还允许我们为数据集声明一个表别名：'
- en: '[PRE53]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can then query this data as if it were a table in a relational database
    system:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像查询关系数据库系统中的表一样查询这些数据：
- en: '[PRE54]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Like the Pandas DataFrames, we can aggregate them by particular columns:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pandas DataFrame 类似，我们可以通过特定列对它们进行聚合：
- en: '[PRE55]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can also access individual columns using similar syntax to Pandas:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用与 Pandas 相似的语法来访问单个列：
- en: '[PRE56]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'If we want to bring all data to a single machine rather than operating on dataset
    partitions which may be spread across several computers, we can call the `collect()`
    command. Use this command with caution: for large datasets it will cause all of
    the partitions of the data to be combined and sent to the Drive, which could potentially
    overload the memory of the Driver. The `collect()` command will return an array
    of row objects, for which we can use `get()` to access individual elements (columns):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望将所有数据带到一台机器上，而不是在可能分布在几台计算机上的数据集分区上操作，我们可以调用 `collect()` 命令。使用此命令时请谨慎：对于大型数据集，它将导致所有数据分区被合并并发送到驱动器，这可能会潜在地超载驱动器的内存。`collect()`
    命令将返回一个行对象数组，我们可以使用 `get()` 来访问单个元素（列）：
- en: '[PRE57]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Not all operations we are interested in performing on our data may be available
    in the DataFrame API, so if necessary we can convert the DataFrame into an RDD
    of rows using the following command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣在数据上执行的所有操作可能都不在 DataFrame API 中可用，因此如果需要，我们可以使用以下命令将 DataFrame 转换为行 RDD：
- en: '[PRE58]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can even convert a PySpark DataFrame into Pandas DataFrame using:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用以下方法将 PySpark DataFrame 转换为 Pandas DataFrame：
- en: '[PRE59]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In later chapters, we will cover setting up applications and building models
    in Spark, but you should now be able to perform many of the same basic data manipulations
    you used in Pandas.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将介绍如何在 Spark 中设置应用程序和构建模型，但你现在应该能够执行许多与 Pandas 中相同的基本数据操作。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have now examined many of the tasks needed to start building analytical applications.
    Using the IPython notebook, we have covered how to load data in a file into a
    DataFrame in Pandas, rename columns in the dataset, filter unwanted rows, convert
    column data types, and create new columns. In addition, we have joined data from
    different sources and performed some basic statistical analyses using aggregations
    and pivots. We have visualized the data using histograms, scatter plots, and density
    plots as well as autocorrelation and log plots for time series. We also visualized
    geospatial data, using coordinate files to overlay data on maps. In addition,
    we processed the movies dataset using PySpark, creating both an RDD and a PySpark
    DataFrame, and performed some basic operations on these datatypes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经检查了许多开始构建分析应用程序所需的任务。使用 IPython 笔记本，我们介绍了如何将文件中的数据加载到 Pandas 的 DataFrame
    中，重命名数据集中的列，过滤掉不需要的行，转换列数据类型，以及创建新列。此外，我们还从不同的来源合并了数据，并使用聚合和交叉操作执行了一些基本的统计分析。我们还使用直方图、散点图和密度图以及自相关和日志图来可视化数据，以及使用坐标文件在地图上叠加地理空间数据。此外，我们还使用
    PySpark 处理了电影数据集，创建了 RDD 和 PySpark DataFrame，并对这些数据类型执行了一些基本操作。
- en: We will build on these tools in future sections, manipulating the raw input
    to develop features for building predictive analytics pipelines. We will later
    utilize similar tools to visualize and understand the features and performance
    of the predictive models we develop, as well as reporting the insights that they
    may deliver.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的部分中构建这些工具，通过操作原始输入来开发用于构建预测分析管道的特征。我们还将利用类似工具来可视化和理解我们开发的预测模型的特征和性能，以及报告它们可能提供的见解。
