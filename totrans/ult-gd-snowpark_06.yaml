- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Data Engineering Pipelines with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the heartbeat of every organization, and data engineering is the lifeblood
    that ensures that current, accurate data is flowing through for various consumption.
    The role of a data engineer is to develop and manage the data engineering pipeline
    and the process that collects, transforms, and delivers data to a different **line
    of business** (**LOB**). As Gartner’s research rightly mentions, “*The increasing
    diversity of data, and the need to provide the right data to the right people
    at the right time, has created a demand for the data engineering practice. Data
    and analytics leaders must integrate the data engineering discipline into their
    data management strategy.*” This chapter discusses a practical approach to building
    efficient data engineering pipelines with Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing resilient data pipelines with Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying efficient DataOps in Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of tasks in Snowflake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing logging and tracing in Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires an active Snowflake account and Python installed with
    Anaconda and configured locally. You can sign up for a Snowflake trial account
    at [https://signup.snowflake.com/](https://signup.snowflake.com/).
  prefs: []
  type: TYPE_NORMAL
- en: The technical requirements for environment setup are the same as in the previous
    chapters. If you haven’t set up your environment yet, please refer to the previous
    chapter. Supporting materials are available at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: Developing resilient data pipelines with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A robust and resilient data pipeline will equip organizations to source, collect,
    analyze, and effectively use insights to grow business and deliver cost-saving
    business processes. Traditional data pipelines are difficult to manage and do
    not support the organization’s evolving data needs. Snowpark solves problems that
    conventional data pipelines have by running them natively on the Snowflake Data
    Cloud and making extracting information from data in the Data Cloud easier and
    faster. This section will cover the various characteristics of resilient data
    pipelines, how to develop them in Snowpark, and their benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional versus modern data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A significant challenge of a traditional data pipeline is that it takes considerable
    time and cost to develop and manage, with high technical debt. It also consists
    of multiple tools that take much time to integrate. Due to the complexity of the
    solution, there is a possibility of delayed data due to latency and support for
    streaming data. The following diagram highlights the traditional method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Traditional data pipeline](img/B19923_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Traditional data pipeline
  prefs: []
  type: TYPE_NORMAL
- en: The architecture shows a complex of technologies and systems being stitched
    together to deliver data from the source to consumers, with multiple points of
    failure at each stage. There are also issues of data governance and security due
    to data silos being present with numerous copies of the same data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern data pipelines work based on a unified platform and multi-workload model.
    They integrate data sources such as batch and streaming to enhance productivity
    with streamlined architecture by enabling various **business intelligence** (**BI**)
    and analytics workloads and supporting internal and external users. The unified
    platform architecture supports continuous, extensible data processing pipelines
    with scalable performance. The following diagram highlights the modern Snowflake
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Modern data pipeline](img/B19923_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Modern data pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark stands out as a modern data pipeline tool due to its native integration
    with Snowflake, a leading cloud data warehouse, enabling seamless data processing
    directly within Spark applications. Offering a unified development experience
    with familiar programming languages such as Scala and Java, Snowpark eliminates
    the complexity associated with traditional Spark setups, allowing for streamlined
    development and maintenance of data pipelines. Snowpark’s optimized performance
    for Snowflake’s architecture ensures efficient data processing and reduced latency,
    enabling quick analysis of large datasets. Moreover, its advanced analytics capabilities,
    scalability, and cost-effectiveness make it a compelling choice for organizations
    seeking to build agile, cloud-native data pipelines with enhanced productivity
    and flexibility compared to traditional Spark setups.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering with Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Snowpark has many data engineering capabilities, making it a fast and flexible
    platform that enables developers to use Python for data engineering. With the
    support of **extract, transform, and load** (**ETL**) and **extract, load, and
    transform** (**ELT**), developers can use the Snowpark client for development
    and interact with the Snowflake engine for processing using their favorite developer
    environment. With the support of Anaconda, you can ensure that required packages
    and dependencies are readily available for Snowpark scripts. And it becomes easier
    to accelerate the growth of product pipelines. Data pipelines in Snowpark can
    be batch or real-time, utilizing scalable, high-performant multi-cluster warehouses
    capable of handling complex data transformations without compromising performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Snowpark data engineering](img/B19923_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Snowpark data engineering
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark enhances the entire data engineering lifecycle with an engine that
    enables expressiveness and flexibility for developers with the simplicity of Data
    Cloud operations. Snowflake can help make data centralized, including structured,
    semi-structured, and unstructured data loaded into Snowflake for processing. Transformations
    can be carried out with the help of the powerful Python-based Snowpark functions.
    Snowpark data engineering workloads can be fully managed alongside other Snowflake
    objects throughout the development lifecycle with built-in monitoring and orchestration
    capabilities that support complex data pipelines at scale powered by the Data
    Cloud. The result of advanced data transformations is stored inside Snowflake
    and can be used for different data consumers. Snowpark data pipelines reduce the
    number of stages data needs to move to actionable insights by removing the step
    that moves data for computation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing programmatic ELT with Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Snowflake supports and recommends a modern ELT implementation pattern for data
    engineering instead of the legacy ETL process. ETL is a pattern where data is
    extracted from various sources, transformed in the data pipeline, and then the
    transformed data is loaded into a destination such as a data warehouse or data
    mart. The following diagram shows the comparison of ETL versus ELT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – ETL versus ELT](img/B19923_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – ETL versus ELT
  prefs: []
  type: TYPE_NORMAL
- en: 'ELT is a pattern that is more suited for the Snowflake Data Cloud, where data
    is extracted from the source and loaded into Snowflake. This data is then transformed
    within Snowflake using Snowpark. Snowpark pipelines are designed to extract and
    load the data first and then transform it in the destination as the transformation
    is done inside Snowflake, which provides better scalability and elasticity. The
    ELT also improves performance and reduces the time it takes to ingest, transform,
    and analyze the data within Snowflake using Snowpark. The following diagram shows
    the different layers of data within Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Data stages in Snowflake](img/B19923_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Data stages in Snowflake
  prefs: []
  type: TYPE_NORMAL
- en: The data pipelines build the different data stages inside Snowflake. These stages
    are databases and schemas with objects such as tables and views inside them. The
    raw data is ingested from the source systems into Snowflake with no transformations
    since Snowflake supports multiple data formats. This data is then transformed
    using Snowpark into the conformed stage containing the de-duped and standardized
    data. This becomes the data that feeds into the next step in the data pipeline.
    The reference stage has the business definition and data mappings with the hierarchies
    and the master data. The final stage has the modeled data, which has the clean
    and transformed data. Snowpark has many functions that help with doing value-added
    transformations that help convert data into a business-ready format accessible
    to users and applications, making it more valuable for the organization.
  prefs: []
  type: TYPE_NORMAL
- en: ETL versus ELT in Snowpark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Snowpark supports both ETL and ELT workloads. While ELT is famous for modern
    pipelines, the ETL pattern is also used in some scenarios. ETL is commonly used
    with structured data where the total volume of data is small. It is also used
    in the system for migrating legacy databases to the Data Cloud where the source
    and target data types differ. ELT provides a significant advantage compared to
    the traditional ETL process. ELT supports large volumes of structured, unstructured,
    and semi-structured data that can be processed using Snowflake. It also allows
    developers and analysts to experiment with data as it is loaded into Snowflake.
    ELT also maximizes the option for them to transform data to get potential insights.
    It also supports low latency and real-time analytics. ELT is better suited for
    Snowflake than the traditional ETL for these reasons. The following section will
    cover how to develop efficient DataOps in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying efficient DataOps in Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataOps helps data teams reduce development times, increase data quality, and
    maximize the business value of data by bringing more rigor to the development
    and management of data pipelines. It also ensures that the data is clean, accurate,
    and up-to-date in a streamlined environment with data governance. Data engineering
    introduces the processes and capabilities required to effectively develop, manage,
    and deploy data engineering pipelines. The following diagram highlights the DataOps
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – DataOps process](img/B19923_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – DataOps process
  prefs: []
  type: TYPE_NORMAL
- en: The DataOps process focuses on bringing agile development to data engineering
    pipelines using an iterative development, testing, and deployment process in loops.
    It also includes **continuous integration and continuous deployment** (**CI/CD**)
    for data, schema changes, and the data versioning and automation of data models
    and artifacts. This section will show an example of a data engineering pipeline
    executed in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a data engineering pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a resilient data engineering pipeline within the Snowpark framework
    requires integrating three core components seamlessly:'
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, data engineers must master the art of loading data into
    Snowflake, setting the stage for subsequent processing. This initial step sets
    the foundation upon which the entire pipeline is built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, the transformative power of Snowpark data functions comes into play,
    enabling engineers to shape and mold the data to meet specific analytical needs.
    [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042)*,* *Simplifying Data Processing
    Using Snowpark* provided a detailed exploration of DataFrame operations, laying
    the groundwork for this pivotal transformation phase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the data journey culminates in bundling these operations as Snowpark
    stored procedures, offering efficiency and repeatability in handling data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we delve into this section, building upon the knowledge garnered from [*Chapter
    2*](B19923_02.xhtml#_idTextAnchor028)*, Establishing a Foundation with Snowpark*
    and [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042), *Simplifying Data Processing
    Using Snowpark* where we elaborated on DataFrame operations and their conversion
    into **user-defined functions** (**UDFs**) and stored procedures, we will unravel
    the intricate process of unifying these elements into a resilient data engineering
    pipeline. This chapter is a testament to the synthesis of theory and practice,
    empowering data professionals to seamlessly interconnect the loading, transformation,
    and bundling phases, resulting in a robust framework for data processing and analysis
    within the Snowpark ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a comprehensive understanding of data loading from our discussions in
    the previous chapters, our journey now pivots toward strategically utilizing this
    data. This pivotal transition places our emphasis on three core steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleanup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These stages constitute the cornerstone of our data engineering voyage, where
    we will sculpt, consolidate, and refine our data, revealing its true potential
    for analysis and valuable insights. We’ll now transform these concepts into practical
    data engineering pipelines, leveraging the valuable insights from our prior discussions
    on stored procedure templates and transformation steps. Our focus will center
    on our marketing campaign data, where the foundational loading steps have been
    thoughtfully outlined in [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042), *Simplifying
    Data Processing Using Snowpark* providing a solid starting point for our data
    preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the progression of our data engineering pipeline, the subsequent imperative
    phase is data preparation, which involves the integration of diverse tables. In
    this section, we will explore techniques for merging these disparate data tables
    using various functions tailored to the task. Additionally, we will elucidate
    the process of registering these functions as stored procedures, ensuring a streamlined
    and efficient data workflow. The first step is joining the purchase history with
    the campaign information. Both tables are entered using the `ID` column, and a
    single ID is retained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The resultant `purchase_campaign` DataFrame holds the data and is used in the
    next step. In the next step, we join the purchase campaign with the complaint
    information using the same `ID` column and then create a `purchase_campaign_complain`
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code joins the column to create a `purchase_campaign_complain`
    DataFrame, which contains the mapped purchase data with complaint information.
    In the final step, a marketing table is created by the union of the data between
    the purchase complaint and the marketing table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a table that contains all the combined data that
    is the final result of the pipeline, which will be written as a table. The Python
    functions representing each step are executed as part of the Snowpark stored procedure.
    The stored procedures can be performed in sequence one after the other and also
    scheduled as Snowflake tasks. The data preparation procedure calls the three Python
    methods, and the final table is written to Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code does the data preparation by loading the required data into
    the DataFrame. We will now call each of the steps to execute it like a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The three previously defined step functions are executed. The resultant data
    is loaded into the new `final_marketing_data` DataFrame, which will then be loaded
    to the Snowflake table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create and execute a stored procedure that contains the preceding
    logic. The procedure is called `data_prep_sproc` and is the first part of the
    data engineering pipeline – data preparation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding stored procedure writes the data into the `Final_Marketing_Data`
    table, which will be used in the next step of data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The subsequent phase in this process involves data transformation, building
    upon the data prepared in the previous step. Here, we’ll take the pivotal action
    of registering another stored procedure after the last stage. This procedure applies
    transformation logic, molding the data into a form primed for analysis. Leveraging
    Snowpark’s array of valuable aggregation and summarization functions, we will
    harness these capabilities to shape and enhance our data, laying a solid foundation
    for rigorous analysis. The following code transforms the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A `data_transform_sproc` stored procedure is created, which reads the `Final_Marketing_Data`
    table and creates a pivot with the education of the customer and the total income.
    When the stored procedure is executed, this is then written to the `Marketing_Pivot`
    table.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleanup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the final step of our data engineering process, we focus on a crucial task:
    cleaning up the data in the `Marketing_Pivot` table. Similar to artists perfecting
    a masterpiece, we carefully go through our data, removing any empty values in
    tables that aren’t important for our analysis. To do this, we rely on the versatile
    `dropna()` function, which acts like a precise tool to cut away unnecessary data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The cleaned-up data from the `market_drop_null` DataFrame is then saved into
    the `Market_Pivot_Cleaned` table. This data is at the last stage of the pipeline
    and is used for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating the pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data pipeline is orchestrated by calling three Snowpark procedures, which
    invokes the three different steps of the data engineering pipeline. The procedures
    are executed in the order of `data_prep_sproc`, `data_transform_sproc`, and `data_cleanup_sproc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The Snowpark procedure is executed, and after each step is executed, the final
    data is written to the `Market_Pivot_Cleaned` table. Snowflake supports scheduling
    and orchestration through tasks. Tasks can be scheduled using the Python API and
    through worksheets, and they can trigger procedures in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Stored procedure execution](img/B19923_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Stored procedure execution
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will explore how we can utilize Snowflake tasks
    and task graphs to execute the preceding pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of tasks in Snowflake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tasks in Snowflake are powerful tools designed to streamline data processing
    workflows and automate various tasks within the Snowflake environment. Offering
    a range of functionalities, tasks execute different types of SQL code, enabling
    users to perform diverse operations on their data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tasks in Snowflake can execute three main types of SQL code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single SQL statement**: Allows the execution of a single SQL statement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call to a stored procedure**: Enables the invocation of a stored procedure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Procedural logic using Snowflake Scripting**: Supports the implementation
    of procedural logic using Snowflake Scripting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks can be integrated with table streams to create continuous ELT workflows.
    By processing recently changed table rows, tasks ensure the maintenance of data
    integrity and provide exactly-once semantics for new or altered data. Tasks in
    Snowflake can be scheduled to run at specified intervals. Snowflake ensures that
    only one instance of a scheduled task is executed at a time, skipping scheduled
    executions if a task is still running.
  prefs: []
  type: TYPE_NORMAL
- en: Compute models for tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the serverless compute model, tasks rely on compute resources managed by
    Snowflake. These resources are automatically resized and scaled based on workload
    demands, ensuring optimal performance and resource utilization. Snowflake dynamically
    determines the appropriate compute size for each task run based on historical
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, users can opt for the user-managed virtual warehouse model, where
    they specify an existing virtual warehouse for individual tasks. This model provides
    users with more control over compute resource management but requires careful
    sizing to ensure efficient task execution.
  prefs: []
  type: TYPE_NORMAL
- en: Task graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Task graphs, also known as **directed acyclic graphs** (**DAGs**), allow for
    the organization of tasks based on dependencies. Each task within a task graph
    has predecessor and subsequent tasks, facilitating complex workflow management.
  prefs: []
  type: TYPE_NORMAL
- en: Task graphs are subject to certain limitations, including a maximum of 1,000
    tasks in total, including the root task. Individual tasks within a task graph
    can have a maximum of 100 predecessors and 100 child tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Users can view and monitor their task graphs using SQL or Snowsight, Snowflake’s
    integrated development environment, providing visibility into task dependencies
    and execution status.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, tasks in Snowflake offer robust capabilities for data processing,
    automation, and workflow management, making them indispensable tools for users
    seeking to optimize their data operations within the Snowflake ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Managing tasks and task graphs with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our primary focus is on Snowpark. Now, we’ll explore how we can utilize Python
    Snowpark to programmatically perform task graph operations instead of using SQL
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, Python can manage Snowflake tasks, allowing users to run SQL statements,
    procedure calls, and Snowflake Scripting logic. The Snowflake Python API introduces
    two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task**: This type represents a task’s properties, such as its schedule, parameters,
    and dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TaskResource**: This type provides methods to interact with **Task** objects,
    enabling task execution and modification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks can be grouped into task graphs, which consist of interconnected tasks
    arranged based on their dependencies. To create a task graph, users first define
    a DAG object, specifying its name and optional properties, such as its schedule.
    The scheduling of a task graph can be customized using either a `timedelta` value
    or a cron expression, allowing for flexible task execution timing and recurrence
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by setting up the necessary functions to implement our DAG. The
    examples provided in this section presuppose that you’ve already written code
    to establish a connection with Snowflake to utilize the Snowflake Python API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code initializes the Snowflake Python API, creating a `root` object
    for utilizing its types and methods. Additionally, it sets up a `timedelta` value
    of 1 hour for the task’s schedule. You can define the schedule using either a
    `timedelta` value or a Cron expression. For one-off runs, you can omit the schedule
    argument to the DAG object without worrying about it running unnecessarily in
    the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a simple DAG that we’ll use to execute our pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this DAG setup, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve named our DAG **Task_Demo**, which by default runs on the specified warehouse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A schedule for daily execution has been defined using **timedelta**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **stage_location** attribute is necessary for storing the serialized version
    of the tasks via the Python API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All tasks under this DAG will run with the default list of packages and the
    specified warehouse. However, both the warehouse and packages for individual tasks
    within the DAG can be overridden with different values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the **use_func_return_value** attribute indicates that the return
    value of Python functions will be treated as the return value of the task, though
    in our case, we’re not utilizing the **return** object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve now defined a series of Python functions representing a three-task pipeline,
    or DAG. However, we haven’t yet created and pushed the DAG to Snowflake. Let’s
    do that now using the Snowflake Python API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For one-off testing or running of the DAG, users can skip specifying the schedule
    and manually trigger a run with `dag_op.run(dag)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The provided code performs several actions using Snowflake’s Snowpark library.
    Firstly, it retrieves the schema named `MY_SCHEMA` from the `SNOWPARK_DEFINITIVE_GUIDE`
    database using the `root` object. Then, it initializes a `DAGOperation` object
    named `dag_op`, specifying the schema where the DAG will be deployed. The `deploy()`
    method is then called on `dag_op` to deploy the specified DAG (named `dag`) in
    the specified schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `orReplace` mode argument indicates that if a DAG with the same name already
    exists in the schema, it will be replaced. Finally, the `run()` method is called
    on `dag_op` to execute the deployed DAG. This code essentially sets up and executes
    a DAG within Snowflake using Snowpark. Now, you can check the graph in Snowsight
    to see how the graph has been set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Snowsight graph](img/B19923_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Snowsight graph
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, note that you can pass the raw function to the dependency definition
    without explicitly creating a `DAGTask` instance, with the library automatically
    creating a task for you with the same name. However, there are exceptions to this
    rule, such as needing to explicitly create a `DAGTask` instance for the first
    task or when utilizing `DAGTaskBranch` or repeating certain functions in multiple
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve deployed and run the DAG, you can easily check its status using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As depicted in the screenshot, our tasks are scheduled to run daily. Additionally,
    we have executed them once to verify the task deployment beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Tasks deployed](img/B19923_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Tasks deployed
  prefs: []
  type: TYPE_NORMAL
- en: As we progress in constructing more complex pipelines, managing and debugging
    become increasingly challenging. It’s crucial to establish a robust logging mechanism
    to facilitate maintenance and streamline error resolution. In the next section,
    we’ll delve into implementing logging and traceback functionalities in Snowpark
    to enhance our pipeline’s manageability and ease troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logging and tracing in Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logging and tracing are crucial for DataOps and are necessary to monitor and
    fix failures in the data engineering pipeline. Snowpark comes with logging and
    tracing functionality that is built in, which can help record the activity of
    Snowpark functions and procedures and capture those in an easy-to-access central
    table inside Snowflake. Log messages are independent, detailed messages with information
    in the form of strings, providing details about the piece of code, and trace events
    are structured data that we can use to get information spanning and grouping multiple
    parts of our code. Once logs are collected, they can be easily queried by SQL
    or accessed via Snowpark. The following diagram highlights the event table and
    alerting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Event table](img/B19923_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Event table
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark stores logs and trace messages inside the event table, a unique table
    with a predefined set of columns. Logs and traces are captured in this table as
    the code is executed. Let’s look at the structure of event tables and how to create
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Event tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An event table is native to Snowflake and needs to be created. There can be
    only one event table for a Snowflake account that captures all the information,
    but multiple views can be made for analysis. An event table contains the following
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column** | **Data Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `TIMESTAMP` | `TIMESTAMP_NTZ` | The UTC timestamp when an event was created.
    This is the end of the period for events representing a period. |'
  prefs: []
  type: TYPE_TB
- en: '| `START_TIMESTAMP` | `TIMESTAMP_NTZ` | For events representing a period, such
    as trace events as the start of the period as a UTC timestamp. |'
  prefs: []
  type: TYPE_TB
- en: '| `OBSERVED_TIMESTAMP` | `TIMESTAMP_NTZ` | A UTC timestamp is used for logs.
    Currently, it has the same value as `TIMESTAMP`. |'
  prefs: []
  type: TYPE_TB
- en: '| `TRACE` | `OBJECT` | Tracing context for all signal types. Contains `trace_id`
    and `span_id` string values. |'
  prefs: []
  type: TYPE_TB
- en: '| `RESOURCE` | `OBJECT` | Reserved for future use. |'
  prefs: []
  type: TYPE_TB
- en: '| `RESOURCE_ATTRIBUTES` | `OBJECT` | Attributes that identify the source of
    an event, such as database, schema, user, warehouse, and so on. |'
  prefs: []
  type: TYPE_TB
- en: '| `SCOPE` | `OBJECT` | Scopes for events; for example, class names for logs.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `SCOPE_ATTRIBUTES` | `OBJECT` | Reserved for future use. |'
  prefs: []
  type: TYPE_TB
- en: '| `RECORD_TYPE` | `STRING` | The event types. One of the following:`LOG` for
    a log message.`SPAN` for UDF invocations performed sequentially on the same thread.
    `SPAN_EVENT` for a single trace event. A single query can emit more than one `SPAN_EVENT`
    event type. |'
  prefs: []
  type: TYPE_TB
- en: '| `RECORD` | `OBJECT` | Fixed values for each record type. |'
  prefs: []
  type: TYPE_TB
- en: '| `RECORD_ATTRIBUTES` | `OBJECT` | Variable attributes for each record type.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `VALUE` | `VARIANT` | Primary event value. |'
  prefs: []
  type: TYPE_TB
- en: '| `EXEMPLARS` | `ARRAY` | Reserved for future use. |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Event table columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Each column can be queried or combined to analyze different outcomes based
    on the logs and the traces. The log type describes the log levels assigned as
    part of the logging and can be set at both objects and the session. The log levels
    can be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **LOG_LEVEL** **Parameter Setting** | **Levels of** **Messages Ingested**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `TRACE` | `TRACE``DEBUG``INFO``WARN``ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: '| `DEBUG` | `DEBUG``INFO``WARN``ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: '| `INFO` | `INFO``WARN``ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: '| `WARN` | `WARN``ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: '| `ERROR` | `ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: '| `FATAL` | `ERROR``FATAL` |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Event table log levels
  prefs: []
  type: TYPE_NORMAL
- en: The log level is a hierarchy applied in the order presented in *Table 4.2*.
    In the next section, we will look at creating an event table.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is best practice to set the necessary log level based on a minor level such
    as **FATAL** and ERROR so that the number of logged messages is fewer. In the
    case of logging **INFO**, it is usually turned on to capture the log and turned
    off in production to avoid catching too many records.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring an event table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to create an event table for the Snowflake account. The name
    of the event table can be specified, and columns for the event table are not required
    to be set when creating the table, as Snowflake automatically creates it with
    the standard columns. The event table is assigned to the account and needs to
    be made in a separate database that does not have Snowflake replication enabled
    using the `ACCOUNTADMIN` role.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an event table, run the following command with the `ACCOUNTADMIN`
    role using Snowpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'An event table called `MY_EVENTS` is created with the default column structure.
    The next step is to assign the event table as the active event table to a particular
    Snowflake account. The event table can be assigned to an account by executing
    the following code with the `ACCOUNTADMIN` role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The parameter is applied at the account level, and all events from the particular
    Snowflake account are captured in this event table. This completes the event table
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: Querying event tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An event table can be accessed just like any other Snowflake table. To get
    the records from an event table, you can query the event table with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns a result with empty records since no information was captured. Records
    in an event table can be filtered with a specific column to get detailed information.
    A Snowflake stream can be set on top of the event table to capture only new events.
    A Stream can be created by running the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `EVENT_APPEND` stream captures the latest inserted records into the event
    table. In the next section, we will set up logging and tracing to capture records
    in an event table.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up logging in Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing logging and tracing capabilities into our pipelines is akin to infusing
    resilience into our standard data engineering processes. This section will delve
    into the integration of logging functionalities into our existing data engineering
    pipeline. By doing so, we not only gain the ability to monitor and trace the flow
    of data but also enhance the robustness of our code, fortifying it against potential
    pitfalls. Join us as we explore how these logging capabilities elevate our data
    engineering practices, making them more reliable and fault-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logging can be enabled for both Snowpark functions and procedures. The first
    step in capturing logs is to set the log level in the Snowpark session. The log
    level in the session can be set by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This sets the log level to `INFO` for the particular session, so all Snowpark
    execution that happens for the specific session is captured with the log level
    as information. Snowpark supports APIs to log messages directly from the handler.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing informational logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now modify the data preparation procedures from the data pipeline to
    capture informational logs. We start with prepping the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The data from the four tables is loaded into the DataFrame. This DataFrame
    is then used to execute each step. Next, we will proceed with calling each step
    in order to process the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all three steps are executed, we get the final marketing data ready to
    be loaded into a Snowflake table for consumption. The following code will load
    the data into a Snowflake table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is loaded into a table called `FINAL_MARKETING_DATA`. The table is
    automatically created with the data in the Snowpark DataFrame. We will now register
    this as a Snowpark stored procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `logging` module from Python’s standard library is used for logging. The
    package is imported, and the logger’s name is specified as `My_Logger`. Different
    logger names can be set for processes to help identify the particular logging
    application. We can execute the stored procedure to capture logs to the event
    table. The stored procedure can be executed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The stored procedure is executed, and we can see the successful output of the
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Stored procedure execution](img/B19923_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Stored procedure execution
  prefs: []
  type: TYPE_NORMAL
- en: The following section will cover how to query logs generated by the stored procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Querying informational logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Logs generated by the stored procedure can be accessed from the event table.
    Records captured from the previous stored procedure can be accessed by running
    the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We filter logs captured only from `My_Logger` by specifying it in the scope.
    The query returns the following records that were generated from executing the
    stored procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Querying logs](img/B19923_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Querying logs
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will set up logging to capture error messages and handle
    exceptions in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Handling exceptions in Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exception handling is integral to data pipelines as it helps identify and handle
    issues. Exception handling can be done by catching exceptions thrown from within
    the `try` block and capturing those error logs. The `ERROR` and `WARN` log levels
    are often used when capturing exceptions, and fatal issues are logged at the `FATAL`
    level. This section will look at capturing error logs and handling the exception
    on the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing error logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will modify the data transformation stored procedure to capture error logs
    and add exception handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The data transformation logic is moved into the `try` block, and a logger named
    `Data_Transform_Logger` is initiated. The exception raised by the code is captured
    in the exception object defined as `err`. This is then logged in the event table
    by the `ERROR` log level. We will now register this stored procedure in Snowpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute the stored procedure purposely with the error to log the error.
    The stored procedure can be triggered by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The stored procedure has raised an exception, and the error has been captured
    in the event table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Error execution](img/B19923_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Error execution
  prefs: []
  type: TYPE_NORMAL
- en: The following section will cover querying error logs generated by the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Querying error logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The error log records from the preceding stored procedure are captured under
    the `Data_Transform_Logger` logger, which can be accessed by filtering the query
    to return logs specific to the logger. The following query can be executed to
    get the records from the event table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The scope name is filtered as the `Data_Transform_Logger` to get the results,
    and errors caused by the exception are logged under `SEVERITY` as `ERROR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Error log messages](img/B19923_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Error log messages
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark makes debugging easy by supporting logging errors and handling exceptions
    through event tables. The following section will cover event traces and how to
    capture trace information in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up tracing in Snowpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trace events are a type of telemetry data that is captured when something has
    happened in the code. It has a structured payload that helps analyze the trace
    by aggregating this information to understand the code’s behavior at a high level.
    When a procedure or function executes, trace events are emitted, which are available
    in the active event table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in capturing events is to turn on the tracing functionality
    by setting the trace level in the Snowpark session. The trace level in the session
    can be set by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will look at capturing traces.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing traces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traces can be captured with the open source Snowflake `telemetry` Python package
    available in the Anaconda Snowflake channel. The package needs to be imported
    into the code, and it will be executed in Snowpark. The `telemetry` package can
    be imported by including the code in the Snowpark handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `telemetry` package helped capture traces generated on the code and logged
    into the event table. We will modify the data cleanup procedure by adding telemetry
    events to capture the trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The procedure is now ready to be executed. We are passing attributes for the
    operations captured on the traces. We can execute the procedure by running the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The procedure is executed, and the data is cleaned from the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Data cleanup procedure execution](img/B19923_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Data cleanup procedure execution
  prefs: []
  type: TYPE_NORMAL
- en: The traces are now generated in the event table. We can directly query the event
    table to obtain trace information.
  prefs: []
  type: TYPE_NORMAL
- en: Querying traces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traces generated can be accessed from the event table. Traces captured from
    the previous stored procedure can be accessed by running the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We are querying using the `data_cleanup` event name. This returns the two traces
    captured when the code was executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Trace capture information](img/B19923_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Trace capture information
  prefs: []
  type: TYPE_NORMAL
- en: We can see the attributes that are captured from the execution of the stored
    procedure. After the `NULL` values have been cleaned up, the details return the
    total data counts, including the `NULL` and `count` values.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of logs and traces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table compares logs and traces and lists scenarios to use each:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Characteristic** | **Log entries** | **Trace events** |'
  prefs: []
  type: TYPE_TB
- en: '| Intended use | Record detailed but unstructured information about the state
    of your code. Use this information to understand what happened during a particular
    invocation of your function or procedure. | Record a brief but structured summary
    of each invocation of your code. Aggregate this information to understand the
    behavior of your code at a high level. |'
  prefs: []
  type: TYPE_TB
- en: '| Structure as a payload | None. A log entry is just a string. | Structured
    with attributes you can attach to trace events. Attributes are key-value pairs
    that can be easily queried with a SQL query. |'
  prefs: []
  type: TYPE_TB
- en: '| Supports grouping | No. Each log entry is an independent event. | Yes. Trace
    events are organized into spans. A span can have its attributes. |'
  prefs: []
  type: TYPE_TB
- en: '| Quantity limits | Unlimited. All log entries emitted by your code are ingested
    into the event table. | The number of trace events per span is capped at 128\.
    There is also a limit on the number of span attributes. |'
  prefs: []
  type: TYPE_TB
- en: '| Complexity of queries against recorded data | Relatively high. Your queries
    must parse each log entry to extract meaningful information from it. | Relatively
    low. Your queries can take advantage of the structured nature of trace events.
    |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Differences between logs and traces
  prefs: []
  type: TYPE_NORMAL
- en: Logs and traces help debug Snowpark and are a practical feature for efficient
    DataOps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered in detail building and deploying resilient data
    pipelines in Snowpark and also how to enable logging and tracking using event
    tables. Building resilient data pipelines with effective DataOps is vital for
    a successful data strategy. Snowpark supports the development of a modern data
    pipeline through a programmatic ELT approach along with features such as logging
    and tracing, making it easy for developers to implement DataOps. We also covered
    how tasks and task graphs can be used in scheduling and deploying pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover using Snowpark to develop data science workloads.
  prefs: []
  type: TYPE_NORMAL
