- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Building Data Engineering Pipelines with Snowpark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snowpark 构建数据工程管道
- en: Data is the heartbeat of every organization, and data engineering is the lifeblood
    that ensures that current, accurate data is flowing through for various consumption.
    The role of a data engineer is to develop and manage the data engineering pipeline
    and the process that collects, transforms, and delivers data to a different **line
    of business** (**LOB**). As Gartner’s research rightly mentions, “*The increasing
    diversity of data, and the need to provide the right data to the right people
    at the right time, has created a demand for the data engineering practice. Data
    and analytics leaders must integrate the data engineering discipline into their
    data management strategy.*” This chapter discusses a practical approach to building
    efficient data engineering pipelines with Snowpark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是每个组织的脉搏，数据工程是确保各种消费有最新、准确数据流动的生命线。数据工程师的角色是开发和管理工作流程，该流程收集、转换并将数据交付到不同的**业务线**（**LOB**）。正如
    Gartner 的研究正确提到的，“*数据的日益多样化以及需要在正确的时间向正确的人提供正确数据的需求，已经产生了对数据工程实践的需求。数据和分析领导者必须将数据工程学科整合到他们的数据管理策略中。*”本章讨论了使用
    Snowpark 构建高效数据工程管道的实用方法。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Developing resilient data pipelines with Snowpark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Snowpark 开发具有弹性的数据管道
- en: Deploying efficient DataOps in Snowpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Snowpark 中部署高效的数据操作
- en: Overview of tasks in Snowflake
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowflake 中任务概述
- en: Implementing logging and tracing in Snowpark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Snowpark 中实现日志记录和跟踪
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires an active Snowflake account and Python installed with
    Anaconda and configured locally. You can sign up for a Snowflake trial account
    at [https://signup.snowflake.com/](https://signup.snowflake.com/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要有效的 Snowflake 账户，并且需要安装 Anaconda 中的 Python 并在本地配置。您可以在 [https://signup.snowflake.com/](https://signup.snowflake.com/)
    注册 Snowflake 试用账户。
- en: The technical requirements for environment setup are the same as in the previous
    chapters. If you haven’t set up your environment yet, please refer to the previous
    chapter. Supporting materials are available at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 环境设置的技术要求与前面章节相同。如果您还没有设置环境，请参考前面的章节。支持材料可在 [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)
    获取。
- en: Developing resilient data pipelines with Snowpark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snowpark 开发具有弹性的数据管道
- en: A robust and resilient data pipeline will equip organizations to source, collect,
    analyze, and effectively use insights to grow business and deliver cost-saving
    business processes. Traditional data pipelines are difficult to manage and do
    not support the organization’s evolving data needs. Snowpark solves problems that
    conventional data pipelines have by running them natively on the Snowflake Data
    Cloud and making extracting information from data in the Data Cloud easier and
    faster. This section will cover the various characteristics of resilient data
    pipelines, how to develop them in Snowpark, and their benefits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强大且具有弹性的数据管道将使组织能够获取、收集、分析和有效地使用洞察力以增长业务并交付节省成本的业务流程。传统的数据管道难以管理，且不支持组织不断变化的数据需求。Snowpark
    通过在 Snowflake 数据云中本地运行来解决问题，使得从数据云中提取信息变得更加容易和快速。本节将涵盖具有弹性的数据管道的各种特性、如何在 Snowpark
    中开发它们以及它们的益处。
- en: Traditional versus modern data pipelines
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统数据管道与现代数据管道的比较
- en: 'A significant challenge of a traditional data pipeline is that it takes considerable
    time and cost to develop and manage, with high technical debt. It also consists
    of multiple tools that take much time to integrate. Due to the complexity of the
    solution, there is a possibility of delayed data due to latency and support for
    streaming data. The following diagram highlights the traditional method:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据管道的一个重大挑战是，开发和管理工作流程需要相当的时间和成本，并且有很高的技术债务。它还包含多个工具，需要花费大量时间进行集成。由于解决方案的复杂性，可能会因为延迟和流数据支持而出现数据延迟。以下图表突出了传统方法：
- en: '![Figure 4.1 – Traditional data pipeline](img/B19923_04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 传统数据管道](img/B19923_04_01.jpg)'
- en: Figure 4.1 – Traditional data pipeline
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 传统数据管道
- en: The architecture shows a complex of technologies and systems being stitched
    together to deliver data from the source to consumers, with multiple points of
    failure at each stage. There are also issues of data governance and security due
    to data silos being present with numerous copies of the same data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构展示了各种技术和系统被拼接在一起，以从源到消费者传递数据，每个阶段都有多个故障点。由于存在数据孤岛，并且有大量相同数据的副本，因此还存在数据治理和安全问题。
- en: 'Modern data pipelines work based on a unified platform and multi-workload model.
    They integrate data sources such as batch and streaming to enhance productivity
    with streamlined architecture by enabling various **business intelligence** (**BI**)
    and analytics workloads and supporting internal and external users. The unified
    platform architecture supports continuous, extensible data processing pipelines
    with scalable performance. The following diagram highlights the modern Snowflake
    approach:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据管道基于统一平台和多工作负载模型运行。它们通过集成数据源，如批量处理和流式处理，通过启用各种**商业智能**（**BI**）和分析工作负载以及支持内部和外部用户，通过简化的架构提高生产力。统一平台架构支持连续、可扩展的数据处理管道，具有可扩展的性能。以下图表突出了现代
    Snowflake 方法：
- en: '![Figure 4.2 – Modern data pipeline](img/B19923_04_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 现代数据管道](img/B19923_04_02.jpg)'
- en: Figure 4.2 – Modern data pipeline
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 现代数据管道
- en: Snowpark stands out as a modern data pipeline tool due to its native integration
    with Snowflake, a leading cloud data warehouse, enabling seamless data processing
    directly within Spark applications. Offering a unified development experience
    with familiar programming languages such as Scala and Java, Snowpark eliminates
    the complexity associated with traditional Spark setups, allowing for streamlined
    development and maintenance of data pipelines. Snowpark’s optimized performance
    for Snowflake’s architecture ensures efficient data processing and reduced latency,
    enabling quick analysis of large datasets. Moreover, its advanced analytics capabilities,
    scalability, and cost-effectiveness make it a compelling choice for organizations
    seeking to build agile, cloud-native data pipelines with enhanced productivity
    and flexibility compared to traditional Spark setups.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 由于与领先的云数据仓库 Snowflake 的原生集成而脱颖而出，使其能够在 Spark 应用程序内直接进行无缝数据处理。它提供了一种统一的开发体验，使用熟悉的编程语言，如
    Scala 和 Java，消除了传统 Spark 设置的复杂性，从而简化了数据管道的开发和维护。Snowpark 对 Snowflake 架构的优化性能确保了高效的数据处理和降低延迟，使得对大型数据集的快速分析成为可能。此外，其高级分析能力、可扩展性和成本效益使其成为寻求构建敏捷、云原生数据管道的组织的一个有吸引力的选择，与传统的
    Spark 设置相比，它提高了生产力和灵活性。
- en: Data engineering with Snowpark
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Snowpark 进行数据工程
- en: 'Snowpark has many data engineering capabilities, making it a fast and flexible
    platform that enables developers to use Python for data engineering. With the
    support of **extract, transform, and load** (**ETL**) and **extract, load, and
    transform** (**ELT**), developers can use the Snowpark client for development
    and interact with the Snowflake engine for processing using their favorite developer
    environment. With the support of Anaconda, you can ensure that required packages
    and dependencies are readily available for Snowpark scripts. And it becomes easier
    to accelerate the growth of product pipelines. Data pipelines in Snowpark can
    be batch or real-time, utilizing scalable, high-performant multi-cluster warehouses
    capable of handling complex data transformations without compromising performance:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 拥有许多数据工程能力，使其成为一个快速灵活的平台，使开发者能够使用 Python 进行数据工程。有了**提取、转换和加载**（**ETL**）和**提取、加载和转换**（**ELT**）的支持，开发者可以使用
    Snowpark 客户端进行开发，并使用他们喜欢的开发环境与 Snowflake 引擎进行交互以处理数据。有了 Anaconda 的支持，你可以确保所需的包和依赖项对
    Snowpark 脚本来说是现成的。这使得加速产品管道的增长变得更容易。Snowpark 中的数据管道可以是批量或实时，利用可扩展、高性能的多集群仓库，能够处理复杂的数据转换而不会影响性能：
- en: '![Figure 4.3 – Snowpark data engineering](img/B19923_04_03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – Snowpark 数据工程](img/B19923_04_03.jpg)'
- en: Figure 4.3 – Snowpark data engineering
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – Snowpark 数据工程
- en: Snowpark enhances the entire data engineering lifecycle with an engine that
    enables expressiveness and flexibility for developers with the simplicity of Data
    Cloud operations. Snowflake can help make data centralized, including structured,
    semi-structured, and unstructured data loaded into Snowflake for processing. Transformations
    can be carried out with the help of the powerful Python-based Snowpark functions.
    Snowpark data engineering workloads can be fully managed alongside other Snowflake
    objects throughout the development lifecycle with built-in monitoring and orchestration
    capabilities that support complex data pipelines at scale powered by the Data
    Cloud. The result of advanced data transformations is stored inside Snowflake
    and can be used for different data consumers. Snowpark data pipelines reduce the
    number of stages data needs to move to actionable insights by removing the step
    that moves data for computation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark通过一个能够为开发者提供数据云操作简单性的引擎，增强了整个数据工程的生命周期，使开发者能够拥有表达性和灵活性。Snowflake可以帮助实现数据的集中化，包括结构化、半结构化和非结构化数据，这些数据被加载到Snowflake中进行处理。可以通过基于Python的强大Snowpark函数来执行转换。在整个开发生命周期中，Snowpark的数据工程工作负载可以与其他Snowflake对象一起完全管理，内置的监控和编排能力支持由数据云驱动的复杂数据管道的扩展。高级数据转换的结果存储在Snowflake内部，可用于不同的数据消费者。Snowpark数据管道通过移除用于计算的数据移动步骤，减少了数据需要移动到可操作见解的阶段数量。
- en: Implementing programmatic ELT with Snowpark
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Snowpark实现程序化ELT
- en: 'Snowflake supports and recommends a modern ELT implementation pattern for data
    engineering instead of the legacy ETL process. ETL is a pattern where data is
    extracted from various sources, transformed in the data pipeline, and then the
    transformed data is loaded into a destination such as a data warehouse or data
    mart. The following diagram shows the comparison of ETL versus ELT:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake支持和推荐使用现代ELT实现模式进行数据工程，而不是传统的ETL过程。ETL是一种模式，其中数据从各种来源提取，在数据管道中进行转换，然后将转换后的数据加载到目的地，如数据仓库或数据集市。以下图表显示了ETL与ELT的比较：
- en: '![Figure 4.4 – ETL versus ELT](img/B19923_04_04.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – ETL与ELT的比较](img/B19923_04_04.jpg)'
- en: Figure 4.4 – ETL versus ELT
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – ETL与ELT的比较
- en: 'ELT is a pattern that is more suited for the Snowflake Data Cloud, where data
    is extracted from the source and loaded into Snowflake. This data is then transformed
    within Snowflake using Snowpark. Snowpark pipelines are designed to extract and
    load the data first and then transform it in the destination as the transformation
    is done inside Snowflake, which provides better scalability and elasticity. The
    ELT also improves performance and reduces the time it takes to ingest, transform,
    and analyze the data within Snowflake using Snowpark. The following diagram shows
    the different layers of data within Snowflake:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ELT是一种更适合Snowflake数据云的模式，其中数据从源提取并加载到Snowflake中。然后使用Snowpark在Snowflake内部对数据进行转换。Snowpark管道被设计为首先提取和加载数据，然后在目的地进行转换，因为转换是在Snowflake内部完成的，这提供了更好的可扩展性和弹性。ELT还提高了性能，并减少了使用Snowpark在Snowflake中摄取、转换和分析数据所需的时间。以下图表显示了Snowflake内部数据的不同层：
- en: '![Figure 4.5 – Data stages in Snowflake](img/B19923_04_05.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – Snowflake中的数据阶段](img/B19923_04_05.jpg)'
- en: Figure 4.5 – Data stages in Snowflake
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – Snowflake中的数据阶段
- en: The data pipelines build the different data stages inside Snowflake. These stages
    are databases and schemas with objects such as tables and views inside them. The
    raw data is ingested from the source systems into Snowflake with no transformations
    since Snowflake supports multiple data formats. This data is then transformed
    using Snowpark into the conformed stage containing the de-duped and standardized
    data. This becomes the data that feeds into the next step in the data pipeline.
    The reference stage has the business definition and data mappings with the hierarchies
    and the master data. The final stage has the modeled data, which has the clean
    and transformed data. Snowpark has many functions that help with doing value-added
    transformations that help convert data into a business-ready format accessible
    to users and applications, making it more valuable for the organization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道在Snowflake中构建不同的数据阶段。这些阶段包含数据库和模式，其中包含诸如表和视图等对象。原始数据从源系统直接导入Snowflake，没有进行转换，因为Snowflake支持多种数据格式。然后，使用Snowpark对这些数据进行转换，形成包含去重和标准化数据的规范阶段。这些数据成为数据管道下一步的输入。参考阶段包含业务定义和数据映射，以及层次结构和主数据。最终阶段包含建模数据，它包含清洁和转换后的数据。Snowpark拥有许多功能，有助于进行增值转换，帮助将数据转换为用户和应用可访问的、业务就绪的格式，从而为组织增加价值。
- en: ETL versus ELT in Snowpark
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Snowpark中的ETL与ELT
- en: Snowpark supports both ETL and ELT workloads. While ELT is famous for modern
    pipelines, the ETL pattern is also used in some scenarios. ETL is commonly used
    with structured data where the total volume of data is small. It is also used
    in the system for migrating legacy databases to the Data Cloud where the source
    and target data types differ. ELT provides a significant advantage compared to
    the traditional ETL process. ELT supports large volumes of structured, unstructured,
    and semi-structured data that can be processed using Snowflake. It also allows
    developers and analysts to experiment with data as it is loaded into Snowflake.
    ELT also maximizes the option for them to transform data to get potential insights.
    It also supports low latency and real-time analytics. ELT is better suited for
    Snowflake than the traditional ETL for these reasons. The following section will
    cover how to develop efficient DataOps in Snowpark.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark支持ETL和ELT工作负载。虽然ELT因现代管道而闻名，但在某些场景中也会使用ETL模式。ETL通常用于结构化数据，其中数据总量较小。它还用于将传统数据库迁移到数据云的系统，其中源数据和目标数据类型不同。与传统的ETL过程相比，ELT提供了显著的优势。ELT支持大量结构化、非结构化和半结构化数据，可以使用Snowflake进行处理。它还允许开发人员和分析师在数据加载到Snowflake时进行数据实验。ELT还最大限度地提高了他们转换数据以获取潜在见解的选项。它还支持低延迟和实时分析。由于这些原因，ELT比传统的ETL更适合Snowflake。以下部分将介绍如何在Snowpark中开发高效的数据操作。
- en: Deploying efficient DataOps in Snowpark
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Snowpark中部署高效的数据操作
- en: 'DataOps helps data teams reduce development times, increase data quality, and
    maximize the business value of data by bringing more rigor to the development
    and management of data pipelines. It also ensures that the data is clean, accurate,
    and up-to-date in a streamlined environment with data governance. Data engineering
    introduces the processes and capabilities required to effectively develop, manage,
    and deploy data engineering pipelines. The following diagram highlights the DataOps
    approach:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为数据管道的开发和管理带来更多严谨性，数据操作帮助数据团队减少开发时间，提高数据质量，并最大化数据的商业价值。它还确保数据在数据治理的简化环境中保持清洁、准确和最新。数据工程引入了有效开发、管理和部署数据工程管道所需的过程和能力。以下图表突出了数据操作方法：
- en: '![Figure 4.6 – DataOps process](img/B19923_04_06.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 数据操作流程](img/B19923_04_06.jpg)'
- en: Figure 4.6 – DataOps process
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 数据操作流程
- en: The DataOps process focuses on bringing agile development to data engineering
    pipelines using an iterative development, testing, and deployment process in loops.
    It also includes **continuous integration and continuous deployment** (**CI/CD**)
    for data, schema changes, and the data versioning and automation of data models
    and artifacts. This section will show an example of a data engineering pipeline
    executed in Snowpark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作流程侧重于通过迭代开发、测试和部署流程在循环中引入敏捷开发到数据工程管道。它还包括数据的**持续集成和持续部署**（**CI/CD**）、模式更改、数据版本以及数据模型和工件自动化的数据版本。本节将展示在Snowpark中执行的数据工程管道的示例。
- en: Developing a data engineering pipeline
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发数据工程管道
- en: 'Creating a resilient data engineering pipeline within the Snowpark framework
    requires integrating three core components seamlessly:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Snowpark框架内创建一个有弹性的数据工程管道需要无缝集成三个核心组件：
- en: First and foremost, data engineers must master the art of loading data into
    Snowflake, setting the stage for subsequent processing. This initial step sets
    the foundation upon which the entire pipeline is built.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先且最重要的是，数据工程师必须掌握将数据加载到Snowflake中的艺术，为后续处理奠定基础。这一初始步骤为整个管道的建设奠定了基础。
- en: Second, the transformative power of Snowpark data functions comes into play,
    enabling engineers to shape and mold the data to meet specific analytical needs.
    [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042)*,* *Simplifying Data Processing
    Using Snowpark* provided a detailed exploration of DataFrame operations, laying
    the groundwork for this pivotal transformation phase.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，Snowpark数据函数的变革力量开始发挥作用，使工程师能够塑造和塑造数据以满足特定的分析需求。[*第三章*](B19923_03.xhtml#_idTextAnchor042)*使用Snowpark简化数据处理*提供了对DataFrame操作的详细探索，为这一关键转换阶段奠定了基础。
- en: Finally, the data journey culminates in bundling these operations as Snowpark
    stored procedures, offering efficiency and repeatability in handling data.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，数据之旅以将这些操作捆绑为Snowpark存储过程而告终，这提供了处理数据的效率和可重复性。
- en: As we delve into this section, building upon the knowledge garnered from [*Chapter
    2*](B19923_02.xhtml#_idTextAnchor028)*, Establishing a Foundation with Snowpark*
    and [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042), *Simplifying Data Processing
    Using Snowpark* where we elaborated on DataFrame operations and their conversion
    into **user-defined functions** (**UDFs**) and stored procedures, we will unravel
    the intricate process of unifying these elements into a resilient data engineering
    pipeline. This chapter is a testament to the synthesis of theory and practice,
    empowering data professionals to seamlessly interconnect the loading, transformation,
    and bundling phases, resulting in a robust framework for data processing and analysis
    within the Snowpark ecosystem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入本节之前，我们将基于从[*第二章*](B19923_02.xhtml#_idTextAnchor028)*建立Snowpark基础*和[*第三章*](B19923_03.xhtml#_idTextAnchor042)*使用Snowpark简化数据处理*所获得的知识，这两章中我们详细阐述了DataFrame操作及其转换为**用户定义函数**（**UDFs**）和存储过程的转换，我们将揭示将这些元素统一到一个有弹性的数据工程管道中的复杂过程。本章是对理论与实践结合的证明，赋予数据专业人士无缝连接加载、转换和捆绑阶段的能力，从而在Snowpark生态系统中构建一个强大的数据处理和分析框架。
- en: 'With a comprehensive understanding of data loading from our discussions in
    the previous chapters, our journey now pivots toward strategically utilizing this
    data. This pivotal transition places our emphasis on three core steps:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中我们对数据加载有了全面的理解之后，我们的旅程现在转向战略性地利用这些数据。这一关键过渡将我们的重点放在三个核心步骤上：
- en: Data preparation
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data transformation
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Data cleanup
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据清理
- en: These stages constitute the cornerstone of our data engineering voyage, where
    we will sculpt, consolidate, and refine our data, revealing its true potential
    for analysis and valuable insights. We’ll now transform these concepts into practical
    data engineering pipelines, leveraging the valuable insights from our prior discussions
    on stored procedure templates and transformation steps. Our focus will center
    on our marketing campaign data, where the foundational loading steps have been
    thoughtfully outlined in [*Chapter 3*](B19923_03.xhtml#_idTextAnchor042), *Simplifying
    Data Processing Using Snowpark* providing a solid starting point for our data
    preparation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阶段构成了我们数据工程之旅的基石，我们将塑造、整合和精炼我们的数据，揭示其分析潜力和宝贵见解。现在，我们将将这些概念转化为实际的数据工程管道，利用我们之前关于存储过程模板和转换步骤的讨论中的宝贵见解。我们的重点将集中在我们的营销活动数据上，其中在[*第三章*](B19923_03.xhtml#_idTextAnchor042)*使用Snowpark简化数据处理*中已经仔细概述了基础加载步骤，为我们的数据准备提供了一个坚实的起点。
- en: Data preparation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'In the progression of our data engineering pipeline, the subsequent imperative
    phase is data preparation, which involves the integration of diverse tables. In
    this section, we will explore techniques for merging these disparate data tables
    using various functions tailored to the task. Additionally, we will elucidate
    the process of registering these functions as stored procedures, ensuring a streamlined
    and efficient data workflow. The first step is joining the purchase history with
    the campaign information. Both tables are entered using the `ID` column, and a
    single ID is retained:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据工程管道进展过程中，下一关键阶段是数据准备，涉及不同表的集成。在本节中，我们将探讨使用针对任务定制的各种函数合并这些不同的数据表的技术。此外，我们将阐明将这些函数注册为存储过程的过程，以确保数据工作流程的流畅和高效。第一步是将购买历史与活动信息连接起来。两个表都使用`ID`列输入，并保留单个ID：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resultant `purchase_campaign` DataFrame holds the data and is used in the
    next step. In the next step, we join the purchase campaign with the complaint
    information using the same `ID` column and then create a `purchase_campaign_complain`
    DataFrame:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`purchase_campaign` DataFrame包含数据，并在下一步中使用。在下一步中，我们将使用相同的`ID`列将购买活动与投诉信息连接起来，然后创建一个`purchase_campaign_complain`
    DataFrame：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code joins the column to create a `purchase_campaign_complain`
    DataFrame, which contains the mapped purchase data with complaint information.
    In the final step, a marketing table is created by the union of the data between
    the purchase complaint and the marketing table:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将列连接起来创建一个`purchase_campaign_complain` DataFrame，其中包含映射的购买数据和投诉信息。在最终步骤中，通过购买投诉和营销表之间的数据合并创建一个营销表：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code produces a table that contains all the combined data that
    is the final result of the pipeline, which will be written as a table. The Python
    functions representing each step are executed as part of the Snowpark stored procedure.
    The stored procedures can be performed in sequence one after the other and also
    scheduled as Snowflake tasks. The data preparation procedure calls the three Python
    methods, and the final table is written to Snowflake:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成一个包含所有合并数据的表，这是管道的最终结果，将被写入为表。代表每个步骤的Python函数作为Snowpark存储过程的一部分执行。存储过程可以依次执行，也可以作为Snowflake任务进行调度。数据准备过程调用三个Python方法，并将最终表写入Snowflake：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code does the data preparation by loading the required data into
    the DataFrame. We will now call each of the steps to execute it like a pipeline:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码通过将所需数据加载到DataFrame中来进行数据准备。我们现在将调用每个步骤来执行它，就像一个管道：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The three previously defined step functions are executed. The resultant data
    is loaded into the new `final_marketing_data` DataFrame, which will then be loaded
    to the Snowflake table:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 执行了之前定义的三个步骤函数。结果数据被加载到新的`final_marketing_data` DataFrame中，然后将被加载到Snowflake表中：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will create and execute a stored procedure that contains the preceding
    logic. The procedure is called `data_prep_sproc` and is the first part of the
    data engineering pipeline – data preparation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建并执行一个包含上述逻辑的存储过程。该过程称为`data_prep_sproc`，是数据工程管道的第一部分——数据准备：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding stored procedure writes the data into the `Final_Marketing_Data`
    table, which will be used in the next step of data transformation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述存储过程将数据写入`Final_Marketing_Data`表，该表将在数据转换的下一步中使用。
- en: Data transformation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'The subsequent phase in this process involves data transformation, building
    upon the data prepared in the previous step. Here, we’ll take the pivotal action
    of registering another stored procedure after the last stage. This procedure applies
    transformation logic, molding the data into a form primed for analysis. Leveraging
    Snowpark’s array of valuable aggregation and summarization functions, we will
    harness these capabilities to shape and enhance our data, laying a solid foundation
    for rigorous analysis. The following code transforms the data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程中的下一阶段涉及数据转换，基于之前步骤准备的数据。在这里，我们将在最后阶段之后注册另一个存储过程。此过程应用转换逻辑，将数据塑造成适合分析的形式。利用Snowpark的一系列有价值的聚合和汇总函数，我们将利用这些功能来塑造和增强我们的数据，为严格的分析奠定坚实的基础。以下代码转换数据：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A `data_transform_sproc` stored procedure is created, which reads the `Final_Marketing_Data`
    table and creates a pivot with the education of the customer and the total income.
    When the stored procedure is executed, this is then written to the `Marketing_Pivot`
    table.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个 `data_transform_sproc` 存储过程，该过程读取 `Final_Marketing_Data` 表并创建一个包含客户教育和总收入的数据透视表。当存储过程执行时，这些数据随后被写入到
    `Marketing_Pivot` 表中。
- en: Data cleanup
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据清理
- en: 'In the final step of our data engineering process, we focus on a crucial task:
    cleaning up the data in the `Marketing_Pivot` table. Similar to artists perfecting
    a masterpiece, we carefully go through our data, removing any empty values in
    tables that aren’t important for our analysis. To do this, we rely on the versatile
    `dropna()` function, which acts like a precise tool to cut away unnecessary data:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们数据工程流程的最后一步，我们专注于一个关键任务：清理 `Marketing_Pivot` 表中的数据。类似于艺术家完善杰作一样，我们仔细检查我们的数据，移除对分析不重要的表格中的任何空值。为此，我们依赖于多功能的
    `dropna()` 函数，它就像一把精确的工具，可以切除不必要的数据：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The cleaned-up data from the `market_drop_null` DataFrame is then saved into
    the `Market_Pivot_Cleaned` table. This data is at the last stage of the pipeline
    and is used for analysis.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `market_drop_null` DataFrame 清理后的数据随后被保存到 `Market_Pivot_Cleaned` 表中。这些数据是管道的最后阶段，用于分析。
- en: Orchestrating the pipeline
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编排管道
- en: 'The data pipeline is orchestrated by calling three Snowpark procedures, which
    invokes the three different steps of the data engineering pipeline. The procedures
    are executed in the order of `data_prep_sproc`, `data_transform_sproc`, and `data_cleanup_sproc`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用三个 Snowpark 过程来编排数据管道，这些过程调用了数据工程管道的三个不同步骤。这些过程按照 `data_prep_sproc`、`data_transform_sproc`
    和 `data_cleanup_sproc` 的顺序执行：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The Snowpark procedure is executed, and after each step is executed, the final
    data is written to the `Market_Pivot_Cleaned` table. Snowflake supports scheduling
    and orchestration through tasks. Tasks can be scheduled using the Python API and
    through worksheets, and they can trigger procedures in sequence:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 过程被执行，并且每执行一步后，最终数据都会写入到 `Market_Pivot_Cleaned` 表中。Snowflake 通过任务支持调度和编排。任务可以通过
    Python API 和工作表进行调度，并且可以按顺序触发过程：
- en: '![Figure 4.7 – Stored procedure execution](img/B19923_04_07.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 存储过程执行](img/B19923_04_07.jpg)'
- en: Figure 4.7 – Stored procedure execution
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 存储过程执行
- en: In the following section, we will explore how we can utilize Snowflake tasks
    and task graphs to execute the preceding pipeline.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何利用 Snowflake 任务和任务图来执行前面的管道。
- en: Overview of tasks in Snowflake
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Snowflake 中任务概述
- en: Tasks in Snowflake are powerful tools designed to streamline data processing
    workflows and automate various tasks within the Snowflake environment. Offering
    a range of functionalities, tasks execute different types of SQL code, enabling
    users to perform diverse operations on their data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 中的任务是有力的工具，旨在简化数据处理工作流程并在 Snowflake 环境中自动化各种任务。提供一系列功能，任务执行不同类型的 SQL
    代码，使用户能够对其数据进行各种操作。
- en: 'Tasks in Snowflake can execute three main types of SQL code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 中的任务可以执行三种主要的 SQL 代码类型：
- en: '**Single SQL statement**: Allows the execution of a single SQL statement'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单个 SQL 语句**：允许执行单个 SQL 语句'
- en: '**Call to a stored procedure**: Enables the invocation of a stored procedure'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调用存储过程**：允许调用存储过程'
- en: '**Procedural logic using Snowflake Scripting**: Supports the implementation
    of procedural logic using Snowflake Scripting'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Snowflake 脚本实现的程序逻辑**：支持使用 Snowflake 脚本实现程序逻辑'
- en: Tasks can be integrated with table streams to create continuous ELT workflows.
    By processing recently changed table rows, tasks ensure the maintenance of data
    integrity and provide exactly-once semantics for new or altered data. Tasks in
    Snowflake can be scheduled to run at specified intervals. Snowflake ensures that
    only one instance of a scheduled task is executed at a time, skipping scheduled
    executions if a task is still running.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 任务可以与表流集成，以创建连续的 ELT 工作流。通过处理最近更改的表行，任务确保数据完整性的维护，并为新数据或更改的数据提供一次且仅一次的语义。Snowflake
    中的任务可以安排在指定的时间间隔运行。Snowflake 确保同一时间只执行一个计划任务的实例，如果任务仍在运行，则跳过计划执行。
- en: Compute models for tasks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为任务计算模型
- en: In the serverless compute model, tasks rely on compute resources managed by
    Snowflake. These resources are automatically resized and scaled based on workload
    demands, ensuring optimal performance and resource utilization. Snowflake dynamically
    determines the appropriate compute size for each task run based on historical
    statistics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器计算模型中，任务依赖于Snowflake管理的计算资源。这些资源会根据工作负载需求自动调整大小和扩展，确保最佳性能和资源利用率。Snowflake会根据历史统计数据动态确定每个任务运行所需的适当计算大小。
- en: Alternatively, users can opt for the user-managed virtual warehouse model, where
    they specify an existing virtual warehouse for individual tasks. This model provides
    users with more control over compute resource management but requires careful
    sizing to ensure efficient task execution.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，用户可以选择用户管理的虚拟仓库模型，为单个任务指定一个现有的虚拟仓库。此模型使用户能够更好地控制计算资源管理，但需要仔细规划以确保任务执行效率。
- en: Task graphs
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务图
- en: Task graphs, also known as **directed acyclic graphs** (**DAGs**), allow for
    the organization of tasks based on dependencies. Each task within a task graph
    has predecessor and subsequent tasks, facilitating complex workflow management.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 任务图，也称为**有向无环图**（**DAGs**），允许根据依赖关系组织任务。任务图中的每个任务都有前驱任务和后续任务，便于复杂工作流程管理。
- en: Task graphs are subject to certain limitations, including a maximum of 1,000
    tasks in total, including the root task. Individual tasks within a task graph
    can have a maximum of 100 predecessors and 100 child tasks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 任务图受到某些限制，包括总任务数最多为1,000个，包括根任务。任务图中的单个任务可以有最多100个前驱任务和100个子任务。
- en: Users can view and monitor their task graphs using SQL or Snowsight, Snowflake’s
    integrated development environment, providing visibility into task dependencies
    and execution status.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用SQL或Snowsight（Snowflake的集成开发环境）查看和监控他们的任务图，从而了解任务依赖关系和执行状态。
- en: In summary, tasks in Snowflake offer robust capabilities for data processing,
    automation, and workflow management, making them indispensable tools for users
    seeking to optimize their data operations within the Snowflake ecosystem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Snowflake中的任务提供了强大的数据处理、自动化和工作流程管理能力，对于寻求在Snowflake生态系统中优化数据操作的用户来说，它们是不可或缺的工具。
- en: Managing tasks and task graphs with Python
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python管理任务和任务图
- en: Our primary focus is on Snowpark. Now, we’ll explore how we can utilize Python
    Snowpark to programmatically perform task graph operations instead of using SQL
    statements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要关注点是Snowpark。现在，我们将探讨如何使用Python Snowpark以编程方式执行任务图操作，而不是使用SQL语句。
- en: 'Now, Python can manage Snowflake tasks, allowing users to run SQL statements,
    procedure calls, and Snowflake Scripting logic. The Snowflake Python API introduces
    two types:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Python可以管理Snowflake任务，使用户能够运行SQL语句、过程调用和Snowflake脚本逻辑。Snowflake Python API引入了两种类型：
- en: '**Task**: This type represents a task’s properties, such as its schedule, parameters,
    and dependencies'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Task**：此类型表示任务的属性，如其调度、参数和依赖关系'
- en: '**TaskResource**: This type provides methods to interact with **Task** objects,
    enabling task execution and modification'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TaskResource**：此类型提供与**Task**对象交互的方法，使任务执行和修改成为可能。'
- en: Tasks can be grouped into task graphs, which consist of interconnected tasks
    arranged based on their dependencies. To create a task graph, users first define
    a DAG object, specifying its name and optional properties, such as its schedule.
    The scheduling of a task graph can be customized using either a `timedelta` value
    or a cron expression, allowing for flexible task execution timing and recurrence
    patterns.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 任务可以被分组到任务图中，这些图基于任务依赖关系相互连接。要创建任务图，用户首先定义一个DAG对象，指定其名称和可选属性，如其调度。任务图的调度可以使用`timedelta`值或cron表达式进行自定义，从而实现灵活的任务执行时间和重复模式。
- en: 'Let’s begin by setting up the necessary functions to implement our DAG. The
    examples provided in this section presuppose that you’ve already written code
    to establish a connection with Snowflake to utilize the Snowflake Python API:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置必要的函数来实现我们的DAG。本节中提供的示例假设您已经编写了代码来与Snowflake建立连接，以利用Snowflake Python API：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code initializes the Snowflake Python API, creating a `root` object
    for utilizing its types and methods. Additionally, it sets up a `timedelta` value
    of 1 hour for the task’s schedule. You can define the schedule using either a
    `timedelta` value or a Cron expression. For one-off runs, you can omit the schedule
    argument to the DAG object without worrying about it running unnecessarily in
    the background.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码初始化 Snowflake Python API，创建一个 `root` 对象以利用其类型和方法。此外，它为任务计划设置了一个 1 小时的 `timedelta`
    值。你可以使用 `timedelta` 值或 Cron 表达式来定义计划。对于一次性运行，你可以省略 DAG 对象的计划参数，无需担心它在不必要的情况下在后台运行。
- en: 'Let’s define a simple DAG that we’ll use to execute our pipelines:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个简单的 DAG，我们将使用它来执行我们的管道：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this DAG setup, the following applies:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在此 DAG 设置中，以下适用：
- en: We’ve named our DAG **Task_Demo**, which by default runs on the specified warehouse.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将我们的 DAG 命名为 **Task_Demo**，它默认在指定的仓库上运行。
- en: A schedule for daily execution has been defined using **timedelta**.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **timedelta** 定义了一个每日执行的调度计划。
- en: A **stage_location** attribute is necessary for storing the serialized version
    of the tasks via the Python API.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储任务的序列化版本需要 **stage_location** 属性通过 Python API 进行。
- en: All tasks under this DAG will run with the default list of packages and the
    specified warehouse. However, both the warehouse and packages for individual tasks
    within the DAG can be overridden with different values.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该 DAG 下的所有任务都将使用默认的包列表和指定的仓库运行。然而，DAG 内部各个任务的仓库和包可以使用不同的值进行覆盖。
- en: In addition, the **use_func_return_value** attribute indicates that the return
    value of Python functions will be treated as the return value of the task, though
    in our case, we’re not utilizing the **return** object.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，**use_func_return_value** 属性表示 Python 函数的返回值将被视为任务的返回值，尽管在我们的案例中，我们没有使用 **return**
    对象。
- en: 'We’ve now defined a series of Python functions representing a three-task pipeline,
    or DAG. However, we haven’t yet created and pushed the DAG to Snowflake. Let’s
    do that now using the Snowflake Python API:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了一系列代表三个任务管道或 DAG 的 Python 函数。然而，我们尚未创建并将 DAG 推送到 Snowflake。现在让我们使用 Snowflake
    Python API 来完成这项工作：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For one-off testing or running of the DAG, users can skip specifying the schedule
    and manually trigger a run with `dag_op.run(dag)`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一次性测试或运行 DAG，用户可以省略指定计划，并使用 `dag_op.run(dag)` 手动触发运行：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The provided code performs several actions using Snowflake’s Snowpark library.
    Firstly, it retrieves the schema named `MY_SCHEMA` from the `SNOWPARK_DEFINITIVE_GUIDE`
    database using the `root` object. Then, it initializes a `DAGOperation` object
    named `dag_op`, specifying the schema where the DAG will be deployed. The `deploy()`
    method is then called on `dag_op` to deploy the specified DAG (named `dag`) in
    the specified schema.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的代码使用 Snowflake 的 Snowpark 库执行多个操作。首先，它使用 `root` 对象从 `SNOWPARK_DEFINITIVE_GUIDE`
    数据库检索名为 `MY_SCHEMA` 的模式。然后，它初始化一个名为 `dag_op` 的 `DAGOperation` 对象，指定 DAG 将部署到的模式。然后，在
    `dag_op` 上调用 `deploy()` 方法来部署指定模式中指定的 DAG（名为 `dag`）。
- en: 'The `orReplace` mode argument indicates that if a DAG with the same name already
    exists in the schema, it will be replaced. Finally, the `run()` method is called
    on `dag_op` to execute the deployed DAG. This code essentially sets up and executes
    a DAG within Snowflake using Snowpark. Now, you can check the graph in Snowsight
    to see how the graph has been set up:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`orReplace` 模式参数表示如果模式中已存在具有相同名称的 DAG，它将被替换。最后，在 `dag_op` 上调用 `run()` 方法来执行已部署的
    DAG。此代码实际上是在 Snowflake 中使用 Snowpark 设置和执行 DAG。现在，你可以在 Snowsight 中检查图以查看图是如何设置的：'
- en: '![Figure 4.8 – Snowsight graph](img/B19923_04_08.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – Snowsight 图](img/B19923_04_08.jpg)'
- en: Figure 4.8 – Snowsight graph
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – Snowsight 图
- en: Additionally, note that you can pass the raw function to the dependency definition
    without explicitly creating a `DAGTask` instance, with the library automatically
    creating a task for you with the same name. However, there are exceptions to this
    rule, such as needing to explicitly create a `DAGTask` instance for the first
    task or when utilizing `DAGTaskBranch` or repeating certain functions in multiple
    tasks.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，你可以将原始函数传递给依赖定义，而无需显式创建 `DAGTask` 实例，库会自动为你创建一个具有相同名称的任务。然而，有一些例外，例如需要显式创建
    `DAGTask` 实例的第一个任务或在使用 `DAGTaskBranch` 或在多个任务中重复某些函数时。
- en: 'Once you’ve deployed and run the DAG, you can easily check its status using
    the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署并运行了 DAG，你可以使用以下代码轻松检查其状态：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As depicted in the screenshot, our tasks are scheduled to run daily. Additionally,
    we have executed them once to verify the task deployment beforehand:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如截图所示，我们的任务被安排为每天运行。此外，我们已经执行了一次以验证任务部署：
- en: '![Figure 4.9 – Tasks deployed](img/B19923_04_09.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 部署的任务](img/B19923_04_09.jpg)'
- en: Figure 4.9 – Tasks deployed
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 部署的任务
- en: As we progress in constructing more complex pipelines, managing and debugging
    become increasingly challenging. It’s crucial to establish a robust logging mechanism
    to facilitate maintenance and streamline error resolution. In the next section,
    we’ll delve into implementing logging and traceback functionalities in Snowpark
    to enhance our pipeline’s manageability and ease troubleshooting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们构建更复杂的管道，管理和调试变得越来越具有挑战性。建立强大的日志机制对于维护和简化错误解决至关重要。在下一节中，我们将深入了解在 Snowpark
    中实现日志和回溯功能，以增强我们管道的可管理性和简化故障排除。
- en: Implementing logging and tracing in Snowpark
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Snowpark 中实现日志和跟踪
- en: 'Logging and tracing are crucial for DataOps and are necessary to monitor and
    fix failures in the data engineering pipeline. Snowpark comes with logging and
    tracing functionality that is built in, which can help record the activity of
    Snowpark functions and procedures and capture those in an easy-to-access central
    table inside Snowflake. Log messages are independent, detailed messages with information
    in the form of strings, providing details about the piece of code, and trace events
    are structured data that we can use to get information spanning and grouping multiple
    parts of our code. Once logs are collected, they can be easily queried by SQL
    or accessed via Snowpark. The following diagram highlights the event table and
    alerting:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 日志和跟踪对于 DataOps 至关重要，并且对于监控和修复数据工程管道中的故障是必要的。Snowpark 内置了日志和跟踪功能，可以帮助记录 Snowpark
    函数和过程的活动，并在 Snowflake 内部的易于访问的中心表中捕获这些信息。日志消息是独立、详细的字符串形式的信息消息，提供了关于代码片段的详细信息，而跟踪事件是有结构的我们可以用来获取跨越和分组我们代码多个部分的信息。一旦收集了日志，就可以通过
    SQL 或通过 Snowpark 容易地查询或访问。以下图表突出了事件表和警报：
- en: '![Figure 4.10 – Event table](img/B19923_04_10.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 事件表](img/B19923_04_10.jpg)'
- en: Figure 4.10 – Event table
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 事件表
- en: Snowpark stores logs and trace messages inside the event table, a unique table
    with a predefined set of columns. Logs and traces are captured in this table as
    the code is executed. Let’s look at the structure of event tables and how to create
    them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 在事件表中存储日志和跟踪消息，这是一个具有预定义列集的唯一表。日志和跟踪在代码执行时捕获在这个表中。让我们看看事件表的结构以及如何创建它们。
- en: Event tables
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件表
- en: 'An event table is native to Snowflake and needs to be created. There can be
    only one event table for a Snowflake account that captures all the information,
    but multiple views can be made for analysis. An event table contains the following
    columns:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 事件表是 Snowflake 的本地表，需要创建。一个 Snowflake 账户只能有一个事件表来捕获所有信息，但可以创建多个视图进行分析。事件表包含以下列：
- en: '| **Column** | **Data Type** | **Description** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **列** | **数据类型** | **描述** |'
- en: '| `TIMESTAMP` | `TIMESTAMP_NTZ` | The UTC timestamp when an event was created.
    This is the end of the period for events representing a period. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `TIMESTAMP` | `TIMESTAMP_NTZ` | 事件创建时的 UTC 时间戳。这是表示一段时间的事件的结束。|'
- en: '| `START_TIMESTAMP` | `TIMESTAMP_NTZ` | For events representing a period, such
    as trace events as the start of the period as a UTC timestamp. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `START_TIMESTAMP` | `TIMESTAMP_NTZ` | 对于表示一段时间的事件，例如作为该时间段开始的时间戳的跟踪事件。|'
- en: '| `OBSERVED_TIMESTAMP` | `TIMESTAMP_NTZ` | A UTC timestamp is used for logs.
    Currently, it has the same value as `TIMESTAMP`. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `OBSERVED_TIMESTAMP` | `TIMESTAMP_NTZ` | 用于日志的 UTC 时间戳。目前，它具有与 `TIMESTAMP`
    相同的值。|'
- en: '| `TRACE` | `OBJECT` | Tracing context for all signal types. Contains `trace_id`
    and `span_id` string values. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `TRACE` | `OBJECT` | 所有信号类型的跟踪上下文。包含 `trace_id` 和 `span_id` 字符串值。|'
- en: '| `RESOURCE` | `OBJECT` | Reserved for future use. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `RESOURCE` | `OBJECT` | 保留供将来使用。|'
- en: '| `RESOURCE_ATTRIBUTES` | `OBJECT` | Attributes that identify the source of
    an event, such as database, schema, user, warehouse, and so on. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `RESOURCE_ATTRIBUTES` | `OBJECT` | 识别事件来源的属性，例如数据库、模式、用户、仓库等。|'
- en: '| `SCOPE` | `OBJECT` | Scopes for events; for example, class names for logs.
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `SCOPE` | `OBJECT` | 事件的作用域；例如，日志的类名。|'
- en: '| `SCOPE_ATTRIBUTES` | `OBJECT` | Reserved for future use. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `SCOPE_ATTRIBUTES` | `OBJECT` | 保留供将来使用。|'
- en: '| `RECORD_TYPE` | `STRING` | The event types. One of the following:`LOG` for
    a log message.`SPAN` for UDF invocations performed sequentially on the same thread.
    `SPAN_EVENT` for a single trace event. A single query can emit more than one `SPAN_EVENT`
    event type. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `RECORD_TYPE` | `STRING` | 事件类型。以下之一：`LOG` 表示日志消息。`SPAN` 表示在同一线程上顺序执行的 UDF
    调用。`SPAN_EVENT` 表示单个跟踪事件。单个查询可以发出多个 `SPAN_EVENT` 事件类型。 |'
- en: '| `RECORD` | `OBJECT` | Fixed values for each record type. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `RECORD` | `OBJECT` | 每种记录类型的固定值。 |'
- en: '| `RECORD_ATTRIBUTES` | `OBJECT` | Variable attributes for each record type.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `RECORD_ATTRIBUTES` | `OBJECT` | 每种记录类型的可变属性。 |'
- en: '| `VALUE` | `VARIANT` | Primary event value. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `VALUE` | `VARIANT` | 主要事件值。 |'
- en: '| `EXEMPLARS` | `ARRAY` | Reserved for future use. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `EXEMPLARS` | `ARRAY` | 保留供将来使用。 |'
- en: Table 4.1 – Event table columns
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 – 事件表列
- en: 'Each column can be queried or combined to analyze different outcomes based
    on the logs and the traces. The log type describes the log levels assigned as
    part of the logging and can be set at both objects and the session. The log levels
    can be the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个列都可以被查询或组合，根据日志和跟踪分析不同的结果。日志类型描述了作为日志一部分分配的日志级别，可以在对象和会话中设置。日志级别可以是以下之一：
- en: '| **LOG_LEVEL** **Parameter Setting** | **Levels of** **Messages Ingested**
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **LOG_LEVEL** **参数设置** | **消息摄入级别** |'
- en: '| `TRACE` | `TRACE``DEBUG``INFO``WARN``ERROR``FATAL` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `TRACE` | `TRACE` `DEBUG` `INFO` `WARN` `ERROR` `FATAL` |'
- en: '| `DEBUG` | `DEBUG``INFO``WARN``ERROR``FATAL` |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `DEBUG` | `DEBUG` `INFO` `WARN` `ERROR` `FATAL` |'
- en: '| `INFO` | `INFO``WARN``ERROR``FATAL` |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `INFO` | `INFO` `WARN` `ERROR` `FATAL` |'
- en: '| `WARN` | `WARN``ERROR``FATAL` |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `WARN` | `WARN` `ERROR` `FATAL` |'
- en: '| `ERROR` | `ERROR``FATAL` |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `ERROR` | `ERROR` `FATAL` |'
- en: '| `FATAL` | `ERROR``FATAL` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `FATAL` | `ERROR` `FATAL` |'
- en: Table 4.2 – Event table log levels
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 – 事件表日志级别
- en: The log level is a hierarchy applied in the order presented in *Table 4.2*.
    In the next section, we will look at creating an event table.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 日志级别是按照 *表 4.2* 中呈现的顺序应用的层次结构。在下一节中，我们将探讨创建事件表。
- en: Note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: It is best practice to set the necessary log level based on a minor level such
    as **FATAL** and ERROR so that the number of logged messages is fewer. In the
    case of logging **INFO**, it is usually turned on to capture the log and turned
    off in production to avoid catching too many records.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是根据次要级别（如 **FATAL** 和 ERROR）设置必要的日志级别，以便日志消息的数量更少。在记录 **INFO** 的情况下，通常在生产中开启以捕获日志，关闭以避免捕获过多的记录。
- en: Creating and configuring an event table
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建和配置事件表
- en: The first step is to create an event table for the Snowflake account. The name
    of the event table can be specified, and columns for the event table are not required
    to be set when creating the table, as Snowflake automatically creates it with
    the standard columns. The event table is assigned to the account and needs to
    be made in a separate database that does not have Snowflake replication enabled
    using the `ACCOUNTADMIN` role.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是为 Snowflake 账户创建一个事件表。事件表的名字可以指定，创建表时不需要设置事件表的列，因为 Snowflake 会自动使用标准列创建它。事件表分配给账户，并且需要在一个没有启用
    Snowflake 复制的单独数据库中使用 `ACCOUNTADMIN` 角色创建。
- en: 'To create an event table, run the following command with the `ACCOUNTADMIN`
    role using Snowpark:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建事件表，请使用 Snowpark 以 `ACCOUNTADMIN` 角色运行以下命令：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'An event table called `MY_EVENTS` is created with the default column structure.
    The next step is to assign the event table as the active event table to a particular
    Snowflake account. The event table can be assigned to an account by executing
    the following code with the `ACCOUNTADMIN` role:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个名为 `MY_EVENTS` 的事件表，具有默认的列结构。下一步是将事件表分配为特定 Snowflake 账户的活动事件表。可以通过使用 `ACCOUNTADMIN`
    角色执行以下代码将事件表分配给账户：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The parameter is applied at the account level, and all events from the particular
    Snowflake account are captured in this event table. This completes the event table
    setup.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数在账户级别应用，特定 Snowflake 账户的所有事件都被捕获在这个事件表中。这完成了事件表的设置。
- en: Querying event tables
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询事件表
- en: 'An event table can be accessed just like any other Snowflake table. To get
    the records from an event table, you can query the event table with the following
    code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 可以像访问任何其他 Snowflake 表一样访问事件表。要从事件表中获取记录，可以使用以下代码查询事件表：
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It returns a result with empty records since no information was captured. Records
    in an event table can be filtered with a specific column to get detailed information.
    A Snowflake stream can be set on top of the event table to capture only new events.
    A Stream can be created by running the following query:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个空记录的结果，因为没有捕获任何信息。可以通过特定列过滤事件表中的记录以获取详细信息。可以在事件表之上设置 Snowflake 流以仅捕获新事件。可以通过运行以下查询来创建流：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `EVENT_APPEND` stream captures the latest inserted records into the event
    table. In the next section, we will set up logging and tracing to capture records
    in an event table.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`EVENT_APPEND` 流捕获事件表中最新插入的记录。在下一节中，我们将设置日志记录和跟踪以捕获事件表中的记录。'
- en: Setting up logging in Snowpark
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Snowpark 中设置日志记录
- en: Introducing logging and tracing capabilities into our pipelines is akin to infusing
    resilience into our standard data engineering processes. This section will delve
    into the integration of logging functionalities into our existing data engineering
    pipeline. By doing so, we not only gain the ability to monitor and trace the flow
    of data but also enhance the robustness of our code, fortifying it against potential
    pitfalls. Join us as we explore how these logging capabilities elevate our data
    engineering practices, making them more reliable and fault-tolerant.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志记录和跟踪功能引入我们的管道类似于将弹性注入到我们的标准数据工程流程中。本节将深入探讨将日志功能集成到我们现有的数据工程管道中。通过这样做，我们不仅获得了监控和跟踪数据流的能力，还增强了代码的健壮性，使其能够抵御潜在的陷阱。随着我们探索这些日志功能如何提升我们的数据工程实践，使它们更加可靠和容错，请加入我们。
- en: 'Logging can be enabled for both Snowpark functions and procedures. The first
    step in capturing logs is to set the log level in the Snowpark session. The log
    level in the session can be set by running the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为 Snowpark 函数和过程启用日志记录。捕获日志的第一步是在 Snowpark 会话中设置日志级别。可以通过运行以下代码来设置会话中的日志级别：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This sets the log level to `INFO` for the particular session, so all Snowpark
    execution that happens for the specific session is captured with the log level
    as information. Snowpark supports APIs to log messages directly from the handler.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将特定会话的日志级别设置为 `INFO`，因此所有针对特定会话发生的 Snowpark 执行都将捕获为信息级别的日志。Snowpark 支持从处理程序直接记录消息的
    API。
- en: Capturing informational logs
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕获信息日志
- en: 'We will now modify the data preparation procedures from the data pipeline to
    capture informational logs. We start with prepping the data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将修改数据管道中的数据准备程序以捕获信息日志。我们从准备数据开始：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The data from the four tables is loaded into the DataFrame. This DataFrame
    is then used to execute each step. Next, we will proceed with calling each step
    in order to process the data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 来自四个表的数据被加载到 DataFrame 中。然后使用此 DataFrame 执行每个步骤。接下来，我们将按顺序调用每个步骤以处理数据：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once all three steps are executed, we get the final marketing data ready to
    be loaded into a Snowflake table for consumption. The following code will load
    the data into a Snowflake table:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所有三个步骤后，我们将获得最终的市场数据，准备将其加载到 Snowflake 表中进行消费。以下代码将数据加载到 Snowflake 表中：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The data is loaded into a table called `FINAL_MARKETING_DATA`. The table is
    automatically created with the data in the Snowpark DataFrame. We will now register
    this as a Snowpark stored procedure:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被加载到一个名为 `FINAL_MARKETING_DATA` 的表中。该表由 Snowpark DataFrame 中的数据自动创建。现在我们将将其注册为
    Snowpark 存储过程：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `logging` module from Python’s standard library is used for logging. The
    package is imported, and the logger’s name is specified as `My_Logger`. Different
    logger names can be set for processes to help identify the particular logging
    application. We can execute the stored procedure to capture logs to the event
    table. The stored procedure can be executed by running the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Python 标准库中的 `logging` 模块用于日志记录。该包被导入，并且记录器的名称被指定为 `My_Logger`。可以为不同的进程设置不同的记录器名称，以帮助识别特定的日志应用。我们可以执行存储过程以将日志捕获到事件表中。可以通过运行以下命令来执行存储过程：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The stored procedure is executed, and we can see the successful output of the
    execution:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 存储过程被执行，我们可以看到执行的输出结果：
- en: '![Figure 4.11 – Stored procedure execution](img/B19923_04_11.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 存储过程执行](img/B19923_04_11.jpg)'
- en: Figure 4.11 – Stored procedure execution
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 存储过程执行
- en: The following section will cover how to query logs generated by the stored procedure.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍如何查询存储过程生成的日志。
- en: Querying informational logs
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询信息日志
- en: 'Logs generated by the stored procedure can be accessed from the event table.
    Records captured from the previous stored procedure can be accessed by running
    the following query:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由存储过程生成的日志可以从事件表中访问。通过运行以下查询可以访问从先前存储过程捕获的记录：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We filter logs captured only from `My_Logger` by specifying it in the scope.
    The query returns the following records that were generated from executing the
    stored procedure:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在作用域中指定它来仅过滤从`My_Logger`捕获的日志。查询返回以下记录，这些记录是从执行存储过程生成的：
- en: '![Figure 4.12 – Querying logs](img/B19923_04_12.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – 查询日志](img/B19923_04_12.jpg)'
- en: Figure 4.12 – Querying logs
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – 查询日志
- en: In the next section, we will set up logging to capture error messages and handle
    exceptions in Snowpark.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将设置日志以捕获错误消息并处理Snowpark中的异常。
- en: Handling exceptions in Snowpark
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Snowpark中处理异常
- en: Exception handling is integral to data pipelines as it helps identify and handle
    issues. Exception handling can be done by catching exceptions thrown from within
    the `try` block and capturing those error logs. The `ERROR` and `WARN` log levels
    are often used when capturing exceptions, and fatal issues are logged at the `FATAL`
    level. This section will look at capturing error logs and handling the exception
    on the pipeline.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 异常处理对于数据管道至关重要，因为它有助于识别和处理问题。可以通过捕获`try`块内部抛出的异常并捕获这些错误日志来完成异常处理。在捕获异常时，通常使用`ERROR`和`WARN`日志级别，而致命问题则记录在`FATAL`级别。本节将探讨捕获错误日志和在管道上处理异常。
- en: Capturing error logs
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕获错误日志
- en: 'We will modify the data transformation stored procedure to capture error logs
    and add exception handling:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改数据转换存储过程以捕获错误日志并添加异常处理：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The data transformation logic is moved into the `try` block, and a logger named
    `Data_Transform_Logger` is initiated. The exception raised by the code is captured
    in the exception object defined as `err`. This is then logged in the event table
    by the `ERROR` log level. We will now register this stored procedure in Snowpark:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换逻辑被移动到`try`块中，并启动了一个名为`Data_Transform_Logger`的记录器。代码引发的异常被捕获在定义为`err`的异常对象中。然后通过`ERROR`日志级别将这些记录在事件表中。现在我们将在此存储过程中注册Snowpark：
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will execute the stored procedure purposely with the error to log the error.
    The stored procedure can be triggered by running the following command:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将故意执行存储过程以记录错误。可以通过运行以下命令来触发存储过程：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The stored procedure has raised an exception, and the error has been captured
    in the event table:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 存储过程已引发异常，错误已记录在事件表中：
- en: '![Figure 4.13 – Error execution](img/B19923_04_13.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13 – 错误执行](img/B19923_04_13.jpg)'
- en: Figure 4.13 – Error execution
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – 错误执行
- en: The following section will cover querying error logs generated by the procedure.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将涵盖查询由存储过程生成的错误日志。
- en: Querying error logs
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询错误日志
- en: 'The error log records from the preceding stored procedure are captured under
    the `Data_Transform_Logger` logger, which can be accessed by filtering the query
    to return logs specific to the logger. The following query can be executed to
    get the records from the event table:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个存储过程生成的错误日志记录在`Data_Transform_Logger`记录器下，可以通过过滤查询以返回特定于记录器的日志来访问。以下查询可以执行以从事件表中获取记录：
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The scope name is filtered as the `Data_Transform_Logger` to get the results,
    and errors caused by the exception are logged under `SEVERITY` as `ERROR`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 作用域名称被过滤为`Data_Transform_Logger`以获取结果，由异常引起的错误在`SEVERITY`下记录为`ERROR`：
- en: '![Figure 4.14 – Error log messages](img/B19923_04_14.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图4.14 – 错误日志消息](img/B19923_04_14.jpg)'
- en: Figure 4.14 – Error log messages
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 错误日志消息
- en: Snowpark makes debugging easy by supporting logging errors and handling exceptions
    through event tables. The following section will cover event traces and how to
    capture trace information in Snowpark.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark通过支持通过事件表记录错误和异常处理来简化调试。以下部分将涵盖事件跟踪以及如何在Snowpark中捕获跟踪信息。
- en: Setting up tracing in Snowpark
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Snowpark中设置跟踪
- en: Trace events are a type of telemetry data that is captured when something has
    happened in the code. It has a structured payload that helps analyze the trace
    by aggregating this information to understand the code’s behavior at a high level.
    When a procedure or function executes, trace events are emitted, which are available
    in the active event table.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪事件是在代码中发生某些事情时捕获的一种遥测数据类型。它有一个结构化有效负载，有助于通过聚合这些信息来分析跟踪，从而理解代码在高级别上的行为。当程序或函数执行时，会发出跟踪事件，这些事件在活动事件表中可用。
- en: 'The first step in capturing events is to turn on the tracing functionality
    by setting the trace level in the Snowpark session. The trace level in the session
    can be set by running the following code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获事件的第一步是通过在 Snowpark 会话中设置跟踪级别来启用跟踪功能。会话中的跟踪级别可以通过运行以下代码来设置：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the next section, we will look at capturing traces.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何捕获跟踪信息。
- en: Capturing traces
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕获跟踪信息
- en: 'Traces can be captured with the open source Snowflake `telemetry` Python package
    available in the Anaconda Snowflake channel. The package needs to be imported
    into the code, and it will be executed in Snowpark. The `telemetry` package can
    be imported by including the code in the Snowpark handler:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用开源 Snowflake `telemetry` Python 包捕获跟踪，该包可在 Anaconda Snowflake 频道中找到。需要将包导入到代码中，并在
    Snowpark 中执行。可以通过在 Snowpark 处理器中包含以下代码来导入 `telemetry` 包：
- en: '[PRE31]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `telemetry` package helped capture traces generated on the code and logged
    into the event table. We will modify the data cleanup procedure by adding telemetry
    events to capture the trace:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`telemetry` 包帮助捕获在代码上生成并记录到事件表中的跟踪信息。我们将通过添加遥测事件来修改数据清理程序以捕获跟踪：'
- en: '[PRE32]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The procedure is now ready to be executed. We are passing attributes for the
    operations captured on the traces. We can execute the procedure by running the
    following code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在已准备好执行。我们正在传递捕获在跟踪上的属性。可以通过运行以下代码来执行程序：
- en: '[PRE33]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The procedure is executed, and the data is cleaned from the table:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 程序执行，数据从表中清理：
- en: '![Figure 4.15 – Data cleanup procedure execution](img/B19923_04_15.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – 数据清理程序执行](img/B19923_04_15.jpg)'
- en: Figure 4.15 – Data cleanup procedure execution
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 数据清理程序执行
- en: The traces are now generated in the event table. We can directly query the event
    table to obtain trace information.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪信息现在在事件表中生成。我们可以直接查询事件表以获取跟踪信息。
- en: Querying traces
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询跟踪
- en: 'Traces generated can be accessed from the event table. Traces captured from
    the previous stored procedure can be accessed by running the following query:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的跟踪可以从事件表中访问。可以通过运行以下查询访问从先前存储过程捕获的跟踪：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We are querying using the `data_cleanup` event name. This returns the two traces
    captured when the code was executed:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `data_cleanup` 事件名称进行查询。这返回了代码执行时捕获的两个跟踪：
- en: '![Figure 4.16 – Trace capture information](img/B19923_04_16.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16 – 跟踪捕获信息](img/B19923_04_16.jpg)'
- en: Figure 4.16 – Trace capture information
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 跟踪捕获信息
- en: We can see the attributes that are captured from the execution of the stored
    procedure. After the `NULL` values have been cleaned up, the details return the
    total data counts, including the `NULL` and `count` values.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到从存储过程的执行中捕获的属性。在清理了 `NULL` 值之后，详细信息返回了总数据计数，包括 `NULL` 和 `count` 值。
- en: Comparison of logs and traces
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志和跟踪的比较
- en: 'The following table compares logs and traces and lists scenarios to use each:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格比较了日志和跟踪，并列出了使用每种情况的场景：
- en: '| **Characteristic** | **Log entries** | **Trace events** |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **日志条目** | **跟踪事件** |'
- en: '| Intended use | Record detailed but unstructured information about the state
    of your code. Use this information to understand what happened during a particular
    invocation of your function or procedure. | Record a brief but structured summary
    of each invocation of your code. Aggregate this information to understand the
    behavior of your code at a high level. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 预期用途 | 记录关于代码状态的详细但非结构化信息。使用这些信息来了解函数或过程特定调用期间发生了什么。 | 记录每次代码调用的简短但结构化摘要。聚合这些信息以了解代码在高级别上的行为。|'
- en: '| Structure as a payload | None. A log entry is just a string. | Structured
    with attributes you can attach to trace events. Attributes are key-value pairs
    that can be easily queried with a SQL query. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 结构作为有效载荷 | 无。日志条目只是一个字符串。 | 结构化，可以附加到跟踪事件上的属性。属性是键值对，可以很容易地通过 SQL 查询进行查询。|'
- en: '| Supports grouping | No. Each log entry is an independent event. | Yes. Trace
    events are organized into spans. A span can have its attributes. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 支持分组 | 否。每个日志条目都是一个独立的事件。 | 是。跟踪事件组织成跨度。一个跨度可以有自己的属性。|'
- en: '| Quantity limits | Unlimited. All log entries emitted by your code are ingested
    into the event table. | The number of trace events per span is capped at 128\.
    There is also a limit on the number of span attributes. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 数量限制 | 无限制。你代码发出的所有日志条目都会被摄入到事件表中。 | 每个跨度中的跟踪事件数量限制为 128。跨度属性的数量也有限制。|'
- en: '| Complexity of queries against recorded data | Relatively high. Your queries
    must parse each log entry to extract meaningful information from it. | Relatively
    low. Your queries can take advantage of the structured nature of trace events.
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 对记录数据的查询复杂度 | 相对较高。您的查询必须解析每个日志条目以从中提取有意义的信息。 | 相对较低。您的查询可以利用跟踪事件的有序性质。 |'
- en: Table 4.3 – Differences between logs and traces
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3 – 日志和跟踪之间的差异
- en: Logs and traces help debug Snowpark and are a practical feature for efficient
    DataOps.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 日志和跟踪有助于调试 Snowpark，并且是高效 DataOps 的实用功能。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered in detail building and deploying resilient data
    pipelines in Snowpark and also how to enable logging and tracking using event
    tables. Building resilient data pipelines with effective DataOps is vital for
    a successful data strategy. Snowpark supports the development of a modern data
    pipeline through a programmatic ELT approach along with features such as logging
    and tracing, making it easy for developers to implement DataOps. We also covered
    how tasks and task graphs can be used in scheduling and deploying pipelines.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了在 Snowpark 中构建和部署具有弹性的数据管道，以及如何使用事件表启用日志记录和跟踪。通过程序化 ELT 方法以及日志记录和跟踪等特性，Snowpark
    支持现代数据管道的开发，这使得开发者能够轻松实现 DataOps。我们还介绍了如何使用任务和任务图进行调度和部署管道。
- en: In the next chapter, we will cover using Snowpark to develop data science workloads.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何使用 Snowpark 开发数据科学工作负载。
