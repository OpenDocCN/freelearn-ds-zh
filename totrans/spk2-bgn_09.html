<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0; Designing Spark Applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9.  Designing Spark Applications </h1></div></div></div><p>Think functionally. Think application functionality designed like a pipeline with each piece plumbed together doing some part of the whole job in hand. It is all about processing data, and that is what Spark does in a highly versatile manner. Data processing starts with the seed data that gets into the processing pipeline. The seed data can be a new piece of data that is ingested into the system, or it can be some kind of master dataset that lives in the enterprise data store and needs to be sliced and diced to produce different views to serve various purposes and business needs. It is this slicing and dicing that is going to be the norm when designing and developing data processing applications.</p><p>Any application development exercise starts with a study of the domain, the business requirement	`s, and the technical tool selection. It is not going to be different here. Even though this chapter is going to see the design and development of a Spark application, the initial focus will be on the overall architecture of data processing applications, use cases, the data, and the applications that transform the data from one state to another. Spark is just a driver that assembles data processing logic and data together, using its highly powerful infrastructure to produce the desired results.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Lambda Architecture</li><li class="listitem" style="list-style-type: disc">Microblogging with Spark</li><li class="listitem" style="list-style-type: disc">Data dictionaries</li><li class="listitem" style="list-style-type: disc">Coding style</li><li class="listitem" style="list-style-type: disc">Data ingestion</li></ul></div><div class="section" title="Lambda Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Lambda Architecture</h1></div></div></div><p>Application architecture is very important for any kind of software development. It is the blueprint that decides how the software has to be built with a good amount of generality and the capability to customize when needed. For common application needs, some popular architectures are available, and there is no need for any ground-up architecture effort in order to use them. These public architecture frameworks are designed by some of the best minds for the benefit of the general public. These popular architectures are very useful because there is no barrier to entry, and they are used by so many people. There are popular architectures available for web application development, data processing, and so on.</p><p>Lambda Architecture is a recent and popular architecture that's ideal for developing data processing applications. There are many tools and technologies available in the market to develop data processing applications. But independent of the technology, how the data processing application components are layered and composed together is driven by the architectural framework. That is why Lambda Architecture is a technology-agnostic architecture framework and, depending on the need, the appropriate technology choice can be made to develop the individual components. <span class="emphasis"><em>Figure 1</em></span> captures the essence of Lambda Architecture:</p><p>
</p><div class="mediaobject"><img alt="Lambda Architecture" src="graphics/image_09_001.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p><p>Lambda Architecture consists of three layers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The batch layer is the main data store. Any kind of processing happens on this dataset. This is the golden dataset. </li><li class="listitem" style="list-style-type: disc">The serving layer processes the master dataset and prepares views for specific purposes, and they are termed purposed views here. This intermediate step of processing is required to serve the queries, or for generating outputs for specific needs. The queries and the specific dataset preparation don't directly access the master dataset. </li><li class="listitem" style="list-style-type: disc">The speed layer is all about data stream processing. The stream of data is processed in a real-time fashion and volatile real-time views are prepared if that is a business need. The queries or specific processes generating outputs may consume data from both the purposed data views and real-time views.</li></ul></div><p>Using the principles of Lambda Architecture to architect a big data processing system, Spark is going to be used here as a data processing tool. Spark fits nicely into all the data processing requirements in all three distinct layers. </p><p>This chapter is going to discuss some selected data processing use cases of a microblogging application. The application functionality, its deployment infrastructure, and the scalability factors are beyond the scope of this work. In a typical batch layer, the master dataset can be plain splittable serialization formats or NoSQL data stores, depending on the data access methods. If the application use cases are all batch operations, then standard serialization formats will be sufficient. But if the use cases mandate random access, NoSQL data stores will be ideal. Here, for the sake of simplicity, all the data files are stored in plain text files locally. </p><p>Typical application development culminates in a completely functional application. But here, the use cases are realized in Spark data processing applications. Data processing always works as a part of the functionality of the main application and it is scheduled to run in batch mode or run as a listener waiting for data and processing it. So, corresponding to each of the use cases, individual Spark applications are developed, and they can be scheduled or made to run in listener mode as the case may be.</p></div></div>
<div class="section" title="Microblogging with Lambda Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Microblogging with Lambda Architecture</h1></div></div></div><p>Blogging has been around for couple of decades in various forms. In the initial days of blogging as a medium of publication, only professional or aspiring writers published articles through the medium of blogs. It spread the false notion that only serious content is published through blogs. In recent years, the concept of microblogging included the general public in the culture of blogging. Microblogs are sudden outbursts of the thought processes of people in the form of a few sentences, photos, videos, or links. Sites such as Twitter and Tumblr popularized this culture at the biggest scale possible with hundreds of millions of active users using the site. </p><div class="section" title="An overview of SfbMicroBlog"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec49"/>An overview of SfbMicroBlog</h2></div></div></div><p>
<span class="strong"><strong>SfbMicroBlog</strong></span>is a microblogging application with millions of users posting short messages. A new user who is going to use this application needs to sign up with a username and password. To post messages, users have to sign in first. The only thing users can do without signing in is read public messages posted by users. Users can follow other users. The act of following is a one-way relationship. If user A follows user B, user A can see all the messages posted by user B; at the same time, user B cannot see the messages posted by user A, because user B is not following user A. By default, all the messages posted by all the users are public messages and can be seen by everybody. But users have settings to make messages visible only to users who are following the message owner. After becoming a follower, unfollowing is also allowed. </p><p>Usernames have to be unique across all users. A username and password are required to sign in. Every user must have a primary e-mail address, and without that the signup process will not be complete. For extra security and password recovery, an alternate e-mail address or mobile phone number can be saved in the profile. </p><p>Messages cannot exceed the a of 140 characters. Messages can contain words prefixed with the # symbol to group them under various topics. Messages can contain usernames prefixed with the @ symbol to directly address users through messages that are posted. In other words, users can address any other user in their messages without being a follower. </p><p>Once posted, the messages cannot be changed. Once posted, the messages cannot be deleted.</p></div><div class="section" title="Getting familiar with data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec50"/>Getting familiar with data</h2></div></div></div><p>All pieces of data that come to the master dataset come through a stream. The data stream is processed, an appropriate header for each message is inspected, and the right action to store it in the data store is done. The following list contains the important data items that come into the store through the same stream:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>User</strong></span>: This dataset contains the user details when a user signs in or when a user's data gets changed</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Follower</strong></span>: This dataset contains the relationship data that gets captured when a user opts to follow another user</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Message</strong></span>: This dataset contains the messages posted by registered users</li></ul></div><p>This list of datasets forms the golden dataset. Based on this master dataset, various views are created that cater to the needs of the vital business functions in the application. The following list contains the important views of the master dataset:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Messages by users</strong></span>: This view contains messages posted by each user in the system. When a given user wants to see the messages posted by him/her, the data generated by this view is used. This is also used by the given user's followers. This is a situation where the main dataset is used for a specific purpose. The message dataset gives all the required data for this view.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Messages to users</strong></span>: In the messages, specific users can be addressed by prefixing the @ symbol followed by the addressee's username. This data view contains the users addressed with the @ symbol and the corresponding messages. There is a limitation enforced in the implementation: you can only have one addressee in one message.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Tagged messages</strong></span>: In the messages, words prefixed with the # symbol becomes searchable messages. For example, the word #spark in a message signifies that the message is searchable by the word #spark. For a given hashtag, users can see all the public messages and the messages of users whom he/she is following in one list. This view contains pairs of the hashtag and the corresponding messages. There is a limitation enforced in the implementation: you can only have one tag in one message.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Follower users</strong></span>: This view contains the list of users who are following a given user. In <span class="emphasis"><em>Figure 2</em></span>, users <span class="strong"><strong>U1</strong></span> and <span class="strong"><strong>U3</strong></span> are in the list of users following <span class="strong"><strong>U4</strong></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Followed users</strong></span>: This view contains the list of users who are followed by a given user. In <span class="emphasis"><em>Figure 2</em></span>, users <span class="strong"><strong>U2</strong></span> and <span class="strong"><strong>U4</strong></span> are in the list of users who are followed by user <span class="strong"><strong>U1</strong></span>:</li></ul></div><p>
</p><div class="mediaobject"><img alt="Getting familiar with data" src="graphics/image_09_002.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p><p>In a nutshell, <span class="emphasis"><em>Figure 3</em></span> gives the Lambda Architecture view of the solution and gives details of the datasets and the corresponding views:</p><p>
</p><div class="mediaobject"><img alt="Getting familiar with data" src="graphics/image_09_003-1.jpg"/><div class="caption"><p>Figure 3</p></div></div><p>
</p></div><div class="section" title="Setting the data dictionary"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec51"/>Setting the data dictionary</h2></div></div></div><p>The data dictionary describes the data, its meaning, and its relationship with other data items. For the SfbMicroBlog application, the data dictionary is going to be a very minimalistic one to implement the selected use cases. Using this as a base, readers can expand and implement their own data items and include data processing use cases. The data dictionary is given for all the master datasets, as well as the data views.</p><p>The following table shows the data items of the user dataset:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>User data</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Type</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>Id</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to uniquely identify a user, as well as being the vertex identifier in the user relationship graph</p>
</td></tr><tr><td>
<p>Username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to uniquely identify users of the system</p>
</td></tr><tr><td>
<p>First name</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the first name of the user</p>
</td></tr><tr><td>
<p>Last name</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the last name of the user</p>
</td></tr><tr><td>
<p>E-mail</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to communicate with users</p>
</td></tr><tr><td>
<p>Alternate e-mail</p>
</td><td>
<p>String</p>
</td><td>
<p>Used for password recovery</p>
</td></tr><tr><td>
<p>Primary phone</p>
</td><td>
<p>String</p>
</td><td>
<p>Used for password recovery</p>
</td></tr></tbody></table></div><p>The following table captures the data items of the follower dataset:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Follower data</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Type</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>Follower username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to identify who the follower is</p>
</td></tr><tr><td>
<p>Followed username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to identify who is being followed</p>
</td></tr></tbody></table></div><p>The following table captures the data items of the message dataset:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Message data</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Type</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>Username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the user who posted the message</p>
</td></tr><tr><td>
<p>Message Id</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to uniquely identify a message</p>
</td></tr><tr><td>
<p>Message</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the message that is being posted</p>
</td></tr><tr><td>
<p>Timestamp</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to capture the time at which the message is posted</p>
</td></tr></tbody></table></div><p>The following table captures the data items of the Message to users view:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Message to users data</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Type</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>From username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the user who posted the message</p>
</td></tr><tr><td>
<p>To username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the user to whom the message is addressed; it is the username that is prefixed with the @ symbol</p>
</td></tr><tr><td>
<p>Message Id</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to uniquely identify a message</p>
</td></tr><tr><td>
<p>Message</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the message that is being posted</p>
</td></tr><tr><td>
<p>Timestamp</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to capture the time at which the message is posted</p>
</td></tr></tbody></table></div><p>The following table captures the data items of the Tagged messages view:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Tagged messages data</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Type</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td></tr><tr><td>
<p>Hashtag</p>
</td><td>
<p>String</p>
</td><td>
<p>The word that is prefixed with the # symbol</p>
</td></tr><tr><td>
<p>Username</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the user who posted the message</p>
</td></tr><tr><td>
<p>Message Id</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to uniquely identify a message</p>
</td></tr><tr><td>
<p>Message</p>
</td><td>
<p>String</p>
</td><td>
<p>Used to capture the message that is being posted</p>
</td></tr><tr><td>
<p>Timestamp</p>
</td><td>
<p>Long</p>
</td><td>
<p>Used to capture the time at which the message is posted</p>
</td></tr></tbody></table></div><p>The follower relationship of the users is pretty straightforward and consists of the pairs of user identification numbers persisted in a data store.</p></div></div>
<div class="section" title="Implementing Lambda Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Implementing Lambda Architecture</h1></div></div></div><p>The concept of Lambda Architecture was introduced in the beginning of this chapter. Since it is a technology-agnostic architecture framework, when designing applications with it, it is imperative to capture the technology choices used in specific implementations. The following sections do exactly that.</p><div class="section" title="Batch layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec52"/>Batch layer</h2></div></div></div><p>The core of the batch layer is a data store. For big data applications, there are plenty of choices for data stores. Typically, <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) in conjunction with Hadoop YARN is the current and accepted platform in which data is stored, mainly because of the ability to partition and distribute data across the Hadoop cluster.</p><p>There are two types of data access any persistence store supports:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Batch write/read</li><li class="listitem" style="list-style-type: disc">Random write/read</li></ul></div><p>Both of these need separate data storage solutions. For batch data operations, typically splittable serialization formats such as Avro and Parquet are used. For random data operations, typically NoSQL data stores are used. Some of these NoSQL solutions sit on top of HDFS and some don't. It doesn't matter whether they are on top of HDFS or not, they provide partitioning and distribution of data. So depending on the use case and the distributed platform that is in use, appropriate solutions can be used.</p><p>When it comes to the storage of the data in HDFS, commonly used formats such as XML and JSON fail because HDFS partitions and distributes the files. When that happens, these formats have opening tags and ending tags, and splits at random locations in the file make the data dirty. Because of that, splittable file formats such as Avro or Parquet are efficient for storing in HDFS.</p><p>When it comes to the NoSQL data store solutions, there are many choices in the market, especially from the open source world. Some of these NoSQL data stores, such as Hbase, sit on top of HDFS. Some of the NoSQL data stores, such as Cassandra and Riak, don't need HDFS, can be deployed on regular operating systems, and can be deployed in master-less fashion so that there is no single point of failure in a cluster. The choice of the NoSQL store is again dependent on the usage of a particular technology within the organization, the production support contracts in place, and many other parameters.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip65"/>Tip</h3><p>This book doesn't recommend a given set of data store technologies for usage in conjunction with Spark because Spark drivers are available in abundance for most popular serialization formats and NoSQL data stores. In other words, most of the data store vendors have started supporting Spark in big way. Another interesting trend these days is that many of the prominent ETL tools have started supporting Spark, and because of that, those who are using such ETL tools may use Spark applications within their ETL processing pipelines.</p></div></div><p>In this application, neither an HDFS-based nor any NoSQL-based data store is being used in order to maintain simplicity and to avoid the complex infrastructure setup required to run the application for the readers. Throughout, the data is stored on the local system in text file formats. Readers who are interested in trying out the examples on HDFS or other NoSQL data stores may go ahead and try them, with some changes to the data write/read part of the application.</p></div><div class="section" title="Serving layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec53"/>Serving layer</h2></div></div></div><p>The serving layer can be implemented in Spark using various methods. If the data is not structured and is purely object-based, then the low-level RDD-based method is suitable. If the data is structured, a DataFrame is ideal. The use case that is being discussed here is dealing with structured data and hence wherever possible the Spark SQL library is going to be used. From the data stores, data is read and RDDs are created. The RDDs are converted to DataFrames and all the serving needs are accomplished using Spark SQL. In this way, the code is going to be succinct and easy to understand.</p></div><div class="section" title="Speed layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec54"/>Speed layer</h2></div></div></div><p>The speed layer is going to be implemented as a Spark Streaming application using Kafka as the broker with its own producers producing the messages. The Spark Streaming application will act as the consumer to the Kafka topics and receive the data that is getting produced. As discussed in the chapter covering Spark Streaming, the producers can be the Kafka console producer or any other producer supported by Kafka. But the Spark Streaming application working as the consumer here is not going to implement the logic of persisting the processed messages to the text file as they are not generally used in real-world use cases. Using this application as a base, readers can implement their own persistence mechanism. </p><div class="section" title="Queries"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec6"/>Queries</h3></div></div></div><p>The queries are all generated from the speed layer and serving layer. Since the data is available in the form of DataFrames, as mentioned before, all the queries for the use case are implemented using Spark SQL. The obvious reason is that Spark SQL works as a consolidation technology that unifies the data sources and destinations. When readers are using the samples from this book and when they are ready to implement it in their real-world use cases, the overall methodology can remain the same, but the data sources and destinations may differ. The following are some of the queries that can be generated from the serving layer. It is up to the imagination of the readers to make the required changes to the data dictionary and be able to write these views or queries: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Find the messages that are grouped by a given hashtag</li><li class="listitem" style="list-style-type: disc">Find the messages that are addressed to a given user</li><li class="listitem" style="list-style-type: disc">Find the followers of a given user</li><li class="listitem" style="list-style-type: disc">Find the followed users of a given user</li></ul></div></div></div></div>
<div class="section" title="Working with Spark applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec77"/>Working with Spark applications</h1></div></div></div><p>The workhorse of this application is the data processing engine consisting of many Spark applications. In general, they can be classified into the following types:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A Spark Streaming application to ingest data: This is the main listener application that receives the data coming as a stream and stores it in the appropriate master dataset.</li><li class="listitem" style="list-style-type: disc">A Spark application to create purposed views and queries: This is the application that is used to create various purposed views from the master datasets. Apart from that, the queries are also included in this application.</li><li class="listitem" style="list-style-type: disc">A Spark GraphX application to do custom data processing: This is the application that is used to process the user-follower relationship.</li></ul></div><p>All these applications are developed independently and they are submitted independently, but the stream processing application will be always running as a listener application to process the incoming messages. Apart from the main data streaming application, all the other applications are scheduled like regular jobs, such as cron jobs in a UNIX system. In this application, all these applications are producing various purposed views. The scheduling depends on the kind of application and how much delay is affordable between the main dataset and the views. It completely depends on the business functions. So this chapter is going to focus on Spark application development rather than scheduling, to keep the focus on the lessons learned in the earlier chapters.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip66"/>Tip</h3><p>It is not ideal to persist the data from the speed layer into text files when implementing real-world use cases. For simplicity, all the data is stored in text files to empower all levels of reader with the simplest setup. The speed layer implementation using Spark Streaming is a skeleton implementation without the persistence logic. Readers can enhance this to introduce persistence to their desired data stores.</p></div></div></div>
<div class="section" title="Coding style"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec78"/>Coding style</h1></div></div></div><p>Coding style has been discussed and lots of Spark application programming has been  done in the earlier chapters. By now, it has been proven in this book that Spark application development can be done in Scala, Python, and R. In most of the earlier chapters, the languages of choice were Scala and Python. In this chapter, the same trend will continue. Only for the Spark GraphX application, since there is no Python support, will the application be developed in Scala alone. </p><p>The style of coding is going to be simple and to the point. The error handling and other best practices of application development are avoided deliberately to focus on the Spark features. In this chapter, wherever possible, the code is run from the appropriate language's Spark REPL. Since the anatomy of the complete application and the scripts to compile, build, and run them as applications have already been covered in the chapter that discussed Spark Streaming, the source code download will have it available as complete ready-to-run applications. Moreover, the chapter covering Spark Streaming discussed the anatomy of a complete Spark application, including the scripts to build and run Spark applications. The same methodology will be used in the applications that are going to be developed in this chapter too. When running such standalone Spark applications, as discussed in the initial chapters of this book, readers can enable Spark monitoring and see how the application is behaving. For the sake of brevity, these discussions will not be taken up again here. </p></div>
<div class="section" title="Setting up the source code"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec79"/>Setting up the source code</h1></div></div></div><p>
<span class="emphasis"><em>Figure 4</em></span> shows the structure of the source code and the data directories that are being used in this chapter. A description of each of them is not provided here as the reader should be familiar with them, and they have been covered in <a class="link" href="ch06.html" title="Chapter 6.  Spark Stream Processing">Chapter 6</a>, <span class="emphasis"><em>Spark Stream Processing</em></span>. There are external library file dependency requirements for running the programs using Kafka. For that, the instructions to download the JAR file are in the <code class="literal">TODO.txt </code>file in the <code class="literal">lib</code>folders. The <code class="literal">submitPy.sh </code>and <code class="literal">submit.sh </code>files use some of the <code class="literal">Kafka</code> libraries in the Kafta installation as well. All these external JAR file dependencies have already been covered in <a class="link" href="ch06.html" title="Chapter 6.  Spark Stream Processing">Chapter 6</a>, <span class="emphasis"><em>Spark Stream Processing</em></span>.</p><p>
</p><div class="mediaobject"><img alt="Setting up the source code" src="graphics/image_09_004.jpg"/><div class="caption"><p>Figure 4</p></div></div><p>
</p></div>
<div class="section" title="Understanding data ingestion"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec80"/>Understanding data ingestion</h1></div></div></div><p>The Spark Streaming application works as the listener application that receives the data from its producers. Since Kafka is going to be used as the message broker, the Spark Streaming application will be its consumer application, listening to the topics for the messages sent by its producers. Since the master dataset in the batch layer has the following datasets, it is ideal to have individual Kafka topics for each of the topics, along with the datasets.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">User dataset:  User</li><li class="listitem" style="list-style-type: disc">Follower dataset: Follower</li><li class="listitem" style="list-style-type: disc">Message dataset: Message</li></ul></div><p>
<span class="emphasis"><em>Figure 5</em></span> provides an overall picture of the Kafka-based Spark Streaming application structure:</p><p>
</p><div class="mediaobject"><img alt="Understanding data ingestion" src="graphics/image_09_005.jpg"/><div class="caption"><p>Figure 5</p></div></div><p>
</p><p>Since the Kafka setup has already been covered in <a class="link" href="ch06.html" title="Chapter 6.  Spark Stream Processing">Chapter 6</a>, <span class="emphasis"><em>Spark Stream Processing</em></span>, only the application code is covered here. </p><p>The following scripts are run from a terminal window. Make sure that the <code class="literal">$KAFKA_HOME</code> environment variable is pointing to the directory where Kafka is installed. Also, it is very important to start Zookeeper, the Kafka server, the Kafka producer, and the Spark Streaming log event data processing application in separate terminal windows. Once the necessary Kafka topics are created as shown in the scripts, the appropriate producers have to start producing messages. Refer to the Kafka setup details that have already been covered in <a class="link" href="ch06.html" title="Chapter 6.  Spark Stream Processing">Chapter 6</a>, <span class="emphasis"><em>Spark Stream Processing</em></span>, before proceeding further.</p><p>Try the following commands in the terminal window prompt:</p><pre class="programlisting">
<span class="strong"><strong>
$ # Start the Zookeeper 
$ cd $KAFKA_HOME
$ $KAFKA_HOME/bin/zookeeper-server-start.sh
 $KAFKA_HOME/config/zookeeper.properties
      [2016-07-30 12:50:15,896] INFO binding to port 0.0.0.0/0.0.0.0:2181
	  (org.apache.zookeeper.server.NIOServerCnxnFactory)
    
	$ # Start the Kafka broker in a separate terminal window
	$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties
      [2016-07-30 12:51:39,206] INFO [Kafka Server 0], started 
	  (kafka.server.KafkaServer)
    
	$ # Create the necessary Kafka topics. This is to be done in a separate terminal window
	$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic user
      Created topic "user".
    $ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic follower
      Created topic "follower".
    
	$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic message
      Created topic "message".
    
	$ # Start producing messages and publish to the topic "message"
	$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 
	--topic message</strong></span>
</pre><p>
	This section provides the details of the Scala code for the Kafka topic consumer application that processes the messages produced by the Kafka producer. The assumption before running the following code snippet is that Kafka is up and running, the required producers are producing messages, and then, if the application is run, it will start consuming the messages. The Scala program for the data ingestion is run by submitting it to the Spark cluster. Starting from the Scala directory, as shown in <span class="emphasis"><em>Figure 4</em></span>, first compile the program and then run it. The <code class="literal">README.txt </code>file is to be consulted for additional instructions. The two following commands are to be executed to compile and run the program:</p><pre class="programlisting">
<span class="strong"><strong>
	$ ./compile.sh
	$ ./submit.sh com.packtpub.sfb.DataIngestionApp 1</strong></span>
</pre><p>
	The following code is the program listing that is to be compiled and run using the preceding commands: </p><pre class="programlisting">
<span class="strong"><strong>
	/**
	The following program can be compiled and run using SBT
	Wrapper scripts have been provided with thisThe following script can be run to compile the code
	./compile.sh
	The following script can be used to run this application in Spark.
	The second command line argument of value 1 is very important.
	This is to flag the shipping of the kafka jar files to the Spark cluster
	./submit.sh com.packtpub.sfb.DataIngestionApp 1
	**/
	package com.packtpub.sfb
	import java.util.HashMap
	import org.apache.spark.streaming._
	import org.apache.spark.sql.{Row, SparkSession}
	import org.apache.spark.streaming.kafka._
	import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}
	import org.apache.spark.storage.StorageLevel
	import org.apache.log4j.{Level, Logger}
	object DataIngestionApp {
	def main(args: Array[String]) {
	// Log level settings
	LogSettings.setLogLevels()
	//Check point directory for the recovery
	val checkPointDir = "/tmp"
    /**
    * The following function has to be used to have checkpointing and driver recovery
    * The way it should be used is to use the StreamingContext.getOrCreate with this function and do a start of that
	* This function example has been discussed but not used in the chapter covering Spark Streaming. But here it is being used    */
    def sscCreateFn(): StreamingContext = {
	// Variables used for creating the Kafka stream
	// Zookeeper host
	val zooKeeperQuorum = "localhost"
	// Kaka message group
	val messageGroup = "sfb-consumer-group"
	// Kafka topic where the programming is listening for the data
	// Reader TODO: Here only one topic is included, it can take a comma separated string containing the list of topics. 
	// Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store
	val topics = "message"
	val numThreads = 1     
	// Create the Spark Session, the spark context and the streaming context      
	val spark = SparkSession
	.builder
	.appName(getClass.getSimpleName)
	.getOrCreate()
	val sc = spark.sparkContext
	val ssc = new StreamingContext(sc, Seconds(10))
	val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
	val messageLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2)
	// This is where the messages are printed to the console. 
	// TODO - As an exercise to the reader, instead of printing messages to the console, implement your own persistence logic
	messageLines.print()
	//Do checkpointing for the recovery
	ssc.checkpoint(checkPointDir)
	// return the Spark Streaming Context
	ssc
    }
	// Note the function that is defined above for creating the Spark streaming context is being used here to create the Spark streaming context. 
	val ssc = StreamingContext.getOrCreate(checkPointDir, sscCreateFn)
	// Start the streaming
    ssc.start()
	// Wait till the application is terminated               
    ssc.awaitTermination() 
	}
	}
	object LogSettings {
	/** 
	Necessary log4j logging level settings are done 
	*/
	def setLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
	// This is to make sure that the console is clean from other INFO messages printed by Spark
	Logger.getRootLogger.setLevel(Level.INFO)
    }
	}
	}</strong></span>
</pre><p>
	The Python program for the data ingestion is run by submitting it to the Spark cluster. Starting from the 
	Python directory, as shown in <span class="emphasis"><em>Figure 4</em></span>, run the program. The <code class="literal">README.txt</code>file is to be consulted for additional instructions. All the Kafka installation requirements are valid, even when running this Python program. The following command is to be followed for running the program. Since Python is an interpreted language, there is no compilation required here:</p><pre class="programlisting">
<span class="strong"><strong>
	$ ./submitPy.sh DataIngestionApp.py 1</strong></span>
</pre><p>
	The following code snippet is the Python implementation of the same application:</p><pre class="programlisting">
<span class="strong"><strong>
	# The following script can be used to run this application in Spark
# ./submitPy.sh DataIngestionApp.py 1
  from __future__ import print_function
  import sys
  from pyspark import SparkContext
  from pyspark.streaming import StreamingContext
  from pyspark.streaming.kafka import KafkaUtils
  if __name__ == "__main__":
# Create the Spark context
  sc = SparkContext(appName="DataIngestionApp")
  log4j = sc._jvm.org.apache.log4j
  log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)
# Create the Spark Streaming Context with 10 seconds batch interval
  ssc = StreamingContext(sc, 10)
# Check point directory setting
  ssc.checkpoint("\tmp")
# Zookeeper host
  zooKeeperQuorum="localhost"
# Kaka message group
  messageGroup="sfb-consumer-group"
# Kafka topic where the programming is listening for the data
# Reader TODO: Here only one topic is included, it can take a comma separated  string containing the list of topics. 
# Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store
topics = "message"
numThreads = 1    
# Create a Kafka DStream
kafkaStream = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, {topics: numThreads})
messageLines = kafkaStream.map(lambda x: x[1])
# This is where the messages are printed to the console. Instead of this, implement your own persistence logic
messageLines.pprint()
# Start the streaming
ssc.start()
# Wait till the application is terminated   
ssc.awaitTermination()
</strong></span>
</pre></div>
<div class="section" title="Generating purposed views and queries"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec81"/>Generating purposed views and queries</h1></div></div></div><p>The following implementations in Scala and Python are the application that creates the purposed views and queries discussed in the earlier sections of this chapter. At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>
//TODO: Change the following directory to point to your data directory
scala&gt; val dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
      dataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/
    scala&gt; //Define the case classes in Scala for the entities
	scala&gt; case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)
      defined class User
    scala&gt; case class Follow(Follower: String, Followed: String)
      defined class Follow
    scala&gt; case class Message(UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class Message
    scala&gt; case class MessageToUsers(FromUserName: String, ToUserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class MessageToUsers
    scala&gt; case class TaggedMessage(HashTag: String, UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class TaggedMessage
    scala&gt; //Define the utility functions that are to be passed in the applications
	scala&gt; def toUser =  (line: Seq[String]) =&gt; User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))
      toUser: Seq[String] =&gt; User
    scala&gt; def toFollow =  (line: Seq[String]) =&gt; Follow(line(0), line(1))
      toFollow: Seq[String] =&gt; Follow
    scala&gt; def toMessage =  (line: Seq[String]) =&gt; Message(line(0), line(1).toLong, line(2), line(3).toLong)
      toMessage: Seq[String] =&gt; Message
    scala&gt; //Load the user data into a Dataset
	scala&gt; val userDataDS = sc.textFile(dataDir + "user.txt").map(_.split("\\|")).map(toUser(_)).toDS()
      userDataDS: org.apache.spark.sql.Dataset[User] = [Id: bigint, UserName: string ... 5 more fields]
    scala&gt; //Convert the Dataset into data frame
	scala&gt; val userDataDF = userDataDS.toDF()
      userDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]
    scala&gt; userDataDF.createOrReplaceTempView("user")
	scala&gt; userDataDF.show()
      +---+--------+---------+--------+--------------------+----------------+--------------+
    
      | Id|UserName|FirstName|LastName|               EMail|  AlternateEmail|         Phone|
    
      +---+--------+---------+--------+--------------------+----------------+--------------+
    
      |  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|+4411860297701|
    
      |  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|+4411860297702|
    
      |  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|+4411860297703|
    
      |  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|+4411860297704|
    
      |  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|+4411860297705|
    
      |  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|+4411860297706|
    
      |  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|+4411860297707|
    
      |  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|+4411860297708|
    
      |  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|+4411860297709|
    
      | 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|+4411860297710|
    
      +---+--------+---------+--------+--------------------+----------------+--------------+
    scala&gt; //Load the follower data into an Dataset
	scala&gt; val followerDataDS = sc.textFile(dataDir + "follower.txt").map(_.split("\\|")).map(toFollow(_)).toDS()
      followerDataDS: org.apache.spark.sql.Dataset[Follow] = [Follower: string, Followed: string]
    scala&gt; //Convert the Dataset into data frame
	scala&gt; val followerDataDF = followerDataDS.toDF()
      followerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]
    scala&gt; followerDataDF.createOrReplaceTempView("follow")
	scala&gt; followerDataDF.show()
      +--------+--------+
    
      |Follower|Followed|
    
      +--------+--------+
    
      | mthomas|mithomas|
    
      | mthomas|  mtwain|
    
      |  thardy| wbryson|
    
      |   wbrad| wbryson|
    
      | eharris| mthomas|
    
      | eharris|   tcook|
    
      | arobert|  jjames|
    
      +--------+--------+
    scala&gt; //Load the message data into an Dataset
	scala&gt; val messageDataDS = sc.textFile(dataDir + "message.txt").map(_.split("\\|")).map(toMessage(_)).toDS()
      messageDataDS: org.apache.spark.sql.Dataset[Message] = [UserName: string, MessageId: bigint ... 2 more fields]
    scala&gt; //Convert the Dataset into data frame
	scala&gt; val messageDataDF = messageDataDS.toDF()
      messageDataDF: org.apache.spark.sql.DataFrame = [UserName: string, MessageId: bigint ... 2 more fields]
    scala&gt; messageDataDF.createOrReplaceTempView("message")
	scala&gt; messageDataDF.show()
      +--------+---------+--------------------+----------+
    
      |UserName|MessageId|        ShortMessage| Timestamp|
    
      +--------+---------+--------------------+----------+
    
      | mthomas|        1|@mithomas Your po...|1459009608|
    
      | mthomas|        2|Feeling awesome t...|1459010608|
    
      |  mtwain|        3|My namesake in th...|1459010776|
    
      |  mtwain|        4|Started the day w...|1459011016|
    
      |  thardy|        5|It is just spring...|1459011199|
    
      | wbryson|        6|Some days are rea...|1459011256|
    
      |   wbrad|        7|@wbryson Stuff ha...|1459011333|
    
      | eharris|        8|Anybody knows goo...|1459011426|
    
      |   tcook|        9|Stock market is p...|1459011483|
    
      |   tcook|       10|Dont do day tradi...|1459011539|
    
      |   tcook|       11|I have never hear...|1459011622|
    
      |   wbrad|       12|#Barcelona has pl...|1459157132|
    
      |  mtwain|       13|@wbryson It is go...|1459164906|
    
      +--------+---------+--------------------+----------+
    </strong></span>
</pre><p>These steps complete the process of loading all the required data from persistent stores into DataFrames. Here, the data comes from text files. In real-world use cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or from Avro or Parquet serialized data stores loaded from HDFS.</p><p>The following section uses these DataFrames and creates various purposed views and queries:</p><pre class="programlisting">
<span class="strong"><strong>
	scala&gt; //Create the purposed view of the message to users
	scala&gt; val messagetoUsersDS = messageDataDS.filter(_.ShortMessage.contains("@")).map(message =&gt; (message.ShortMessage.split(" ").filter(_.contains("@")).mkString(" ").substring(1), message)).map(msgTuple =&gt; MessageToUsers(msgTuple._2.UserName, msgTuple._1, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))
      messagetoUsersDS: org.apache.spark.sql.Dataset[MessageToUsers] = [FromUserName: string, ToUserName: string ... 3 more fields]
    
	scala&gt; //Convert the Dataset into data frame
	scala&gt; val messagetoUsersDF = messagetoUsersDS.toDF()
      messagetoUsersDF: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]
    
	scala&gt; messagetoUsersDF.createOrReplaceTempView("messageToUsers")
	scala&gt; messagetoUsersDF.show()
      +------------+----------+---------+--------------------+----------+
    
      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|
    
      +------------+----------+---------+--------------------+----------+
    
      |     mthomas|  mithomas|        1|@mithomas Your po...|1459009608|
    
      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|
    
      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|
    
      +------------+----------+---------+--------------------+----------+
    scala&gt; //Create the purposed view of tagged messages 
	scala&gt; val taggedMessageDS = messageDataDS.filter(_.ShortMessage.contains("#")).map(message =&gt; (message.ShortMessage.split(" ").filter(_.contains("#")).mkString(" "), message)).map(msgTuple =&gt; TaggedMessage(msgTuple._1, msgTuple._2.UserName, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))
      taggedMessageDS: org.apache.spark.sql.Dataset[TaggedMessage] = [HashTag: string, UserName: string ... 3 more fields]
    
	scala&gt; //Convert the Dataset into data frame
	scala&gt; val taggedMessageDF = taggedMessageDS.toDF()
      taggedMessageDF: org.apache.spark.sql.DataFrame = [HashTag: string, UserName: string ... 3 more fields]
    
	scala&gt; taggedMessageDF.createOrReplaceTempView("taggedMessages")
	scala&gt; taggedMessageDF.show()
      +----------+--------+---------+--------------------+----------+
    
      |   HashTag|UserName|MessageId|        ShortMessage| Timestamp|
    
      +----------+--------+---------+--------------------+----------+
    
      |#Barcelona| eharris|        8|Anybody knows goo...|1459011426|
    
      |#Barcelona|   wbrad|       12|#Barcelona has pl...|1459157132|
    
      +----------+--------+---------+--------------------+----------+
    
	scala&gt; //The following are the queries given in the use cases
	scala&gt; //Find the messages that are grouped by a given hash tag
	scala&gt; val byHashTag = spark.sql("SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp")
      byHashTag: org.apache.spark.sql.DataFrame = [UserName: string, FirstName: string ... 4 more fields]
    
	scala&gt; byHashTag.show()
      +--------+---------+--------+---------+--------------------+----------+
    
      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|
    
      +--------+---------+--------+---------+--------------------+----------+
    
      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|
    
      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|
    
      +--------+---------+--------+---------+--------------------+----------+
    
	scala&gt; //Find the messages that are addressed to a given user
	scala&gt; val byToUser = spark.sql("SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp")
      byToUser: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]
    
	scala&gt; byToUser.show()
      +------------+----------+---------+--------------------+----------+
    
      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|
    
      +------------+----------+---------+--------------------+----------+
    
      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|
    
      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|
    
      +------------+----------+---------+--------------------+----------+
    scala&gt; //Find the followers of a given user
	scala&gt; val followers = spark.sql("SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'")
      followers: org.apache.spark.sql.DataFrame = [FollowerFirstName: string, FollowerLastName: string ... 1 more field]
    scala&gt; followers.show()
      +-----------------+----------------+--------+
    
      |FollowerFirstName|FollowerLastName|Followed|
    
      +-----------------+----------------+--------+
    
      |          William|        Bradford| wbryson|
    
      |           Thomas|           Hardy| wbryson|
    
      +-----------------+----------------+--------+
    
	scala&gt; //Find the followedUsers of a given user
	scala&gt; val followedUsers = spark.sql("SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'")
      followedUsers: org.apache.spark.sql.DataFrame = [FollowedFirstName: string, FollowedLastName: string ... 1 more field]
    scala&gt; followedUsers.show()
      +-----------------+----------------+--------+
    
      |FollowedFirstName|FollowedLastName|Follower|
    
      +-----------------+----------------+--------+
    
      |           Thomas|            Cook| eharris|
    
      |             Mark|          Thomas| eharris|
    
      +-----------------+----------------+--------+
    </strong></span>
</pre><p>In the preceding Scala code snippet, the dataset and DataFrame-based programming model is being used because the programming language of choice was Scala. Now, since Python is not a strongly typed language, the Dataset API is not supported in Python, hence the following Python code uses the traditional RDD-based programming model of Spark in conjunction with the DataFrame-based programming model. At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>
	&gt;&gt;&gt; from pyspark.sql import Row
	&gt;&gt;&gt; #TODO: Change the following directory to point to your data directory
	&gt;&gt;&gt; dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
	&gt;&gt;&gt; #Load the user data into an RDD
	&gt;&gt;&gt; userDataRDD = sc.textFile(dataDir + "user.txt").map(lambda line: line.split("|")).map(lambda p: Row(Id=int(p[0]), UserName=p[1], FirstName=p[2], LastName=p[3], EMail=p[4], AlternateEmail=p[5], Phone=p[6]))
	&gt;&gt;&gt; #Convert the RDD into data frame
	&gt;&gt;&gt; userDataDF = userDataRDD.toDF()
	&gt;&gt;&gt; userDataDF.createOrReplaceTempView("user")
	&gt;&gt;&gt; userDataDF.show()
      +----------------+--------------------+---------+---+--------+--------------+--------+
    
      |  AlternateEmail|               EMail|FirstName| Id|LastName|         Phone|UserName|
    
      +----------------+--------------------+---------+---+--------+--------------+--------+
    
      |mt12@example.com| mthomas@example.com|     Mark|  1|  Thomas|+4411860297701| mthomas|
    
      | mit@example.com|mithomas@example.com|  Michael|  2|  Thomas|+4411860297702|mithomas|
    
      | mtw@example.com|  mtwain@example.com|     Mark|  3|   Twain|+4411860297703|  mtwain|
    
      |  th@example.com|  thardy@example.com|   Thomas|  4|   Hardy|+4411860297704|  thardy|
    
      |  bb@example.com| wbryson@example.com|  William|  5|  Bryson|+4411860297705| wbryson|
    
      |  wb@example.com|   wbrad@example.com|  William|  6|Bradford|+4411860297706|   wbrad|
    
      |  eh@example.com| eharris@example.com|       Ed|  7|  Harris|+4411860297707| eharris|
    
      |  tk@example.com|   tcook@example.com|   Thomas|  8|    Cook|+4411860297708|   tcook|
    
      |  ar@example.com| arobert@example.com|     Adam|  9|  Robert|+4411860297709| arobert|
    
      |  jj@example.com|  jjames@example.com|    Jacob| 10|   James|+4411860297710|  jjames|
    
      +----------------+--------------------+---------+---+--------+--------------+--------+
    
	&gt;&gt;&gt; #Load the follower data into an RDD
	&gt;&gt;&gt; followerDataRDD = sc.textFile(dataDir + "follower.txt").map(lambda line: line.split("|")).map(lambda p: Row(Follower=p[0], Followed=p[1]))
	&gt;&gt;&gt; #Convert the RDD into data frame
	&gt;&gt;&gt; followerDataDF = followerDataRDD.toDF()
	&gt;&gt;&gt; followerDataDF.createOrReplaceTempView("follow")
	&gt;&gt;&gt; followerDataDF.show()
      +--------+--------+
    
      |Followed|Follower|
    
      +--------+--------+
    
      |mithomas| mthomas|
    
      |  mtwain| mthomas|
    
      | wbryson|  thardy|
    
      | wbryson|   wbrad|
    
      | mthomas| eharris|
    
      |   tcook| eharris|
    
      |  jjames| arobert|
    
      +--------+--------+
    
	&gt;&gt;&gt; #Load the message data into an RDD
	&gt;&gt;&gt; messageDataRDD = sc.textFile(dataDir + "message.txt").map(lambda line: line.split("|")).map(lambda p: Row(UserName=p[0], MessageId=int(p[1]), ShortMessage=p[2], Timestamp=int(p[3])))
	&gt;&gt;&gt; #Convert the RDD into data frame
	&gt;&gt;&gt; messageDataDF = messageDataRDD.toDF()
	&gt;&gt;&gt; messageDataDF.createOrReplaceTempView("message")
	&gt;&gt;&gt; messageDataDF.show()
      +---------+--------------------+----------+--------+
    
      |MessageId|        ShortMessage| Timestamp|UserName|
    
      +---------+--------------------+----------+--------+
    
      |        1|@mithomas Your po...|1459009608| mthomas|
    
      |        2|Feeling awesome t...|1459010608| mthomas|
    
      |        3|My namesake in th...|1459010776|  mtwain|
    
      |        4|Started the day w...|1459011016|  mtwain|
    
      |        5|It is just spring...|1459011199|  thardy|
    
      |        6|Some days are rea...|1459011256| wbryson|
    
      |        7|@wbryson Stuff ha...|1459011333|   wbrad|
    
      |        8|Anybody knows goo...|1459011426| eharris|
    
      |        9|Stock market is p...|1459011483|   tcook|
    
      |       10|Dont do day tradi...|1459011539|   tcook|
    
      |       11|I have never hear...|1459011622|   tcook|
    
      |       12|#Barcelona has pl...|1459157132|   wbrad|
    
      |       13|@wbryson It is go...|1459164906|  mtwain|
    
      +---------+--------------------+----------+--------+
    </strong></span>
</pre><p>These steps complete the process of loading all the required data from persistent stores into DataFrames. Here, the data comes from text files. In real-world use cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or from Avro or Parquet serialized data stores loaded from HDFS. The following section uses these DataFrames and creates various purposed views and queries:</p><pre class="programlisting">
<span class="strong"><strong>
	&gt;&gt;&gt; #Create the purposed view of the message to users
	&gt;&gt;&gt; messagetoUsersRDD = messageDataRDD.filter(lambda message: "@" in message.ShortMessage).map(lambda message : (message, " ".join(filter(lambda s: s[0] == '@', message.ShortMessage.split(" "))))).map(lambda msgTuple: Row(FromUserName=msgTuple[0].UserName, ToUserName=msgTuple[1][1:], MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))
	&gt;&gt;&gt; #Convert the RDD into data frame
	&gt;&gt;&gt; messagetoUsersDF = messagetoUsersRDD.toDF()
	&gt;&gt;&gt; messagetoUsersDF.createOrReplaceTempView("messageToUsers")
	&gt;&gt;&gt; messagetoUsersDF.show()
      +------------+---------+--------------------+----------+----------+
    
      |FromUserName|MessageId|        ShortMessage| Timestamp|ToUserName|
    
      +------------+---------+--------------------+----------+----------+
    
      |     mthomas|        1|@mithomas Your po...|1459009608|  mithomas|
    
      |       wbrad|        7|@wbryson Stuff ha...|1459011333|   wbryson|
    
      |      mtwain|       13|@wbryson It is go...|1459164906|   wbryson|
    
      +------------+---------+--------------------+----------+----------+
    
	&gt;&gt;&gt; #Create the purposed view of tagged messages 
	&gt;&gt;&gt; taggedMessageRDD = messageDataRDD.filter(lambda message: "#" in message.ShortMessage).map(lambda message : (message, " ".join(filter(lambda s: s[0] == '#', message.ShortMessage.split(" "))))).map(lambda msgTuple: Row(HashTag=msgTuple[1], UserName=msgTuple[0].UserName, MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))
	&gt;&gt;&gt; #Convert the RDD into data frame
	&gt;&gt;&gt; taggedMessageDF = taggedMessageRDD.toDF()
	&gt;&gt;&gt; taggedMessageDF.createOrReplaceTempView("taggedMessages")
	&gt;&gt;&gt; taggedMessageDF.show()
      +----------+---------+--------------------+----------+--------+
    
      |   HashTag|MessageId|        ShortMessage| Timestamp|UserName|
    
      +----------+---------+--------------------+----------+--------+
    
      |#Barcelona|        8|Anybody knows goo...|1459011426| eharris|
    
      |#Barcelona|       12|#Barcelona has pl...|1459157132|   wbrad|
    
      +----------+---------+--------------------+----------+--------+
    
	&gt;&gt;&gt; #The following are the queries given in the use cases
	&gt;&gt;&gt; #Find the messages that are grouped by a given hash tag
	&gt;&gt;&gt; byHashTag = spark.sql("SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp")
	&gt;&gt;&gt; byHashTag.show()
      +--------+---------+--------+---------+--------------------+----------+
    
      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|
    
      +--------+---------+--------+---------+--------------------+----------+
    
      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|
    
      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|
    
      +--------+---------+--------+---------+--------------------+----------+
    
	&gt;&gt;&gt; #Find the messages that are addressed to a given user
	&gt;&gt;&gt; byToUser = spark.sql("SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp")
	&gt;&gt;&gt; byToUser.show()
      +------------+----------+---------+--------------------+----------+
    
      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|
    
      +------------+----------+---------+--------------------+----------+
    
      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|
    
      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|
    
      +------------+----------+---------+--------------------+----------+
    
	&gt;&gt;&gt; #Find the followers of a given user
	&gt;&gt;&gt; followers = spark.sql("SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'")&gt;&gt;&gt; followers.show()
      +-----------------+----------------+--------+
    
      |FollowerFirstName|FollowerLastName|Followed|
    
      +-----------------+----------------+--------+
    
      |          William|        Bradford| wbryson|
    
      |           Thomas|           Hardy| wbryson|
    
      +-----------------+----------------+--------+
    
	&gt;&gt;&gt; #Find the followed users of a given user
	&gt;&gt;&gt; followedUsers = spark.sql("SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'")
	&gt;&gt;&gt; followedUsers.show()
      +-----------------+----------------+--------+
    
      |FollowedFirstName|FollowedLastName|Follower|
    
      +-----------------+----------------+--------+
    
      |           Thomas|            Cook| eharris|
    
      |             Mark|          Thomas| eharris|
    </strong></span>
<span class="strong"><strong>
      +-----------------+----------------+--------+
   </strong></span>
</pre><p>The purposed views and queries required to implement the use cases are developed as a single application. But in reality, it is not a good design practice to have all the views and queries in one application. It is good to separate them by persisting the views and refreshing them at regular intervals. If using only one application, caching and the use of custom-made context objects that are broadcasted to the Spark cluster could be employed to access the views.</p></div>
<div class="section" title="Understanding custom data processes"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec82"/>Understanding custom data processes</h1></div></div></div><p>The views created here were created to serve various queries and to produce desired outputs. There are some other classes of data processing applications that are often developed to implement real-world use cases. From the Lambda Architecture perspective, this also falls into the serving layer. The reason why these custom data processes fall into the serving layer is mainly because most of these use or process data from the master dataset and create views or outputs. It is also very possible for the custom processed data to remain as a view, and the following use case is one of such cases.</p><p>In the SfbMicroBlog microblogging application, it is a very common requirement to see whether a given user A is in some way connected to user B in a direct follower relationship or in a transitive way. This use case can be implemented using a graph data structure to see whether the two users in question are in the same connected component, whether they are connected in a transitive way, or whether they are not connected at all. For this, a graph is constructed with all the users as the vertices and the follow relationship as edges using a Spark GraphX library-based Spark application. At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>
   scala&gt; import org.apache.spark.rdd.RDD
    import org.apache.spark.rdd.RDD    
	scala&gt; import org.apache.spark.graphx._
    import org.apache.spark.graphx._    
	scala&gt; //TODO: Change the following directory to point to your data directory
	scala&gt; val dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
dataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/
    
	scala&gt; //Define the case classes in Scala for the entities
	scala&gt; case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)
      defined class User
    
	scala&gt; case class Follow(Follower: String, Followed: String)
      defined class Follow
    
	scala&gt; case class ConnectedUser(CCId: Long, UserName: String)
      defined class ConnectedUser
    
	scala&gt; //Define the utility functions that are to be passed in the applications
	scala&gt; def toUser =  (line: Seq[String]) =&gt; User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))
      toUser: Seq[String] =&gt; User
    
	scala&gt; def toFollow =  (line: Seq[String]) =&gt; Follow(line(0), line(1))
      toFollow: Seq[String] =&gt; Follow
    
	scala&gt; //Load the user data into an RDD
	scala&gt; val userDataRDD = sc.textFile(dataDir + "user.txt").map(_.split("\\|")).map(toUser(_))
userDataRDD: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[160] at map at &lt;console&gt;:34
    
	scala&gt; //Convert the RDD into data frame
	scala&gt; val userDataDF = userDataRDD.toDF()
userDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]
    
	scala&gt; userDataDF.createOrReplaceTempView("user")
	scala&gt; userDataDF.show()
      +---+--------+---------+--------+-----------+----------------+--------------+
    
Id|UserName|FirstName|LastName| EMail|  AlternateEmail|   Phone|
    
      +---+--------+---------+--------+----------+-------------+--------------+
    
|  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|
+4411860297701|
    
|  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|
+4411860297702|
    
|  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|
+4411860297703|
    
|  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|
+4411860297704|
    
|  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|
+4411860297705|
    
|  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|
+4411860297706|
    
|  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|
+4411860297707|
    
|  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|
+4411860297708|
    
|  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|
+4411860297709|
    
| 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|
+4411860297710|    
      +---+--------+---------+--------+-------------+--------------+--------------+
    
	scala&gt; //Load the follower data into an RDD
	scala&gt; val followerDataRDD = sc.textFile(dataDir + "follower.txt").map(_.split("\\|")).map(toFollow(_))
followerDataRDD: org.apache.spark.rdd.RDD[Follow] = MapPartitionsRDD[168] at map at &lt;console&gt;:34
    
	scala&gt; //Convert the RDD into data frame
	scala&gt; val followerDataDF = followerDataRDD.toDF()
followerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]
    
	scala&gt; followerDataDF.createOrReplaceTempView("follow")
	scala&gt; followerDataDF.show()
      +--------+--------+
    
      |Follower|Followed|
    
      +--------+--------+
    
      | mthomas|mithomas|
    
      | mthomas|  mtwain|
    
      |  thardy| wbryson|
    
      |   wbrad| wbryson|
    
      | eharris| mthomas|
    
      | eharris|   tcook|
    
      | arobert|  jjames|
    
      +--------+--------+
    
	scala&gt; //By joining with the follower and followee users with the master user data frame for extracting the unique ids
	scala&gt; val fullFollowerDetails = spark.sql("SELECT b.Id as FollowerId, c.Id as FollowedId, a.Follower, a.Followed FROM follow a, user b, user c WHERE a.Follower = b.UserName AND a.Followed = c.UserName")
fullFollowerDetails: org.apache.spark.sql.DataFrame = [FollowerId: bigint, FollowedId: bigint ... 2 more fields]
    
	scala&gt; fullFollowerDetails.show()
      +----------+----------+--------+--------+
    
      |FollowerId|FollowedId|Follower|Followed|
    
      +----------+----------+--------+--------+
    
      |         9|        10| arobert|  jjames|
    
      |         1|         2| mthomas|mithomas|
    
      |         7|         8| eharris|   tcook|
    
      |         7|         1| eharris| mthomas|
    
      |         1|         3| mthomas|  mtwain|
    
      |         6|         5|   wbrad| wbryson|
    
      |         4|         5|  thardy| wbryson|
    
      +----------+----------+--------+--------+
    
	scala&gt; //Create the vertices of the connections graph
	scala&gt; val userVertices: RDD[(Long, String)] = userDataRDD.map(user =&gt; (user.Id, user.UserName))
userVertices: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[194] at map at &lt;console&gt;:36
    
	scala&gt; userVertices.foreach(println)
      (6,wbrad)
    
      (7,eharris)
    
      (8,tcook)
    
      (9,arobert)
    
      (10,jjames)
    
      (1,mthomas)
    
      (2,mithomas)
    
      (3,mtwain)
    
      (4,thardy)
    
      (5,wbryson)
    
	scala&gt; //Create the edges of the connections graph 
	scala&gt; val connections: RDD[Edge[String]] = fullFollowerDetails.rdd.map(conn =&gt; Edge(conn.getAs[Long]("FollowerId"), conn.getAs[Long]("FollowedId"), "Follows"))
      connections: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = MapPartitionsRDD[217] at map at &lt;console&gt;:29
    
	scala&gt; connections.foreach(println)
	Edge(9,10,Follows)
	Edge(7,8,Follows)
	Edge(1,2,Follows)
	Edge(7,1,Follows)
	Edge(1,3,Follows)
	Edge(6,5,Follows)
	Edge(4,5,Follows)
	scala&gt; //Create the graph using the vertices and the edges
	scala&gt; val connectionGraph = Graph(userVertices, connections)
      connectionGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@3c207acd
    </strong></span>
</pre><p>
	The user graph with the users in the vertices and the connection relationship forming the edges is done. On this graph data structure, run the graph processing algorithm, the connected component algorithm. The following code snippet does this:</p><pre class="programlisting">
<span class="strong"><strong>
	scala&gt; //Calculate the connected users
	scala&gt; val cc = connectionGraph.connectedComponents()
      cc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,String] = org.apache.spark.graphx.impl.GraphImpl@73f0bd11
    
	scala&gt; // Extract the triplets of the connected users
	scala&gt; val ccTriplets = cc.triplets
      ccTriplets: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,String]] = MapPartitionsRDD[285] at mapPartitions at GraphImpl.scala:48
    
	scala&gt; // Print the structure of the triplets
	scala&gt; ccTriplets.foreach(println)
      ((9,9),(10,9),Follows)
    
      ((1,1),(2,1),Follows)
    
      ((7,1),(8,1),Follows)
    
      ((7,1),(1,1),Follows)
    
      ((1,1),(3,1),Follows)
    
      ((4,4),(5,4),Follows)
    </strong></span>
<span class="strong"><strong>
      ((6,4),(5,4),Follows)
   </strong></span>
</pre><p>The connected component graph, <code class="literal">cc</code>, and its triplets, <code class="literal">ccTriplets</code>, are created, and this can now be used to run various queries. Since the graph is an RDD-based data structure, if it is necessary to do queries, converting the graph RDD to DataFrames is a common practice. The following code demonstrates this:</p><pre class="programlisting">
<span class="strong"><strong>
   scala&gt; //Print the vertex numbers and the corresponding connected component id. The connected component id is generated by the system and it is to be taken only as a unique identifier for the connected component
   scala&gt; val ccProperties = ccTriplets.map(triplet =&gt; "Vertex " + triplet.srcId + " and " + triplet.dstId + " are part of the CC with id " + triplet.srcAttr)
      ccProperties: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[288] at map at &lt;console&gt;:48
    
	scala&gt; ccProperties.foreach(println)
      Vertex 9 and 10 are part of the CC with id 9
    
      Vertex 1 and 2 are part of the CC with id 1
    
      Vertex 7 and 8 are part of the CC with id 1
    
      Vertex 7 and 1 are part of the CC with id 1
    
      Vertex 1 and 3 are part of the CC with id 1
    
      Vertex 4 and 5 are part of the CC with id 4
    
      Vertex 6 and 5 are part of the CC with id 4
    
	scala&gt; //Find the users in the source vertex with their CC id
	scala&gt; val srcUsersAndTheirCC = ccTriplets.map(triplet =&gt; (triplet.srcId, triplet.srcAttr))
      srcUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[289] at map at &lt;console&gt;:48
    
	scala&gt; //Find the users in the destination vertex with their CC id
	scala&gt; val dstUsersAndTheirCC = ccTriplets.map(triplet =&gt; (triplet.dstId, triplet.dstAttr))
      dstUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[290] at map at &lt;console&gt;:48
    
	scala&gt; //Find the union
	scala&gt; val usersAndTheirCC = srcUsersAndTheirCC.union(dstUsersAndTheirCC)
      usersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = UnionRDD[291] at union at &lt;console&gt;:52
    
	scala&gt; //Join with the name of the users
	scala&gt; //Convert the RDD to DataFrame
	scala&gt; val usersAndTheirCCWithName = usersAndTheirCC.join(userVertices).map{case (userId,(ccId,userName)) =&gt; (ccId, userName)}.distinct.sortByKey().map{case (ccId,userName) =&gt; ConnectedUser(ccId, userName)}.toDF()
      usersAndTheirCCWithName: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string]
    
	scala&gt; usersAndTheirCCWithName.createOrReplaceTempView("connecteduser")
	scala&gt; val usersAndTheirCCWithDetails = spark.sql("SELECT a.CCId, a.UserName, b.FirstName, b.LastName FROM connecteduser a, user b WHERE a.UserName = b.UserName ORDER BY CCId")
      usersAndTheirCCWithDetails: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string ... 2 more fields]
    
	scala&gt; //Print the usernames with their CC component id. If two users share the same CC id, then they are connected
	scala&gt; usersAndTheirCCWithDetails.show()
      +----+--------+---------+--------+
    
      |CCId|UserName|FirstName|LastName|
    
      +----+--------+---------+--------+
    
      |   1|mithomas|  Michael|  Thomas|
    
      |   1|  mtwain|     Mark|   Twain|
    
      |   1|   tcook|   Thomas|    Cook|
    
      |   1| eharris|       Ed|  Harris|
    
      |   1| mthomas|     Mark|  Thomas|
    
      |   4|   wbrad|  William|Bradford|
    
      |   4| wbryson|  William|  Bryson|
    
      |   4|  thardy|   Thomas|   Hardy|
    
      |   9|  jjames|    Jacob|   James|
    
      |   9| arobert|     Adam|  Robert|
    </strong></span>
<span class="strong"><strong>
      +----+--------+---------+--------+
   </strong></span>
</pre><p>Using the preceding implementation of a purposed view to get a list of users and their connected component identification numbers, if there is a need to find out whether two users are connected, just read the records of those two users and see whether they have the same connected component identification number.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec83"/>References</h1></div></div></div><p>For more information, visit the following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://lambda-architecture.net/">http://lambda-architecture.net/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf">https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf</a></li></ul></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec84"/>Summary</h1></div></div></div><p>This chapter concludes the book with one single application's use cases, implemented using the Spark concepts learned in the earlier chapters of the book. From a data processing application architecture perspective, this chapter covered the Lambda Architecture as a technology-agnostic architectural framework for data processing applications, which has huge applicability in the big data application development space.</p><p>From a data processing application development perspective, RDD-based Spark programming, Dataset-based Spark programming, Spark SQL-based DataFrames to process structured data, the Spark Streaming-based listener program that constantly listens to the incoming messages and processes them, and the Spark GraphX-based application to process follower relationships have been covered. The use cases covered so far have immense scope for readers to add their own functionalities and enhance the application use cases discussed in this chapter.</p></div></body></html>