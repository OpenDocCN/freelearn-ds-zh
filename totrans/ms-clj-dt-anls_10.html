<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Modeling Stock Data</h1></div></div></div><p>Automated stock analysis<a id="id792" class="indexterm"/> has gotten a lot of press recently. High-frequency trading firms are a flashpoint. People either believe that they're great for the markets and increasing liquidity, or that they're precursors to the apocalypse. Smaller traders have also gotten into the mix in a slower fashion. Some sites, such as <a id="id793" class="indexterm"/>Quantopian (<a class="ulink" href="https://www.quantopian.com/">https://www.quantopian.com/</a>) and <a id="id794" class="indexterm"/>AlgoTrader (<a class="ulink" href="http://www.algotrader.ch/">http://www.algotrader.ch/</a>) provide services that allow you to create models for automated trading. Many others allow you to use automated analysis to inform your trading decisions.</p><p>Whatever your view of this phenomena, it's an area with a lot of data begging to be analyzed. It's also a nice domain in which to experiment with some analysis and machine learning techniques.</p><p>For this chapter, we're going to look for relationships between news articles and stock prices in the future.</p><p>In the course of this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learn about financial data analysis</li><li class="listitem" style="list-style-type: disc">Set up our project and acquire our data</li><li class="listitem" style="list-style-type: disc">Prepare the data</li><li class="listitem" style="list-style-type: disc">Analyze the text</li><li class="listitem" style="list-style-type: disc">Analyze the stock prices</li><li class="listitem" style="list-style-type: disc">Learn patterns in both text and stock prices with neural networks</li><li class="listitem" style="list-style-type: disc">Use this system to predict the future</li><li class="listitem" style="list-style-type: disc">Talk about the limitations of these systems</li></ul></div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec61"/>Learning about financial data analysis</h1></div></div></div><p>Finance has always relied heavily on data. Earnings statements, forecasting, and portfolio management are just some of the areas that make use of data to quantify their decisions. Because of this, financial data analysis and its related field, financial engineering, are extremely broad fields that are difficult to summarize in a short amount of space.</p><p>However, lately, <a id="id795" class="indexterm"/>quantitative finance, high-frequency trading, and similar fields have gotten a lot of press and really come into their own. As I mentioned, some people hate them and the added volatility that the markets seem to have. Others maintain that they bring the necessary liquidity that helps the market function better.</p><p>All of these fields apply statistical or machine learning methods to financial data. Some of these techniques can be quite simple. Others are more sophisticated. Some of these analyses are used to inform a human analyst or manager to make better financial decisions. Others are used as inputs to automated algorithmic processes that operate with varying degrees of human oversight, but perhaps with little to no intervention.</p><p>For this chapter, we'll focus on adding information to the human analyst's repertoire. We'll develop a simple machine learning system to look at past, current, and future stock prices, alongside the text of news articles, in order to identify potentially interesting articles that may indicate future fluctuations in stock price. These articles, with the possible future price vector, could provide important information to an investor or analyst attempting to decide how to shuffle his/her money around. We'll talk more about the purpose and limitations of this system toward the end of the chapter.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec62"/>Setting up the basics</h1></div></div></div><p>Before we really dig into the<a id="id796" class="indexterm"/> project and the data, we need to prepare. We'll set up the code and the library, and then we'll download the data.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec77"/>Setting up the library</h2></div></div></div><p>First, we'll need to<a id="id797" class="indexterm"/> initialize the library. We can do this using Leiningen 2<a id="id798" class="indexterm"/> (<a class="ulink" href="http://leiningen.org/">http://leiningen.org/</a>) and Stuart Sierra's reloaded plugin for it (<a class="ulink" href="http://https://github.com/stuartsierra/reloaded">https://github.com/stuartsierra/reloaded</a>). This will initialize the development environment and project.</p><p>To do this, just execute the following command at the prompt (I've named the project <code class="literal">financial</code> in this case):</p><div><pre class="programlisting">
<strong>lein new reloaded financial</strong>
</pre></div><p>Now, we can specify the libraries that we'll need to use. We can do this in the <code class="literal">project.clj</code> file. Open it and replace its current contents with the following lines:</p><div><pre class="programlisting">(defproject financial "0.1.0-SNAPSHOT":dependencies [[org.clojure/clojure "1.5.1"][org.clojure/data.xml "0.0.7"][org.clojure/data.csv "0.1.2"][clj-time "0.6.0"][me.raynes/fs "1.4.4"][org.encog/encog-core "3.1.0"][enclog "0.6.3"]]:profiles
  {:dev {:dependencies [[org.clojure/tools.namespace "0.2.4"]]
            :source-paths ["dev"]}})</pre></div><p>The primary library that we'll use is<a id="id799" class="indexterm"/> Enclog (<a class="ulink" href="https://github.com/jimpil/enclog">https://github.com/jimpil/enclog</a>). This is a Clojure <a id="id800" class="indexterm"/>wrapper around the Java library Encog (<a class="ulink" href="http://www.heatonresearch.com/encog">http://www.heatonresearch.com/encog</a>), which is a machine learning library, including classes for artificial neural networks.</p><p>We now have the basics in place. We can get the data at this point.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec78"/>Getting the data</h2></div></div></div><p>We'll need data from two<a id="id801" class="indexterm"/> different sources. To begin with, we'll focus on getting the stock data.</p><p>In this case, we're going to use the historical stock data for Dominion Resources, Inc. They're a power company that operates in the eastern United States. Their New York Stock Exchange symbol is D. Focusing on one stock like this will reduce possible noise and allow us to focus on the simple system that we'll be working on in this chapter.</p><p>To download the stock data, I went to Google Finance<a id="id802" class="indexterm"/> (<a class="ulink" href="https://finance.google.com/">https://finance.google.com/</a>). In the search box, I entered <code class="literal">NYSE:D</code>. On the left-hand side menu bar, there is an option to download <strong>Historical prices</strong>. Click on it.</p><p>In the table header, set the date range to be from <code class="literal">Sept 1, 1995</code> to <code class="literal">Jan 1, 2001</code>. Refer to the following screenshot as an example:</p><div><img src="img/4139OS_10_01.jpg" alt="Getting the data"/></div><p>If you look at the<a id="id803" class="indexterm"/> lower-right corner of the screenshot, there's a link that reads <strong>Download to spreadsheet</strong>. Click on this link to download the data. By default, the filename is <code class="literal">d.csv</code>. I moved it into a directory named <code class="literal">d</code> inside my project folder and renamed it to <code class="literal">d-1995-2001.csv</code>.</p><p>We'll also need some news article data to correlate with the stock data. Freely available news articles are difficult to come by. There are good corpora available for modest fees (several hundred dollars). However, in order to make this exercise as accessible as possible, I've limited the data to what's freely available.</p><p>At the moment, the best collection appears to be the journalism segment of the Open American National Corpus (<a class="ulink" href="http://www.anc.org/data/oanc/">http://www.anc.org/data/oanc/</a>). The <strong>American National Corpus</strong> (<strong>ANC</strong>)<a id="id804" class="indexterm"/> is a collection of texts from a variety of registers and genres that are assembled for linguistic research. The <strong>Open ANC</strong> (<strong>OANC</strong>)<a id="id805" class="indexterm"/> is the subset of the ANC that is available for open access downloading. The journalism genre is represented by articles from<a id="id806" class="indexterm"/> Slate (<a class="ulink" href="http://www.slate.com/">http://www.slate.com/</a>). This has some benefits and introduces some problems. The primary benefit is that the data will be quite manageable. It means that we won't have a lot of documents to use for training and testing, and we'll need to be pickier about what features we pull from the documents. We'll see how we need to handle this later.</p><p>To download the dataset, visit the<a id="id807" class="indexterm"/> download page at <a class="ulink" href="http://www.anc.org/data/oanc/download/">http://www.anc.org/data/oanc/download/</a> and get the data in your preferred format, either a TAR ball or a ZIP file. I decompressed that data into the <code class="literal">d</code> directory. It created a directory named <code class="literal">OANC-GrAF</code> that contained the data.</p><p>Your <code class="literal">d</code> directory should now look something as follows:</p><div><img src="img/4139OS_10_02.jpg" alt="Getting the data"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec63"/>Getting prepared with data</h1></div></div></div><p>As usual, now we <a id="id808" class="indexterm"/>need to clean up the data and put it into a shape that we can work with. The news article dataset particularly will require some attention, so let's turn our attention to it first.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec79"/>Working with news articles</h2></div></div></div><p>The OANC is<a id="id809" class="indexterm"/> published in an XML format that includes a lot of <a id="id810" class="indexterm"/>information and annotations about the data. Specifically, this marks off:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sections and chapters</li><li class="listitem" style="list-style-type: disc">Sentences</li><li class="listitem" style="list-style-type: disc">Words with part-of-speech lemma</li><li class="listitem" style="list-style-type: disc">Noun chunks</li><li class="listitem" style="list-style-type: disc">Verb chunks</li><li class="listitem" style="list-style-type: disc">Named entities</li></ul></div><p>However, we want the option to use raw text later when the system is actually being used. Because of that, we will ignore the annotations and just extract the raw tokens. In fact, all we're really interested in is each document's text—either as a raw string or a feature vector—and the date it was published. Let's create a record type for this.</p><p>We'll put this into the <code class="literal">types.clj</code> file in <code class="literal">src/financial/</code>. Put this simple namespace header into the file:</p><div><pre class="programlisting">(ns financial.types)</pre></div><p>This data record will be similarly simple. It can be defined as follows:</p><div><pre class="programlisting">(defrecord NewsArticle [title pub-date text])</pre></div><p>So let's see what the XML looks like and what we need to do to get it to work with the data structures we just defined.</p><p>The Slate data is in the <code class="literal">OANC-GrAF/data/written_1/journal/slate/</code> directory. The data files are spread through 55 subdirectories as follows:</p><div><pre class="programlisting">
<strong>$ ls d/OANC-GrAF/data/written_1/journal/slate/</strong>
<strong>. .. 1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 28 29 3 30 31 32</strong>
<strong>33 34 35 36 37 38 39 4 40 41 42 43 44 45 46 47 48 49 5 50 51 52 53 54 55 6 7 8</strong>
<strong>9</strong>
</pre></div><p>Digging in <a id="id811" class="indexterm"/>deeper, each document is represented by a<a id="id812" class="indexterm"/> number of files. From the <code class="literal">slate</code> directory, we can see the following details:</p><div><pre class="programlisting">
<strong>$ ls 1/Article247_99*</strong>
<strong>1/Article247_99-hepple.xml  1/Article247_99-s.xml       </strong>
<strong>1/Article247_99.txt</strong>
<strong>1/Article247_99-logical.xml 1/Article247_99-vp.xml</strong>
<strong>1/Article247_99-np.xml      1/Article247_99.anc</strong>
</pre></div><p>So we can see the different annotations files are the files with the <code class="literal">xml</code> extension. The ANC file contains metadata about the file. We'll need to access that file for the date and other information. But most importantly, there's also a <code class="literal">.txt</code> file containing the raw text of the document. That will make working with this dataset much easier!</p><p>But let's take a minute to write some functions that will help us work with each document's text and its metadata as an entity. These will represent the knowledge we've just gained about the directory and file structure of the OANC corpus.</p><p>We'll call this file <code class="literal">src/financial/oanc.clj</code>, and its namespace header should look as follows:</p><div><pre class="programlisting">(ns financial.oanc
  (:require [clojure.data.xml :as xml]
            [clojure.java.io :as io]
            [clojure.string :as str]
            [me.raynes.fs :as fs]
            [clj-time.core :as clj-time]
            [clj-time.format :as time-format])
  (:use [financial types utils]))</pre></div><p>If we examine the directory structure that the OANC uses, we can see that it's divided into a clear hierarchy. Let's trace that structure in the <code class="literal">slate</code> directory that we discussed earlier, <code class="literal">OANC-GrAF/data/written_1/journal/slate/</code>. In this example, <code class="literal">written_1</code> represents a category, <code class="literal">journal</code> is a genre, and <code class="literal">slate</code> is a source. We can leverage this information as we walk the directory structure to get to the data files.</p><p>Our first bit of code contains four functions. Let's list them first, and then we can talk about them:</p><div><pre class="programlisting">(defn list-category-genres [category-dir]
  (map #(hash-map :genre % :dirname (io/file category-dir %))
       (fs/list-dir category-dir)))
(defn list-genres [oanc-dir]
  (mapcat list-category-genres (ls (io/file oanc-dir "data"))))
(defn find-genre-dir [genre oanc-dir]
  (-&gt;&gt; oanc-dir
    list-genres
    (filter #(= (:genre %) genre))
    first
    :dirname))
(defn find-source-data [genre source oanc-dir]
  (-&gt; (find-genre-dir genre oanc-dir)
    (io/file source)
    (fs/find-files #".*\.anc")))</pre></div><p>The functions<a id="id813" class="indexterm"/> used in the preceding code are <a id="id814" class="indexterm"/>described as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first of these functions, <code class="literal">list-category-genre</code>, takes a category directory (<code class="literal">OANC-GrAF/data/written_1/</code>) and returns the genres that it contains. This could be <code class="literal">journal</code>, as in our example here, or fiction, letters, or a number of other options. Each item returned is a hash map of the full directory and the name of the genre.</li><li class="listitem" style="list-style-type: disc">The second function is <code class="literal">list-genres</code>. It lists all of the genres within the OANC data directory.</li><li class="listitem" style="list-style-type: disc">The third function is <code class="literal">find-genre-dir</code>. It looks for one particular genre and returns the full directory for it.</li><li class="listitem" style="list-style-type: disc">Finally, we have <code class="literal">find-source-data</code>. This takes a genre and source and lists all of the files with an <code class="literal">anc</code> extension.</li></ul></div><p>Using these functions, we can iterate over the documents for a source. We can see how to do that in the next function, <code class="literal">find-slate-files</code>, which returns a sequence of maps pointing to each document's metadata ANC file and to its raw text file, as shown in the following code:</p><div><pre class="programlisting">(defn find-slate-files [oanc-dir]
  (map #(hash-map :anc % :txt (chext % ".txt"))
       (find-source-data "journal" "slate" oanc-dir)))</pre></div><p>Now we can get at the metadata in the ANC file. We'll use the <code class="literal">clojure.data.xml</code> library to parse the file, and we'll define a couple of utility functions to make descending into the file easier. Look at the following code:</p><div><pre class="programlisting">(defn find-all [xml tag-name]
  (lazy-seq
    (if (= (:tag xml) tag-name)
      (cons xml (mapcat #(find-all % tag-name) (:content xml)))
      (mapcat #(find-all % tag-name) (:content xml)))))
(defn content-str [xml]
  (apply str (filter string? (:content xml))))</pre></div><p>The first utility function, <code class="literal">find-all</code>, lazily walks the XML document and returns all elements with a given tag name. The second function, <code class="literal">content-str</code>, returns all the text children of a tag.</p><p>Also, we'll need to parse the date from the <code class="literal">pubDate</code> elements. Some of these have a <code class="literal">value</code> attribute, but this isn't consistent. Instead, we'll parse the elements' content directly using the <code class="literal">clj-time</code> library (<a class="ulink" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>), which is a wrapper over the Joda time library for Java (<a class="ulink" href="http://joda-time.sourceforge.net/">http://joda-time.sourceforge.net/</a>). From our end, we'll use a few functions.</p><p>Before we do, <a id="id815" class="indexterm"/>though, we'll need to define a date <a id="id816" class="indexterm"/>format string. The dates inside the <code class="literal">pubDate</code> functions look like <em>2/13/97 4:30:00 PM</em>. The formatting string, then, should look as follows:</p><div><pre class="programlisting">(def date-time-format
     (time-format/formatter "M/d/yyyy h:mm:ss a"))</pre></div><p>We can use this formatter to pull data out of a <code class="literal">pubDate</code> element and parse it into an <code class="literal">org.joda.time.DateTime</code> object as follows:</p><div><pre class="programlisting">(defn parse-pub-date [pub-date-el]
  (time-format/parse date-time-format (content-str pub-date-el)))</pre></div><p>Unfortunately, some of these dates are about 2000 years off. We can normalize the dates and correct these errors fairly quickly, as shown in the following code:</p><div><pre class="programlisting">(defn norm-date [date]
  (cond
    (= (clj-time/year date) 0)
      (clj-time/plus date (clj-time/years 2000))
    (&lt; (clj-time/year date) 100)
      (clj-time/plus date (clj-time/years 1900))
    :else date))</pre></div><p>With all of these parts in place, we can write a function that takes the XML from an ANC file and returns date and time for the publication date as follows:</p><div><pre class="programlisting">(defn find-pub-date [anc-xml]
  (-&gt; anc-xml
    (find-all :pubDate)
    first
    parse-pub-date
    norm-date))</pre></div><p>The other piece of data that we'll load from the ANC metadata XML is the title. We get that from the <code class="literal">title</code> element, of course, as follows:</p><div><pre class="programlisting">(defn find-title [anc-xml]
  (content-str (first (find-all anc-xml :title))))</pre></div><p>Now, loading a <code class="literal">NewsArticle</code> object is straightforward. In fact, it's so simple that we'll also include a version of this that reads in the text from a plain file. Look at the following code:</p><div><pre class="programlisting">(defn load-article [data-info]
  (let [{:keys [anc txt]} data-info
        anc-xml (xml/parse (io/reader anc))]
    (-&gt;NewsArticle (find-title anc-xml)
                   (find-pub-date anc-xml)
                   (slurp txt))))
(defn load-text-file [data filename]
  (-&gt;NewsArticle filename date (slurp filename)))</pre></div><p>And using these<a id="id817" class="indexterm"/> functions to load all of the Slate<a id="id818" class="indexterm"/> articles just involves repeating the earlier steps, as shown in the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def articles (doall (map oanc/load-article</strong>
<strong>                                 (oanc/find-slate-files</strong>
<strong>                                   (io/file "d/OANC-GrAF")))))</strong>
<strong>user=&gt; (count articles)</strong>
<strong>4531</strong>
<strong>user=&gt; (let [a (first articles)]</strong>
<strong>         [(:title a) (:pub-date a) (count (:text a))])</strong>
<strong>["Article247_4" #&lt;DateTime 1999-03-09T07:47:21.000Z&gt; 3662]</strong>
</pre></div><p>The last command in the preceding code just prints the title, publication date, and the length of the text in the document.</p><p>With these functions in place, we now have access to the article dataset.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec80"/>Working with stock data</h2></div></div></div><p>Loading the news articles was<a id="id819" class="indexterm"/> complicated. Fortunately, the stock price data is in <a id="id820" class="indexterm"/>
<strong>comma-separated values</strong> (<strong>CSV</strong>) format. Although not the richest data format, it is very popular, and <code class="literal">clojure.data.csv</code> (<a class="ulink" href="https://github.com/clojure/data.csv/">https://github.com/clojure/data.csv/</a>) is an excellent library for loading it.</p><p>As I just mentioned, though, <a id="id821" class="indexterm"/>CSV isn't the richest data format. We will want to convert this data into a richer format, so we'll still create a record type and some wrapper functions to make it easier to work with the data as we read it in.</p><p>The data in this will closely follow the columns in the CSV file that we downloaded from Google Finance earlier. Open <code class="literal">src/financial/types.clj</code> again and add the following line to represent the data type for the stock data:</p><div><pre class="programlisting">(defrecord StockData [date open high low close volume])</pre></div><p>For the rest of the code in this section, we'll use a new namespace. Open the <code class="literal">src/financial/cvs_data.clj</code> file and add the following namespace declaration:</p><div><pre class="programlisting">(ns financial.csv-data
  (:require [clojure.data.csv :as csv]
            [clojure.java.io :as io]
            [clj-time.core :as clj-time]
            [clj-time.format :as time-format])
  (:use [financial types utils]))</pre></div><p>Just like the Slate news article data, this data also has a field with a date, which we'll need to parse. Unlike the Slate data, this value is formatted differently. Glancing at the first few lines of the file gives us all the information that we need, as follows:</p><div><pre class="programlisting">Date,Open,High,Low,Close,Volume
29-Dec-00,33.47,33.56,33.09,33.50,857800
28-Dec-00,33.62,33.62,32.94,33.47,961200
27-Dec-00,33.50,33.97,33.19,33.56,992400
26-Dec-00,32.88,33.69,32.88,33.62,660600</pre></div><p>To parse dates in this format (29-Dec-00), we can use the following format specification:</p><div><pre class="programlisting">(def date-format (time-format/formatter "d-MMM-YY"))</pre></div><p>Now, we build on this and a few other function—which you can find in the code download in the file <code class="literal">src/financial/utils.clj</code>—to create a <code class="literal">StockData</code> instance from a row of data, as shown in the following code:</p><div><pre class="programlisting">(defn row-&gt;StockData [row]
  (let [[date open high low close vol] row]
    (-&gt;StockData (time-format/parse date-format date)
                 (-&gt;double open)
                 (-&gt;double high)
                 (-&gt;double low)
                 (-&gt;double close)
                 (-&gt;long vol))))</pre></div><p>This is all straightforward. Basically, every value in the row must be converted to a native Clojure/Java type, and then all of those values are used to create the <code class="literal">StockData</code> instance.</p><p>To read in an entire file, we just do this for every row returned by the CSV library as follows:</p><div><pre class="programlisting">(defn read-stock-prices [filename]
  (with-open [f (io/reader filename)]
    (doall (map row-&gt;StockData (drop 1 (csv/read-csv f))))))</pre></div><p>The only wrinkle is that we have to drop the first row, since it's the header.</p><p>And now, to load the data, we just call the following function (we've aliased the <code class="literal">financial.csv-data</code> namespace to <code class="literal">csvd</code>):</p><div><pre class="programlisting">
<strong>user=&gt; (def sp (csvd/read-stock-prices "d/d-1995-2001.csv"))</strong>
<strong>user=&gt; (first sp)</strong>
<strong>#financial.types.StockData{:date #&lt;DateTime 2000-12-29T00:00:00.000Z&gt;,</strong>
<strong>   :open 33.47, :high 33.56, :low 33.09, :close 33.5, :volume 857800}</strong>
<strong>user=&gt; (count sp)</strong>
<strong>1263</strong>
</pre></div><p>Everything <a id="id822" class="indexterm"/>appears to be working correctly. Let's turn our attention back to the news article dataset and begin analyzing it.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec64"/>Analyzing the text</h1></div></div></div><p>Our goal for analyzing the <a id="id823" class="indexterm"/>news articles is to generate a vector <a id="id824" class="indexterm"/>space model of the collection of documents. This attempts to pull the salient features for the documents into a vector of floating-point numbers. Features can be words or information from the documents' metadata encoded for the vector. The feature values can be 0 or 1 for presence, an integer for raw frequency, or the frequency scaled in some form.</p><p>In our case, we'll use the feature vector to represent a selection of the tokens in a document. Often, we can use all the tokens, or all the tokens that occur more than once or twice. However, in this case, we don't have a lot of data, so we'll need to be more selective in the features that we include. We'll consider how we select these in a few sections.</p><p>For the feature values, we'll use a scaled version of the token frequency called <strong>term frequency-inverse document frequency</strong> (<strong>tf-idf</strong>). There are <a id="id825" class="indexterm"/>good libraries for this, but this is a basic metric in working with free text data, so we'll take this algorithm apart and implement it ourselves for this chapter. That way, we'll understand it better.</p><p>For the rest of this section, we'll put the code into <code class="literal">src/financial/nlp.clj</code>. Open this file and add the following for the namespace header:</p><div><pre class="programlisting">(ns financial.nlp
  (:require [clojure.string :as str]
            [clojure.set :as set])
  (:use [financial types utils]))</pre></div><p>With this in place, <a id="id826" class="indexterm"/>we can<a id="id827" class="indexterm"/> now start to pick the documents apart.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec81"/>Analyzing vocabulary</h2></div></div></div><p>The first step for<a id="id828" class="indexterm"/> analyzing a <a id="id829" class="indexterm"/>document, of course, is <a id="id830" class="indexterm"/>tokenizing. We'll use a simple tokenize function that just pulls out sequences of letters or numbers, including any single punctuation marks.</p><p>Now, we can use this function to see what words are present in the text and how frequent they are. The core Clojure function, <code class="literal">frequencies</code>, makes this especially easy, but we do still need to pull out the data that we'll use.</p><p>For each step, we'll first work on raw input, and then we'll write an additional utility function that modifies the <code class="literal">:text</code> property of the input <code class="literal">NewsArticle</code>.</p><p>To tokenize the text, we'll search for the matches for a regular expression and convert the output to lowercase. This won't work well for a lot of cases—contractions, abbreviations, and hyphenations in English, for example—but it will take care of simple needs. Look at the following code:</p><div><pre class="programlisting">(defn tokenize [string]
  (map str/lower-case (re-seq #"[\p{L}\p{M}]+" string)))
(defn tokenize-text [m] (update-in m [:text] tokenize))</pre></div><p>The actual tokenization is handled by the <code class="literal">tokenize</code> function. The <code class="literal">tokenize-text</code> function takes a <code class="literal">NewsArticle</code> instance and replaces its raw text property with the sequence of tokens generated from the text.</p><p>The function <code class="literal">token-freqs</code> replaces the sequence of tokens with a mapping of their frequencies. It uses the Clojure core function frequencies as shown in the following code:</p><div><pre class="programlisting">(defn token-freqs [m] (update-in m [:text] frequencies))</pre></div><p>We can then take a sequence of <code class="literal">NewsArticle</code> instances that contain the token frequencies and generate the frequencies for the entire corpus. The function <code class="literal">corpus-freqs</code> takes care of that. Look at the following code:</p><div><pre class="programlisting">(defn corpus-freqs [coll]
  (reduce #(merge-with + %1 %2) {} (map :text coll)))</pre></div><p>Let's use the <a id="id831" class="indexterm"/>following functions to get the <a id="id832" class="indexterm"/>frequencies:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We'll get the tokens for each article. Then we'll print out the first ten tokens from the first article, as follows:<div><pre class="programlisting">
<strong>user=&gt; (def tokens (map nlp/tokenize-text articles))</strong>
<strong>user=&gt; (take 10 (:text (first tokens)))</strong>
<strong>("harmonic" "convergences" "you" "re" "right" "maxim" "s" "strong" "point" "is")</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">Now, we'll get the frequencies of the tokens in each document and print out ten of the token-frequency pairs from the first document, as follows:<div><pre class="programlisting">
<strong>user=&gt; (def freqs (map nlp/token-freqs tokens))</strong>
<strong>user=&gt; (take 10 (:text (first freqs)))</strong>
<strong>(["sillier" 1] ["partly" 2] ["mags" 4] ["new" 1] ["advisor" 1] ["a" 13] ["worry" 1] ["unsentimental" 1] ["method" 1] ["pampering" 1])</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">Finally, we can reduce those down into one set of frequencies over the entire collection. I've pretty-printed out the top ten most frequent tokens. Look at the following code:<div><pre class="programlisting">
<strong>user=&gt; (def c-freqs (nlp/corpus-freqs freqs))</strong>
<strong>user=&gt; (take 10 (reverse (sort-by second c-freqs)))</strong>
<strong>(["the" 266011]</strong>
<strong> ["of" 115973]</strong>
<strong> ["to" 107951]</strong>
<strong> ["a" 101017]</strong>
<strong> ["and" 96375]</strong>
<strong> ["in" 74558]</strong>
<strong> ["s" 66349]</strong>
<strong> ["that" 64447]</strong>
<strong> ["is" 49311]</strong>
<strong> ["it" 38175])</strong>
</pre></div></li></ul></div><p>We can see that the most frequent words are common words with little semantic value. In the next section, we'll see what we need to do with them.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec82"/>Stop lists</h2></div></div></div><p>The words identified <a id="id833" class="indexterm"/>as the most<a id="id834" class="indexterm"/> common words in the code in the previous section are often referred to as <a id="id835" class="indexterm"/>
<strong>function</strong> words, because they're performing functions in the sentence, but not really carrying meaning. For some kinds of analyses, such as grammatical and stylistic analyses, these are vitally important. However, for this particular chapter, we're more interested in the documents' content words, or the words that carry semantic meaning.</p><p>To filter these out, the typical technique is to use a stop word list. This is a list of common words to remove from the list of tokens.</p><p>If you type <code class="literal">english stop list</code> into Google, you'll get a lot of workable stop lists. I've downloaded one from <a class="ulink" href="http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop">http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop</a>. Download this file too, and place it into the <code class="literal">d</code> directory along with the data files.</p><p>To load the stop words, we'll use the following function. It simply takes the filename and returns a set of the tokens in it.</p><div><pre class="programlisting">(defn load-stop-words [filename]
  (set (tokenize (slurp filename))))</pre></div><p>Using this set directly is easy enough on raw strings. However, we'll want a function to make calling it on <code class="literal">NewsArticle</code> instances easier. Look at the following code:</p><div><pre class="programlisting">(defn remove-stop-words [stop-words m]
  (update-in m [:text] #(remove stop-words %)))</pre></div><p>Now, we can load those words and remove them from the lists of tokens. We'll start with the definition of tokens that we just created. Look at the following code:</p><div><pre class="programlisting">
<strong>user=&gt; (def stop-words (nlp/load-stop-words "d/english.stop"))</strong>
<strong>user=&gt; (def filtered</strong>
<strong>         (map #(nlp/remove-stop-words stop-words %) tokens))</strong>
<strong>user=&gt; (take 10 (:text (first filtered)))</strong>
<strong>("harmonic" "convergences" "maxim" "strong" "point" "totally" "unsentimental" "ungenteel" "sendup" "model")</strong>
</pre></div><p>First, we can tell that we've removed a number of tokens that weren't really adding much. <code class="literal">You</code>, <code class="literal">re</code>, and <code class="literal">s</code> were all taken out, along with others.</p><p>Now let's regenerate the corpus frequencies with the following code:</p><div><pre class="programlisting">
<strong>user=&gt; (def freqs (map nlp/token-freqs filtered))</strong>
<strong>user=&gt; (def c-freqs (nlp/corpus-freqs freqs))</strong>
<strong>user=&gt; (pprint (take 10 (reverse (sort-by second c-freqs))))</strong>
<strong>(["clinton" 8567]</strong>
<strong> ["times" 6528]</strong>
<strong> ["people" 6351]</strong>
<strong> ["time" 6091]</strong>
<strong> ["story" 5645]</strong>
<strong> ["president" 5223]</strong>
<strong> ["year" 4539]</strong>
<strong> ["york" 4516]</strong>
<strong> ["world" 4256]</strong>
<strong> ["years" 4144])</strong>
</pre></div><p>This list seems<a id="id836" class="indexterm"/> much more reasonable. It focuses on Bill Clinton, <a id="id837" class="indexterm"/>who was the US President during this period.</p><p>Another way of dealing with this is to use a white list. This would be a set of words or features that represent the entire collection of those that we want to deal with. We could implement this as a simple function, <code class="literal">keep-white-list</code>, as shown in the following code:</p><div><pre class="programlisting">(defn keep-white-list [white-list-set m]
  (over :text #(filter white-list-set %) m))</pre></div><p>This function seems academic now, but we'll need it before we're done.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec83"/>Hapax and Dis Legomena</h2></div></div></div><p>Now, let's look at a <a id="id838" class="indexterm"/>graph of the frequencies:</p><div><img src="img/4139OS_10_03.jpg" alt="Hapax and Dis Legomena"/></div><p>That's a lot of words<a id="id839" class="indexterm"/> that don't occur very much. This is actually expected. A few words occur a lot, but most just don't.</p><p>We can get another view of the data by looking at the log-log plot of the frequencies and ranks. Functions that represent a value raised to a power should be linear in these line charts. We can see that the relationship isn't quite on a line in this plot, but it's very close. Look at the following graph:</p><div><img src="img/4139OS_10_04.jpg" alt="Hapax and Dis Legomena"/></div><p>In fact, let's turn<a id="id840" class="indexterm"/> the frequency mapping around in the following code to look at how often different frequencies occur:</p><div><pre class="programlisting">
<strong>user=&gt; (def ffreqs (frequencies (vals c-freqs)))</strong>
<strong>user=&gt; (pprint (take 10 (reverse (sort-by second ffreqs))))</strong>
<strong>([1 23342]</strong>
<strong> [2 8814]</strong>
<strong> [3 5310]</strong>
<strong> [4 3749]</strong>
<strong> [5 2809]</strong>
<strong> [6 2320]</strong>
<strong> [7 1870]</strong>
<strong> [8 1593]</strong>
<strong> [9 1352]</strong>
<strong> [10 1183])</strong>
</pre></div><p>So there are more than 23,000 words that only occur once and more than 8,000 words that only occur twice. Words like these are very interesting for authorship studies. The words that are found only once are referred to as<a id="id841" class="indexterm"/> <strong>hapax legomena</strong>, from Greek for "said once", and<a id="id842" class="indexterm"/> words that occur only<a id="id843" class="indexterm"/> twice are<a id="id844" class="indexterm"/> <strong>dis legomena</strong>.</p><p>Looking at a random <a id="id845" class="indexterm"/>10 hapax legomena gives us a good indication of the types of words these are. The 10 words are: shanties, merrifield, cyberguru, alighting, roomfor, sciaretto, borisyeltsin, vermes, fugs, and gandhian. Some of these appear to be unusual or rare words. Others are mistakes or two words that were joined together for some reason, possibly by a dash.</p><p>Unfortunately, they do not contribute much to our study, since they don't occur often enough to contribute to the results statistically. In fact, we'll just get rid of any words that occur less than 10 times. This will form a second stop list, this time of rare words. Let's generate this list. Another, probably better performing, option is to create a whitelist of the words that aren't rare, but we can easily integrate this with our existing stop-list infrastructure, so we'll do it by just creating another list here.</p><p>To create it from the frequencies, we'll define a <code class="literal">make-rare-word-list</code> function. It takes a frequency mapping and returns the items with fewer than <em>n</em> occurrences, as follows:</p><div><pre class="programlisting">(defn make-rare-word-list [freqs n]
  (map first (filter #(&lt; (second %) n) freqs)))</pre></div><p>We can now use this function to generate the <code class="literal">d/english.rare</code> file. We can use this file just like we used the stop list to remove items that aren't common and to further clean up the tokens that we'll have to deal with (you can also find this file in the code download for this chapter):</p><div><pre class="programlisting">(with-open [f (io/writer "d/english.rare")]
  (binding [*out* f]
    (doseq [t (sort (nlp/make-rare-word-list c-freqs 8))]
      (println t))))</pre></div><p>Now, we have a list of more than 48,000 tokens that will get removed. For perspective, after removing the common stop words, there were more than 71,000 token types.</p><p>We can now use that just like we did for the previous stop word list. Starting from <code class="literal">filtered</code>, which we defined in the earlier code after removing the common stop words, we'll now define <code class="literal">filtered2</code> and recalculate the frequencies as follows:</p><div><pre class="programlisting">
<strong>user=&gt; (def rare (nlp/load-stop-words "d/english.rare"))</strong>
<strong>user=&gt; (def filtered2</strong>
<strong>         (map #(nlp/remove-stop-words rare %) filtered))</strong>
<strong>user=&gt; (take 10 (:text (first filtered2)))</strong>
<strong>("maxim" "strong" "point" "totally" "unsentimental" "sendup" "model" "hustler" "difference" "surprise")</strong>
</pre></div><p>So we can see that the process has removed some uncommon words, such as <code class="literal">harmonic</code> and <code class="literal">convergences</code>.</p><p>This process is pretty piecemeal so far, but it's one that we would need to do multiple times, probably. Many natural language processing and text analysis tasks begin by taking a text, converting it to a sequence of features (tokenization, normalization, and filtering), and then counting them. Let's package that into one function as follows:</p><div><pre class="programlisting">(defn process-articles
  ([articles]
   (process-articles
      articles ["d/english.stop" "d/english.rare"]))
  ([articles stop-files]
   (let [stop-words (reduce set/union #{}
                            (map load-stop-words stop-files))
         process (fn [text]
                   (frequencies
                     (remove stop-words (tokenize text))))]
     (map #(over :text process %) articles))))</pre></div><p>The preceding <a id="id846" class="indexterm"/>function allows us to call it with just a list of articles. We can also specify a list of stop word files. The entries in all the lists are added together to create a master list of stop words. Then the articles' text is tokenized, filtered by the stop words, and counted. Doing it this way should save on creating and possibly hanging on to multiple lists of intermediate processing stages that we won't ever use later.</p><p>Now we can skip to the document-level frequencies with the following command:</p><div><pre class="programlisting">
<strong>user=&gt; (def freqs (nlp/process-articles articles))</strong>
</pre></div><p>Now that we've filtered these out, let's look at the graph of token frequencies again:</p><div><img src="img/4139OS_10_05.jpg" alt="Hapax and Dis Legomena"/></div><p>The <a id="id847" class="indexterm"/>distribution stayed the same, as we would expect, but the number of words should be more manageable.</p><p>Again, we can see from the following log-log plot that the previous power relationship—almost, but not quite, linear—holds for this frequency distribution as well:</p><div><img src="img/4139OS_10_06.jpg" alt="Hapax and Dis Legomena"/></div><p>Another <a id="id848" class="indexterm"/>way to approach this would be to use a whitelist, as we mentioned earlier. We could load files and keep only the tokens that we've seen before by using the following function:</p><div><pre class="programlisting">(defn load-articles [white-list-set articles]
  (let [process (fn [text]
                  (frequencies
                    (filter white-list-set (tokenize text))))]
    (map #(over :text process %) articles)))</pre></div><p>Again, this will come up later. We'll find this necessary when we need to load unseen documents to analyze.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec84"/>TF-IDF</h2></div></div></div><p>The frequencies <a id="id849" class="indexterm"/>as we currently have them will <a id="id850" class="indexterm"/>present a couple of difficulties. For one thing, if we have a document with 100 words and one with 500 words, we can't really compare the frequencies. For another thing, if a word occurs three times in every document, say in a header, it's not as interesting as one that occurs in only a few documents three times and nowhere else.</p><p>In order to work around both of these, we'll use a metric called <strong>term frequency-inverse document frequency</strong> (<strong>TF-IDF</strong>). This <a id="id851" class="indexterm"/>combines some kind of document-term frequency with the log of the percentage of documents that contain that term.</p><p>For the first part, <a id="id852" class="indexterm"/>term frequency, we could use a number of metrics. We could use a boolean 0 or 1 to show absence or presence. We could use the raw frequency or the raw frequency scaled. In this case, we're going to use an augmented frequency that scales the raw frequency by the maximum frequency of any word in the document. Look at the following code:</p><div><pre class="programlisting">(defn tf [term-freq max-freq]
  (+ 0.5 (/ (* 0.5 term-freq) max-freq)))
(defn tf-article [article term]
  (let [freqs (:text article)]
    (tf (freqs term 0) (reduce max 0 (vals freqs)))))</pre></div><p>The first function in the preceding code, <code class="literal">tf</code>, is a basic augmented frequency equation and takes the raw values as parameters. The second function, <code class="literal">tf-article</code>, wraps <code class="literal">tf</code> but takes a <code class="literal">NewsArticle</code> instance and a word and generates the TF value for that pair.</p><p>For the second part of this equation, the inverse document frequency, we'll use the log of the total number of documents divided by the number of documents containing that term. We'll also add one to the last number to protect against division-by-zero errors.</p><p>The <code class="literal">idf</code> function calculates the inverse document frequency for a term over the given corpus, as shown in the following code:</p><div><pre class="programlisting">(defn has-term?
  ([term] (fn [a] (has-term? term a)))
  ([term a] (not (nil? (get (:text a) term)))))
(defn idf [corpus term]
  (Math/log
    (/ (count corpus)
       (inc (count (filter (has-term? term) corpus))))))</pre></div><p>The IDF for a word won't change between different documents. Because of this, we can calculate all of the IDF values for all the words represented in the corpus once and cache them. The following two functions take care of this scenario:</p><div><pre class="programlisting">(defn get-vocabulary [corpus]
  (reduce set/union #{} (map #(set (keys (:text %))) corpus)))
(defn get-idf-cache [corpus]
  (reduce #(assoc %1 %2 (idf corpus %2)) {}
          (get-vocabulary corpus)))</pre></div><p>The first function in the preceding code, <code class="literal">get-vocabulary</code>, returns a set of all the words used in the corpus. The next function, <code class="literal">get-idf-cache</code>, iterates over the vocabulary set to construct a mapping of the cached IDF values. We'll use this cache to generate the TF-IDF values for each document.</p><p>The <code class="literal">tf-idf</code> function combines the output of <code class="literal">tf</code> and <code class="literal">idf</code> (via <code class="literal">get-idf-cache</code>) to calculate the TF-IDF value. In this case, we simply take the raw frequencies and the IDF value and multiply them together as shown in the following code:</p><div><pre class="programlisting">(defn tf-idf [idf-value freq max-freq]
  (* (tf freq max-freq) idf-value))</pre></div><p>This works at the most basic level; however, we'll want some adapters to work with <code class="literal">NewsArticle</code> instances and higher-level Clojure data structures.</p><p>The first level up will take the <a id="id853" class="indexterm"/>IDF cache and a map of frequencies and return a new map of TF-IDF values based off of those frequencies. To do this, we have to find the maximum frequency represented in the mapping. Then we can calculate the TF-IDF for each token type in the frequency map as follows:</p><div><pre class="programlisting">(defn tf-idf-freqs [idf-cache freqs]
  (let [max-freq (reduce max 0 (vals freqs))]
    (into {}
          (map #(vector (first %)
                        (tf-idf
                          (idf-cache (first %))
                          (second %)
                          max-freq))
               freqs))))</pre></div><p>The<a id="id854" class="indexterm"/> <code class="literal">tf-idf-freqs</code> function <a id="id855" class="indexterm"/>does most of the work for us. Now we can build on it further. First, we'll write <code class="literal">tf-idf-over</code> to calculate the TF-IDF values for all the tokens in a <code class="literal">NewsArticle</code> instance. Then we'll write <code class="literal">tf-idf-cached</code>, which takes a cache of IDF values for each word in a corpus. It returns those documents with their frequencies converted if TF-IDF. Finally, <code class="literal">tf-idf-all</code> will call this function on a collection of <code class="literal">NewsArticle</code> instances as shown in the following code:</p><div><pre class="programlisting">(defn tf-idf-over [idf-cache article]
  (over :text (fn [f] (tf-idf-freqs idf-cache f)) article))
(defn tf-idf-cached [idf-cache corpus]
  (map #(tf-idf-over idf-cache %) corpus))
(defn tf-idf-all [corpus]
  (tf-idf-cached (get-idf-cache corpus) corpus))</pre></div><p>We've implemented TF-IDF, but now we should play with it some more to get a feel for how it works in practice.</p><p>We'll start with the definition of <code class="literal">filtered2</code> that we implemented in the <em>Hapax and Dis Legomena</em> section. This section contained the corpus of <code class="literal">NewsArticles</code> instances, and the <code class="literal">:text</code> property is the frequency of tokens without the tokens from the stop word lists of both rare and common words.</p><p>Now we can generate the scaled TF-IDF frequencies for these articles by calling <code class="literal">tf-idf-all</code> on them. Once we have that, we can compare the frequencies for one article. Look at the following code:</p><div><pre class="programlisting">(def tf-idfs (nlp/tf-idf-all filtered2))
(doseq [[t f] (sort-by second (:text (first filtered2)))]
  (println t \tab f \tab (get (:text (first tf-idfs)) t)))</pre></div><p>The table's too <a id="id856" class="indexterm"/>long to reproduce here (176 tokens). Instead, I'll just pick 10 interesting terms to look at more closely. The following table<a id="id857" class="indexterm"/> includes not only each term's raw frequencies and TF-IDF scores, but also the number of documents that they are found in:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Token</p>
</th><th style="text-align: left" valign="bottom">
<p>Raw frequency</p>
</th><th style="text-align: left" valign="bottom">
<p>Document frequency</p>
</th><th style="text-align: left" valign="bottom">
<p>TF-IDF</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>sillier</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>3.35002</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>politics</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>749</p>
</td><td style="text-align: left" valign="top">
<p>0.96849</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>british</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>594</p>
</td><td style="text-align: left" valign="top">
<p>1.09315</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>reason</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>851</p>
</td><td style="text-align: left" valign="top">
<p>0.96410</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>make</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2,350</p>
</td><td style="text-align: left" valign="top">
<p>0.37852</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>military</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>700</p>
</td><td style="text-align: left" valign="top">
<p>1.14842</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>time</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>2,810</p>
</td><td style="text-align: left" valign="top">
<p>0.29378</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>mags</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>18</p>
</td><td style="text-align: left" valign="top">
<p>3.57932</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>women</p>
</td><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>930</p>
</td><td style="text-align: left" valign="top">
<p>1.46071</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>men</p>
</td><td style="text-align: left" valign="top">
<p>13</p>
</td><td style="text-align: left" valign="top">
<p>856</p>
</td><td style="text-align: left" valign="top">
<p>1.66526</p>
</td></tr></tbody></table></div><p>The tokens in the preceding table are ordered by their raw frequencies. However, notice how badly that correlates with the TF-IDF.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, notice the numbers for "sillier" and "politics". Both are found once in this document. But "sillier" probably doesn't occur much in the entire collection, and it has a TF-IDF score of more than 3. However, "politics" is common, so it scores slightly less than 1.</li><li class="listitem" style="list-style-type: disc">Next, notice the numbers for "time" (raw frequency of 3) and "mags" (4). "Time" is a very common word that kind of straddles the categories of function words and content words. On the one hand, you can be using expressions like "time after time", but you can also talk about "time" as an abstract concept. "Mags" is a slangy version of "magazines", and it occurs roughly the same number of times as "time". However, since "mags" is rarely found in the entire corpus (only 18 times), it has the highest TF-IDF score of any word in this document.</li><li class="listitem" style="list-style-type: disc">Finally, look at "women" and "men". These are the two most common words in this article. However, because they're found in so many documents, both are given TF-IDF scores of around 1.5.</li></ul></div><p>What we <a id="id858" class="indexterm"/>wind up with is a measure of how <a id="id859" class="indexterm"/>important a term is in that document. Words that are more common have to appear more to be considered significant. Words that are found in only a few documents can be important with just one mention.</p><p>As a final step before we move on, we can also write a utility function that loads a set of articles, given a token whitelist and an IDF cache. This will be important }after we've trained the neural network when we're actually using it. That's because we will need to keep the same features, in the same order, and to scale between the two runs. Thus, it's important to scale by the same IDF values. Look at the following code:</p><div><pre class="programlisting">(defn load-text-files [token-white-list idf-cache articles]
  (tf-idf-cached idf-cache
                 (load-articles token-white-list articles)))</pre></div><p>The preceding code will allow us to analyze documents and actually use our neural network after we've trained it.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec65"/>Inspecting the stock prices</h1></div></div></div><p>Now that we have <a id="id860" class="indexterm"/>some<a id="id861" class="indexterm"/> hold on the textual data, let's turn our attention to the stock prices. Previously, we loaded it from the CSV file using the <code class="literal">financial.csv-data/read-stock-prices</code> function. Let's reload that data with the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def stock (csvd/read-stock-prices "d/d-1995-2001.csv"))</strong>
<strong>user=&gt; (count stock)</strong>
<strong>1263</strong>
</pre></div><p>Let's start with a graph that shows how the closing price has changed over the years:</p><div><img src="img/4139OS_10_07.jpg" alt="Inspecting the stock prices"/></div><p>So the <a id="id862" class="indexterm"/>price started in the low 30s, fluctuated <a id="id863" class="indexterm"/>a bit, and finished in the low 20s. During that time, there were some periods where it climbed rapidly. Hopefully, we'll be able to capture and predict those changes.</p></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec66"/>Merging text and stock features</h1></div></div></div><p>Before we can start to train the <a id="id864" class="indexterm"/>neural network, however, <a id="id865" class="indexterm"/>we'll need to figure out how we need to represent the data and what information the neural network needs to have.</p><p>The code for this section will be present in the <code class="literal">src/financial/nn.clj</code> file. Open it up and add the following namespace header:</p><div><pre class="programlisting">(ns financial.nn
  (:require [clj-time.core :as time]
            [clj-time.coerce :as time-coerce]
            [clojure.java.io :as io]
            [enclog.nnets :as nnets]
            [enclog.training :as training]
            [financial.utils :as u]
            [financial.validate :as v])
  (:import [org.encog.neural.networks PersistBasicNetwork]))</pre></div><p>However, we first need to be clear about what we're trying to do. That will allow us to properly format and present the data.</p><p>Let's break it down like this: for each document, based on the previous stock prices and the tokens in a document, can we predict the direction of future stock prices.</p><p>So one set of features will be the tokens in the document. We already have those identified earlier. Other features can represent the stock prices. Since we're actually interested in the direction of the future prices, we can actually use the difference between the stock prices of a point in the past and of the day the article was published. Offhand, we're not sure what time frames will be helpful, so we can select several and include them all.</p><p>The output is another difference in stock prices. Again, we don't know at what difference in time we'll be able to get good results (if any!), so we'll try to look out into the future at various distances.</p><p>For the ranges of<a id="id866" class="indexterm"/> time, we'll use some <a id="id867" class="indexterm"/>standard time periods, gradually getting further and further out: a day, two days, three days, four days, five days, two weeks, three weeks, one month, two months, six months, and one year. Days that fall on a weekend have the value of the previous business day. Months will be 30 days, and a year is 365 days. This way, the time periods will be more or less regular.</p><p>We can represent those periods in Clojure using the <code class="literal">clj-time</code> library (<a class="ulink" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>) as follows:</p><div><pre class="programlisting">(def periods [(time/days 1)
              (time/days 2)
              (time/days 3)
              (time/days 4)
              (time/days 5)
              (time/days (* 7 2))
              (time/days (* 7 3))
              (time/days 30)
              (time/days (* 30 2))
              (time/days (* 30 6))
              (time/days 365)])</pre></div><p>For the features, we'll use the difference in price over those periods. The easiest way to get at that information would be to index the stock prices by date and then access the prices from there using some utility functions. Let's see what that would look like:</p><div><pre class="programlisting">(defn index-by [key-fn coll]
  (into {} (map #(vector (key-fn %) %) coll)))
(defn get-stock-date [stock-index date]
  (if-let [price (stock-index date)]
    price
    (if (&lt;= (time/year date) 1990)
      nil
      (get-stock-date
        stock-index (time/minus date (time/days 1))))))</pre></div><p>We can use <code class="literal">index-by</code> to index a collection of anything into a map. The other function, <code class="literal">get-stock-date</code>, then attempts to get the <code class="literal">StockData</code> instance from the index. If it doesn't find one, it tries the previous day. If it ever works its way before 1990, it just returns nil.</p><p>Now let's get the input feature vector from a <code class="literal">NewsArticle</code> instance and the stock index.</p><p>The easy part of this will be getting the token vector. Getting the price vector will be more complicated, and we'll be doing almost the same thing twice: once looking backward from the article for the input vector, and once looking forward from the article for the output vector. Since generating these two vectors will be mostly the same, we'll write a function that does it and accepts function parameters for the differences, as shown in the following code:</p><div><pre class="programlisting">(defn make-price-vector [stock-index article date-op]
  (let [pub-date (:pub-date article)
        base-price (:close (get-stock-date stock-index pub-date))
        price-feature
        (fn [period]
          (let [date-key (date-op pub-date period)]
            (if-let [stock (get-stock-date stock-index date-key)]
              (/ (price-diff base-price (:close stock))
                 base-price)
              0.0)))]
    (vec (remove nil? (map price-feature periods)))))</pre></div><p>The <code class="literal">make-price-vector</code> function gets the base price from the day the article was published. It then gets <a id="id868" class="indexterm"/>the day offsets that <a id="id869" class="indexterm"/>we outlined previously and finds the closing stock price for each of those days. It finds the difference between the two prices.</p><p>The parameter for this function is <code class="literal">date-op</code>, which gets the second day to find the stock price for. It will either add the period to the article's publish date or subtract it, depending on whether we're looking in the future or the past.</p><p>We can build on this to make the input vector, which will contain the token vector and the price vector, as shown in the following code:</p><div><pre class="programlisting">(defn make-feature-vector [stock-index vocab article]
  (let [freqs (:text article)
        token-features (map #(freqs % 0.0) (sort vocab))
        price-features (make-price-vector
                         stock-index article time/minus)]
    (vec (concat token-features price-features))))</pre></div><p>For the token vector, we get the frequencies from the <code class="literal">NewsArticle</code> instance in the order given by the vocab collection. This should be the same across all <code class="literal">NewsArticle</code> instances. We call <code class="literal">make-price-vector</code> to get the prices for the offset days. Then we concatenate all of them into one (Clojure) vector.</p><p>The following code gives us the input vector. However, we'll also want to have future prices as the output vector.</p><div><pre class="programlisting">(defn make-training-vector [stock-index article]
  (vec (make-price-vector stock-index article time/plus)))</pre></div><p>The preceding <a id="id870" class="indexterm"/>code is just a thin wrapper<a id="id871" class="indexterm"/> over <code class="literal">make-price-vector</code>. It calls this function with the appropriate arguments to get the future stock price.</p><p>Finally, we'll write a function that takes a stock index, a vocabulary, and a collection of articles. It will generate both the input vector and the expected output vector, and it will return both stored in a hash map. The code for this function is given as follows:</p><div><pre class="programlisting">(defn make-training-set [stock-index vocab articles]
  (let [make-pair
        (fn [article]
          {:input (make-feature-vector stock-index vocab article)
           :outputs (zipmap periods
                            (make-training-vector
                              stock-index article))})]
    (map make-pair articles)))</pre></div><p>This code will make it easy to generate a training set from the data that we've been working with.</p></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec67"/>Analyzing both text and stock features together with neural nets</h1></div></div></div><p>We now<a id="id872" class="indexterm"/> have <a id="id873" class="indexterm"/>everything <a id="id874" class="indexterm"/>ready<a id="id875" class="indexterm"/> to perform the<a id="id876" class="indexterm"/> analysis, except <a id="id877" class="indexterm"/>for the engine that will actually attempt to learn the training data.</p><p>In this instance, we're going to try to train an artificial neural network to learn the direction of change of the future prices of the input data. In other words, we'll try to train it to tell whether the price will go up or down in the near future. We want to create a simple binary classifier from the past price changes and the text of an article.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec85"/>Understanding neural nets</h2></div></div></div><p>As the name implies, artificial<a id="id878" class="indexterm"/> neural networks are machine learning structures modeled on the architecture and behavior of neurons, such as the ones found in the human brain. Artificial neural networks come in many forms, but today we're going to use one of the oldest and most common forms: the three-layer feed-forward network.</p><p>We can see the structure of a unit outlined in the following figure:</p><div><img src="img/4139OS_10_08.jpg" alt="Understanding neural nets"/></div><p>Each unit is able to realize<a id="id879" class="indexterm"/> linearly separable functions. That is, functions that divide their n-dimensional output space along a hyperplane. To emulate more complex functions, however, we have to go beyond a single unit and create a network of them.</p><p>These networks have three layers: an input layer, a hidden layer, and an output layer. Each layer is made up of one or more neurons. Each neuron takes one or more inputs and produces an output, which is broadcast to one or more outputs. The inputs are weighted, and each input is weighted individually. All of the inputs are added together, and the sum is passed through an activation function that normalizes and scales the input. The inputs are <em>x</em>, the weights are <em>w</em>, and the outputs are <em>y</em>.</p><p>A simple schematic of<a id="id880" class="indexterm"/> this structure is shown as follows:</p><div><img src="img/4139OS_10_09.jpg" alt="Understanding neural nets"/></div><p>The network operates in a fairly simple manner, following the process called feed forward activation. It is described as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">The input vector is fed to the input layer of the network. Depending on how the network is set up, these may be passed through the activation function for each neuron. This determines the activation of each neuron, or the amount of signal coming into it from that channel and how excited it is.</li><li class="listitem">The weighted connections between the input and hidden layers are then activated and used to excite the nodes in the hidden layer. This is done by getting the dot product of the input neurons with the weights going into each hidden node. These values are then passed through the activation function for the hidden neurons.</li><li class="listitem">The forward propagation process is repeated again between the hidden layer and the output layer.</li><li class="listitem">The activation of the neurons in the output layer is the output of the network.</li></ol></div><p>Initially, the <a id="id881" class="indexterm"/>weights are usually randomly selected. Then the weights are trained using a variety of techniques. A common one is called backward propagation. This involves computing the error between the output neurons and the desired outputs. This error is then fed backward into the network. This is used to dampen some weights and increase others. The net effect is to nudge the output of the network slightly closer to the target.</p><p>Other training methods work differently, but attempt to do the same thing: each tries to modify the weights so that the outputs are close to the targets for each input in the training set.</p><p>Note that I said <em>close to the targets</em>. When training a neural network, you don't want the outputs to align exactly. When this happens, the network is said to have memorized the training set. This means that the network will perform great for inputs that it has seen previously. But when it encounters new inputs, it is brittle and won't perform well. It has learned the training set too well, but it won't be able to generalize that information to new inputs.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec86"/>Setting up the neural net</h2></div></div></div><p>Implementing <a id="id882" class="indexterm"/>neural networks isn't difficult—and doing so is a useful exercise—but there are good libraries for neural networks available for Java and Clojure, and we'll choose one of those here. For our case, we'll use the Encog Machine Learning Framework (<a class="ulink" href="http://www.heatonresearch.com/encog">http://www.heatonresearch.com/encog</a>), which specializes in neural networks. But we'll primarily be using it through the Clojure wrapper library Enclog (<a class="ulink" href="https://github.com/jimpil/enclog/">https://github.com/jimpil/enclog/</a>). We'll build on these to write some facade functions over Enclog to customize this library for our processes.</p><p>The first step is to create the neural network. The <code class="literal">make-network</code> function takes the vocabulary size and the number of hidden nodes (the variables for our purposes), but it defines the rest of the parameters internally, as follows:</p><div><pre class="programlisting">(defn make-network [vocab-size hidden-nodes]
  (nnets/network (nnets/neural-pattern :feed-forward)
                 :activation :sigmoid
                 :input (+ vocab-size (count periods))
                 :hidden [hidden-nodes]
                 :output 1))</pre></div><p>The number<a id="id883" class="indexterm"/> of input nodes is a function of the size of the vocabulary in addition to the number of periods. (Periods is a non-dynamic, namespace-level binding. We may want to rethink that and make it dynamic to provide a little more flexibility, but for our needs right now this is sufficient.) And since from the output node, we just want a single value indicating whether the stock went up or down, we hardcoded the number of output nodes to one. However, the number of hidden nodes that will perform best is an open question. We'll include that as a parameter so we can experiment with it.</p><p>For the output, we'll need a way to take our expected output and run it through the same activation function as that output. That way, we can directly compare the two as follows:</p><div><pre class="programlisting">(defn activated [act-fn output]
  (let [a (double-array 1 [output]))]
    (.activationFunction act-fn a 0 1)
    a)</pre></div><p>The activated function takes an object that implements <code class="literal">org.encog.engine.network.activation.ActivationFunction</code>. We can get these from the neural network. It then puts the output for a period into a double array. The activation function scales this and then returns the array.</p><p>We will also need to prepare the data and insert it into a data structure that Encog can work with. The primary transformation in the following code is pulling out the output prices for the period that we're training for:</p><div><pre class="programlisting">(defn build-data [nnet period training-set]
  (let [act (.getActivation nnet (dec (.getLayerCount nnet)))
        output (mapv #(activated act (get (:outputs %) period))
                     training-set)]
    (training/data :basic-dataset
                   (mapv :input training-set)
                   output)))</pre></div><p>There's nothing particularly exciting here. We pull the inputs and the outputs for one time period out into two separate vectors and create a dataset with them.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec87"/>Training the neural net</h2></div></div></div><p>Now we have a <a id="id884" class="indexterm"/>neural network, but it's been initialized to random weights, so it will perform very, very poorly. We'll need to train it immediately.</p><p>To do this, we will put the training set together with the network in the following code. Like the previous functions, <code class="literal">train-for</code> accepts the parameters that we're interested in being able to change, uses reasonable defaults for ones that we'll probably leave alone, but hardcodes parameters that we won't touch. The function creates a trainer object and calls its <code class="literal">train</code> method. Finally, we return the neural network, which was modified in place.</p><div><pre class="programlisting">(defn train-for
  ([nnet period training-set]
    (train-for nnet period training-set 0.01 500 []))
  ([nnet period training-set error-tolerance
    iterations strategies]
    (let [data (build-data nnet period training-set)
          trainer (training/trainer :back-prop
                                    :network nnet:training-set data)]
      (training/train
        trainer error-tolerance iterations strategies)
      nnet)))</pre></div><p>When it is time to validate a network, it will be a little easier to combine creating a network with training it into one function. We'll do that with <code class="literal">make-train</code> as follows:</p><div><pre class="programlisting">(defn make-train [vocab-size hidden-count period coll]
  (let [nn (make-network vocab-size hidden-count)]
    (train-for nn period coll 0.01 100 [])
    nn))</pre></div><p>This allows us to <a id="id885" class="indexterm"/>train a new neural network in one call.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec88"/>Running the neural net</h2></div></div></div><p>Once we've trained<a id="id886" class="indexterm"/> the network, we'll want to run it on new inputs, ones for which we don't know the expected output. We can do that with the <code class="literal">run-network</code> function. This takes a trained network and an input collection and returns an array of the network's output as follows:</p><div><pre class="programlisting">(defn run-network [nnet input]
  (let [input (double-array (count input) input)
        output (double-array (.getOutputCount nnet))]
    (.compute nnet input output)
    output))</pre></div><p>We can use this function in one of two ways:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We can pass it data that we don't know the output for to see how the network classifies it.</li><li class="listitem" style="list-style-type: disc">We can pass it input data that we do know the output for in order to evaluate how well this network performs against data it hasn't previously encountered.</li></ul></div><p>We'll see an example of the latter in the next section.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec89"/>Validating the neural net</h2></div></div></div><p>We can build<a id="id887" class="indexterm"/> on all of these functions to validate the neural network, train it, test it against new data, and evaluate how it does.</p><p>The <code class="literal">test-on</code> utility<a id="id888" class="indexterm"/> gets the <strong>sum of squared errors</strong> (<strong>SSE</strong>)<a id="id889" class="indexterm"/> for running the network on a test set for a given period. This trains and runs a neural network on the training set for a given period. It then returns the SSE for that run as follows:</p><div><pre class="programlisting">(defn test-on [nnet period test-set]
  "Runs the net on the test set and calculates the SSE."
  (let [act (.getActivation nnet (dec (.getLayerCount nnet)))
        sqr (fn [x] (* x x))
        error (fn [{:keys [input outputs]}]
                (- (first (activated act (get outputs period)))
                   (first (run-network nnet input))))]
    (reduce + 0.0 (map sqr (map error test-set)))))</pre></div><p>Running this train-test combination once gives us a very rough idea of how the network will perform with those parameters. However, if we want a better idea, we can use K-fold cross-validation. This divides the data into <em>K</em> equally sized groups. It then runs the train-test combination <em>K</em> times. Each time, it holds out a different partition as a test group. It trains the network on the rest of the partitions and evaluates it on the test group. The errors returned by <code class="literal">test-on</code> can be averaged to get a better idea of how the network will perform with those parameters.</p><p>For example, say we use K=4. We'll divide the training input into four groups: A, B, C, and D. This means that we'll train the following four different classifiers:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We'll use A as the test set and train on B, C, and D combined</li><li class="listitem" style="list-style-type: disc">We'll use B as the test set and train on A, C, and D</li><li class="listitem" style="list-style-type: disc">We'll use C as the test set and train on A, B, and D</li><li class="listitem" style="list-style-type: disc">We'll use D as the test set and train on A, B, and C</li></ul></div><p>For each classifier, we'll compute the SSE, and we'll take the mean of these to see how well the classification should perform with those parameters, on average.</p><div><div><h3 class="title"><a id="note02"/>Note</h3><p>I've defined the K-fold function in the <code class="literal">validate.clj</code> file at <code class="literal">src/financial/</code>. You can see how it's implemented in the source code download. I've also aliased that namespace to <code class="literal">v</code> in the current namespace.</p></div></div><p>The <code class="literal">x-validate</code> function will perform the cross validation on the inputs. The other function, <code class="literal">accum</code>, is simply a small utility that accumulates the error values into a vector. The <code class="literal">v/k-fold</code> function expects the accumulator to return the base case (an empty vector) when called with no arguments, as shown in the following code:</p><div><pre class="programlisting">(defn accum
  ([] [])
  ([v x] (conj v x)))
(defn x-validate [vocab-size hidden-count period coll]
  (v/k-fold #(make-train vocab-size hidden-count period %)
            #(test-on %1 period %2)
            accum
            10
            coll))</pre></div><p>The <code class="literal">x-validate</code> function <a id="id890" class="indexterm"/>uses <code class="literal">make-train</code> to create a new network and train it. It tests that network using <code class="literal">test-on</code>, and it gathers the resulting error rates together with <code class="literal">accum</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec90"/>Finding the best parameters</h2></div></div></div><p>We've defined this <a id="id891" class="indexterm"/>system to let us play with a couple of parameters. First, we can set the number of neurons in the hidden layer. Also, we can set the time period that we are to predict for into the future (one day, two days, three days, a month, a year, and so on).</p><p>These parameters create a large space of possible solutions, some of which may perform better than others. We can make some educated guesses about some of the parameters—that it will predict the movement of the stock prices one day in the future better than it will the movement a year in the future—but we don't know that, and we should perhaps try it out.</p><p>These parameters present a search space. It would take too much time to try all the combinations, but we can try a number of them, just to see how they perform. This lets us tune the neural network to get the best results.</p><p>To explore this search space, let's first define what happens when we test one point, one combination of time period in the future, and a number of hidden nodes. The <code class="literal">explore-point</code> function will take care of this in the following code:</p><div><pre class="programlisting">(defn explore-point [vocab-count period hidden-count training]
  (println period hidden-count)
  (let [error (x-validate
                vocab-count hidden-count period training)]
    (println period hidden-count
             '=&gt; \tab (u/mean error) \tab error)
    (println)
    error))</pre></div><p>The preceding code basically just takes the information and passes it to <code class="literal">x-validate</code>. It returns that function's return value (<code class="literal">error</code>) too. Along the way, it prints out a number of status messages. Then we need something that walks over the search space, calls <code class="literal">explore-point</code>, and collects the error rates returned for the output.</p><p>We'll define a dynamic global called <code class="literal">*hidden-counts*</code> that defines the range of hidden neuron counts that we're interested in exploring. The <code class="literal">periods</code> value that we bound earlier will define the search space for how far to look into the future.</p><p>To make sure that we <a id="id892" class="indexterm"/>don't train the networks too specifically on the data that we're using to find the best parameters, we'll first break the data into a development set and a test set. We'll use the development set to try out the different parameters, further breaking it up into a training set and a development-test set. At the end, we'll take the best set of parameters and test those against the test set that we originally held out. This will give us a better idea of how the neural network performs. The <code class="literal">final-eval</code> function will perform this last set and return the information that it creates.</p><p>The following function walks over these values and is named <code class="literal">explore-params</code>:</p><div><pre class="programlisting">(def ^:dynamic *hidden-counts* [5 10 25 50 75])
(defn final-eval [vocab-size period hidden-count
                  training-set test-set]
  (let [nnet (make-train
               vocab-size hidden-count period training-set)
        error (test-on nnet period test-set)]
    {:period period
     :hidden-count hidden-count
     :nnet nnet
     :error error}))

(defn explore-params
  ([error-ref vocab-count training]
   (explore-params
     error-ref vocab-count training *hidden-counts* 0.2))
  ([error-ref vocab-count training hidden-counts test-ratio]
   (let [[test-set dev-set] (u/rand-split training test-ratio)
         search-space (for [p periods, h hidden-counts] [p h])]
     (doseq [pair search-space]
       (let [[p h] pair,
             error (explore-point vocab-count p h dev-set)]
         (dosync
           (commute error-ref assoc pair error))))
     (println "Final evaluation against the test set.")
     (let [[period hidden-count]
           (first (min-key #(u/mean (second %)) @error-ref))
           final (final-eval
                   vocab-count period hidden-count
                   dev-set test-set)]
       (dosync
         (commute error-ref assoc :final final))))
   @error-ref))</pre></div><p>I've made a slightly unusual design decision in writing <code class="literal">explore-params</code>. Instead of initializing a hash map to contain the period-hidden count pairs and their associated error rates, I need the caller to pass in a reference containing a hash map. During the course of the processing, <code class="literal">explore-params</code> fills the hash map and finally returns it.</p><p>I've done this for one <a id="id893" class="indexterm"/>reason: exploring this search space still takes a long time. Over the course of writing this chapter, I needed to stop the validation, tweak the possible parameter values, and start it again. Setting up the function this way allowed me to be able to stop the processing, but still have access to what's happened thus far. I can look at the values, play around with them, and allow a more thorough examination of them to influence my decisions about what direction to take.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec68"/>Predicting the future</h1></div></div></div><p>Now is the time to<a id="id894" class="indexterm"/> bring together everything that we've assembled<a id="id895" class="indexterm"/> over the course of this chapter, so it seems appropriate to start over from scratch, just using the Clojure source code that we've written over the course of the chapter.</p><p>We'll take this one block at a time, loading and processing the data, creating training and test sets, training and validating the neural network, and finally viewing and analyzing its results.</p><p>Before we do any of this, we'll need to load the proper namespaces into the REPL. We can do that with the following <code class="literal">require</code> statement:</p><div><pre class="programlisting">
<strong>user=&gt; (require</strong>
<strong>         [me.raynes.fs :as fs]</strong>
<strong>         [financial]</strong>
<strong>         [financial.types :as t]</strong>
<strong>         [financial.nlp :as nlp]</strong>
<strong>         [financial.nn :as nn]</strong>
<strong>         [financial.oanc :as oanc]</strong>
<strong>         [financial.csv-data :as csvd]</strong>
<strong>         [financial.utils :as u])</strong>
</pre></div><p>This will give us access to everything that we've implemented so far.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec91"/>Loading stock prices</h2></div></div></div><p>First, we'll load the<a id="id896" class="indexterm"/> stock prices with<a id="id897" class="indexterm"/> the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def stocks (csvd/read-stock-prices "d/d-1996-2001.csv"))</strong>
<strong>user=&gt; (def stock-index (nn/index-by :date stocks))</strong>
</pre></div><p>The preceding code loads the stock prices from the CSV file and indexes them by date. This will make it easy to integrate them with the new article data in a few steps.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec92"/>Loading news articles</h2></div></div></div><p>Now we <a id="id898" class="indexterm"/>can load the <a id="id899" class="indexterm"/>news articles. We'll need two pieces of data from them: the TF-IDF scaled frequencies and the vocabulary list. Look at the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def slate (doall</strong>
<strong>                    (map oanc/load-article</strong>
<strong>                         (oanc/find-slate-files</strong>
<strong>                           (io/file "d/OANC-GrAF")))))</strong>
<strong>user=&gt; (def corpus (nlp/process-articles slate))</strong>
<strong>user=&gt; (def freqs (nlp/tf-idf-all corpus))</strong>
<strong>user=&gt; (def vocab (nlp/get-vocabulary corpus))</strong>
</pre></div><p>This code binds the frequencies as <code class="literal">freqs</code> and the vocabulary as <code class="literal">vocab</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec93"/>Creating training and test sets</h2></div></div></div><p>Since we bundled the entire <a id="id900" class="indexterm"/>process into<a id="id901" class="indexterm"/> one function, merging our two data sources together into one training set is simple, as shown in the following command:</p><div><pre class="programlisting">
<strong>user=&gt; (def training</strong>
<strong>         (nn/make-training-set stock-index vocab freqs))</strong>
</pre></div><p>Now, for each article, we have an input vector and a series of output for different stock prices related to the data.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec94"/>Finding the best parameters for the neural network</h2></div></div></div><p>The training data and the <a id="id902" class="indexterm"/>parameters' value ranges are the input for exploring the network parameter space. Look at the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def error-rates (ref {}))</strong>
<strong>user=&gt; (nn/explore-params error-rates (count vocab) training)</strong>
</pre></div><p>This takes a very long time to run. Actually, I looked at the output it was producing and realized that it wouldn't be able to predict well beyond a day or two, so I stopped it after that. Thanks to my decision to pass in a reference, I was able to stop it and still have access to the results generated by that point.</p><p>The output is a <a id="id903" class="indexterm"/>mapping from the period and number of hidden nodes to a list of SSE values generated from each partition in the K-fold cross-validation. A more meaningful metric would be the average of the errors. We can generate that here and print out the results as follows:</p><div><pre class="programlisting">
<strong>user=&gt; (def error-means</strong>
<strong>         (into {}</strong>
<strong>               (map #(vector (first %) (u/mean (second %)))</strong>
<strong>                    @error-rates)))</strong>
<strong>user=&gt; (pprint (sort-by second error-means))</strong>
<strong>([[#&lt;Days P1D&gt; 10] 1.0435393]</strong>
<strong> [[#&lt;Days P1D&gt; 5] 1.5253379]</strong>
<strong> [[#&lt;Days P1D&gt; 25] 5.0099998]</strong>
<strong> [[#&lt;Days P1D&gt; 50] 32.00977]</strong>
<strong> [[#&lt;Days P1D&gt; 100] 34.264244]</strong>
<strong> [[#&lt;Days P1D&gt; 200] 60.73007]</strong>
<strong> [[#&lt;Days P1D&gt; 300] 100.29568])</strong>
</pre></div><p>So the squared sum of errors for predicting one day ahead go from about 1 for 10 hidden units to 100 for 300 hidden units. So, based on that, we'll only train a network to predict one day into the future and to use 10 hidden nodes.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec95"/>Training and validating the neural network</h2></div></div></div><p>Actually, training<a id="id904" class="indexterm"/> the neural<a id="id905" class="indexterm"/> network is pretty easy from our end, but it does take a while. The following commands should somewhat produce better results than we saw before, but at the cost of some time. Remember that the training process may not take this long, but we should probably be prepared.</p><div><pre class="programlisting">
<strong>user=&gt; (def nn (nn/make-network (count vocab) 10))</strong>
<strong>user=&gt; (def day1 (first nn/periods))</strong>
<strong>user=&gt; (nn/train-for nn day1 training)</strong>
<strong>Iteration # 1 Error: 22.025400% Target-Error: 1.000000%</strong>
<strong>Iteration # 2 Error: 19.332094% Target-Error: 1.000000%</strong>
<strong>Iteration # 3 Error: 14.241920% Target-Error: 1.000000%</strong>
<strong>Iteration # 4 Error: 6.283643% Target-Error: 1.000000%</strong>
<strong>Iteration # 5 Error: 0.766110% Target-Error: 1.000000%</strong>
</pre></div><p>Well, that was quick.</p><p>This gives us a trained, ready-to-use neural network bound to the name <code class="literal">nn</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec96"/>Running the network on new data</h2></div></div></div><p>We can now run <a id="id906" class="indexterm"/>our trained network on some new data. Just to have something to look at, I downloaded 10 articles off the Slate website and saved them to files in the directory <code class="literal">d/slate/</code>. I also downloaded the stock prices for Dominion, Inc.</p><p>Now, how would I analyze this data?</p><p>Before we really start, we'll need to pull some data from the processes we've been using, and we'll need to set up some reference values, such as the date of the documents. Look at the following code:</p><div><pre class="programlisting">(def idf-cache (nlp/get-idf-cache corpus))
(def sample-day (time/date-time 2014 3 20 0 0 0))
(def used-vocab (set (map first idf-cache)))</pre></div><p>So we get the IDF cache, the date the articles were downloaded on, and the vocabulary that we used in training. That vocabulary set will serve as the token whitelist for loading the news articles.</p><p>Let's see how to get the documents ready to analyze. Look at the following code:</p><div><pre class="programlisting">(def articles (doall
                (-&gt;&gt; "d/slate/"
                  fs/list-dir
                  (map #(str "d/slate/" %))
                  (map #(oanc/load-text-file sample-day %))
                  (nlp/load-text-files used-vocab idf-cache))))</pre></div><p>This is a little more complicated than it was when we loaded them earlier. Basically, we just read the directory list and load the text from each one. Then we tokenize and filter it before determining the TF-IDF value for each token.</p><p>On the other hand, reading the stocks is very similar to what we just did. Look at the following code:</p><div><pre class="programlisting">(def recent-stocks (csvd/read-stock-prices "d/d-2013-2014.csv"))
(def recent-index (nn/index-by :date recent-stocks))</pre></div><p>With these in hand, we can put both together to make the input vectors as shown in the following code:</p><div><pre class="programlisting">(def inputs
  (map #(nn/make-feature-vector recent-index used-vocab %)
       articles))</pre></div><p>Now let's see how to run the network and see what happens. Look at the following:</p><div><pre class="programlisting">
<strong>user=&gt; (pprint</strong>
<strong>         (flatten</strong>
<strong>           (map vec</strong>
<strong>                (map #(nn/run-network nn %) inputs))))</strong>
<strong>(0.5046613110846201</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613135395166</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613110846201</strong>
<strong> 0.5046613112651592</strong>
<strong> 0.5046613110846201)</strong>
</pre></div><p>These items<a id="id907" class="indexterm"/> are very consistent. To quite a few decimal places, they're all clustered right around 0.5. From the sigmoid function, this means that it doesn't really anticipate a stock change over the next day.</p><p>In fact, this tracks what actually happened fairly well. On March 20, the stock closed at $69.77, and on March 21, it closed at $70.06. This was a gain of $0.29.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec69"/>Taking it with a grain of salt</h1></div></div></div><p>Any analysis like the one presented in this chapter has a number of things that we need to question. This chapter is no exception.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec97"/>Related to this project</h2></div></div></div><p>The main weakness of <a id="id908" class="indexterm"/>this project was that it was carried out on far too little data. This cuts in several ways:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We need articles from a number of data sources</li><li class="listitem" style="list-style-type: disc">We need articles from a wider range of time</li><li class="listitem" style="list-style-type: disc">We need more density of articles in the time period</li></ul></div><p>For all of these, there are reasons we didn't address the issues in this chapter. However, if you plan to take this further, you'd need to figure out some way around these.</p><p>There are several ways to look at the results too. The day we looked at, the results all clustered close to zero. In fact, this stock if relatively stable, so if it always indicated little change, then it would always have a fairly low SSE. Large changes seem to happen occasionally, and the <a id="id909" class="indexterm"/>error from not predicting them has a low impact on the SSE.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec98"/>Related to machine learning and market modeling in general</h2></div></div></div><p>Second, and more importantly, simply putting some stock data into a jar with some machine learning and shaking it is a risky endeavor. This isn't a get-rich-quick scheme, and by approaching it so naively, you're asking for trouble. In this case, that means losing money.</p><p>For one thing, there's not much noise in news articles, and the relationship between their content and stock prices is tenuous enough that in general, stock prices may not be predictable from news reports in the first place, whatever results we achieve is this study, particularly given how small it is.</p><p>Really, to do this well, you need to understand at least two things:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Financial modeling</strong>: You <a id="id910" class="indexterm"/>need to understand how to model financial transactions and dynamics mathematically</li><li class="listitem" style="list-style-type: disc"><strong>Machine learning</strong>: You need <a id="id911" class="indexterm"/>to understand how machine learning works and how it models things</li></ul></div><p>With this knowledge, you should be able to formulate a better model of how the stock prices change and which prices you should pay attention to.</p><p>But keep in mind, André Christoffer Andersen and Stian Mikelsen have published a master's thesis in 2012 showing that it's very, very difficult to do better than buying and holding index funds (<a class="ulink" href="http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf">http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf</a>). So, if you do try this route, you have a hard, hard task in front of you.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec70"/>Summary</h1></div></div></div><p>Over the course of this chapter, we've gotten a hold of some news articles and some stock prices, and we've managed to train a neural network that projects just a little into the future. This is a risky thing to put into production, but we've also outlined what we'd need to learn to do this correctly.</p><p>And this is also the end of this book. Thank you for staying with me this far. You've been a great reader. I hope that you've learned something as we've looked at the 10 data analysis projects that we've covered. If programming and data are both eating this world, hopefully you've seen how to have fun with both.</p></div></body></html>