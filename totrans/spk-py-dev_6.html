<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Visualizing Insights and Trends"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Visualizing Insights and Trends</h1></div></div></div><p>So far, we have focused on the collection, analysis, and processing of data from Twitter. We have set the stage to use our data for visual rendering and extracting insights and trends. We will give a quick lay of the land about visualization tools in the Python ecosystem. We will highlight Bokeh as a powerful tool for rendering and viewing large datasets. Bokeh is part of the Python Anaconda Distribution ecosystem.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Gauging the key words and memes within a social network community using charts and wordcloud</li><li class="listitem" style="list-style-type: disc">Mapping the most active location where communities are growing around certain themes or topics</li></ul></div><div class="section" title="Revisiting the data-intensive apps architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Revisiting the data-intensive apps architecture</h1></div></div></div><p>We have <a id="id359" class="indexterm"/>reached the final layer of the data-intensive apps architecture: the engagement layer. This layer focuses on how to synthesize, emphasize, and visualize the key context relevant information for the data consumers. A bunch of numbers in a console will not suffice to engage with end-users. It is critical to present the mass of information in a rapid, digestible, and attractive fashion.</p><p>The following diagram sets the context of the chapter's focus highlighting the engagement layer.</p><div class="mediaobject"><img src="graphics/B03968_06_01.jpg" alt="Revisiting the data-intensive apps architecture"/></div><p>For Python <a id="id360" class="indexterm"/>plotting and visualizations, we have quite a few tools and libraries. The most interesting and relevant ones for our purpose are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Matplotlib</strong></span> is <a id="id361" class="indexterm"/>the grandfather of the Python plotting libraries. Matplotlib was originally the brainchild of <span class="emphasis"><em>John Hunter</em></span> who was an open source software proponent and established Matplotlib as one of the most prevalent plotting libraries both in the academic and the data scientific communities. Matplotlib allows the generation of plots, histograms, power spectra, bar charts, error charts, scatterplots, and so on. Examples can be found on <a id="id362" class="indexterm"/>the Matplotlib dedicated website at <a class="ulink" href="http://matplotlib.org/examples/index.html">http://matplotlib.org/examples/index.html</a>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Seaborn</strong></span>, developed by <span class="emphasis"><em>Michael Waskom</em></span>, is a great library to quickly visualize<a id="id363" class="indexterm"/> statistical information. It is built on top of Matplotlib and integrates seamlessly with Pandas and the Python data stack, including Numpy. A gallery of graphs from Seaborn at <a class="ulink" href="http://stanford.edu/~mwaskom/software/seaborn/examples/index.html">http://stanford.edu/~mwaskom/software/seaborn/examples/index.html</a> shows<a id="id364" class="indexterm"/> the potential of the library.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ggplot</strong></span> is <a id="id365" class="indexterm"/>relatively new and aims to offer the equivalent of the famous ggplot2 from the R ecosystem for the Python data wranglers. It has the same look and feel of ggplot2 and uses the same grammar of graphics as expounded by Hadley Wickham. The ggplot the Python port is developed by the team<a id="id366" class="indexterm"/> at <code class="literal">yhat</code>. More information can be found at <a class="ulink" href="http://ggplot.yhathq.com">http://ggplot.yhathq.com</a>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>D3.js</strong></span> is a <a id="id367" class="indexterm"/>very popular, JavaScript library developed by <span class="emphasis"><em>Mike Bostock</em></span>. <span class="strong"><strong>D3</strong></span> stands for <span class="strong"><strong>Data Driven Documents</strong></span> and brings data to life on any <a id="id368" class="indexterm"/>modern browser leveraging HTML, SVG, and CSS. It <a id="id369" class="indexterm"/>delivers dynamic, powerful, interactive visualizations by manipulating the DOM, the Document Object Model. The Python community could not wait to integrate D3 with Matplotlib. Under the impulse of Jake Vanderplas, mpld3 was created with the aim of bringing <code class="literal">matplotlib</code> to the browser. Examples graphics are hosted at the following address: <a class="ulink" href="http://mpld3.github.io/index.html">http://mpld3.github.io/index.html</a>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bokeh</strong></span> aims<a id="id370" class="indexterm"/> to deliver high-performance interactivity over very large or streaming datasets whilst leveraging lot of the concepts of <code class="literal">D3.js</code> without the burden of writing some intimidating <code class="literal">javascript</code> and <code class="literal">css</code> code. Bokeh delivers dynamic visualizations on the browser with or without a server. It integrates seamlessly with Matplotlib, Seaborn and ggplot and renders beautifully in IPython notebooks or Jupyter notebooks. Bokeh is actively developed by the team at Continuum.io and is an integral part of the Anaconda Python data stack.</li></ul></div><p>Bokeh server provides a full-fledged, dynamic plotting engine that materializes a reactive scene graph from JSON. It uses web sockets to keep state and update the HTML5 canvas using Backbone.js and Coffee-script under the hoods. Bokeh, as it is fueled by data in JSON, creates easy bindings for other languages such as R, Scala, and Julia.</p><p>This gives a high-level overview of the main plotting and visualization library. It is not exhaustive. Let's move to concrete examples of visualizations.</p></div></div>
<div class="section" title="Preprocessing the data for visualization"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Preprocessing the data for visualization</h1></div></div></div><p>Before <a id="id371" class="indexterm"/>jumping into the visualizations, we will do<a id="id372" class="indexterm"/> some preparatory work on the data harvested:</p><div class="informalexample"><pre class="programlisting">In [16]:
# Read harvested data stored in csv in a Panda DF
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'
pddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')
In [20]:
print('tweets pandas dataframe - count:', pddf_in.count())
print('tweets pandas dataframe - shape:', pddf_in.shape)
print('tweets pandas dataframe - colns:', pddf_in.columns)
('tweets pandas dataframe - count:', Unnamed: 0    7540
id            7540
created_at    7540
user_id       7540
user_name     7538
tweet_text    7540
dtype: int64)
('tweets pandas dataframe - shape:', (7540, 6))
('tweets pandas dataframe - colns:', Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object'))</pre></div><p>For <a id="id373" class="indexterm"/>the purpose of our visualization activity, we <a id="id374" class="indexterm"/>will use a dataset of 7,540 tweets. The key information is stored in the <code class="literal">tweet_text</code> column. We preview the data stored in the dataframe calling the <code class="literal">head()</code> function on the dataframe:</p><div class="informalexample"><pre class="programlisting">In [21]:
pddf_in.head()
Out[21]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text
0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...</pre></div><p>We will now create some utility functions to clean up the tweet text and parse the twitter date. First, we import the Python regular expression regex library <code class="literal">re</code> and the time library to parse dates and time:</p><div class="informalexample"><pre class="programlisting">In [72]:
import re
import time</pre></div><p>We create a dictionary of regex that will be compiled and then passed as function:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RT</strong></span>: The first regex with key <code class="literal">RT</code> looks for the keyword <code class="literal">RT</code> at the beginning of the tweet text:<div class="informalexample"><pre class="programlisting">re.compile(r'^RT'),</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ALNUM</strong></span>: The <a id="id375" class="indexterm"/>second regex with key <code class="literal">ALNUM</code> looks for words including alphanumeric characters and underscore <a id="id376" class="indexterm"/>sign preceded by the <code class="literal">@</code> symbol in the tweet text:<div class="informalexample"><pre class="programlisting">re.compile(r'(@[a-zA-Z0-9_]+)'),</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>HASHTAG</strong></span>: The third regex with key <code class="literal">HASHTAG</code> looks for words including alphanumeric characters preceded by the <code class="literal">#</code> symbol in the tweet text:<div class="informalexample"><pre class="programlisting">re.compile(r'(#[\w\d]+)'),</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SPACES</strong></span>: The fourth regex with key <code class="literal">SPACES</code> looks for blank or line space characters in the tweet text:<div class="informalexample"><pre class="programlisting">re.compile(r'\s+'), </pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>URL</strong></span>: The fifth regex with key <code class="literal">URL</code> looks for <code class="literal">url</code> addresses including alphanumeric characters preceded with <code class="literal">https://</code> or <code class="literal">http://</code> markers in the tweet text:<div class="informalexample"><pre class="programlisting">re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')
In [24]:
regexp = {"RT": "^RT", "ALNUM": r"(@[a-zA-Z0-9_]+)",
          "HASHTAG": r"(#[\w\d]+)", "URL": r"([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)",
          "SPACES":r"\s+"}
regexp = dict((key, re.compile(value)) for key, value in regexp.items())
In [25]:
regexp
Out[25]:
{'ALNUM': re.compile(r'(@[a-zA-Z0-9_]+)'),
 'HASHTAG': re.compile(r'(#[\w\d]+)'),
 'RT': re.compile(r'^RT'),
 'SPACES': re.compile(r'\s+'),
 'URL': re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')}</pre></div></li></ul></div><p>We create a utility function to identify whether a tweet is a retweet or an original tweet:</p><div class="informalexample"><pre class="programlisting">In [77]:
def getAttributeRT(tweet):
    """ see if tweet is a RT """
    return re.search(regexp["RT"], tweet.strip()) != None</pre></div><p>Then, we extract all user handles in a tweet:</p><div class="informalexample"><pre class="programlisting">def getUserHandles(tweet):
    """ given a tweet we try and extract all user handles"""
    return re.findall(regexp["ALNUM"], tweet)</pre></div><p>We<a id="id377" class="indexterm"/> also extract all hashtags in a tweet:</p><div class="informalexample"><pre class="programlisting">def getHashtags(tweet):
    """ return all hashtags"""
    return re.findall(regexp["HASHTAG"], tweet)</pre></div><p>Extract <a id="id378" class="indexterm"/>all URL links in a tweet as follows:</p><div class="informalexample"><pre class="programlisting">def getURLs(tweet):
    """ URL : [http://]?[\w\.?/]+"""
    return re.findall(regexp["URL"], tweet)</pre></div><p>We strip all URL links and user handles preceded by <code class="literal">@</code> sign in a tweet text. This function will be the basis of the wordcloud we will build soon:</p><div class="informalexample"><pre class="programlisting">def getTextNoURLsUsers(tweet):
    """ return parsed text terms stripped of URLs and User Names in tweet text
        ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x).split()) """
    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|(RT)"," ", tweet).lower().split())</pre></div><p>We label the data so we can create groups of datasets for the wordcloud:</p><div class="informalexample"><pre class="programlisting">def setTag(tweet):
    """ set tags to tweet_text based on search terms from tags_list"""
    tags_list = ['spark', 'python', 'clinton', 'trump', 'gaga', 'bieber']
    lower_text = tweet.lower()
    return filter(lambda x:x.lower() in lower_text,tags_list)</pre></div><p>We parse the twitter date in the <code class="literal">yyyy-mm-dd hh:mm:ss</code> format:</p><div class="informalexample"><pre class="programlisting">def decode_date(s):
    """ parse Twitter date into format yyyy-mm-dd hh:mm:ss"""
    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))</pre></div><p>We preview the data prior to processing:</p><div class="informalexample"><pre class="programlisting">In [43]:
pddf_in.columns
Out[43]:
Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object')
In [45]:
# df.drop([Column Name or list],inplace=True,axis=1)
pddf_in.drop(['Unnamed: 0'], inplace=True, axis=1)
In [46]:
pddf_in.head()
Out[46]:
  id   created_at   user_id   user_name   tweet_text
0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...</pre></div><p>We <a id="id379" class="indexterm"/>create new dataframe columns by applying the <a id="id380" class="indexterm"/>utility functions described. We create a new column for <code class="literal">htag</code>, user handles, URLs, the text terms stripped from URLs, and unwanted characters and the labels. We finally parse the date:</p><div class="informalexample"><pre class="programlisting">In [82]:
pddf_in['htag'] = pddf_in.tweet_text.apply(getHashtags)
pddf_in['user_handles'] = pddf_in.tweet_text.apply(getUserHandles)
pddf_in['urls'] = pddf_in.tweet_text.apply(getURLs)
pddf_in['txt_terms'] = pddf_in.tweet_text.apply(getTextNoURLsUsers)
pddf_in['search_grp'] = pddf_in.tweet_text.apply(setTag)
pddf_in['date'] = pddf_in.created_at.apply(decode_date)</pre></div><p>The following code gives a quick snapshot of the newly generated dataframe:</p><div class="informalexample"><pre class="programlisting">In [83]:
pddf_in[2200:2210]
Out[83]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
2200   638242693374681088   Mon Aug 31 06:51:30 +0000 2015   19525954   CENATIC   El impacto de @ApacheSpark en el procesamiento...   [#sparkSpecial]   [://t.co/4PQmJNuEJB]   el impacto de en el procesamiento de datos y e...   [spark]   2015-08-31 06:51:30   [@ApacheSpark]   el impacto de en el procesamiento de datos y e...   [spark]
2201   638238014695575552   Mon Aug 31 06:32:55 +0000 2015   51115854   Nawfal   Real Time Streaming with Apache Spark\nhttp://...   [#IoT, #SmartMelboune, #BigData, #Apachespark]   [://t.co/GW5PaqwVab]   real time streaming with apache spark iot smar...   [spark]   2015-08-31 06:32:55   []   real time streaming with apache spark iot smar...   [spark]
2202   638236084124516352   Mon Aug 31 06:25:14 +0000 2015   62885987   Mithun Katti   RT @differentsachin: Spark the flame of digita...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   []   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 06:25:14   [@differentsachin, @ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2203   638234734649176064   Mon Aug 31 06:19:53 +0000 2015   140462395   solaimurugan v   Installing @ApacheMahout with @ApacheSpark 1.4...   []   [1.4.1, ://t.co/3c5dGbfaZe.]   installing with 1 4 1 got many more issue whil...   [spark]   2015-08-31 06:19:53   [@ApacheMahout, @ApacheSpark]   installing with 1 4 1 got many more issue whil...   [spark]
2204   638233517307072512   Mon Aug 31 06:15:02 +0000 2015   2428473836   Ralf Heineke   RT @RomeoKienzler: Join me @velocityconf on #m...   [#machinelearning, #devOps, #Bl]   [://t.co/U5xL7pYEmF]   join me on machinelearning based devops operat...   [spark]   2015-08-31 06:15:02   [@RomeoKienzler, @velocityconf, @ApacheSpark]   join me on machinelearning based devops operat...   [spark]
2205   638230184848687106   Mon Aug 31 06:01:48 +0000 2015   289355748   Akim Boyko   RT @databricks: Watch live today at 10am PT is...   []   [1.5, ://t.co/16cix6ASti]   watch live today at 10am pt is 1 5 presented b...   [spark]   2015-08-31 06:01:48   [@databricks, @ApacheSpark, @databricks, @pwen...   watch live today at 10am pt is 1 5 presented b...   [spark]
2206   638227830443110400   Mon Aug 31 05:52:27 +0000 2015   145001241   sachin aggarwal   Spark the flame of digital India @ #IBMHackath...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   [://t.co/C1AO3uNexe]   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 05:52:27   [@ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2207   638227031268810752   Mon Aug 31 05:49:16 +0000 2015   145001241   sachin aggarwal   RT @pravin_gadakh: Imagine, innovate and Igni...   [#IBMHackathon, #ISLconnectIN2015]   []   gadakh imagine innovate and ignite digital ind...   [spark]   2015-08-31 05:49:16   [@pravin_gadakh, @ApacheSpark]   gadakh imagine innovate and ignite digital ind...   [spark]
2208   638224591920336896   Mon Aug 31 05:39:35 +0000 2015   494725634   IBM Asia Pacific   RT @sachinparmar: Passionate about Spark?? Hav...   [#IBMHackathon, #ISLconnectIN]   [India..]   passionate about spark have dreams of clean sa...   [spark]   2015-08-31 05:39:35   [@sachinparmar]   passionate about spark have dreams of clean sa...   [spark]
2209   638223327467692032   Mon Aug 31 05:34:33 +0000 2015   3158070968   Open Source India   "Game Changer" #ApacheSpark speeds up #bigdata...   [#ApacheSpark, #bigdata]   [://t.co/ieTQ9ocMim]   game changer apachespark speeds up bigdata pro...   [spark]   2015-08-31 05:34:33   []   game changer apachespark speeds up bigdata pro...   [spark]</pre></div><p>We save the processed information in a CSV format. We have 7,540 records and 13 columns. In <a id="id381" class="indexterm"/>your case, the output will vary according<a id="id382" class="indexterm"/> to the dataset you chose:</p><div class="informalexample"><pre class="programlisting">In [84]:
f_name = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweets_processed.csv'
pddf_in.to_csv(f_name, sep=';', encoding='utf-8', index=False)
In [85]:
pddf_in.shape
Out[85]:
(7540, 13)</pre></div></div>
<div class="section" title="Gauging words, moods, and memes at a glance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Gauging words, moods, and memes at a glance</h1></div></div></div><p>We are now ready to proceed with building the wordclouds which will give us a sense of the important <a id="id383" class="indexterm"/>words carried in those tweets. We will create wordclouds for the datasets harvested. Wordclouds extract the top words in a list of words and create a scatterplot of the words where the size of the word is correlated to its frequency. The more frequent the word in the dataset, the bigger will be the font size in the wordcloud rendering. They include three very different themes and two competing or analogous entities. Our first theme is obviously data processing and analytics, with Apache Spark and Python as our entities. Our second theme is the 2016 presidential election campaign, with the two contenders: Hilary Clinton and Donald Trump. Our last theme is the world of pop music with Justin Bieber and Lady Gaga as the two exponents.</p><div class="section" title="Setting up wordcloud"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec56"/>Setting up wordcloud</h2></div></div></div><p>We will<a id="id384" class="indexterm"/> illustrate the programming steps by analyzing the spark related tweets. We load the data and preview the dataframe:</p><div class="informalexample"><pre class="programlisting">In [21]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets.csv'
tspark_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [3]:
tspark_df.head(3)
Out[3]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
0   638818911773856000   Tue Sep 01 21:01:11 +0000 2015   2511247075   Noor Din   RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]   [://t.co/3bsaTT7eUs]   r leads rapidminer python catches up big data ...   [spark, python]   2015-09-01 21:01:11   [@kdnuggets]   r leads rapidminer python catches up big data ...   [spark, python]
1   622142176768737000   Fri Jul 17 20:33:48 +0000 2015   24537879   IBM Cloudant   Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]   be one of the first to sign up for ibm analyti...   [spark]   2015-07-17 20:33:48   []   be one of the first to sign up for ibm analyti...   [spark]
2   622140453069169000   Fri Jul 17 20:26:57 +0000 2015   515145898   Arno Candel   Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]   nice article on apachespark hadoop and datasci...   [spark]   2015-07-17 20:26:57   [@h2oai]   nice article on apachespark hadoop and datasci...   [spark]</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>The <a id="id385" class="indexterm"/>wordcloud library we will use is the one developed by Andreas Mueller and hosted on his GitHub account at <a class="ulink" href="https://github.com/amueller/word_cloud">https://github.com/amueller/word_cloud</a>.</p></div></div><p>The <a id="id386" class="indexterm"/>library requires <span class="strong"><strong>PIL</strong></span> (short for <span class="strong"><strong>Python Imaging Library</strong></span>). PIL is <a id="id387" class="indexterm"/>easily installable by invoking <code class="literal">conda install pil</code>. PIL is a complex library to install and is not yet ported on Python 3.4, so we need to run a Python 2.7+ environment to be able to see our wordcloud:</p><div class="informalexample"><pre class="programlisting">#
# Install PIL (does not work with Python 3.4)
#
an@an-VB:~$ conda install pil

Fetching package metadata: ....
Solving package specifications: ..................
Package plan for installation in environment /home/an/anaconda:</pre></div><p>The following packages will be downloaded:</p><div class="informalexample"><pre class="programlisting">    package                    |            build
    ---------------------------|-----------------
    libpng-1.6.17              |                0         214 KB
    freetype-2.5.5             |                0         2.2 MB
    conda-env-2.4.4            |           py27_0          24 KB
    pil-1.1.7                  |           py27_2         650 KB
    ------------------------------------------------------------
                                           Total:         3.0 MB</pre></div><p>The following packages will be UPDATED:</p><div class="informalexample"><pre class="programlisting">    conda-env: 2.4.2-py27_0 --&gt; 2.4.4-py27_0
    freetype:  2.5.2-0      --&gt; 2.5.5-0     
    libpng:    1.5.13-1     --&gt; 1.6.17-0    
    pil:       1.1.7-py27_1 --&gt; 1.1.7-py27_2

Proceed ([y]/n)? y</pre></div><p>Next, we<a id="id388" class="indexterm"/> install the wordcloud library:</p><div class="informalexample"><pre class="programlisting">#
# Install wordcloud
# Andreas Mueller
# https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py
#

an@an-VB:~$ pip install wordcloud
Collecting wordcloud
  Downloading wordcloud-1.1.3.tar.gz (163kB)
    100% |████████████████████████████████| 163kB 548kB/s 
Building wheels for collected packages: wordcloud
  Running setup.py bdist_wheel for wordcloud
  Stored in directory: /home/an/.cache/pip/wheels/32/a9/74/58e379e5dc614bfd9dd9832d67608faac9b2bc6c194d6f6df5
Successfully built wordcloud
Installing collected packages: wordcloud
Successfully installed wordcloud-1.1.3</pre></div></div><div class="section" title="Creating wordclouds"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec57"/>Creating wordclouds</h2></div></div></div><p>At this stage, we<a id="id389" class="indexterm"/> are ready to invoke the wordcloud program with the generated list of terms from the tweet text.</p><p>Let's get started with the wordcloud program by first calling <code class="literal">%matplotlib</code> inline to display the wordcloud in our notebook:</p><div class="informalexample"><pre class="programlisting">In [4]:
%matplotlib inline
In [11]:</pre></div><p>We convert the dataframe <code class="literal">txt_terms</code> column into a list of words. We make sure it is all converted into the <code class="literal">str</code> type to avoid any bad surprises and check the list's first four records:</p><div class="informalexample"><pre class="programlisting">len(tspark_df['txt_terms'].tolist())
Out[11]:
2024
In [22]:
tspark_ls_str = [str(t) for t in tspark_df['txt_terms'].tolist()]
In [14]:
len(tspark_ls_str)
Out[14]:
2024
In [15]:
tspark_ls_str[:4]
Out[15]:
['r leads rapidminer python catches up big data tools grow spark ignites kdn',
 'be one of the first to sign up for ibm analytics for apachespark today sparkinsight',
 'nice article on apachespark hadoop and datascience',
 'spark 101 running spark and mapreduce together in production hadoopsummit2015 apachespark altiscale']</pre></div><p>We <a id="id390" class="indexterm"/>first call the Matplotlib and the wordcloud libraries:</p><div class="informalexample"><pre class="programlisting">import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS</pre></div><p>From the input list of terms, we create a unified string of terms separated by a whitespace as the input to the wordcloud program. The wordcloud program removes stopwords:</p><div class="informalexample"><pre class="programlisting"># join tweets to a single string
words = ' '.join(tspark_ls_str)

# create wordcloud 
wordcloud = WordCloud(
                      # remove stopwords
                      stopwords=STOPWORDS,
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(words)

# render wordcloud image
plt.imshow(wordcloud)
plt.axis('off')

# save wordcloud image on disk
plt.savefig('./spark_tweets_wordcloud_1.png', dpi=300)

# display image in Jupyter notebook
plt.show()</pre></div><p>Here, we can visualize the wordclouds for Apache Spark and Python. Clearly, in the case of Spark, <span class="emphasis"><em>Hadoop</em></span>, <span class="emphasis"><em>big data</em></span>, and <span class="emphasis"><em>analytics</em></span> are the memes, while Python recalls the root of its name Monty Python with a strong focus on <span class="emphasis"><em>developer</em></span>, <span class="emphasis"><em>apache spark</em></span>, and programming with some hints to java and ruby.</p><div class="mediaobject"><img src="graphics/B03968_06_02.jpg" alt="Creating wordclouds"/></div><p>We can <a id="id391" class="indexterm"/>also get a glimpse in the following wordclouds of the words preoccupying the North American 2016 presidential election candidates: Hilary Clinton and Donald Trump. Seemingly Hilary Clinton is overshadowed by the presence of her opponents Donald Trump and Bernie Sanders, while Trump is heavily centered only on himself:</p><div class="mediaobject"><img src="graphics/B03968_06_03.jpg" alt="Creating wordclouds"/></div><p>Interestingly, in the case of Justin Bieber and Lady Gaga, the word <span class="emphasis"><em>love</em></span> appears. In the case of Bieber, <span class="emphasis"><em>follow</em></span> and <span class="emphasis"><em>belieber</em></span> are key words, while <span class="emphasis"><em>diet</em></span>, <span class="emphasis"><em>weight loss</em></span>, and <span class="emphasis"><em>fashion</em></span> are the preoccupations for the Lady Gaga crowd.</p><div class="mediaobject"><img src="graphics/B03968_06_04.jpg" alt="Creating wordclouds"/></div></div></div>
<div class="section" title="Geo-locating tweets and mapping meetups"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Geo-locating tweets and mapping meetups</h1></div></div></div><p>Now, we <a id="id392" class="indexterm"/>will dive into the creation of interactive maps with Bokeh. First, we <a id="id393" class="indexterm"/>create a world map where we geo-locate sample tweets and, on moving our mouse over these locations, we can see the users and their respective tweets in a hover box.</p><p>The second map is focused on mapping upcoming meetups in London. It could be an interactive map that would act as a reminder of date, time, and location for upcoming meetups in a specific city.</p><div class="section" title="Geo-locating tweets"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec58"/>Geo-locating tweets</h2></div></div></div><p>The objective is to create a world map scatter plot of the locations of important tweets on the map, and the tweets and authors are revealed on hovering over these points. We will go through three steps to build this interactive visualization:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create <a id="id394" class="indexterm"/>the background world map by first loading a dictionary of all the world country boundaries defined by their respective longitude and latitudes.</li><li class="listitem">Load the important tweets we wish to geo-locate with their respective coordinates and authors.</li><li class="listitem">Finally, scatter plot on the world map the tweets coordinates and activate the hover tool to visualize interactively the tweets and author on the highlighted dots on the map.</li></ol></div><p>In step<a id="id395" class="indexterm"/> one, we create a Python list called data that will contain all the world countries boundaries with their respective latitude and longitude:</p><div class="informalexample"><pre class="programlisting">In [4]:
#
# This module exposes geometry data for World Country Boundaries.
#
import csv
import codecs
import gzip
import xml.etree.cElementTree as et
import os
from os.path import dirname, join

nan = float('NaN')
__file__ = os.getcwd()

data = {}
with gzip.open(join(dirname(__file__), 'AN_Spark/data/World_Country_Boundaries.csv.gz')) as f:
    decoded = codecs.iterdecode(f, "utf-8")
    next(decoded)
    reader = csv.reader(decoded, delimiter=',', quotechar='"')
    for row in reader:
        geometry, code, name = row
        xml = et.fromstring(geometry)
        lats = []
        lons = []
        for i, poly in enumerate(xml.findall('.//outerBoundaryIs/LinearRing/coordinates')):
            if i &gt; 0:
                lats.append(nan)
                lons.append(nan)
            coords = (c.split(',')[:2] for c in poly.text.split())
            lat, lon = list(zip(*[(float(lat), float(lon)) for lon, lat in
                coords]))
            lats.extend(lat)
            lons.extend(lon)
        data[code] = {
            'name'   : name,
            'lats'   : lats,
            'lons'   : lons,
        }
In [5]:
len(data)
Out[5]:
235</pre></div><p>In step two, we load a sample set of important tweets that we wish to visualize with their respective<a id="id396" class="indexterm"/> geo-location information:</p><div class="informalexample"><pre class="programlisting">In [69]:
# data
#
#
In [8]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets_20.csv'
t20_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [9]:
t20_df.head(3)
Out[9]:
    id  created_at  user_id     user_name   tweet_text  htag    urls    ptxt    tgrp    date    user_handles    txt_terms   search_grp  lat     lon
0   638818911773856000  Tue Sep 01 21:01:11 +0000 2015  2511247075  Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]  [://t.co/3bsaTT7eUs]    r leads rapidminer python catches up big data ...   [spark, python]     2015-09-01 21:01:11     [@kdnuggets]    r leads rapidminer python catches up big data ...   [spark, python]     37.279518   -121.867905
1   622142176768737000  Fri Jul 17 20:33:48 +0000 2015  24537879    IBM Cloudant    Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]    be one of the first to sign up for ibm analyti...   [spark]     2015-07-17 20:33:48     []  be one of the first to sign up for ibm analyti...   [spark]     37.774930   -122.419420
2   622140453069169000  Fri Jul 17 20:26:57 +0000 2015  515145898   Arno Candel     Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]    nice article on apachespark hadoop and datasci...   [spark]     2015-07-17 20:26:57     [@h2oai]    nice article on apachespark hadoop and datasci...   [spark]     51.500130   -0.126305
In [98]:
len(t20_df.user_id.unique())
Out[98]:
19
In [17]:
t20_geo = t20_df[['date', 'lat', 'lon', 'user_name', 'tweet_text']]
In [24]:
# 
t20_geo.rename(columns={'user_name':'user', 'tweet_text':'text' }, inplace=True)
In [25]:
t20_geo.head(4)
Out[25]:
    date    lat     lon     user    text
0   2015-09-01 21:01:11     37.279518   -121.867905     Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...
1   2015-07-17 20:33:48     37.774930   -122.419420     IBM Cloudant    Be one of the first to sign-up for IBM Analyti...
2   2015-07-17 20:26:57     51.500130   -0.126305   Arno Candel     Nice article on #apachespark, #hadoop and #dat...
3   2015-07-17 19:35:31     51.500130   -0.126305   Ira Michael Blonder     Spark 101: Running Spark and #MapReduce togeth...
In [22]:
df = t20_geo
#</pre></div><p>In step <a id="id397" class="indexterm"/>three, we first imported all the necessary Bokeh libraries. We will instantiate the output in the Jupyter Notebook. We get the world countries boundary information loaded. We get the geo-located tweet data. We instantiate the Bokeh interactive tools such as wheel and box zoom as well as the hover tool.</p><div class="informalexample"><pre class="programlisting">In [29]:
#
# Bokeh Visualization of tweets on world map
#
from bokeh.plotting import *
from bokeh.models import HoverTool, ColumnDataSource
from collections import OrderedDict

# Output in Jupiter Notebook
output_notebook()

# Get the world map
world_countries = data.copy()

# Get the tweet data
tweets_source = ColumnDataSource(df)

# Create world map 
countries_source = ColumnDataSource(data= dict(
    countries_xs=[world_countries[code]['lons'] for code in world_countries],
    countries_ys=[world_countries[code]['lats'] for code in world_countries],
    country = [world_countries[code]['name'] for code in world_countries],
))

# Instantiate the bokeh interactive tools 
TOOLS="pan,wheel_zoom,box_zoom,reset,resize,hover,save"</pre></div><p>We are <a id="id398" class="indexterm"/>now ready to layer the various elements gathered into an object figure called <span class="strong"><strong>p</strong></span>. Define the title, width, and height of <span class="strong"><strong>p</strong></span>. Attach the tools. Create the world map background by patches with a light background color and borders. Scatter plot the tweets according to their respective geo-coordinates. Then, activate the hover tool with the users and their respective tweet. Finally, render the picture on the browser. The code is as follows:</p><div class="informalexample"><pre class="programlisting"># Instantiante the figure object
p = figure(
    title="%s tweets " %(str(len(df.index))),
    title_text_font_size="20pt",
    plot_width=1000,
    plot_height=600,
    tools=TOOLS)

# Create world patches background
p.patches(xs="countries_xs", ys="countries_ys", source = countries_source, fill_color="#F1EEF6", fill_alpha=0.3,
        line_color="#999999", line_width=0.5)

# Scatter plots by longitude and latitude
p.scatter(x="lon", y="lat", source=tweets_source, fill_color="#FF0000", line_color="#FF0000")
# 

# Activate hover tool with user and corresponding tweet information
hover = p.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("user", "@user"),
   ("tweet", "@text"),
])

# Render the figure on the browser
show(p)
BokehJS successfully loaded.
    
inspect
    
#
#</pre></div><p>The following <a id="id399" class="indexterm"/>code gives an overview of the world map with the red dots representing the locations of the tweets' origins:</p><div class="mediaobject"><img src="graphics/B03968_06_05.jpg" alt="Geo-locating tweets"/></div><p>We can hover on a specific dot to reveal the tweets in that location:</p><div class="mediaobject"><img src="graphics/B03968_06_06.jpg" alt="Geo-locating tweets"/></div><p>We <a id="id400" class="indexterm"/>can zoom into a specific location:</p><div class="mediaobject"><img src="graphics/B03968_06_07.jpg" alt="Geo-locating tweets"/></div><p>Finally, we <a id="id401" class="indexterm"/>can reveal the tweets in the given zoomed-in location:</p><div class="mediaobject"><img src="graphics/B03968_06_08.jpg" alt="Geo-locating tweets"/></div></div><div class="section" title="Displaying upcoming meetups on Google Maps"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec59"/>Displaying upcoming meetups on Google Maps</h2></div></div></div><p>Now, our<a id="id402" class="indexterm"/> objective is to focus on <a id="id403" class="indexterm"/>upcoming meetups in London. We are mapping three <a id="id404" class="indexterm"/>meetups <span class="strong"><strong>Data Science London</strong></span>, <span class="strong"><strong>Apache Spark</strong></span>, and <span class="strong"><strong>Machine Learning</strong></span>. We <a id="id405" class="indexterm"/>embed a Google Map within a Bokeh visualization and geo-locate the three meetups according to their coordinates and get information such as the name of the upcoming event for each meetup with a hover tool.</p><p>First, import all the necessary Bokeh libraries:</p><div class="informalexample"><pre class="programlisting">In [ ]:
#
# Bokeh Google Map Visualization of London with hover on specific points
#
#
from __future__ import print_function

from bokeh.browserlib import view
from bokeh.document import Document
from bokeh.embed import file_html
from bokeh.models.glyphs import Circle
from bokeh.models import (
    GMapPlot, Range1d, ColumnDataSource,
    PanTool, WheelZoomTool, BoxSelectTool,
    HoverTool, ResetTool,
    BoxSelectionOverlay, GMapOptions)
from bokeh.resources import INLINE

x_range = Range1d()
y_range = Range1d()</pre></div><p>We <a id="id406" class="indexterm"/>will instantiate the Google Map that will act as the substrate upon which our Bokeh visualization will be layered:</p><div class="informalexample"><pre class="programlisting"># JSON style string taken from: https://snazzymaps.com/style/1/pale-dawn
map_options = GMapOptions(lat=51.50013, lng=-0.126305, map_type="roadmap", zoom=13, styles="""
[{"featureType":"administrative","elementType":"all","stylers":[{"visibility":"on"},{"lightness":33}]},
 {"featureType":"landscape","elementType":"all","stylers":[{"color":"#f2e5d4"}]},
 {"featureType":"poi.park","elementType":"geometry","stylers":[{"color":"#c5dac6"}]},
 {"featureType":"poi.park","elementType":"labels","stylers":[{"visibility":"on"},{"lightness":20}]},
 {"featureType":"road","elementType":"all","stylers":[{"lightness":20}]},
 {"featureType":"road.highway","elementType":"geometry","stylers":[{"color":"#c5c6c6"}]},
 {"featureType":"road.arterial","elementType":"geometry","stylers":[{"color":"#e4d7c6"}]},
 {"featureType":"road.local","elementType":"geometry","stylers":[{"color":"#fbfaf7"}]},
 {"featureType":"water","elementType":"all","stylers":[{"visibility":"on"},{"color":"#acbcc9"}]}]
""")</pre></div><p>Instantiate the Bokeh object plot from the class <code class="literal">GMapPlot</code> with the dimensions and map options from the previous step:</p><div class="informalexample"><pre class="programlisting"># Instantiate Google Map Plot
plot = GMapPlot(
    x_range=x_range, y_range=y_range,
    map_options=map_options,
    title="London Meetups"
)</pre></div><p>Bring in<a id="id407" class="indexterm"/> the information from our three meetups we wish to plot and get the information by hovering above the respective coordinates:</p><div class="informalexample"><pre class="programlisting">source = ColumnDataSource(
    data=dict(
        lat=[51.49013, 51.50013, 51.51013],
        lon=[-0.130305, -0.126305, -0.120305],
        fill=['orange', 'blue', 'green'],
        name=['LondonDataScience', 'Spark', 'MachineLearning'],
        text=['Graph Data &amp; Algorithms','Spark Internals','Deep Learning on Spark']
    )
)</pre></div><p>Define the dots to be drawn on the Google Map:</p><div class="informalexample"><pre class="programlisting">circle = Circle(x="lon", y="lat", size=15, fill_color="fill", line_color=None)
plot.add_glyph(source, circle)</pre></div><p>Define the stings for the Bokeh tools to be used in this visualization:</p><div class="informalexample"><pre class="programlisting"># TOOLS="pan,wheel_zoom,box_zoom,reset,hover,save"
pan = PanTool()
wheel_zoom = WheelZoomTool()
box_select = BoxSelectTool()
reset = ResetTool()
hover = HoverTool()
# save = SaveTool()

plot.add_tools(pan, wheel_zoom, box_select, reset, hover)
overlay = BoxSelectionOverlay(tool=box_select)
plot.add_layout(overlay)</pre></div><p>Activate the <code class="literal">hover</code> tool with the information that will be carried:</p><div class="informalexample"><pre class="programlisting">hover = plot.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("Name", "@name"),
    ("Text", "@text"),
    ("(Long, Lat)", "(@lon, @lat)"),
])

show(plot)</pre></div><p>Render<a id="id408" class="indexterm"/> the plot that gives a pretty good view of London:</p><div class="mediaobject"><img src="graphics/B03968_06_09.jpg" alt="Displaying upcoming meetups on Google Maps"/></div><p>Once we<a id="id409" class="indexterm"/> hover on a highlighted dot, we can get the information of the given meetup:</p><div class="mediaobject"><img src="graphics/B03968_06_10.jpg" alt="Displaying upcoming meetups on Google Maps"/></div><p>Full smooth zooming capability is preserved, as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B03968_06_11.jpg" alt="Displaying upcoming meetups on Google Maps"/></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Summary</h1></div></div></div><p>In this chapter, we focused on few visualization techniques. We saw how to build wordclouds and their intuitive power to reveal, at a glance, lots of the key words, moods, and memes carried through thousands of tweets.</p><p>We then discussed interactive mapping visualizations using Bokeh. We built a world map from the ground up and created a scatter plot of critical tweets. Once the map was rendered on the browser, we could interactively hover from dot to dot and reveal the tweets originating from different parts of the world.</p><p>Our final visualization was focused on mapping upcoming meetups in London on Spark, data science, and machine learning and their respective topics, making a beautiful interactive visualization with an actual Google Map.</p></div></body></html>