<html><head></head><body>
		<div><h1 id="_idParaDest-130"><em class="italic"><a id="_idTextAnchor132"/>Chapter 12</em>: Building a Kafka Cluster</h1>
			<p>In this chapter, you will move beyond batch processing – running queries on a complete set of data – and learn about the tools used in stream processing. In stream processing, the data may be infinite and incomplete at the time of a query. One of the leading tools in handling streaming data is Apache Kafka. Kafka is a tool that allows you to send data in real time to topics. These topics can be read by consumers who process the data. This chapter will teach you how to build a three-node Apache Kafka cluster. You will also learn how to create and send messages (<strong class="bold">produce</strong>) and read data from topics (<strong class="bold">consume</strong>).</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Creating ZooKeeper and Kafka clusters</li>
				<li>Testing the Kafka cluster</li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor133"/>Creating ZooKeeper and Kafka clusters</h1>
			<p>Most <a id="_idIndexMarker621"/>tutorials on running applications that can be distributed<a id="_idIndexMarker622"/> often only show how to run a single node and then you are left wondering how you would run this in production. In this section, you will build a three-node ZooKeeper and Kafka cluster. It will run on a single machine. However, I will split each instance into its own folder and each folder simulates a server. The only modification when running on different servers would be to change localhost to the server IP. </p>
			<p>The next chapter will go into detail on the topic of Apache Kafka, but for now it is enough to understand that Kafka is a tool for building real-time data streams. Kafka was developed at LinkedIn and is now an Apache project. You can find Kafka<a id="_idIndexMarker623"/> on the web at <a href="http://kafka.apache.org">http://kafka.apache.org</a>. The website is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_12.1_B15739.jpg" alt="Figure 12.1 – Apache Kafka website&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Apache Kafka website</p>
			<p>Kafka <a id="_idIndexMarker624"/>requires another application, ZooKeeper, to manage information<a id="_idIndexMarker625"/> about the cluster, to handle discovery, and to elect leaders. You can install and build a ZooKeeper cluster on your own, but for this example, you will use the ZooKeeper scripts provided by Kafka. To learn more about ZooKeeper, you can find<a id="_idIndexMarker626"/> it at <a href="http://zookeeper.apache.org">http://zookeeper.apache.org</a>. The website is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_12.2_B15739.jpg" alt="Figure 12.2 – The Apache ZooKeeper website&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – The Apache ZooKeeper website</p>
			<p>The following section will walk you through building the cluster.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor134"/>Downloading Kafka and setting up the environment</h2>
			<p>You can download Apache<a id="_idIndexMarker627"/> Kafka from the website under the <code>wget</code> to download it from the command line. From your home directory, run the following commands:</p>
			<pre>Wget <a href="https://downloads.apache.org/kafka/2.5.0/kafka_2.12-2.5.0.tgz">https://downloads.apache.org/kafka/2.5.0/kafka_2.12-2.5.0.tgz</a>
tar -xvzf kafka_2.12-2.5.0.tgz </pre>
			<p>The preceding commands download the current Kafka version and extract it into the current directory. Because you will run three nodes, you will need to create three separate folders for Kafka. Use the following commands to create the directories:</p>
			<pre>cp kafka_2.12-2.5.0 kafka_1
cp kafka_2.12-2.5.0 kafka_2
cp kafka_2.12-2.5.0 kafka_3</pre>
			<p>You will now have three Kafka folders. You will also need to specify a log directory for each instance of Kafka. You can create three folders using the <code>mkdir</code> command, as shown:</p>
			<pre>mkdir logs_1
mkdir logs_2
mkdir logs_2</pre>
			<p>Next, you will need a <code>data</code> folder<a id="_idIndexMarker628"/> for ZooKeeper. Create the directory, and then enter it using <code>cd</code>, as shown:</p>
			<pre>mkdir data
cd data</pre>
			<p>You will run three ZooKeeper instances, so you will need to create a folder for each instance. You can do that using <code>mkdir</code>, as shown:</p>
			<pre>mkdir zookeeper_1
mkdir zookeeper_2
mkdir zookeeper_3</pre>
			<p>Each ZooKeeper instance needs an ID. It will look for a file named <code>myid</code> with an integer value in it. In each folder, create the corresponding <code>myid</code> file with the correct value. The following commands will create the file:</p>
			<pre>echo 1 &gt; zookeeper_1/myid
echo 2 &gt; zookeeper_2/myid
echo 3 &gt; zookeeper_3/myid</pre>
			<p>You have completed the prerequisite tasks for configuring ZooKeeper and Kafka. Now you can edit the configuration files for both. The next section will walk you through the process.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor135"/>Configuring ZooKeeper and Kafka</h2>
			<p>The configuration files for both <a id="_idIndexMarker629"/>ZooKeeper and Kafka are in the Kafka directory in the <code>conf</code> folder. Since<a id="_idIndexMarker630"/> you have three Kafka directories, I will walk through using <code>Kafka_1</code> and the steps will need to be applied to every other directory.</p>
			<p>From the <code>~/kafka_1/conf</code> directory, you will need to edit the <code>zookeeper.properties</code> file. You will edit the data directory and the servers, as well as adding properties. The configuration file is shown in the following code block, with the modifications in bold (for the full file, refer to the GitHub repo):</p>
			<pre># the directory where the snapshot is stored.
<strong class="bold">dataDir=/home/paulcrickard/data/zookeeper_1</strong>
# the port at which the clients will connect
<strong class="bold">clientPort=2181</strong>
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080
<strong class="bold">tickTime=2000</strong>
<strong class="bold">initLimit=5</strong>
<strong class="bold">syncLimit=2</strong>
<strong class="bold">server.1=localhost:2666:3666</strong>
<strong class="bold">server.2=localhost:2667:3667</strong>
<strong class="bold">server.3=localhost:2668:3668</strong></pre>
			<p>After making the changes, you can save the file. You will now need to modify this file in the <code>kafka_2</code> and <code>kafka_3</code> directories. Note that the <code>dataDir</code> setting will end in <code>zookeeper_2</code> and <code>zookeeper_3</code>, respectively. Also, the port number should increment by one to <code>2182</code> and <code>2183</code>. Everything else will remain the same. Again, the only reason you are changing the directory and ports is so that you can run three servers on a single machine. On three distinct servers, you would leave the settings as they are, only changing localhost to the IP address of the server.</p>
			<p>Now that ZooKeeper is <a id="_idIndexMarker631"/>configured, you can configure Kafka. In the same <code>conf</code> directory, open the <code>server.properties</code> file. The file is shown with the edits in<a id="_idIndexMarker632"/> bold (for the full file, refer to the GitHub repo):</p>
			<pre>############################# Server Basics #############################
# The id of the broker. This must be set to a unique integer for each broker.
<strong class="bold">broker.id=1</strong>
############################# Socket Server Settings #############################
# The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
<strong class="bold">listeners=PLAINTEXT://localhost:9092</strong>
############################# Log Basics #############################
# A comma separated list of directories under which to store log files
<strong class="bold">log.dirs=/home/paulcrickard/logs_1</strong>
############################# Zookeeper #############################
# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
<strong class="bold">zookeeper.connect=localhost:2181,localhost:2182,localhost:2183</strong></pre>
			<p>For each <a id="_idIndexMarker633"/>Kafka directory, you will modify the <code>server.properties</code> file to have a broker ID of <code>1</code>, <code>2</code>, and <code>3</code>. You can use any integer, but I am keeping them the same as the folder names. Also, you will set the listeners to <code>localhost:9092</code>, <code>localhost:9093</code>, and <code>localhost:9094</code>. The <code>log.dirs</code> property will be set to<a id="_idIndexMarker634"/> each of the <code>log_1</code>, <code>log_2</code>, and <code>log_3</code> folders. All three configurations will have the same value for the <code>zookeeper.connect</code> property.</p>
			<p>You have created all the necessary directories to simulate three servers and have configured both ZooKeeper and Kafka. You can now move on to starting the clusters.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor136"/>Starting the ZooKeeper and Kafka clusters</h2>
			<p>To run<a id="_idIndexMarker635"/> the servers, you will<a id="_idIndexMarker636"/> need to open six terminals – you will not run them in the background.</p>
			<p class="callout-heading">Docker</p>
			<p class="callout">You could use Docker Compose to run multiple containers and launch everything with a single file. Containers are an excellent tool, but beyond the scope of this book. </p>
			<p>In the first three terminals, you will launch the ZooKeeper cluster. In each terminal, enter the Kafka folder for each instance. Run the following command:</p>
			<pre>bin/zookeeper-server-start.sh config/zookeeper.properties </pre>
			<p>When you start all of the servers, a lot of text will scroll by as the servers look for others and hold an election. Once they connect, the text will stop, and the cluster will be ready.</p>
			<p>To start the Kafka cluster, enter an instance of the <code>kafka</code> directory in each of the three remaining terminals. You can then run the following command in each terminal:</p>
			<pre>bin/kafka-server-start.sh config/server.properties</pre>
			<p>When you are<a id="_idIndexMarker637"/> finished, you will have a line in each terminal that should look<a id="_idIndexMarker638"/> like the following line:</p>
			<pre>INFO [ZookeeperClient Kafka server] Connected. (kafka.zookeeper.zookeeperClient)</pre>
			<p>You now have two clusters of three nodes running for both ZooKeeper and Kafka. To test out the clusters and make sure everything is working properly, the next section will create a topic, a consumer and, a producer, and send some messages.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor137"/>Testing the Kafka cluster</h1>
			<p>Kafka comes <a id="_idIndexMarker639"/>with scripts to allow you to perform some basic functions from the command line. To test the cluster, you can create a topic, create a producer, send some messages, and then create a consumer to read them. If the consumer can read them, your cluster is running.</p>
			<p>To create a topic, run the following command from your <code>kafka_1</code> directory:</p>
			<pre>bin/kafka-topics.sh --create --zookeeper localhost:2181,localhost:2182,localhost:2183 --replication-factor 2 --partitions 1 --topic dataengineering</pre>
			<p>The preceding command runs the <code>kafka-topics</code> script with the <code>create</code> flag. It then specifies the ZooKeeper cluster IP addresses and the topic. If the topic was created, the terminal will have printed the following line:</p>
			<pre>created topic dataengineering</pre>
			<p>You can verify this by listing all the topics in the Kafka cluster using the same script, but with the <code>list</code> flag:</p>
			<pre>bin/kafka-topics.sh –list --zookeeper localhost:2181,localhost:2182,localhost:2183</pre>
			<p>The result <a id="_idIndexMarker640"/>should be a single line: <code>dataengineering</code>. Now that you have a topic, you can send and receive messages on it. The next section will show you how.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor138"/>Testing the cluster with messages</h2>
			<p>In the<a id="_idIndexMarker641"/> next chapters, you will use Apache NiFi and Python to <a id="_idIndexMarker642"/>send and receive messages, but for a quick test of the cluster, you can use the scripts provided to do this as well. To create a producer, use the following command:</p>
			<pre>bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic dataengineering</pre>
			<p>The preceding command uses the <code>kafka-console-producer</code> script with the <code>broker-list</code> flag that passes the <code>kafka</code> cluster servers. Lastly, it takes a topic, and since we only have one, it is <code>dataengineering</code>. When it is ready, you will have a <code>&gt;</code> prompt to type messages into. </p>
			<p>To read the messages, you will need to use the <code>kafka-console-consumer</code> script. The command is as shown:</p>
			<pre>bin/kafka-console-consumer.sh --zookeeper localhost:2181,localhost:2182,localhost:2183 --topic dataengineering –from-beginning</pre>
			<p>The consumer passes the <code>zookeeper</code> flag with the list of servers. It also specifies the topic and the <code>from-beginning</code> flag. If you had already read messages, you could specify an <code>offset</code> flag with the index of the last message so that you start from your last position. </p>
			<p>Putting the producer and consumer terminals next to each other, you should have something like the following screenshot:</p>
			<div><div><img src="img/Figure_12.3_B15739.jpg" alt="Figure 12.3 – Producer and consumer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Producer and consumer</p>
			<p>In the preceding <a id="_idIndexMarker643"/>screenshot, you will notice that I typed <em class="italic">first message</em> and <em class="italic">second message</em> twice. When the consumer turned on, it read all the messages<a id="_idIndexMarker644"/> on the topic. Once it has read them all, it will await new messages. If you type a message in the producer, it will show up in the consumer window after a short lag.</p>
			<p>You now have a fully functional Kafka cluster and are ready to move on to stream processing with NiFi and Python in the next chapter.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor139"/>Summary</h1>
			<p>In this chapter, you learned how to create a Kafka cluster, which required the creation of a ZooKeeper cluster. While you ran all of the instances on a single machine, the steps you took will work on different servers too. Kafka allows the creation of real-time data streams and will require a different way of thinking than the batch processing you have been doing.</p>
			<p>The next chapter will explain the concepts involved in streams in depth. You will also learn how to process streams in both NiFi and Python.</p>
		</div>
	</body></html>