- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Deep Dive into Data Wrangling with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform subsetting, filtering, and grouping on pandas DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply Boolean filtering and indexing from a DataFrame to choose specific elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform JOIN operations in pandas that are analogous to the SQL command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify missing or corrupted data and choose to drop or apply imputation techniques
    on missing or corrupted data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn about pandas DataFrames in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will learn about several advanced operations involving pandas
    DataFrames and NumPy arrays. On completing the detailed activity for this chapter,
    you will have handled real-life datasets and understood the process of data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: Subsetting, Filtering, and Grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important aspects of data wrangling is to curate the data carefully
    from the deluge of streaming data that pours into an organization or business
    entity from various sources. Lots of data is not always a good thing; rather,
    data needs to be useful and of high-quality to be effectively used in downstream
    activities of a data science pipeline such as machine learning and predictive
    model building. Moreover, one data source can be used for multiple purposes and
    this often requires different subsets of data to be processed by a data wrangling
    module. This is then passed on to separate analytics modules.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say you are doing data wrangling on US State level economic
    output. It is a fairly common scenario that one machine learning model may require
    data for large and populous states (such as California, Texas, and so on), while
    another model demands processed data for small and sparsely populated states (such
    as Montana or North Dakota). As the frontline of the data science process, it
    is the responsibility of the data wrangling module to satisfy the requirements
    of both these machine learning models. Therefore, as a data wrangling engineer,
    you have to filter and group data accordingly (based on the population of the
    state) before processing them and producing separate datasets as the final output
    for separate machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in some cases, data sources may be biased, or the measurement may corrupt
    the incoming data occasionally. It is a good idea to try to filter only the error-free,
    good data for downstream modeling. From these examples and discussions, it is
    clear that filtering and grouping/bucketing data is an essential skill to have
    for any engineer that's engaged in the task of data wrangling. Let's proceed to
    learn about a few of these skills with pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 48: Loading and Examining a Superstore''s Sales Data from an Excel
    File'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will load and examine an Excel file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read an Excel file into pandas, you will need a small package called `xlrd`
    to be installed on your system. If you are working from inside this book''s Docker
    container, then this package may not be available next time you start your container,
    and you have to follow the same step. Use the following code to install the xlrd
    package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Excel file from GitHub by using the simple pandas method `read_excel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine all the columns and check if they are useful for analysis:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 Output of the Excel file in a DataFrame](img/C11065_04_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 4.1 Output of the Excel file in a DataFrame
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: On examining the file, we can see that the first column, called **Row ID**,
    is not very useful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Drop this column altogether from the DataFrame by using the `drop` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the number of rows and columns in the newly created dataset. We will
    use the `shape` function here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the dataset has 9,994 rows and 20 columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Subsetting the DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Subsetting** involves the extraction of partial data based on specific columns
    and rows, as per business needs. Suppose we are interested only in the following
    information from this dataset: Customer ID, Customer Name, City, Postal Code,
    and Sales. For demonstration purposes, let''s assume that we are only interested
    in 5 records – rows 5-9\. We can subset the DataFrame to extract only this much
    information using a single line of Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `loc` method to index the dataset by name of the columns and index
    of the rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: DataFrame indexed by name of the columns](img/C11065_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: DataFrame indexed by name of the columns'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We need to pass on two arguments to the `loc` method – one for indicating the
    rows, and another for indicating the columns. These should be Python lists.
  prefs: []
  type: TYPE_NORMAL
- en: For the rows, we have to pass a list [5,6,7,8,9], but instead of writing that
    explicitly, we use a list comprehension, that is, `[i for i in range(5,10)]`.
  prefs: []
  type: TYPE_NORMAL
- en: Because the columns we are interested in are not contiguous, we cannot just
    put a continuous range and need to pass on a list containing the specific names.
    So, the second argument is just a simple list with specific column names.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset shows the fundamental concepts of the process of subsetting a DataFrame
    based on business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example Use Case: Determining Statistics on Sales and Profit'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This quick section shows a typical use case of subsetting. Suppose we want
    to calculate descriptive statistics (mean, median, standard deviation, and so
    on) of records 100-199 for sales and profit. This is how subsetting helps us to
    achieve that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 Output of descriptive statistics of data](img/C11065_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Output of descriptive statistics of data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Furthermore, we can create boxplots of sales and profit figures from this final
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply extract records 100-199 and run the `describe` function on it because
    we don''t want to process all the data! For this particular business question,
    we are only interested in sales and profit numbers and therefore we should not
    take the easy route and run a describe function on all the data. For a real-life
    dataset, the number of rows and columns could often be in the millions, and we
    don''t want to compute anything that is not asked for in the data wrangling task.
    We always aim to subset the exact data that is needed to be processed and run
    statistical or plotting functions on that partial data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Boxplot of sales and profit](img/C11065_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Boxplot of sales and profit'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 49: The unique Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before continuing further with filtering methods, let's take a quick detour
    and explore a super useful function called `unique`. As the name suggests, this
    function is used to scan through the data quickly and extract only the unique
    values in a column or row.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the superstore sales data, you will notice that there are columns
    like "Country", "State", and "City". A natural question will be to ask how many
    countries/states/cities are present in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the countries/states/cities for which the information is in the database,
    with one simple line of code, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5: Different states present in the dataset](img/C11065_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.5: Different states present in the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: You will see a list of all the states whose data is present in the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `nunique` method to count the number of unique values, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This returns 49 for this dataset. So, one out of 50 states in the US does not
    appear in this dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similarly, if we run this function on the Country column, we get an array with
    only one element, `United States`. Immediately, we can see that we don't need
    to keep the country column at all, because there is no useful information in that
    column except that all the entries are the same. This is how a simple function
    helped us to decide about dropping a column altogether – that is, removing 9,994
    pieces of unnecessary data!
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Selection and Boolean Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often, we don't want to process the whole dataset and would like to select only
    a partial dataset whose contents satisfy a particular condition. This is probably
    the most common use case of any data wrangling task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of our superstore sales dataset, think of these common questions
    that may arise from the daily activity of the business analytics team:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the average sales and profit figures in California?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which states have the highest and lowest total sales?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What consumer segment has the most variance in sales/profit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the top 5 states in sales, which shipping mode and product category are
    the most popular choices?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Countless examples can be given where the business analytics team or the executive
    management want to glean insight from a particular subset of data that meet certain
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any prior experience with SQL, you will know that these kinds of
    questions require fairly complex SQL query writing. Remember the WHERE clause?
  prefs: []
  type: TYPE_NORMAL
- en: We will show you how to use conditional subsetting and Boolean filtering to
    answer such questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to understand the critical concept of boolean indexing. This
    process essentially accepts a conditional expression as an argument and returns
    a dataset of booleans in which the `TRUE` value appears in places where the condition
    was satisfied. A simple example is shown in the following code. For demonstration
    purposes, we subset a small dataset of 10 records and 3 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Sample dataset](img/C11065_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Sample dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, if we just want to know the records with sales higher than $100, then
    we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following boolean DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: Records with sales higher than $100](img/C11065_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Records with sales higher than $100'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note the True and False entries in the **Sales** column. Values in the **Ship
    Mode** and **State** columns were not impacted by this code because the comparison
    was with a numerical quantity, and the only numeric column in the original DataFrame
    was **Sales**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see what happens if we pass this boolean DataFrame as an index
    to the original DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Results after passing the boolean DataFrame as an index to original
    DataFrame](img/C11065_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Results after passing the boolean DataFrame as an index to the
    original DataFrame'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The NaN values came from the fact that the preceding code tried to create a
    DataFrame with TRUE indices (in the Boolean DataFrame) only.
  prefs: []
  type: TYPE_NORMAL
- en: The values which were TRUE in the boolen DataFrame were retained in the final
    output DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The program inserted **NaN** values for the rows where data was not available
    (because they were discarded due to the Sales value being < $100).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we probably don''t want to work with this resulting DataFrame with `Sales
    > $100`. We can achieve that by simply passing only the `Sales` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the expected result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Results after removing the NaN values](img/C11065_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Results after removing the NaN values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We are not limited to conditional expressions involving numeric quantities only.
    Let's try to extract high sales values (> $100) for entries that do not involve
    Colorado.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the following code to accomplish that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of a conditional involving string. In this expression, we are joining
    two conditionals by an & operator. Both conditions must be wrapped inside parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first conditional expression simply matches the entries in the `State`
    column to the string `Colorado` and assigns TRUE/FALSE accordingly. The second
    conditional is the same as before. Together, joined by the & operator, they extract
    only those rows for which `State` is not `Colorado` and `Sales` is `> $100`. We
    get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Results where State is not California and Sales is higher than
    $100'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Note**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although, in theory, there is no limit on how complex a conditional you can
    build using individual expressions and & (LOGICAL AND) and | (LOGICAL OR) operators,
    it is advisable to create intermediate boolean DataFrames with limited conditional
    expressions and build your final DataFrame step by step. This keeps the code legible
    and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 50: Setting and Resetting the Index'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, we may need to reset or eliminate the default index of a DataFrame
    and assign a new column as an index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `matrix_data`, `row_labels`, and `column_headings` functions using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a DataFrame using the `matrix_data`, `row_labels`, and `column_headings`
    functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.11: The original DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Reset the index, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C11065_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.12: DataFrame after resetting the index'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Reset the index with `drop` set to `True`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.13: DataFrame after resetting the index with the drop option set
    to true](img/C11065_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.13: DataFrame after resetting the index with the drop option set to
    true'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add a new column using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.14: DataFrame after adding a new column called Profession'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, set the `Profession` column as an `index` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: DataFrame after setting the Profession as an index'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 51: The GroupBy Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Group by refers to a process involving one or more of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into groups based on some criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a function to each group independently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the results into a data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In many situations, we can split the dataset into groups and do something with
    those groups. In the apply step, we might wish to do one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregation**: Compute a summary statistic (or statistics) for each group
    – sum, mean, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation**: Perform a group-specific computation and return a like-indexed
    object – z-transformation or filling missing data with a value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtration**: Discard few groups, according to a group-wise computation that
    evaluates TRUE or FALSE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is, of course, a describe method to this `GroupBy` object, which produces
    the summary statistics in the form of a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '`GroupBy` is not limited to a single variable. If you pass on multiple variables
    (as a list), then you will get back a structure essentially similar to a Pivot
    Table (from Excel). The following is an example where we group together all the
    states and cities from the whole dataset (the snapshot is a partial view only).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The name `GroupBy` should be quite familiar to those who have used a SQL-based
    tool before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 10-record subset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a pandas DataFrame using the `groupby` object, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the mean sales figure by state by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: Output after grouping the state with the listing mean sales](img/C11065_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.16: Output after grouping the state with the listing mean sales'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate the total sales figure by state by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: The output after grouping the state with the listing sum of
    sales](img/C11065_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.17: The output after grouping the state with the listing sum of sales'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Subset that DataFrame for a particular state and show the statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18: Checking the statistics of a particular state](img/C11065_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.18: Checking the statistics of a particular state'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform a similar summarization by using the `Ship Mode` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19: Checking the sales by summarizing the Ship Mode attribute](img/C11065_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.19: Checking the sales by summarizing the Ship Mode attribute'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how pandas has grouped the data by `State` first and then by cities under
    each state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the complete summary statistics of sales by every city in each state
    – all by two lines of code by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: Checking the summary statistics of sales](img/C11065_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: Checking the summary statistics of sales'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Detecting Outliers and Handling Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outlier detection and handling missing values fall under the subtle art of data
    quality checking. A modeling or data mining process is fundamentally a complex
    series of computations whose output quality largely depends on the quality and
    consistency of the input data being fed. The responsibility of maintaining and
    gate keeping that quality often falls on the shoulders of a data wrangling team.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the obvious issue of poor quality data, missing data can sometimes
    wreak havoc with the machine learning (ML) model downstream. A few ML models,
    like Bayesian learning, are inherently robust to outliers and missing data, but
    commonly techniques like Decision Trees and Random Forest have an issue with missing
    data because the fundamental splitting strategy employed by these techniques depends
    on an individual piece of data and not a cluster. Therefore, it is almost always
    imperative to impute missing data before handing it over to such a ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outlier detection is a subtle art. Often, there is no universally agreed definition
    of an outlier. In a statistical sense, a data point that falls outside a certain
    range may often be classified as an outlier, but to apply that definition, you
    need to have a fairly high degree of certainty about the assumption of the nature
    and parameters of the inherent statistical distribution about the data. It takes
    a lot of data to build that statistical certainty and even after that, an outlier
    may not be just an unimportant noise but a clue to something deeper. Let''s take
    an example with some fictitious sales data from an American fast food chain restaurant.
    If we want to model the daily sales data as a time series, we observe an unusual
    spike in the data somewhere around mid-April:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Fictitious sales data of an American fast food chain restaurant'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A good data scientist or data wrangler should develop curiosity about this data
    point rather than just rejecting it just because it falls outside the statistical
    range. In the actual anecdote, the sales figure really spiked that day because
    of an unusual reason. So, the data was real. But just because it was real does
    not mean it is useful. In the final goal of building a smoothly varying time series
    model, this one point should not matter and should be rejected. But the chapter
    here is that we cannot reject outliers without paying some attention to them.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the key to outliers is their systematic and timely detection in an
    incoming stream of millions of data or while reading data from a cloud-based storage.
    In this topic, we will quickly go over some basic statistical tests for detecting
    outliers and some basic imputation techniques for filling up missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Missing Values in Pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most useful functions to detect missing values is `isnull`. Here,
    we have a snapshot of a `DataFrame` called `df_missing` (sampled partially from
    the superstore DataFrame we are working with) with some missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22: DataFrame with missing values](img/C11065_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: DataFrame with missing values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, if we simply run the following code, we will get a DataFrame that''s the
    same size as the original with boolean values as TRUE for the places where a **NaN**
    was encountered. Therefore, it is simple to test for the presence of any **NaN**/missing
    value for any row or column of the DataFrame. You just have to add the particular
    row and column of this boolean DataFrame. If the result is greater than zero,
    then you know there are some TRUE values (because FALSE here is denoted by 0 and
    TRUE here is denoted by 1) and correspondingly some missing values. Try the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: DataFrame with the Excel values](img/C11065_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: DataFrame with the Excel values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use the `isnull` function on the DataFrame and observe the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.24 Output highlighting the missing values](img/C11065_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 Output highlighting the missing values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is an example of some very simple code to detect, count, and print out
    missing values in every column of a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This code scans every column of the DataFrame, calls the `isnull` function,
    and sums up the returned object (a pandas Series object, in this case) to count
    the number of missing values. If the missing value is greater than zero, it prints
    out the message accordingly. The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25: Output of counting the missing values](img/C11065_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: Output of counting the missing values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 52: Filling in the Missing Values with fillna'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To handle missing values, you should first look for ways not to drop them altogether
    but to fill them somehow. The `fillna` method is a useful function for performing
    this task on pandas DataFrames. The `fillna` method may work for string data,
    but not for numerical columns like sales or profits. So, we should restrict ourselves
    in regards to this fixed string replacement to non-numeric text-based columns
    only. The `Pad` or `ffill` function is used to fill forward the data, that is,
    copy it from the preceding data of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mean` function can be used to fill using the average of the two values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill all missing values with the string `FILL` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.26: Missing values replaced with FILL](img/C11065_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Missing values replaced with FILL'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fill in the specified columns with the string `FILL` by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.27: Specified columns replaced with FILL'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: In all of these cases, the function works on a copy of the original DataFrame.
    So, if you want to make the changes permanent, you have to assign the DataFrames
    that are returned by these functions to the original DataFrame object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the values using pad or backfill by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `backfill` or `bfill` to fill backward, that is, copy from the next data
    in the series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.28: Using forward fill and backward fill to fill in missing data](img/C11065_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: Using forward fill and backward fill to fill in missing data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also fill by using a function average of DataFrames. For example, we
    may want to fill the missing values in Sales by the average sales amount. Here
    is how we can do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C11065_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.29: Using average to fill in missing data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 53: Dropping Missing Values with dropna'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function is used to simply drop the rows or columns that contain NaN/missing
    values. However, there is some choice involved.
  prefs: []
  type: TYPE_NORMAL
- en: If the axis parameter is set to zero, then rows containing missing values are
    dropped; if the axis parameter is set to one, then columns containing missing
    values are dropped. These are useful if we don't want to drop a particular row/column
    if the NaN values do not exceed a certain percentage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two arguments that are useful for the `dropna`() method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `how` argument determines if a row or column is removed from a DataFrame,
    when we have at least one NaN or all NaNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `thresh` argument requires that many non-NaN values to keep the row/column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To set the axis parameter to zero and drop all missing rows, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To set the axis parameter to one and drop all missing rows, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.30: Dropping rows or columns to handle missing data](img/C11065_04_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.30: Dropping rows or columns to handle missing data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Drop the values with the axis set to one and thresh set to 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.31: DataFrame with values dropped with axis=1 and thresh=10](img/C11065_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: DataFrame with values dropped with axis=1 and thresh=10'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All of these methods work on a temporary copy. To make a permanent change, you
    have to set `inplace=True` or assign the result to the original DataFrame, that
    is, overwrite it.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier Detection Using a Simple Statistical Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we''ve already discussed, outliers in a dataset can occur due to many factors
    and in many ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Data entry errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimental errors (data extraction related)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurement errors due to noise or instrumental failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing errors (data manipulation or mutations due to coding error)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling errors (extracting or mixing data from wrong or various sources)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is impossible to pin-point one universal method for outlier detection. Here,
    we will show you some simple tricks for numeric data using standard statistical
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boxplots may show unusual values. Corrupt two sales values by assigning negative,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the boxplot, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_04_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.32: Boxplot of sales and profit'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can create simple boxplots to check for any unusual/nonsensical values. For
    example, in the preceding example, we intentionally corrupted two sales values
    to be negative and they were readily caught in a boxplot.
  prefs: []
  type: TYPE_NORMAL
- en: Note that profit may be negative, so those negative points are generally not
    suspicious. But sales cannot be negative in general, so they are detected as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a distribution of a numerical quantity and check for values that
    lie at the extreme end to see if they are truly part of the data or outlier. For
    example, if a distribution is almost normal, then any value more than 4 or 5 standard
    deviations away may be a suspect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33: Value away from the main outliers](img/C11065_04_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.33: Value away from the main outliers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Concatenating, Merging, and Joining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Merging and joining tables or datasets are highly common operations in the day-to-day
    job of a data wrangling professional. These operations are akin to the JOIN query
    in SQL for relational database tables. Often, the key data is present in multiple
    tables, and those records need to be brought into one combined table that's matching
    on that common key. This is an extremely common operation in any type of sales
    or transactional data, and therefore must be mastered by a data wrangler. The
    pandas library offers nice and intuitive built-in methods to perform various types
    of JOIN queries involving multiple DataFrame objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 54: Concatenation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will start by learning the concatenation of DataFrames along various axes
    (rows or columns). This is a very useful operation as it allows you to grow a
    DataFrame as the new data comes in or new feature columns need to be inserted
    in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample 4 records each to create three DataFrames at random from the original
    sales dataset we are working with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a combined DataFrame with all the rows concatenated by using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.34: Concatenating DataFrames together'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C11065_04_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.34: Concatenating DataFrames together'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also try concatenating along the columns, although that does not make
    any practical sense for this particular example. However, pandas fills in the
    unavailable values with **NaN** for that operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.35: Output after concatenating the DataFrames](img/C11065_04_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.35: Output after concatenating the DataFrames'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 55: Merging by a Common Key'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Merging by a common key is an extremely common operation for data tables as
    it allows you to rationalize multiple sources of data in one master database –
    that is, if they have some common features/keys.
  prefs: []
  type: TYPE_NORMAL
- en: This is often the first step in building a large database for machine learning
    tasks where daily incoming data may be put into separate tables. However, at the
    end of the day, the most recent table needs to be merged with the master data
    table to be fed into the backend machine learning server, which will then update
    the model and its prediction capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will show a simple example of an inner join with Customer Name as
    the key:'
  prefs: []
  type: TYPE_NORMAL
- en: 'One DataFrame, `df_1`, had shipping information associated with the customer
    name, and another table, `df_2`, had the product information tabulated. Our goal
    is to merge these tables into one DataFrame on the common customer name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.36: Entries in table df_1](img/C11065_04_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.36: Entries in table df_1'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The second DataFrame is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.37: Entries in table df_2](img/C11065_04_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.37: Entries in table df_2'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Join these two tables by inner join by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.38: Inner join on table df_1 and table df_2](img/C11065_04_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.38: Inner join on table df_1 and table df_2'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Drop the duplicates by using the following command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.39: Inner join on table df_1 and table df_2 after dropping the duplicates](img/C11065_04_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.39: Inner join on table df_1 and table df_2 after dropping the duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract another small table called `df_3` to show the concept of an outer join:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.40: Creating table df_3'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform an inner join on `df_1` and `df_3` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.41: Merging table df_1 and table df_3 and dropping duplicates](img/C11065_04_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.41: Merging table df_1 and table df_3 and dropping duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform an outer join on `df_1` and `df_3` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.42: Outer join on table df_1 and table df_2 and dropping the duplicates](img/C11065_04_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.42: Outer join on table df_1 and table df_2 and dropping the duplicates'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how some `NaN` and `NaT` values are inserted automatically because no
    corresponding entries could be found for those records, as those are the entries
    with unique customer names from their respective tables. `NaT` represents a Not
    a Time object, as the objects in the Ship Date column are of the nature of Timestamp
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 56: The join Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Joining is performed based on **index** **keys** and is done by combining the
    columns of two potentially differently indexed DataFrames into a single one. It
    offers a faster way to accomplish merging by row indices. This is useful if the
    records in different tables are indexed differently but represent the same inherent
    data and you want to merge them into a single table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the following tables with customer name as the index by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The outputs is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.43: DataFrames df_1 and df_2'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform a left join on `df_1` and `df_2` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.44: Left join on table df_1 and table df_2 after dropping the duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform a right join on `df_1` and `df_2` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.45: Right join on table df_1 and table df_2 after dropping the duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform an inner join on `df_1` and `df_2` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.46: Inner join on table df_1 and table df_2 after dropping the duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform an outer join on `df_1` and `df_2` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.47: Outer join on table df_1 and table df_2 after dropping the duplicates'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Useful Methods of Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this topic, we will discuss some small utility functions that are offered
    by pandas so that we can work efficiently with DataFrames. They don't fall under
    any particular group of function, so they are mentioned here under the Miscellaneous
    category.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 57: Randomized Sampling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sampling a random fraction of a big DataFrame is often very useful so that we
    can practice other methods on them and test our ideas. If you have a database
    table of 1 million records, then it is not computationally effective to run your
    test scripts on the full table.
  prefs: []
  type: TYPE_NORMAL
- en: However, you may also not want to extract only the first 100 elements as the
    data may have been sorted by a particular key and you may get an uninteresting
    table back, which may not represent the full statistical diversity of the parent
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these situations, the `sample` method comes in super handy so that we can
    randomly choose a controlled fraction of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the number of samples that you require from the DataFrame by using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_48.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.48: DataFrame with 5 samples'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Specify a definite fraction (percentage) of data to be sampled by using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.49: DataFrame with 0.1% data sampled'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also choose if sampling is done with replacement, that is, whether the
    same record can be chosen more than once. The default replace choice is FALSE,
    that is, no repetition, and sampling will try to choose new elements only.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose the sampling by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.50: DataFrame with 0.1% data sampled and repetition enabled'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The value_counts Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discussed the `unique` method before, which finds and counts the unique records
    from a DataFrame. Another useful function in a similar vein is `value_counts`.
    This function returns an object containing counts of unique values. In the object
    that is returned, the first element is the most frequently used object. The elements
    are arranged in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a practical application of this method to illustrate the utility.
    Suppose your manager asks you to list the top 10 customers from the big sales
    database that you have. So, the business question is: which 10 customers'' names
    occur the most frequently in the sales table? You can achieve the same with an
    SQL query if the data is in a RDBMS, but in pandas, this can be done by using
    one simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_04_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.51: List of top 10 customers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `value_counts` method returns a series of the counts of all unique customer
    names sorted by the frequency of the count. By asking for only the first 10 elements
    of that list, this code returns a series of the most frequently occurring top
    10 customer names.
  prefs: []
  type: TYPE_NORMAL
- en: Pivot Table Functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to group by, pandas also offer pivot table functionality, which works
    the same as a pivot table in spreadsheet programs like MS Excel. For example,
    in this sales database, you want to know the average sales, profit, and quantity
    sold, by Region and State (two levels of index).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract this information by using one simple piece of code (we sample
    100 records first for keeping the computation fast and then apply the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (note that your specific output may be different due
    to random sampling):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_04_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.52: Sample of 100 records'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 58: Sorting by Column Values – the sort_values Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sorting a table by a particular column is one of the most frequently used operations
    in the daily work of an analyst. Not surprisingly, pandas provide a simple and
    intuitive method for sorting called the `sort_values` method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a random sample of 15 records and then show how we can sort by the Sales
    column and then by both the Sales and State columns together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.53: Sample of 15 records](img/C11065_04_53.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.53: Sample of 15 records'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sort the values with respect to `Sales` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.54: DataFrame with the Sales value sorted](img/C11065_04_54.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.54: DataFrame with the Sales value sorted'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sort the values with respect to Sales and State:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.55: DataFrame sorted with respect to Sales and State'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 59: Flexibility for User-Defined Functions with the apply Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pandas library provides great flexibility to work with user-defined functions
    of arbitrary complexity through the `apply` method. Much like the native Python
    `apply` function, this method accepts a user-defined function and additional arguments
    and returns a new column after applying the function on a particular column element-wise.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, suppose we want to create a column of categorical features like
    high/medium/low based on the sales price column. Note that it is a conversion
    from a numeric value to a categorical factor (string) based on certain conditions
    (threshold values of sales):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a user-defined function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sample 100 records randomly from the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_56.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.56: 100 sample records from the database'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use the `apply` method to apply the categorization function onto the `Sales`
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_57.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.57: DataFrame with 10 rows after using the apply function on the Sales
    column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `apply` method also works with the built-in native Python functions. For
    practice, let''s create another column for storing the length of the name of the
    customer. We can do that using the familiar `len` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_58.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.58: DataFrame with a new column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of writing out a separate function, we can even insert lambda expressions
    directly into the apply method for short functions. For example, let''s say we
    are promoting our product and want to show the discounted sales price if the original
    price is *> $200*. We can do this using a `lambda` function and the `apply` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_04_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.59: Lambda function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The lambda function contains a conditional, and a discount is applied to those
    records where the original sales price is > $200.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Working with the Adult Income Dataset (UCI)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, you will work with the Adult Income Dataset from the UCI machine
    learning portal. The Adult Income dataset has been used in many machine learning
    papers that address classification problems. You will read the data from a CSV
    file into a pandas DataFrame and do some practice on the advanced data wrangling
    you learned about in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this activity is to practice various advanced pandas DataFrame operations,
    for example, for subsetting, applying user-defined functions, summary statistics,
    visualizations, boolean indexing, group by, and outlier detection on a real-life
    dataset. We have the data downloaded as a CSV file on the disk for your ease.
    However, it is recommended to practice data downloading on your own so that you
    are familiar with the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the URL for the dataset: [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the URL for the description of the dataset and the variables: [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps that will help you solve this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read the adult income dataset from the following URL: [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter04/Activity06/](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson04/Activity06/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a script that will read a text file line by line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a name of `Income` for the response variable to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a DataFrame with only age, education, and occupation by using subsetting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot a histogram of age with a bin size of 20.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function to strip the whitespace characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `apply` method to apply this function to all the columns with string
    values, create a new column, copy the values from this new column to the old column,
    and drop the new column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the number of people who are aged between 30 and 50.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group the records based on age and education to find how the mean age is distributed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group by occupation and show the summary statistics of age. Find which profession
    has the oldest workers on average and which profession has its largest share of
    the workforce above the 75th percentile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use subset and groupby to find outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the values on a bar chart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the data using common keys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 297.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we dived deep into the pandas library to learn advanced data
    wrangling techniques. We started with some advanced subsetting and filtering on
    DataFrames and round this up by learning about boolean indexing and conditional
    selection of a subset of data. We also covered how to set and reset the index
    of a DataFrame, especially while initializing.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about a particular topic that has a deep connection with traditional
    relational database systems – the group by method. Then, we dived deep into an
    important skill for data wrangling - checking for and handling missing data. We
    showed you how pandas help in handling missing data using various imputation techniques.
    We also discussed methods for dropping missing values. Furthermore, methods and
    usage examples of concatenation and merging of DataFrame objects were shown. We
    saw the join method and how it compares to a similar operation in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, miscellaneous useful methods on DataFrames, such as randomized sampling,
    `unique`, `value_count`, `sort_values`, and pivot table functionality were covered.
    We also showed an example of running an arbitrary user-defined function on a DataFrame
    using the `apply` method.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about the basic and advanced data wrangling techniques with NumPy
    and pandas libraries, the natural question of data acquiring rises. In the next
    chapter, we will show you how to work with a wide variety of data sources, that
    is, you will learn how to read data in tabular format in pandas from different
    sources.
  prefs: []
  type: TYPE_NORMAL
