- en: Chapter 4. Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 Spark SQL
- en: 'Spark SQL is a Spark module for processing a structured data. This chapter
    is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark模块，用于处理结构化数据。本章分为以下食谱：
- en: Understanding the Catalyst optimizer
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Catalyst优化器
- en: Creating HiveContext
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建HiveContext
- en: Inferring schema using case classes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用case类推断模式
- en: Programmatically specifying the schema
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以编程方式指定模式
- en: Loading and saving data using the Parquet format
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Parquet格式加载数据和保存数据
- en: Loading and saving data using the JSON format
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用JSON格式加载数据和保存数据
- en: Loading and saving data from relational databases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系数据库加载数据和保存数据
- en: Loading and saving data from an arbitrary source
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任意源加载数据和保存数据
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Spark can process data from various data sources such as HDFS, Cassandra, HBase,
    and relational databases, including HDFS. Big data frameworks (unlike relational
    database systems) do not enforce schema while writing. HDFS is a perfect example
    where any arbitrary file is welcome during the write phase. Reading data is a
    different story, however. You need to give some structure to even completely unstructured
    data to make sense out of it. With this structured data, SQL comes very handy
    when it comes to analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以从各种数据源处理数据，例如HDFS、Cassandra、HBase和关系数据库，包括HDFS。大数据框架（与关系数据库系统不同）在写入时不会强制执行模式。HDFS是写入阶段欢迎任何任意文件的完美例子。然而，读取数据是另一回事。即使是完全非结构化的数据，也需要提供一些结构，以便从中获得意义。有了这种结构化数据，当涉及到分析时，SQL就非常方便了。
- en: Spark SQL is a relatively new component in Spark ecosystem, introduced in Spark
    1.0 for the first time. It incorporates a project named Shark, which was an attempt
    to make Hive run on Spark.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark生态系统中的一个相对较新的组件，首次在Spark 1.0版本中引入。它包含一个名为Shark的项目，该项目的目的是让Hive在Spark上运行。
- en: Hive is essentially a relational abstraction, which converts SQL queries to
    MapReduce jobs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Hive本质上是一种关系抽象，它将SQL查询转换为MapReduce作业。
- en: '![Introduction](img/3056_04_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![简介](img/3056_04_01.jpg)'
- en: Shark replaced the MapReduce part with Spark while retaining most of the code
    base.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Shark用Spark替换了MapReduce部分，同时保留了大部分代码库。
- en: '![Introduction](img/3056_04_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![简介](img/3056_04_02.jpg)'
- en: Initially, it worked fine, but very soon, Spark developers hit roadblocks and
    could not optimize it any further. Finally, they decided to write the SQL Engine
    from scratch and that gave birth to Spark SQL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，它运行良好，但很快Spark的开发者遇到了瓶颈，无法进一步优化它。最终，他们决定从头开始编写SQL引擎，这催生了Spark SQL。
- en: '![Introduction](img/3056_04_03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![简介](img/3056_04_03.jpg)'
- en: Spark SQL took care of all the performance challenges, but it had to provide
    compatibility with Hive and for that reason, a new wrapper context, `HiveContext`,
    was created on top of `SQLContext`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL解决了所有性能挑战，但必须与Hive保持兼容，因此，在`SQLContext`之上创建了一个新的包装上下文`HiveContext`。
- en: Spark SQL supports accessing data using standard SQL queries and HiveQL, a SQL-like
    query language that Hive uses. In this chapter, we will explore different features
    of Spark SQL. It supports a subset of HiveQL as well as a subset of SQL 92\. It
    runs SQL/HiveQL queries alongside, or replacing the existing Hive deployments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL支持使用标准SQL查询和HiveQL（Hive使用的类似SQL的查询语言）访问数据。在本章中，我们将探讨Spark SQL的不同特性。它支持HiveQL和SQL
    92的子集。它可以在现有的Hive部署旁边或替换它们运行SQL/HiveQL查询。
- en: Running SQL is only a part of the reason for the creation of Spark SQL. One
    big reason is that it helps to create and run Spark programs faster. It lets developers
    write less code, program read less data, and let the catalyst optimizer do all
    the heavy lifting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行SQL只是创建Spark SQL的部分原因。一个很大的原因是它有助于更快地创建和运行Spark程序。它让开发者编写更少的代码，读取更少的数据，并让Catalyst优化器完成所有繁重的工作。
- en: Spark SQL uses a programming abstraction called **DataFrame**. It is a distributed
    collection of data organized in named columns. DataFrame is equivalent to a database
    table, but provides much finer level of optimization. The DataFrame API also ensures
    that Spark's performance is consistent across different language bindings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL使用一种名为**DataFrame**的编程抽象。它是有名列组织的数据的分布式集合。DataFrame相当于数据库表，但提供了更细粒度的优化。DataFrame
    API还确保Spark的性能在不同语言绑定之间保持一致。
- en: Let's contrast DataFrames with RDDs. An RDD is an opaque collection of objects
    with no idea about the format of the underlying data. In contrast, DataFrames
    have schema associated with them. You can also look at DataFrames as RDDs with
    schema added to them. In fact, until Spark 1.2, there was an artifact called **SchemaRDD**,
    which has now evolved into DataFrame. They provide much richer functionality than
    SchemaRDDs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对比DataFrame和RDD。RDD是一个不透明的对象集合，对底层数据的格式一无所知。相比之下，DataFrame与它们关联了模式。您也可以将DataFrame看作是添加了模式的RDD。实际上，直到Spark
    1.2，有一个名为**SchemaRDD**的工件，现在已经演变成了DataFrame。它们提供了比SchemaRDDs更丰富的功能。
- en: This extra information about schema makes possible to do a lot of optimizations,
    which were not otherwise possible.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这额外的模式信息使得进行许多优化成为可能，而这些优化在其他情况下是不可能的。
- en: DataFrames also transparently load from various data sources, such as Hive tables,
    Parquet files, JSON files, and external databases using JDBC. DataFrames can be
    viewed as RDDs of row objects, allowing users to call the procedural Spark APIs
    such as map.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames也可以透明地从各种数据源加载，例如Hive表、Parquet文件、JSON文件和外部数据库使用JDBC。DataFrames可以被看作是行对象的RDD，使用户能够调用如map这样的过程式Spark
    API。
- en: The DataFrame API is available in Scala, Java, Python, and also R starting Spark
    1.4.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API在Scala、Java、Python中可用，从Spark 1.4开始也支持R。
- en: Users can perform relational operations on DataFrames using a **domain-specific
    language** (**DSL**). DataFrames support all the common relational operators and
    they all take expression objects in a limited DSL that lets Spark capture the
    structure of the expression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用**领域特定语言**（**DSL**）在DataFrames上执行关系操作。DataFrames支持所有常见的关联操作，并且它们都接受在有限的DSL中的表达式对象，这使得Spark能够捕获表达式的结构。
- en: We will start with the entry point into Spark SQL, that is, SQLContext. We will
    also cover HiveContext that is a wrapper around SQLContext to support Hive functionality.
    Please note that HiveContext is more battle-tested and provides a richer functionality,
    so it is strongly recommended to use it even if you do not plan to connect to
    Hive. Slowly, SQLContext will come to the same level of functionality as HiveContext
    is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Spark SQL的入口点开始，即SQLContext。我们还将涵盖HiveContext，它是SQLContext的包装器，用于支持Hive功能。请注意，HiveContext经过了更多的实战检验，并提供了更丰富的功能，因此即使您不打算连接到Hive，也强烈建议使用它。慢慢地，SQLContext将达到与HiveContext相同的功能水平。
- en: There are two ways to associate schema with RDDs to create DataFrames. The easy
    way is to leverage Scala case classes, which we are going to cover first. Spark
    uses Java reflection to deduce schema from case classes. There is also a way to
    programmatically specify schema for advanced needs, which we will cover next.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以将模式与RDD关联起来以创建DataFrame。简单的方法是利用Scala的case类，我们将会首先介绍。Spark使用Java反射从case类中推断模式。还有一种方法可以程序化地指定用于高级需求的模式，我们将在下一部分介绍。
- en: Spark SQL provides an easy way to both load and save the Parquet files, which
    will also be covered. Lastly, we will cover loading from and saving data to JSON.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一个简单的方法来加载和保存Parquet文件，这也会被涵盖。最后，我们将涵盖从JSON加载和保存数据。
- en: Understanding the Catalyst optimizer
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Catalyst优化器
- en: Most of the power of Spark SQL comes due to Catalyst optimizer, so it makes
    sense to spend some time understanding it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的大部分功能都得益于Catalyst优化器，因此花些时间来理解它是很有意义的。
- en: '![Understanding the Catalyst optimizer](img/3056_04_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![理解Catalyst优化器](img/3056_04_04.jpg)'
- en: How it works…
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Catalyst optimizer primarily leverages functional programming constructs of
    Scala such as pattern matching. It offers a general framework for transforming
    trees, which we use to perform analysis, optimization, planning, and runtime code
    generation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器主要利用Scala的函数式编程结构，如模式匹配。它提供了一个通用的框架来转换树，我们使用它来进行分析、优化、规划和运行时代码生成。
- en: 'Catalyst optimizer has two primary goals:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器有两个主要目标：
- en: Make adding new optimization techniques easy
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使添加新的优化技术变得容易
- en: Enable external developers to extend the optimizer
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许外部开发者扩展优化器
- en: 'Spark SQL uses Catalyst''s transformation framework in four phases:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL在四个阶段使用Catalyst的转换框架：
- en: Analyzing a logical plan to resolve references
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析逻辑计划以解析引用
- en: Logical plan optimization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑计划优化
- en: Physical planning
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理规划
- en: Code generation to compile the parts of the query to Java bytecode
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成，将查询的部分编译成Java字节码
- en: Analysis
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析
- en: 'The analysis phase involved looking at a SQL query or a DataFrame, creating
    a logical plan out of it, which is still unresolved (the columns referred may
    not exist or may be of wrong datatype) and then resolving this plan using the
    Catalog object (which connects to the physical data source), and creating a logical
    plan, as shown in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分析阶段涉及查看一个SQL查询或DataFrame，从中创建一个逻辑计划，该计划仍然未解决（所引用的列可能不存在或数据类型可能不正确），然后使用目录对象（该对象连接到物理数据源）解决此计划，并创建一个逻辑计划，如下所示图所示：
- en: '![Analysis](img/3056_04_05.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![分析](img/3056_04_05.jpg)'
- en: Logical plan optimization
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑计划优化
- en: The logical plan optimization phase applies standard rule-based optimization
    to the logical plan. These include constant folding, predicate pushdown, projection
    pruning, null propagation, Boolean expression simplification, and other rules.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑计划优化阶段将对逻辑计划应用基于规则的标准化优化。这包括常量折叠、谓词下沉、投影剪枝、空值传播、布尔表达式简化以及其他规则。
- en: I would like to draw special attention to predicate the pushdown rule here.
    The concept is simple; if you issue a query in one place to run against the massive
    data, which is another place, it can lead to a lot of unnecessary data moving
    across the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我要特别关注谓词下沉规则。概念很简单；如果你在一个地方发出查询以运行针对大量数据，而数据存储在另一个地方，这可能导致大量不必要的数据在网络中移动。
- en: If we can push down the part of the query to where the data is stored, and thus
    filter out unnecessary data, it reduces network traffic significantly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将查询的一部分推送到数据存储的地方，从而过滤掉不必要的数据，这将显著减少网络流量。
- en: '![Logical plan optimization](img/3056_04_06.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑计划优化](img/3056_04_06.jpg)'
- en: Physical planning
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物理规划
- en: In the physical planning phase, Spark SQL takes a logical plan and generates
    one or more physical plans. It then measures the cost of each physical plan and
    generates one physical plan based on that.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理规划阶段，Spark SQL将一个逻辑计划生成一个或多个物理计划。然后它测量每个物理计划的成本，并基于此生成一个物理计划。
- en: '![Physical planning](img/3056_04_07.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![物理规划](img/3056_04_07.jpg)'
- en: Code generation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码生成
- en: The final phase of query optimization involves generating Java bytecode to run
    on each machine. It uses a special Scala feature called **Quasi quotes** to accomplish
    that.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查询优化的最后阶段涉及生成在每个机器上运行的Java字节码。它使用一个名为**Quasi quotes**的特殊Scala功能来完成此操作。
- en: Creating HiveContext
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建HiveContext
- en: '`SQLContext` and its descendant `HiveContext` are the two entry points into
    the world of Spark SQL. `HiveContext` provides a superset of functionality provided
    by SQLContext. The additional features are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`SQLContext`及其子类`HiveContext`是进入Spark SQL世界的两个入口点。`HiveContext`提供了比`SQLContext`提供的功能更全面的功能。附加功能包括：'
- en: More complete and battle-tested HiveQL parser
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更完整且经过实战检验的HiveQL解析器
- en: Access to Hive UDFs
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问Hive UDFs
- en: Ability to read data from Hive tables
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够从Hive表中读取数据
- en: 'From Spark 1.3 onwards, the Spark shell comes loaded with sqlContext (which
    is an instance of `HiveContext` not `SQLContext`). If you are creating `SQLContext`
    in Scala code, it can be created using `SparkContext`, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark 1.3版本开始，Spark shell自带sqlContext（它是`HiveContext`的实例，而不是`SQLContext`）。如果你在Scala代码中创建`SQLContext`，可以使用`SparkContext`创建，如下所示：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this recipe, we will cover how to create instance of `HiveContext`, and then
    access Hive functionality through Spark SQL.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将介绍如何创建`HiveContext`实例，然后通过Spark SQL访问Hive功能。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To enable Hive functionality, make sure that you have Hive enabled (-Phive)
    assembly JAR is available on all worker nodes; also, copy `hive-site.xml` into
    the `conf` directory of the Spark installation. It is important that Spark has
    access to `hive-site.xml`; otherwise, it will create its own Hive metastore and
    will not connect to your existing Hive warehouse.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Hive功能，请确保你已经在所有工作节点上启用了Hive（-Phive）assembly JAR，并且已将`hive-site.xml`复制到Spark安装的`conf`目录中。重要的是Spark能够访问`hive-site.xml`；否则，它将创建自己的Hive元数据存储，并且不会连接到你的现有Hive仓库。
- en: By default, all the tables created by Spark SQL are Hive-managed tables, that
    is, Hive has complete control on life cycle of a table, including deleting it
    if table metadata is dropped using the `drop table` command. This holds true only
    for persistent tables. Spark SQL also has mechanism to create temporary tables
    out of DataFrames for ease of writing queries, and they are not managed by Hive.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Please note that Spark 1.4 supports Hive versions 0.13.1\. You can specify a
    version of Hive you would like to build against using the `-Phive-<version> build`
    option while building with Maven. For example, to build with 0.12.0, you can use
    `-Phive-0.12.0`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create an instance of `HiveContext`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a Hive table `Person` with `first_name`, `last_name`, and `age` as columns:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Open another shell and create the `person` data in a local file:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Load the data in the `person` table:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Alternatively, load that data in the `person` table from HDFS:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that using `load data inpath` moves the data from another HDFS location
    to the Hive's `warehouse` directory, which is, by default, `/user/hive/warehouse`.
    You can also specify fully qualified path such as `hdfs://localhost:9000/user/hduser/person`.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the person data using HiveQL:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a new table from the output of a `select` query:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also copy directly from one table to another:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create two tables `people_by_last_name` and `people_by_age` to keep counts:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also insert records into multiple tables using a HiveQL query:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inferring schema using case classes
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Case classes are special classes in Scala that provide you with the boiler plate
    implementation of the constructor, getters (accessors), equals and hashCode, and
    implement `Serializable`. Case classes work really well to encapsulate data as
    objects. Readers, familiar with Java, can relate it to **plain old Java objects**
    (**POJOs**) or Java bean.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of case classes is that all that grunt work, which is required in
    Java, can be done with case classes in a single line of code. Spark uses reflection
    on case classes to infer schema.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import for the implicit conversions:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a `Person` case class:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In another shell, create some sample data to be put in HDFS:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Load the `person` directory as an RDD:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Convert the RDD of Array[String] into the RDD of `Person` case objects:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Convert the `personRDD` into the `personDF` DataFrame:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Register the `personDF` as a table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run a SQL query against it:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Get the output values from `persons`:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Programmatically specifying the schema
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are few cases where case classes might not work; one of these cases is
    that the case classes cannot take more than 22 fields. Another case can be that
    you do not know about schema beforehand. In this approach, the data is loaded
    as an RDD of the `Row` objects. Schema is created separately using the `StructType`
    and `StructField` objects, which represent a table and a field respectively. Schema
    is applied to the `Row` RDD to create a DataFrame.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import for the implicit conversion:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Import the Spark SQL datatypes and `Row` objects:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In another shell, create some sample data to be put in HDFS:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Load the `person` data in an RDD:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Convert the RDD of array[string] to the RDD of the `Row` objects:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create schema using the `StructType` and `StructField` objects. The `StructField`
    object takes parameters in the form of param name, param type, and nullability:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Apply schema to create the `personDF` DataFrame:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Register the `personDF` as a table:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Run a SQL query against it:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the output values from `persons`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this recipe, we learned how to create a DataFrame by programmatically specifying
    schema.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A `StructType` object defines the schema. You can consider it equivalent to
    a table or a row in the relational world. `StructType` takes in an array of the
    `StructField` objects, as in the following signature:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A `StructField` object has the following signature:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is some more information on the parameters used:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This represents the name of the field.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataType`: This shows the datatype of this field.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following datatypes are allowed:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `IntegerType` | `FloatType` |'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `BooleanType` | `ShortType` |'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `LongType` | `ByteType` |'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `DoubleType` | `StringType` |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`nullable`: This shows whether this field can be null.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: This shows the metadata of this field. Metadata is a wrapper over
    `Map[String,Any]` so that it can contain any arbitrary metadata.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data using the Parquet format
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Parquet is a columnar data storage format, specifically designed for
    big data storage and processing. Parquet is based on record shredding and assembly
    algorithm in the Google Dremel paper. In Parquet, data in a single column is stored
    contiguously.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The columnar format gives Parquet some unique benefits. For example, if you
    have a table with 100 columns and you mostly access 10 columns, in a row-based
    format you will have to load all 100 columns, as granularity level is at row level.
    But, in Parquet, you will only load 10 columns. Another benefit is that since
    all the data in a given column is of the same datatype (by definition), compression
    is much more efficient.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open the terminal and create the `person` data in a local file:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Upload the `person` directory to HDFS:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Import for the implicit conversion:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a case class for `Person`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Load the `person` directory from HDFS and map it to the `Person` case class:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Convert the `personRDD` into the `person` DataFrame:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Register the `person` DataFrame as a temp table so that SQL queries can be run
    against it. Please note that the DataFrame name does not have to be the same as
    the table name.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Select all the person with age over 60 years:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Print values:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s save this `sixtyPlus` RDD in the Parquet format:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The previous step created a directory called `sp.parquet` in the HDFS root.
    You can run the `hdfs dfs -ls` command in another shell to make sure that it''s
    created:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Load contents of the Parquet files in the Spark shell:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Register the loaded `parquet` DF as a `temp` table:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Run a query against the preceding `temp` table:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s spend some time understanding the Parquet format deeper. The following
    is sample data represented in the table format:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '| First_Name | Last_Name | Age |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| Barack | Obama | 53 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| George | Bush | 68 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Bill | Clinton | 68 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: 'In the row format, the data will be stored like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '| Barack | Obama | 53 | George | Bush | 68 | Bill | Clinton | 68 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: 'In the columnar layout, the data will be stored like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '| Row group => | Barack | George | Bill | Obama | Bush | Clinton | 53 | 68
    | 68 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '|   | Column chunk | Column chunk | Column chunk |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: 'Here''s a brief description about the different parts:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '**Row group**: This shows the horizontal partitioning of data into rows. A
    row group consists of column chunks.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column chunk**: A column chunk has data for a given column in a row group.
    A column chunk is always physically contiguous. A row group has only one column
    chunk per column.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Page**: A column chunk is divided into pages. A page is a unit of storage
    and cannot be further divided. Pages are written back to back in column chunk.
    The data for a page can be compressed.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there is already data in a Hive table, say, the `person` table, you can
    directly save it in the Parquet format by performing the following steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a table named `person_parquet` with schema, the same as `person`, but
    in the Parquet storage format (for Hive 0.13 onwards):'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Insert data in the `person_parquet` table by importing it from the `person`
    table:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, data imported from other sources, such as Impala, saves string as
    binary. To convert it to string while reading, set the following property in `SparkConf`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There's more…
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using Spark 1.4 or later, there is a new interface both to write
    to and read from Parquet. To write the data to Parquet (step 11 rewritten), let''s
    save this `sixtyPlus` RDD to the Parquet format (RDD implicitly gets converted
    to DataFrame):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To read from Parquet (step 13 rewritten; the result is DataFrame), load the
    contents of the Parquet files in the Spark shell:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Loading and saving data using the JSON format
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JSON is a lightweight data-interchange format. It is based on a subset of the
    JavaScript programming language. JSON's popularity is directly related to XML
    getting unpopular. XML was a great solution to provide a structure to the data
    in a plain text format. With time, XML documents became more and more heavy and
    the overhead was not worth it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是一种轻量级的数据交换格式。它基于 JavaScript 编程语言的一个子集。JSON 的流行度与 XML 的不流行度直接相关。XML 是一种为纯文本格式中的数据提供结构的优秀解决方案。随着时间的推移，XML
    文档变得越来越重，开销不值得。
- en: JSON solved this problem by providing structure with minimal overhead. Some
    people call JSON **fat-free XML**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 通过提供最小开销的结构解决了这个问题。有些人称 JSON 为 **无脂肪 XML**。
- en: 'The JSON syntax follows these rules:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 语法遵循以下规则：
- en: 'Data is in the form of key-value pairs:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据以键值对的形式存在：
- en: '[PRE57]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'There are four datatypes in JSON:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON 有四种数据类型：
- en: 'String ("firstName" : "Barack")'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '字符串 ("firstName" : "Barack")'
- en: 'Number ("age" : 53)'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '数字 ("age" : 53)'
- en: 'Boolean ("alive": true)'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '布尔 ("alive": true)'
- en: 'null ("manager" : null)'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '空值 ("manager" : null)'
- en: Data is delimited by commas
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据由逗号分隔
- en: 'Curly braces {} represents an object:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花括号 {} 代表一个对象：
- en: '[PRE58]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Square brackets [] represent an array:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号 [] 代表一个数组：
- en: '[PRE59]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In this recipe, we will explore how to save and load it in the JSON format.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探讨如何以 JSON 格式保存和加载数据。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Open the terminal and create the `person` data in the JSON format:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并在 JSON 格式中创建 `person` 数据：
- en: '[PRE60]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Upload the `jsondata` directory to HDFS:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `jsondata` 目录上传到 HDFS：
- en: '[PRE61]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Spark shell 并给它一些额外的内存：
- en: '[PRE62]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create an instance of `SQLContext`:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `SQLContext` 的实例：
- en: '[PRE63]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Import for the implicit conversion:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE64]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the `jsondata` directory from HDFS:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 HDFS 加载 `jsondata` 目录：
- en: '[PRE65]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Register the `person` DF as a `temp` table so that the SQL queries can be run
    against it:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `person` DF 注册为 `temp` 表，以便可以针对它运行 SQL 查询：
- en: '[PRE66]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Select all the persons with age over 60 years:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择所有年龄超过 60 岁的人：
- en: '[PRE67]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Print values:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印值：
- en: '[PRE68]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Let's save this `sixtyPlus` DF in the JSON format
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将这个 `sixtyPlus` DF 以 JSON 格式保存
- en: '[PRE69]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Last step created a directory called `sp` in the HDFS root. You can run the
    `hdfs dfs -ls` command in another shell to make sure it''s created:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步在 HDFS 根目录下创建了一个名为 `sp` 的目录。您可以在另一个 shell 中运行 `hdfs dfs -ls` 命令以确保它已创建：
- en: '[PRE70]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works…
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The `sc.jsonFile` internally uses `TextInputFormat`, which processes one line
    at a time. Therefore, one JSON record cannot be on multiple lines. It would be
    a valid JSON format if you use multiple lines, but it will not work with Spark
    and will throw an exception.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc.jsonFile` 内部使用 `TextInputFormat`，它一次处理一行。因此，一个 JSON 记录不能跨多行。如果您使用多行，它将是有效的
    JSON 格式，但与 Spark 不兼容，并会抛出异常。'
- en: 'It is allowed to have more than one object in a line. For example, you can
    have the information of two persons in one line as an array, as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一行中可以包含多个对象。例如，您可以将两个人的信息放在一行中作为一个数组，如下所示：
- en: '[PRE71]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This recipe concludes saving and loading data in the JSON format using Spark.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱总结了使用 Spark 保存和加载数据的 JSON 格式。
- en: There's more…
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'If you are using Spark Version 1.4 or later, `SqlContext` provides an easier
    interface to load the `jsondata` directory from HDFS:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Spark 版本 1.4 或更高版本，`SqlContext` 提供了一个更简单的接口来从 HDFS 加载 `jsondata` 目录：
- en: '[PRE72]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `sqlContext.jsonFile` is deprecated in version 1.4, and `sqlContext.read.json`
    is the recommend approach.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`sqlContext.jsonFile` 在 1.4 版本中已弃用，`sqlContext.read.json` 是推荐的方法。'
- en: Loading and saving data from relational databases
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从关系型数据库加载数据和保存数据
- en: In the previous chapter, we learned how to load data from a relational data
    into an RDD using JdbcRDD. Spark 1.4 has support to load data directly into Dataframe
    from a JDBC resource. This recipe will explore how to do it.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用 JdbcRDD 从关系型数据中加载数据到 RDD。Spark 1.4 支持直接从 JDBC 资源加载数据到 Dataframe。本食谱将探讨如何实现它。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please make sure that JDBC driver JAR is visible on the client node and all
    the slaves nodes on which executor will run.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保 JDBC 驱动程序 JAR 文件在客户端节点以及所有将运行执行器的从节点上可见。
- en: How to do it...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Create a table named `person` in MySQL using the following DDL:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下 DDL 在 MySQL 中创建一个名为 `person` 的表：
- en: '[PRE73]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Insert some data:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入一些数据：
- en: '[PRE74]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)
    下载 `mysql-connector-java-x.x.xx-bin.jar`。
- en: 'Make MySQL driver available to the Spark shell and launch it:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 MySQL 驱动程序提供给 Spark shell 并启动它：
- en: '[PRE75]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that `path-to-mysql-jar` is not the actual path name. You need to
    use your path name.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Construct a JDBC URL:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create a connection properties object with username and password:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Load DataFrame with JDBC data source (url, table name, properties):'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Show the results in a nice tabular format by executing the following command:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This has loaded the whole table. What if I only would like to load males (url,
    table name, predicates, properties)? To do this, run the following command:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Show only first names by executing the following command:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Show only people below age 60 by executing the following command:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Group people by gender as follows:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Find the number of males and females by executing the following command:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Find the average age of males and females by executing the following command:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now if you''d like to save this `avg_age` data to a new table, run the following
    command:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Save the people DataFrame in the Parquet format:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Save the people DataFrame in the JSON format:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Loading and saving data from an arbitrary source
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered three data sources that are inbuilt with DataFrames—`parquet`
    (default), `json`, and `jdbc`. Dataframes are not limited to these three and can
    load and save to any arbitrary data source by specifying the format manually.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover loading and saving data from arbitrary sources.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Load the data from Parquet; since `parquet` is the default data source, you
    do not have to specify it:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Load the data from Parquet by manually specifying the format:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'For inbuilt datatypes (`parquet`,`json`, and `jdbc`), you do not have to specify
    the full format name, only specifying `"parquet"`, `"json"`, or `"jdbc"` works:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Note
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When writing data, there are four save modes: `append`, `overwrite`, `errorIfExists`,
    and `ignore`. The `append` mode adds data to data source, `overwrite` overwrites
    it, `errorIfExists` throws an exception that data already exists, and `ignore`
    does nothing when data already exists.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save people as JSON in the `append` mode:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: There's more…
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark SQL's data source API saves to a variety of data sources. To find
    more information, visit [http://spark-packages.org/](http://spark-packages.org/).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
