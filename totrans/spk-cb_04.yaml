- en: Chapter 4. Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark SQL is a Spark module for processing a structured data. This chapter
    is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Catalyst optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating HiveContext
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring schema using case classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatically specifying the schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data using the Parquet format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data using the JSON format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data from relational databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data from an arbitrary source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can process data from various data sources such as HDFS, Cassandra, HBase,
    and relational databases, including HDFS. Big data frameworks (unlike relational
    database systems) do not enforce schema while writing. HDFS is a perfect example
    where any arbitrary file is welcome during the write phase. Reading data is a
    different story, however. You need to give some structure to even completely unstructured
    data to make sense out of it. With this structured data, SQL comes very handy
    when it comes to analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL is a relatively new component in Spark ecosystem, introduced in Spark
    1.0 for the first time. It incorporates a project named Shark, which was an attempt
    to make Hive run on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Hive is essentially a relational abstraction, which converts SQL queries to
    MapReduce jobs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shark replaced the MapReduce part with Spark while retaining most of the code
    base.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Initially, it worked fine, but very soon, Spark developers hit roadblocks and
    could not optimize it any further. Finally, they decided to write the SQL Engine
    from scratch and that gave birth to Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark SQL took care of all the performance challenges, but it had to provide
    compatibility with Hive and for that reason, a new wrapper context, `HiveContext`,
    was created on top of `SQLContext`.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL supports accessing data using standard SQL queries and HiveQL, a SQL-like
    query language that Hive uses. In this chapter, we will explore different features
    of Spark SQL. It supports a subset of HiveQL as well as a subset of SQL 92\. It
    runs SQL/HiveQL queries alongside, or replacing the existing Hive deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL is only a part of the reason for the creation of Spark SQL. One
    big reason is that it helps to create and run Spark programs faster. It lets developers
    write less code, program read less data, and let the catalyst optimizer do all
    the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL uses a programming abstraction called **DataFrame**. It is a distributed
    collection of data organized in named columns. DataFrame is equivalent to a database
    table, but provides much finer level of optimization. The DataFrame API also ensures
    that Spark's performance is consistent across different language bindings.
  prefs: []
  type: TYPE_NORMAL
- en: Let's contrast DataFrames with RDDs. An RDD is an opaque collection of objects
    with no idea about the format of the underlying data. In contrast, DataFrames
    have schema associated with them. You can also look at DataFrames as RDDs with
    schema added to them. In fact, until Spark 1.2, there was an artifact called **SchemaRDD**,
    which has now evolved into DataFrame. They provide much richer functionality than
    SchemaRDDs.
  prefs: []
  type: TYPE_NORMAL
- en: This extra information about schema makes possible to do a lot of optimizations,
    which were not otherwise possible.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames also transparently load from various data sources, such as Hive tables,
    Parquet files, JSON files, and external databases using JDBC. DataFrames can be
    viewed as RDDs of row objects, allowing users to call the procedural Spark APIs
    such as map.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API is available in Scala, Java, Python, and also R starting Spark
    1.4.
  prefs: []
  type: TYPE_NORMAL
- en: Users can perform relational operations on DataFrames using a **domain-specific
    language** (**DSL**). DataFrames support all the common relational operators and
    they all take expression objects in a limited DSL that lets Spark capture the
    structure of the expression.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the entry point into Spark SQL, that is, SQLContext. We will
    also cover HiveContext that is a wrapper around SQLContext to support Hive functionality.
    Please note that HiveContext is more battle-tested and provides a richer functionality,
    so it is strongly recommended to use it even if you do not plan to connect to
    Hive. Slowly, SQLContext will come to the same level of functionality as HiveContext
    is.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to associate schema with RDDs to create DataFrames. The easy
    way is to leverage Scala case classes, which we are going to cover first. Spark
    uses Java reflection to deduce schema from case classes. There is also a way to
    programmatically specify schema for advanced needs, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL provides an easy way to both load and save the Parquet files, which
    will also be covered. Lastly, we will cover loading from and saving data to JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Catalyst optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the power of Spark SQL comes due to Catalyst optimizer, so it makes
    sense to spend some time understanding it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Catalyst optimizer](img/3056_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Catalyst optimizer primarily leverages functional programming constructs of
    Scala such as pattern matching. It offers a general framework for transforming
    trees, which we use to perform analysis, optimization, planning, and runtime code
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Catalyst optimizer has two primary goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Make adding new optimization techniques easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable external developers to extend the optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark SQL uses Catalyst''s transformation framework in four phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing a logical plan to resolve references
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical plan optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation to compile the parts of the query to Java bytecode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The analysis phase involved looking at a SQL query or a DataFrame, creating
    a logical plan out of it, which is still unresolved (the columns referred may
    not exist or may be of wrong datatype) and then resolving this plan using the
    Catalog object (which connects to the physical data source), and creating a logical
    plan, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis](img/3056_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logical plan optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The logical plan optimization phase applies standard rule-based optimization
    to the logical plan. These include constant folding, predicate pushdown, projection
    pruning, null propagation, Boolean expression simplification, and other rules.
  prefs: []
  type: TYPE_NORMAL
- en: I would like to draw special attention to predicate the pushdown rule here.
    The concept is simple; if you issue a query in one place to run against the massive
    data, which is another place, it can lead to a lot of unnecessary data moving
    across the network.
  prefs: []
  type: TYPE_NORMAL
- en: If we can push down the part of the query to where the data is stored, and thus
    filter out unnecessary data, it reduces network traffic significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Logical plan optimization](img/3056_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Physical planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the physical planning phase, Spark SQL takes a logical plan and generates
    one or more physical plans. It then measures the cost of each physical plan and
    generates one physical plan based on that.
  prefs: []
  type: TYPE_NORMAL
- en: '![Physical planning](img/3056_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Code generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final phase of query optimization involves generating Java bytecode to run
    on each machine. It uses a special Scala feature called **Quasi quotes** to accomplish
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Creating HiveContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`SQLContext` and its descendant `HiveContext` are the two entry points into
    the world of Spark SQL. `HiveContext` provides a superset of functionality provided
    by SQLContext. The additional features are:'
  prefs: []
  type: TYPE_NORMAL
- en: More complete and battle-tested HiveQL parser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Hive UDFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to read data from Hive tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From Spark 1.3 onwards, the Spark shell comes loaded with sqlContext (which
    is an instance of `HiveContext` not `SQLContext`). If you are creating `SQLContext`
    in Scala code, it can be created using `SparkContext`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we will cover how to create instance of `HiveContext`, and then
    access Hive functionality through Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enable Hive functionality, make sure that you have Hive enabled (-Phive)
    assembly JAR is available on all worker nodes; also, copy `hive-site.xml` into
    the `conf` directory of the Spark installation. It is important that Spark has
    access to `hive-site.xml`; otherwise, it will create its own Hive metastore and
    will not connect to your existing Hive warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all the tables created by Spark SQL are Hive-managed tables, that
    is, Hive has complete control on life cycle of a table, including deleting it
    if table metadata is dropped using the `drop table` command. This holds true only
    for persistent tables. Spark SQL also has mechanism to create temporary tables
    out of DataFrames for ease of writing queries, and they are not managed by Hive.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that Spark 1.4 supports Hive versions 0.13.1\. You can specify a
    version of Hive you would like to build against using the `-Phive-<version> build`
    option while building with Maven. For example, to build with 0.12.0, you can use
    `-Phive-0.12.0`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of `HiveContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a Hive table `Person` with `first_name`, `last_name`, and `age` as columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open another shell and create the `person` data in a local file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data in the `person` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, load that data in the `person` table from HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that using `load data inpath` moves the data from another HDFS location
    to the Hive's `warehouse` directory, which is, by default, `/user/hive/warehouse`.
    You can also specify fully qualified path such as `hdfs://localhost:9000/user/hduser/person`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the person data using HiveQL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new table from the output of a `select` query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also copy directly from one table to another:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two tables `people_by_last_name` and `people_by_age` to keep counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also insert records into multiple tables using a HiveQL query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inferring schema using case classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Case classes are special classes in Scala that provide you with the boiler plate
    implementation of the constructor, getters (accessors), equals and hashCode, and
    implement `Serializable`. Case classes work really well to encapsulate data as
    objects. Readers, familiar with Java, can relate it to **plain old Java objects**
    (**POJOs**) or Java bean.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of case classes is that all that grunt work, which is required in
    Java, can be done with case classes in a single line of code. Spark uses reflection
    on case classes to infer schema.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import for the implicit conversions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `Person` case class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In another shell, create some sample data to be put in HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `person` directory as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the RDD of Array[String] into the RDD of `Person` case objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the `personRDD` into the `personDF` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the `personDF` as a table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a SQL query against it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the output values from `persons`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Programmatically specifying the schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are few cases where case classes might not work; one of these cases is
    that the case classes cannot take more than 22 fields. Another case can be that
    you do not know about schema beforehand. In this approach, the data is loaded
    as an RDD of the `Row` objects. Schema is created separately using the `StructType`
    and `StructField` objects, which represent a table and a field respectively. Schema
    is applied to the `Row` RDD to create a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import for the implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the Spark SQL datatypes and `Row` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In another shell, create some sample data to be put in HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `person` data in an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the RDD of array[string] to the RDD of the `Row` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create schema using the `StructType` and `StructField` objects. The `StructField`
    object takes parameters in the form of param name, param type, and nullability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply schema to create the `personDF` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the `personDF` as a table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a SQL query against it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the output values from `persons`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we learned how to create a DataFrame by programmatically specifying
    schema.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A `StructType` object defines the schema. You can consider it equivalent to
    a table or a row in the relational world. `StructType` takes in an array of the
    `StructField` objects, as in the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A `StructField` object has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is some more information on the parameters used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This represents the name of the field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataType`: This shows the datatype of this field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following datatypes are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `IntegerType` | `FloatType` |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `BooleanType` | `ShortType` |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `LongType` | `ByteType` |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `DoubleType` | `StringType` |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`nullable`: This shows whether this field can be null.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: This shows the metadata of this field. Metadata is a wrapper over
    `Map[String,Any]` so that it can contain any arbitrary metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data using the Parquet format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Parquet is a columnar data storage format, specifically designed for
    big data storage and processing. Parquet is based on record shredding and assembly
    algorithm in the Google Dremel paper. In Parquet, data in a single column is stored
    contiguously.
  prefs: []
  type: TYPE_NORMAL
- en: The columnar format gives Parquet some unique benefits. For example, if you
    have a table with 100 columns and you mostly access 10 columns, in a row-based
    format you will have to load all 100 columns, as granularity level is at row level.
    But, in Parquet, you will only load 10 columns. Another benefit is that since
    all the data in a given column is of the same datatype (by definition), compression
    is much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open the terminal and create the `person` data in a local file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the `person` directory to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import for the implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a case class for `Person`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `person` directory from HDFS and map it to the `Person` case class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the `personRDD` into the `person` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Register the `person` DataFrame as a temp table so that SQL queries can be run
    against it. Please note that the DataFrame name does not have to be the same as
    the table name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select all the person with age over 60 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s save this `sixtyPlus` RDD in the Parquet format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous step created a directory called `sp.parquet` in the HDFS root.
    You can run the `hdfs dfs -ls` command in another shell to make sure that it''s
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load contents of the Parquet files in the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the loaded `parquet` DF as a `temp` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a query against the preceding `temp` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s spend some time understanding the Parquet format deeper. The following
    is sample data represented in the table format:'
  prefs: []
  type: TYPE_NORMAL
- en: '| First_Name | Last_Name | Age |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Barack | Obama | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| George | Bush | 68 |'
  prefs: []
  type: TYPE_TB
- en: '| Bill | Clinton | 68 |'
  prefs: []
  type: TYPE_TB
- en: 'In the row format, the data will be stored like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Barack | Obama | 53 | George | Bush | 68 | Bill | Clinton | 68 |'
  prefs: []
  type: TYPE_TB
- en: 'In the columnar layout, the data will be stored like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Row group => | Barack | George | Bill | Obama | Bush | Clinton | 53 | 68
    | 68 |'
  prefs: []
  type: TYPE_TB
- en: '|   | Column chunk | Column chunk | Column chunk |'
  prefs: []
  type: TYPE_TB
- en: 'Here''s a brief description about the different parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Row group**: This shows the horizontal partitioning of data into rows. A
    row group consists of column chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column chunk**: A column chunk has data for a given column in a row group.
    A column chunk is always physically contiguous. A row group has only one column
    chunk per column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Page**: A column chunk is divided into pages. A page is a unit of storage
    and cannot be further divided. Pages are written back to back in column chunk.
    The data for a page can be compressed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there is already data in a Hive table, say, the `person` table, you can
    directly save it in the Parquet format by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a table named `person_parquet` with schema, the same as `person`, but
    in the Parquet storage format (for Hive 0.13 onwards):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Insert data in the `person_parquet` table by importing it from the `person`
    table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, data imported from other sources, such as Impala, saves string as
    binary. To convert it to string while reading, set the following property in `SparkConf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using Spark 1.4 or later, there is a new interface both to write
    to and read from Parquet. To write the data to Parquet (step 11 rewritten), let''s
    save this `sixtyPlus` RDD to the Parquet format (RDD implicitly gets converted
    to DataFrame):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To read from Parquet (step 13 rewritten; the result is DataFrame), load the
    contents of the Parquet files in the Spark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Loading and saving data using the JSON format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JSON is a lightweight data-interchange format. It is based on a subset of the
    JavaScript programming language. JSON's popularity is directly related to XML
    getting unpopular. XML was a great solution to provide a structure to the data
    in a plain text format. With time, XML documents became more and more heavy and
    the overhead was not worth it.
  prefs: []
  type: TYPE_NORMAL
- en: JSON solved this problem by providing structure with minimal overhead. Some
    people call JSON **fat-free XML**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON syntax follows these rules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is in the form of key-value pairs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are four datatypes in JSON:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'String ("firstName" : "Barack")'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number ("age" : 53)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boolean ("alive": true)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'null ("manager" : null)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is delimited by commas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Curly braces {} represents an object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Square brackets [] represent an array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we will explore how to save and load it in the JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open the terminal and create the `person` data in the JSON format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the `jsondata` directory to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of `SQLContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import for the implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `jsondata` directory from HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the `person` DF as a `temp` table so that the SQL queries can be run
    against it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select all the persons with age over 60 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's save this `sixtyPlus` DF in the JSON format
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Last step created a directory called `sp` in the HDFS root. You can run the
    `hdfs dfs -ls` command in another shell to make sure it''s created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sc.jsonFile` internally uses `TextInputFormat`, which processes one line
    at a time. Therefore, one JSON record cannot be on multiple lines. It would be
    a valid JSON format if you use multiple lines, but it will not work with Spark
    and will throw an exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is allowed to have more than one object in a line. For example, you can
    have the information of two persons in one line as an array, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This recipe concludes saving and loading data in the JSON format using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using Spark Version 1.4 or later, `SqlContext` provides an easier
    interface to load the `jsondata` directory from HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `sqlContext.jsonFile` is deprecated in version 1.4, and `sqlContext.read.json`
    is the recommend approach.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and saving data from relational databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to load data from a relational data
    into an RDD using JdbcRDD. Spark 1.4 has support to load data directly into Dataframe
    from a JDBC resource. This recipe will explore how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please make sure that JDBC driver JAR is visible on the client node and all
    the slaves nodes on which executor will run.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a table named `person` in MySQL using the following DDL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Insert some data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make MySQL driver available to the Spark shell and launch it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that `path-to-mysql-jar` is not the actual path name. You need to
    use your path name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Construct a JDBC URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a connection properties object with username and password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load DataFrame with JDBC data source (url, table name, properties):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show the results in a nice tabular format by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This has loaded the whole table. What if I only would like to load males (url,
    table name, predicates, properties)? To do this, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show only first names by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show only people below age 60 by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Group people by gender as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the number of males and females by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the average age of males and females by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now if you''d like to save this `avg_age` data to a new table, run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the people DataFrame in the Parquet format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the people DataFrame in the JSON format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Loading and saving data from an arbitrary source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered three data sources that are inbuilt with DataFrames—`parquet`
    (default), `json`, and `jdbc`. Dataframes are not limited to these three and can
    load and save to any arbitrary data source by specifying the format manually.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover loading and saving data from arbitrary sources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from Parquet; since `parquet` is the default data source, you
    do not have to specify it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from Parquet by manually specifying the format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For inbuilt datatypes (`parquet`,`json`, and `jdbc`), you do not have to specify
    the full format name, only specifying `"parquet"`, `"json"`, or `"jdbc"` works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When writing data, there are four save modes: `append`, `overwrite`, `errorIfExists`,
    and `ignore`. The `append` mode adds data to data source, `overwrite` overwrites
    it, `errorIfExists` throws an exception that data already exists, and `ignore`
    does nothing when data already exists.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save people as JSON in the `append` mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark SQL's data source API saves to a variety of data sources. To find
    more information, visit [http://spark-packages.org/](http://spark-packages.org/).
  prefs: []
  type: TYPE_NORMAL
