<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 8. Sharing Models with Prediction Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Sharing Models with Prediction Services</h1></div></div></div><p>Thus far, we have examined how to build a variety of models with data sources ranging from standard 'tabular' data to text and images. However, this only accomplishes part of our goal in business analysis: we can generate predictions from a dataset, but we cannot easily share the results with colleagues or with other software systems within a company. We also cannot easily replicate the results as new data becomes available without manually re-running the sorts of analyses discussed in previous chapters or scale it to larger datasets over time. We will also have difficulty to use our models in a public setting, such as a company's website, without revealing the details of the analysis through the model parameters exposed in our code.</p><p>To overcome these challenges, the following chapter will describe how to build 'prediction services', web applications that encapsulate and automate the core components of data transformation, model fitting, and scoring of new observations that we have discussed in the context of predicative algorithms in prior sections. By packaging our analysis into a web application, we can both easily scale the modeling system and change implementations in the underlying algorithm, all the while making such changes invisible to the consumer (whether a human or other software system), who interacts with our predictive models by making requests to our application through web URLs and a standard REST <a id="id502" class="indexterm"/>
<span class="strong"><strong>application programmer interface</strong></span> (<span class="strong"><strong>API</strong></span>). It also allows initialization and updates to the analysis to be automated through calls to the service, making the predictive modeling task consistent and replicable. Finally, by carefully parameterizing many of the steps, we can use the same service to interact with interchangeable data sources computation frameworks.</p><p>In essences, building a prediction service involves linking several of the components we have already discussed, such as data transformation and predictive modeling, with a set of new components that we will discuss in this chapter for the first time. To this end, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to instrument a basic web application and server using the Cherrypy and Flask frameworks</li><li class="listitem" style="list-style-type: disc">How to automate a generic modeling framework using a RESTful API</li><li class="listitem" style="list-style-type: disc">Scaling our system using a Spark computation framework</li><li class="listitem" style="list-style-type: disc">Storing the results of our predictive model in database systems for reporting applications we will discuss in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span></li></ul></div><div class="section" title="The architecture of a prediction service"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec42"/>The architecture of a prediction service</h1></div></div></div><p>Now with a clear goal in<a id="id503" class="indexterm"/> mind—to share and scale the results of our predictive modeling using a web application—what are the components required to accomplish this objective?</p><p>The first is the <span class="emphasis"><em>client</em></span>: this could be either a web browser or simply a user entering a <code class="literal">curl</code> command in the terminal (see Aside). In either case, the client sends requests using <a id="id504" class="indexterm"/>
<span class="strong"><strong>hypertext transfer protocol</strong></span> (<span class="strong"><strong>HTTP</strong></span>), a standard transport convention to retrieve or transmit information over a network (Berners-Lee, Tim, Roy Fielding, and Henrik Frystyk. <span class="emphasis"><em>Hypertext transfer protocol--HTTP/1.0</em></span>. No. RFC 1945. 1996). An important feature of the HTTP standard is that the client and server do not have to 'know' anything about how the other is implemented (for example, which programming language is used to write these components) because the message will remain consistent between them regardless by virtue of following the HTTP standard.</p><p>The next component is the <span class="emphasis"><em>server</em></span>, which receives HTTP requests from a client and forwards them to the application. You can think of it as the gateway for the requests from the client to our actual predictive modeling application. In Python, web servers and applications each conform to the <span class="strong"><strong>Web Server Gateway Interface</strong></span> (<span class="strong"><strong>WSGI</strong></span>), which <a id="id505" class="indexterm"/>specifies how the server and application should communicate. Like the HTTP requests between client and server, this standard allows the server and application to be modular as long as both consistently implement the interface. In fact, there could even be intervening middleware between the server and application that further modifies communication between the two: as long as the format of this communication remains constant, the details of each side of the interface are flexible. While we will use the Cherrypy library to build a server for our application, other common choices are Apache Tomcat and Nginx, both written in the Java programming language.</p><p>After the client request has been received and forwarded by the server, the application performs operations in response to the requests, and returns a value indicating the success or failure of the task. These requests could, for example, obtain for the predicted score for a particular user, update to the training dataset, or perform a round<a id="id506" class="indexterm"/> of model training.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip12"/>Tip</h3><p><span class="strong"><strong>Aside: The curl command</strong></span></p><p>As part of <a id="id507" class="indexterm"/>testing our prediction service, it is useful to have a way to quickly issue commands to the server and observe the response we receive back. While we could do some of this interactively using the address bar of a web browser, it is not easy to script browser activities in cases where we want to run a number of tests or replicate a particular command. The <code class="literal">curl</code> command, found in most Linux command line terminals, is very useful for this purpose: the same requests (in terms of a URL) can be issued to the prediction service using the <code class="literal">curl</code> command<a id="id508" class="indexterm"/> as would be given in the browser, and this call can be automated using shell scripting. The <code class="literal">curl</code> application can be installed from <a class="ulink" href="https://curl.haxx.se/">https://curl.haxx.se/</a>.</p></div></div><p>The web application relies upon server-side code to perform commands in response to requests from the web server. In our example, this server-side code is divided into several components: the first is a generic interface for the modeling logic, which specifies a standard way to construct predictive models, train them with an input dataset, and score incoming data. The second is an implementation of this framework using the logistic regression algorithm from <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <span class="emphasis"><em>Putting Data in its Place – Classification Methods and Analysis</em></span>. This code relies upon executing Spark jobs, which could be carried out either locally (on the same machine as the web application) or remotely (on a separate cluster).</p><p>The final piece of this chain is database systems that can persist information used by the prediction service This database could be as simple as file system on the same machine as the web server or as complex as a distributed database software. In our example we will use both Redis (a simple key-value store) and MongoDB (a NoSQL database) to store the data used in modeling, transient information about our application, and the model results themselves.</p><p>As we have emphasized, an important feature of these three components is that they are largely independent: because the WGSI standard defines how the web server and application communicate, we could change server and predictive model implementation, and as long as the commands used in the web application are the same, the code will still work since these commands are formatted in a consistent way.</p><p>Now that we have covered the basic components of a prediction service and how they communicate with one another, let us examine each in greater detail.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Clients and making requests"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec43"/>Clients and making requests</h1></div></div></div><p>When a client issues requests to the server and the downstream application, we might potentially have <a id="id509" class="indexterm"/>a major design problem: how do we know in advance what kind of requests we might receive? If we had to re-implement a new set of standard requests every time we developed a web application, it would be difficult to reuse code and write generic services that other programs could call, since their requests would potentially have to change for every web application a client might interact with.</p><p>This is the problem solved by the HTTP standard, which describes a standard language and format in which requests are sent between servers and clients, allowing us to rely upon a common command syntax, which could be consumed by many different applications. While we could, in theory, issue some of these commands to our prediction service by pasting a URL into the address bar of our browser (such as GET, described below), this will only cover a subset of the kinds of requests we want to issue. The standard sorts of requests we typically implement in a web application are:</p><div class="section" title="The GET requests"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec77"/>The GET requests</h2></div></div></div><p>The <code class="literal">GET</code> requests<a id="id510" class="indexterm"/> only retrieve information, which will then be rendered in our web browser depending upon the kind of response. We could receive back an actual <code class="literal">html</code> page, or simply a piece of text. In order to specify what information we want to receive, a GET request will include variables in the URL in the form <code class="literal">url?key1=value1&amp;key2=value2</code>. URL is the web address given to the prediction service, which in our example will just be the local machine, but could also be any valid IP address or URL. This URL is separated by a question mark (<span class="strong"><strong>?</strong></span>) from the (key, value) pairs that define the parameters of our request for information. Multiple parameters may be specified: for example, we could indicate a pair of parameters for a user and item dataset using the string <code class="literal">userid=12894&amp;itemid=93819</code>, with each key, value pair separated by the ampersand symbol (<code class="literal">&amp;</code>).</p><p>We can directly issue a <code class="literal">GET</code> request by pasting the URL format described previously into the address bar of a browser or by issuing a <code class="literal">curl</code> command to the same address by typing the following into a terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curl &lt;address&gt;</strong></span>
</pre></div><p>We can also use the <a id="id511" class="indexterm"/>Python requests library (<a class="ulink" href="http://docs.python-requests.org/en/master/">http://docs.python-requests.org/en/master/</a>), which allows us to not worry about the details of formatting the URL. Using this library, the same GET request is called in the following way:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r = requests.get(url,params)</strong></span>
</pre></div><p>Here, <code class="literal">params</code> is a dictionary of key-value pairs that we would have passed in the URL. The requests library performs this formatting for us, as we can see by printing the resulting URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(r.url)</strong></span>
</pre></div><p>Once we have issued<a id="id512" class="indexterm"/> the request, we can check the result using either of the following two commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r.json()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r.text</strong></span>
</pre></div><p>We can also check the status code of the response to see if there was an error or not (see aside on standard response codes):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r.status_code</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p><span class="strong"><strong>Aside: HTTP Status Codes</strong></span></p><p>When we issue <a id="id513" class="indexterm"/>a request to a web application using the methods discussed in this chapter, one way to check the success of the request is to examine the response code, which gives a standard number corresponding to the response of the web application to the request. You may have even seen these codes before without realizing it, such as the 404 error that is returned when a webpage cannot be displayed in your browser. The standard codes to be aware of are:</p><p>200: success, we usually check this value to make sure we received a correct response.</p><p>404: not found, indicating that the web application could not find the resource we requested.</p><p>500: server error, which we will often receive if the code run by our web application runs into problems.</p><p>For a more comprehensive list please, see (Nottingham, Mark, and Roy Fielding. "Additional HTTP Status Codes." (2012); Berners-Lee, Tim, Roy Fielding, and Henrik Frystyk. Hypertext transfer protocol--HTTP/1.0. No. RFC 1945. 1996).</p></div></div></div><div class="section" title="The POST request"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec78"/>The POST request</h2></div></div></div><p>Unlike the <a id="id514" class="indexterm"/>GET command, the POST request does not use data contained in the URL, but rather transmits information separate from the URL. If you have ever entered your credit card information in an online store, this information is probably transmitted using a POST request, which is fortunate since it then remains hidden. However, the fact that the information for the request is not contained in the URL means that we cannot simply paste the request into the address bar of our web browser: we would need a form on the webpage that issues the POST request or make the request programmatically ourselves. Without an actual form on a webpage,  we can use a <code class="literal">curl</code> command to issue a POST request using the following syntax:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curl –x POST  -d  &lt;data&gt; &lt;url&gt;</strong></span>
</pre></div><p>We can also use the Python requests library:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r = requests.post(url,data)</strong></span>
</pre></div><p>In the preceding code, <code class="literal">data</code> is a Python dictionary of information that the web application can access in fulfilling the POST request.</p></div><div class="section" title="The HEAD request"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec79"/>The HEAD request</h2></div></div></div><p>Like<a id="id515" class="indexterm"/> the GET request, HEAD retrieves information, but instead of the body of the response (such as a webpage or JSON), it only retrieves metadata about the response (such as the encoding). We can issue a HEAD request using the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curl –i –X HEAD &lt;url&gt;</strong></span>
</pre></div><p>Note that we have added the <code class="literal">–i</code> flag to this request; normally, the <code class="literal">curl</code> command will not print header information without this option. Using the Python requests library we would use the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  requests.head(url)</strong></span>
</pre></div></div><div class="section" title="The PUT request"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec80"/>The PUT request</h2></div></div></div><p>In cases where <a id="id516" class="indexterm"/>our web application has access to a database system, we issue PUT commands in order to store new information. Using <code class="literal">curl</code>, we make this request using the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curl -X PUT -d key1=value1 -d key2=value2 &lt;url&gt;</strong></span>
</pre></div><p>We can also make this request using the requests library:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  r = requests.put(url,data)</strong></span>
</pre></div><p>Here, data is a <a id="id517" class="indexterm"/>dictionary of the arguments we wish to place in the applications storage system.</p></div><div class="section" title="The DELETE request"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec81"/>The DELETE request</h2></div></div></div><p>The opposite of the <a id="id518" class="indexterm"/>PUT command, DELETE requests are issued to remove a piece of data from the application's storage system. The curl command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curl -X DELETE -d key1=value1 -d key2=value2 &lt;url&gt;</strong></span>
</pre></div><p>While the same request using the requests library is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  r = requests.delete(url,data)</strong></span>
</pre></div><p>Here, data is a dictionary of the arguments we wish to remove from the applications storage system.</p><p>While there are other requests types available, we will not cover them in this discussion; for more details please see (Berners-Lee, Tim, Roy Fielding, and Henrik Frystyk. Hypertext transfer protocol--HTTP/1.0. No. RFC 1945. 1996). Note that since we can issue these requests using the Python request library, we can actually test our web application in the Python notebooks we have been using in the exercises in this volume.</p><p>For our purposes, the client will be the Jupyter notebook itself or the command line of the terminal; however, we could imagine other cases where the client is actually another web application that issues these commands and acts on the response. Again, since the server only needs to guarantee a particular message format rather than the details of the sender, either option is interchangeable.</p><p>Now that we know how to issue HTTP requests to our service, let us look at the server.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Server – the web traffic controller"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec44"/>Server – the web traffic controller</h1></div></div></div><p>To run our <a id="id519" class="indexterm"/>prediction<a id="id520" class="indexterm"/> service, we need to communicate with external systems to receive requests to train a model, score new data, evaluate existing performance, or provide model parameter information. The web server performs this function, accepting incoming HTTP requests and forwarding them on to our web application either directly or through whatever middleware may be used.</p><p>Though we could have made many different choices of server in illustrating this example, we have chosen the CherryPy library because unlike other popular servers such as Apache Tomcat or Nginx, it is written in Python (allowing us to demonstrate its functionality inside a notebook) and is scalable, processing many requests in only a few milliseconds (<a class="ulink" href="http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet">http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet</a>.). The server is attached to a particular port, or endpoint (this is usually given in the format <code class="literal">url:port</code>), to which we direct requests that are then forwarded to<a id="id521" class="indexterm"/> the web application. The use of ports means that we could in theory have multiple servers on a given URL, each listening to requests on a<a id="id522" class="indexterm"/> different endpoint.</p><p>As we discussed previously, the server uses the WGSI specification to communicate with the application itself. In concrete terms, the server has a function known as a callable (for example, any object with a <code class="literal">__call__</code> method) that is executed every time it receives a request, whose result is handed off to the application. In our example in this chapter, the WGSI is already implemented by CherryPy, and we will simply illustrate how it does so. Complete documentation of the interface is available at (<a class="ulink" href="https://www.python.org/dev/peps/pep-0333/">https://www.python.org/dev/peps/pep-0333/</a>). In a way, the WGSI solves the same problem as HTTP in the communication between servers and applications: it provides a common way in which the two systems exchange information, allowing us to swap the components or event place intervening components without altering the fundamental way in which information is transferred.</p><p>In cases where we might wish to scale the application to a larger load, we could imagine middleware such as a load balancer between the server and the application. The middleware would receive the callable output and pass it along to the web application. In the case of a load balancer, this could potentially redistribute requests to many separate instances of the same predictive service, allowing us to scale the service horizontally (see Aside). Each of these services would then return their response to the server before it is sent back to the client.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p><span class="strong"><strong>Aside: horizontal and vertical scaling</strong></span></p><p>As the volume <a id="id523" class="indexterm"/>of data <a id="id524" class="indexterm"/>or computational complexity of our prediction services increases, we have two primary ways to increase the performance of the service. The first, known as horizontal scaling, might involve adding more instances of our application. Separately, we might also increase the number of resources in our underlying computing layer, such as Spark. In contrast, vertical scaling involves improving the existing resources by adding more RAM, CPU, or disk space. While horizontal scaling is more easily implemented using software alone, the right solution for such resources constraints will depend on the problem domain and organizational budget.</p></div></div><div class="section" title="Application – the engine of the predictive services"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec82"/>Application – the engine of the predictive services</h2></div></div></div><p>Once a request<a id="id525" class="indexterm"/> has made its way from the client to the application, we need to provide the logic that will execute these commands and return a response to the server and subsequently client upstream. To do so, we must attach a function to the particular endpoint and requests we anticipate receiving.</p><p>In this chapter, we will be using the <a id="id526" class="indexterm"/>Flask framework to develop our web application (<a class="ulink" href="http://flask.pocoo.org/">http://flask.pocoo.org/</a>). While Flask can also support template generation of HTML pages, in this chapter we will be using it purely to implement various requests to the underlying predictive algorithm code through URL endpoints corresponding to the HTTP requests discussed previously. Implementing these endpoints allows a consistent interface through which many other software systems could interact with our application—they just need to point to the appropriate web address and process the response returned from our service. In case you are concerned we will not generate any actual 'webpages' in our application, do not be worried: we will use the same Flask framework in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span>, to develop a dashboard system based on the data we will generate through the predictive modeling service in this chapter.</p><p>In writing the logic for our predictive modeling application, it is important to keep in mind that the functions that are called in response to client requests can themselves be interfaces specifying a generic, modular service. While we could directly implement a particular machine learning algorithm in the code for the web application itself, we have chosen to abstract this design, with the web application instead making a generic call to construct a model with some parameters, train, and score using an algorithm, regardless of the data or particular model used in the application. This allows us to reuse the web application code with many different algorithms while also affording the flexibility to implement these algorithms in different ways over time. It also forces us to determine a consistent set of operations for our algorithms since the web application will only interact with them through this abstraction layer.</p><p>Finally, we have the algorithm itself, which is called by the web application code. This program needs to implement functions, such as training a model and scoring records using a set of data, specified in the web application. The details can change substantially over time without need to modify the web application, allowing us to flexibly develop new models or experiment with different libraries.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Persisting information with database systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec45"/>Persisting information with database systems</h1></div></div></div><p>Our prediction <a id="id527" class="indexterm"/>service will use data in a number of ways. When we start the service, we have standard configurations we would like to retrieve (for example, the model parameters), and we might also like to log records of the requests that the application responds to for debugging purposes. As we score data or prepare trained models, we would ideally like to store these somewhere in case the prediction service needs to be restarted. Finally, as we will discuss in more detail, a database can allow us to keep track of application state (such as which tasks are in progress). For all these uses, a number of <a id="id528" class="indexterm"/>database systems can be applied.</p><p>Databases are generally categorized into two groups: relational and non-relational. Relational databases<a id="id529" class="indexterm"/> are probably familiar to you, as they are used in most business data warehouses. Data is stored in the form of tables, often with facts (such as purchases or search events) containing columns (such as user account IDs or an item identifier) that may be joined to dimensional tables (containing information on an item or user) or relational information (such as a hierarchy of items IDs that define the contents of an online store). In a web application, a relational system can be used behind the scenes to retrieve information (for example, in response to a GET request for user information), to insert new information, or delete rows from the database. Because the data in a relational system is stored in tables, it needs to follow a common series of columns, and these sorts of systems are not designed with nested structures such as JSON in mind. If we know there are columns we will frequently query (such as an item ID), we can design indices on the tables in these systems that speed up retrieval. Some common popular (and open source) relational systems are MySQL, PostGreSQL, and SQLite.</p><p>Non-relational databases, also known as 'NoSQL', follow a very different data model. Instead of being <a id="id530" class="indexterm"/>formed of tables with multiple columns, these systems are designed as with alternative layouts such as key-value stores, where a row of information (such as a customer account) has a key (such as an item index) and an arbitrary amount of information in the value field. For example, the value could be a single item or a nested series of other key-values. This flexibility means that NoSQL databases can store information with diverse schema even in the same table, since the fields in the value do not need to be specifically defined. Some of these applications allow us to create indices on particular fields within the value, just as for relational systems. In addition to key-value databases (such as Redis) and document stores (such as MongoDB), NoSQL systems also include columnar stores where data are co-located in files based primarily on column chunks rather than rows (examples include Cassandra and Druid), and graph databases such as Neo4j which are optimized for data composed of nodes and edges (such as what we studied in the context of spectral clustering in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <span class="emphasis"><em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em></span>). We will use MongoDB and Redis in our example in this chapter.</p><p>In addition to storing data with flexible schema, such as the nested JSON strings we might encounter in REST API calls, key-value stores can server another function in a web application by allowing us to persist the state of a task. For quickly answered requests such as a GET class for information, this is not necessary. However, prediction services might frequently have long-running tasks that are launched by a POST request and <a id="id531" class="indexterm"/>take time to compute a response. Even if the task is not complete though, we want to return an immediate response to the client that initiated the task. Otherwise, the<a id="id532" class="indexterm"/> client will stall waiting for the server to complete, and this can potentially affect performance of the client and is very much against the philosophy of decoupling the components of the system described previously. Instead, we want to return a task identifier to the client immediately, which will allow the client to poll the service to check on the progress of the task and retrieve the result when it is available. We can store the state of a task using a key-value database and provide both update methods to allow us to provide information on intermediate progress by editing the task records and GET methods to allow clients to retrieve the current status of the task. In our example, we will be using Redis as the backend to store task results for long-running applications, and also as the message queue by which tasks can communicate, a role known as<a id="id533" class="indexterm"/> a "broker".</p><p>Now that we have covered the basic structure of our prediction service, let us examine a concrete example that ties together many of the patterns we have developed in predictive modeling tasks over the previous sections.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Case study – logistic regression service"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec46"/>Case study – logistic regression service</h1></div></div></div><p>As an illustration <a id="id534" class="indexterm"/>of the architecture covered previously, let us look at an example of a prediction service that implements a logistic regression model. The model is both trained and scores new data using information passed through URLs (either through the web browser or invoking curl on the command line), and illustrates how these components fit together. We will also examine how we can interactively test these components using the same IPython notebooks as before, while also allowing us to seamlessly deploying the resulting code in an independent application.</p><p>Our first task is to set up the databases used to store the information used in modeling, as well as the result and model parameters.</p><div class="section" title="Setting up the database"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec83"/>Setting up the database</h2></div></div></div><p>As a first step in <a id="id535" class="indexterm"/>our application, we will set up the database to store our training data and models, and scores obtained for new data. The examples for this exercise consist of data from a marketing campaign, where the objective was to convince customers to subscribe for a term deposit (Moro, Sérgio, Paulo Cortez, and Paulo Rita. "A data-driven approach to predict the success of bank telemarketing."Decision Support Systems 62 (2014): 22-31). Thus, the objective with this data is to predict based on a customer's feature variables whether they are likely to pay for this service. The data is contained in the <code class="literal">bank-full.csv</code> file, which we need to load into MongoDB (<a class="ulink" href="https://www.mongodb.org/">https://www.mongodb.org/</a>).</p><p>After installing MongoDB for your system, you can test the database by running the following command in your terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mongodb</strong></span>
</pre></div><p>The preceding command should start the database. Now, to import our training data, we can use the following command in a separate terminal window:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mongoimport -d datasets -c bank --type csv --file bank-full.csv —headerline</strong></span>
</pre></div><p>This will allow us to import the data into a database called 'datasets', in a collection called bank. We can test if the data has been successfully loaded by opening a mongo client in the terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mongo</strong></span>
</pre></div><p>If we run the following command, we should be able to see our dataset listed under the datasets database:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ use datasets</strong></span>
<span class="strong"><strong>$ show collections</strong></span>
</pre></div><p>We can verify that the data has been correctly parsed by examining one record:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ db.bank.findOne()</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>The code here is inspired by examples in <a class="ulink" href="https://github.com/jadianes/spark-movie-lens">https://github.com/jadianes/spark-movie-lens</a> and <a class="ulink" href="http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server">http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server</a>.</p></div></div><p>You can see that record appears like a Python dictionary. To retrieve elements with particular values, we can use findOne with key:values set to the filters we want to apply:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ db.bank.findOne({},{key:value,..})</strong></span>
</pre></div><p>Now that we have the data loaded, we can interact with it through Python using the pymongo client. We initialize a client with access to the database we just created using the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pymongo import MongoClient</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; MONGODB_HOST = 'localhost'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; MONGODB_PORT = 27017</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; DBS_NAME = 'datasets'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; COLLECTION_NAME = 'bank'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; connection = MongoClient(MONGODB_HOST, MONGODB_PORT)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; collection = connection[DBS_NAME][COLLECTION_NAME]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; customers = collection.find(projection=FIELDS)</strong></span>
</pre></div><p>Note that the <code class="literal">mongod</code> command still needs to be running in a separate terminal window for you to <a id="id536" class="indexterm"/>access the database through Python. The customers object will then contain each customer's records. While for the current example we will primarily access MongoDB using the SparkConnector, the commands above will be useful in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span> when we analyze the output of our model. Indeed, the MongoDB database allows us to store information used by our model service, but also can be a source of shared information for the reporting service we will build in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span>, by visualizing the results of our modeling.</p><p>As we mentioned previously, we will also use the <a id="id537" class="indexterm"/>Redis (<a class="ulink" href="http://redis.io/">http://redis.io/</a>) key-value store to log the intermediate state of long-running tasks, and also to store the serialized output from training models in Spark. After installing Redis DB on your system, you should be able to start the server by typing the following command in the terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> &gt; redis-server</strong></span>
</pre></div><p>Which, if successful, should give and output like the following:</p><div class="mediaobject"><img src="images/B04881_08_01.jpg" alt="Setting up the database"/></div><p>The Python interface<a id="id538" class="indexterm"/> for Redis in the redis-py package (which, like many of the libraries we have seen in prior chapters, may be installed using <code class="literal">pip</code> or <code class="literal">easy_install</code>) is comparable to MongoDB. If we wanted to retrieve a record from our redis database, we could the following commands to start a client and issue a query or store data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import redis</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r.get(key)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r.set(key,value)</strong></span>
</pre></div><p>When we start a new client using 'StrictRedis', we specify the port the redis-server is listening on (default of 6379) and the database identifier. By issuing get and set commands, we can respectively retrieve prior results or update the database with new information. As with the Python mongo client, we will need to have the redis-server command running in a separate command line window to allow us to issue commands to the database in Python.</p><p>Now that we have our databases set up, let us look at the server that will manage requests for the applications using this data.</p></div><div class="section" title="The web server"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec84"/>The web server</h2></div></div></div><p>As described<a id="id539" class="indexterm"/> previously, the web server receives requests and forwards them to the web application. For our example, we start the server using the main function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;if __name__ == "__main__":</strong></span>

<span class="strong"><strong>    modelparameters = json.loads(open(sys.argv[1]).readline())</strong></span>

<span class="strong"><strong>    service = modelservice(modelparameters)</strong></span>

<span class="strong"><strong>    run_server(service)</strong></span>
</pre></div><p>There are three steps: we read the parameters for this service (here, just the name of the algorithm used), which is passed as command line argument, create the web application (using the same parameter file passed in during creation in the constructor), and then start the server. As you can see, the algorithm run by the prediction service is specified using a string argument. Later we will examine how this allows us to write a generic prediction service class, rather than a specific web application for each new algorithm we might use. When we start the server; it is registered on localhost on port 5000, as you can see by examining the body of the <code class="literal">run_server</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  def run_server(app):</strong></span>
<span class="strong"><strong>    import paste</strong></span>
<span class="strong"><strong>    from paste.translogger import TransLogger</strong></span>
<span class="strong"><strong>    app_ = TransLogger(app)</strong></span>
<span class="strong"><strong>    cherrypy.tree.graft(app_, '/')</strong></span>
<span class="strong"><strong>    cherrypy.config.update({</strong></span>
<span class="strong"><strong>        'engine.autoreload.on': True,</strong></span>
<span class="strong"><strong>        'log.screen': True,</strong></span>
<span class="strong"><strong>        'server.socket_port': 5000,</strong></span>
<span class="strong"><strong>        'server.socket_host': '0.0.0.0'</strong></span>
<span class="strong"><strong>    })</strong></span>
<span class="strong"><strong>    cherrypy.engine.start()</strong></span>
<span class="strong"><strong>    cherrypy.engine.block()</strong></span>
</pre></div><p>There are a few key things happening in this function. Firstly, we see middleware in action since the TransLogger class from the paste library passes requests between the server and the application. The TransLogger object then represents a valid WGSI application since it has a callable (the application). We use the <code class="literal">tree.graft</code> command to attach the application (the model service itself) so that the object is called by the CherryPy modelserver whenever it receives an HTTP request.</p><p>When we start the <a id="id540" class="indexterm"/>cherrypy server, we provide a few configurations. The enable.autoreload.on parameter controls whether the application will refresh when we change the source files it is pointing to, in this case our Flask application. Log.screen directs the output of error and access message to the stdout, which is useful when we are still debugging. Finally, the last two settings specify the URL and endpoint where we will send requests to the application.</p><p>Once we start the application, we also set it to block, which means it must finish processing one request before considering another. If we want to tune performance, we could remove this configuration, which would allow the application to receive multiple requests without waiting for the first to finish. The URL for this server is thus accessed by <code class="literal">http://0.0.0.0:5000</code> once it is running—this is the address where we will send our various commands to the prediction service. To start the server, type the following in the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; python modelserver.py parameters.json</strong></span>
</pre></div><p>The <code class="literal">parameters.json</code> file could contain parameters for the <code class="literal">modelservice</code> application that will be used when starting the modeling application, but for now we actually place nothing in this file. If successful, you should see the following output in the terminal:</p><div class="mediaobject"><img src="images/B04881_08_02.jpg" alt="The web server"/></div><p>As we issue <code class="literal">curl</code> commands to the server, we will see the responses displayed in this output as well.</p></div><div class="section" title="The web application"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec85"/>The web application</h2></div></div></div><p>Now that we have started the server and can begin receiving commands from the client, let us look <a id="id541" class="indexterm"/>at the commands that will be executed by our application, such as HTTP requests issued through the Python notebook or curl commands. The code that is executed when we send requests to the <code class="literal">CherryPy</code> server is contained in the <code class="literal">modelservice.py</code> file.</p><p>The constructor for the application, called by the <code class="literal">CherryPy</code> server when we started it, returns an app object specified using the Flask framework:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def modelservice(model_parameters):</strong></span>
<span class="strong"><strong>  …return app</strong></span>
<span class="strong"><strong>What is the definition of app? If we examine the beginning of the modelservice.py file, we see that app is defined using the Flask library:</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; app = Flask(__name__)</strong></span>
<span class="strong"><strong>… app.config.update(CELERY_BROKER_URL='redis://localhost:6379',CELERY_RESULT_BACKEND='redis://localhost:6379')</strong></span>
<span class="strong"><strong>… celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],broker=app.config['CELERY_BROKER_URL'])</strong></span>
<span class="strong"><strong>… celery.conf.update(app.config)</strong></span>
</pre></div><p>In addition to creating the Flask object app, we also generate a celery object. What is this celery object? As mentioned previously, we do not want to have our clients wait on long-running tasks to respond, as this would cause the client applications to potentially hang or timeout. Thus, our application needs to be non-blocking and return an immediate value for a long-running task, which is an ID that allows us to access the progress and results of the task through a REST API. We want to run the long-running task in a secondary process and have it report back the results or intermediate state as they become available. For our application, we will be using the <a id="id542" class="indexterm"/>
<code class="literal">Celery</code> library (<a class="ulink" href="http://www.celeryproject.org/">http://www.celeryproject.org/</a>), an asynchronous task queuing system that is ideal for this sort of application. Celery consists of a client that submits jobs to a queue, and worker tasks, which read from this queue, perform work, and return the results to the client. The client and workers communicate via a messaging queue, such as the Redis key-value store we mentioned previously, and results are also persisted to this database. The arguments <code class="literal">CELERY_BROKER_URL</code> and <code class="literal">CELERY_RESULT_BACKEND</code> are used to specify, respectively, where the worker tasks retrieve information on scheduled tasks, and where we can look up information on the status of currently running tasks. In our example, both functions are served by Redis, but we could <a id="id543" class="indexterm"/>substitute other systems, such as the message queue system RabbitMQ (<a class="ulink" href="https://www.rabbitmq.com/">https://www.rabbitmq.com/</a>).</p><p>In order for us to issue HTTP requests to the Celery worker tasks, we need to make sure that redis is already running, and then start the Celery workers using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; celery worker -A modelservice.celery</strong></span>
</pre></div><p>This starts celery <a id="id544" class="indexterm"/>worker processes with access to the commands specified in <code class="literal">modelservice.py</code> which we will cover below. If successful, you will see the following in your terminal.</p><div class="mediaobject"><img src="images/B04881_08_03.jpg" alt="The web application"/></div><p>As we later send requests to the service which are passed off to the Celery workers, information (such as Spark outputs) will be printed in this window as well.</p><div class="section" title="The flow of a prediction service – training a model"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec01"/>The flow of a prediction service – training a model</h3></div></div></div><p>So now that we <a id="id545" class="indexterm"/>have the Celery process running along with the Flask application, how can we define the functions executed by the workers in response to our HTTP requests? How can we specify the URLs to which we will issue curl commands? We will illustrate the flow of events by showing how a call to the training function will kick off a series of Spark jobs to perform cross validation and store a LogisticRegression model.</p><p>We start by issuing a curl command to the <code class="literal">train</code> function with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X POST http://0.0.0.0:5000/train/ -d @job.json --header "Content-Type: application/json"</strong></span>
</pre></div><p>We could have similarly used the Python requests library to transmit the information in <code class="literal">job.json</code> to the model training task. The <code class="literal">job.json</code> file contains all the parameters we might need to use in the various stages of parsing the data and training the model, as we will see as we walk through the flow of this request through our application. When this command is received by the CherryPy modelserver, it is forwarded to the Flask app defined in <code class="literal">modelservice.py</code>. How can we make the Flask application respond to this request? It is as easy as providing a decorator specifying a<a id="id546" class="indexterm"/> function to run in response to requests to this URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route("/train/",methods=["POST"])</strong></span>
<span class="strong"><strong>… def train():</strong></span>
<span class="strong"><strong>…    try:</strong></span>
<span class="strong"><strong> …       parsed_parameters = request.json</strong></span>
<span class="strong"><strong>     …   trainTask = train_task.apply_async(args=[parsed_parameters])</strong></span>
<span class="strong"><strong>     …   return json.dumps( {"job_id": trainTask.id } )</strong></span>
<span class="strong"><strong>    except:</strong></span>
<span class="strong"><strong>    …    print(traceback.format_exc())</strong></span>
</pre></div><p>The <code class="literal">@app.route</code> decorator indicates that the Flask object app listens for POST commands to a URL<a id="id547" class="indexterm"/> given as an argument to route. In responses, it extracts the dictionary of parameters from the POST request and passes them to a <code class="literal">train_task</code>, which will be run on a Celery worker process through the <code class="literal">apply_async</code> function. We then immediately return a task identifier associated with this task, which we can use to check the status or, as we will see, identify the output of the resulting model.</p><p>How do we specify the Celery task <code class="literal">train_task</code>? Similarly, we provide a decorator indicating that this function will be run on a worker process:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @celery.task(bind=True)</strong></span>
<span class="strong"><strong>… def train_task(self,parameters):</strong></span>
<span class="strong"><strong>…   try: </strong></span>
<span class="strong"><strong> …       spark_conf = start_conf(parameters)</strong></span>
<span class="strong"><strong>…        model.set_model(parameters)</strong></span>
<span class="strong"><strong>…        messagehandler = MessageHandler(self)</strong></span>
<span class="strong"><strong> …       model.train(parameters,messagehandler=messagehandler,sc=spark_conf)</strong></span>
<span class="strong"><strong> …   except:</strong></span>
<span class="strong"><strong>…        messagehandler.update('FAILURE',traceback.format_exc())</strong></span>
</pre></div><p>There are a few important details here. First, along with annotating the function with <code class="literal">@celery.task</code>, we provide the argument <code class="literal">bind=True</code>. This ensures that the function has a 'self' argument. Why would we need a self argument? In our example, we attach a <code class="literal">MessangeHandler</code> object to the training task using a reference to the function (self), allowing us to inject updates on the status of the task as it proceeds, and also retrieve the identifier for the task which was returned after we issued the POST request. The <code class="literal">MessageHandler</code> class is relatively simple and defined as follows in the <code class="literal">messagehandler.py</code> file in the <a id="id548" class="indexterm"/>code examples for this chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; class MessageHandler:</strong></span>
<span class="strong"><strong>    …</strong></span>
<span class="strong"><strong>    …def __init__(self,parent):</strong></span>
<span class="strong"><strong>    …    self.parent = parent</strong></span>
<span class="strong"><strong>    …   self.task_id = parent.request.id</strong></span>
<span class="strong"><strong>    …</strong></span>
<span class="strong"><strong>    … def update(self,state,message):</strong></span>
<span class="strong"><strong>    …    self.parent.update_state(state=state,meta={"message": message})</strong></span>
<span class="strong"><strong>    …</strong></span>
<span class="strong"><strong>    …def get_id(self):</strong></span>
<span class="strong"><strong>    …return self.task_id</strong></span>
</pre></div><p>When we construct the <code class="literal">MessageHandler</code> object, we retrieve the ID associated with the tasks from the <code class="literal">request.id</code> field. If we had not used the <code class="literal">bind=True</code> argument above, we would not be able to access this field, since we would not have a reference (self) to the task object to pass to the <code class="literal">MessageHandler</code>. This is also needed for the <code class="literal">update</code> function, which allows us to inject status updates about the progress of the task using the reference to the train task above. Finally, if we need to access the training task identifier anywhere else in our application, we can do so using <code class="literal">get_id</code>.</p><p>How could we access the tasks status modified by update? If you recall, when we initialized the Celery application, we provided the Redis database as a storage location for task status information. Using the identifier returned in response to our POST request, we could use a GET method to look up the status of this task, which we specify through another Flask app endpoint:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route('/training/status/&lt;task_id&gt;')</strong></span>
<span class="strong"><strong>… def training_status(task_id):</strong></span>
<span class="strong"><strong>…    try: </strong></span>
<span class="strong"><strong>…        task = train_task.AsyncResult(task_id)</strong></span>
<span class="strong"><strong>…        message = ""</strong></span>
<span class="strong"><strong>…        if task.state == 'PENDING':</strong></span>
<span class="strong"><strong> …           response = {</strong></span>
<span class="strong"><strong> …               'status': task.status,</strong></span>
<span class="strong"><strong> …               'message': "waiting for job {0} to start".format(task_id)</strong></span>
<span class="strong"><strong> …           }</strong></span>
<span class="strong"><strong> …       elif task.state != 'FAILED':</strong></span>
<span class="strong"><strong> …           if task.info is not None:</strong></span>
<span class="strong"><strong> …               message = task.info.get('message','no message')</strong></span>
<span class="strong"><strong> …           response = {</strong></span>
<span class="strong"><strong> …               'status': task.status,</strong></span>
<span class="strong"><strong> …               'message': message</strong></span>
<span class="strong"><strong> …           }</strong></span>
<span class="strong"><strong> …       else:</strong></span>
<span class="strong"><strong>…            if task.info is not None:</strong></span>
<span class="strong"><strong> …               message = task.info.get('message','no message')</strong></span>
<span class="strong"><strong> …           response = {</strong></span>
<span class="strong"><strong>  …              'status': task.status,</strong></span>
<span class="strong"><strong>  …              'message': message </strong></span>
<span class="strong"><strong> …           }</strong></span>
<span class="strong"><strong> …       return json.dumps(response)</strong></span>
<span class="strong"><strong> …   except:</strong></span>
<span class="strong"><strong> …       print(traceback.format_exc())</strong></span>
</pre></div><p>Thus, using a <code class="literal">curl</code> command, we could issue a GET to obtain the status of our training task, either <a id="id549" class="indexterm"/>printing it to the console or, if we made this application more complex, using it to generate a dashboard of job states in a pipeline or system.</p><p>Now that we have a way to inject updates about the status of our tasks, let us return to the <code class="literal">train_task</code> definition. In addition to creating the <code class="literal">MessageHandler</code> for this task, we also generate a <code class="literal">SparkConfiguration</code> and initialize a model object. The SparkConfiguration will probably look familiar from some of the examples in previous chapters, and is returned from the following function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def start_conf(jobparameters):</strong></span>
<span class="strong"><strong>    … conf = SparkConf().setAppName("prediction-service")</strong></span>
<span class="strong"><strong>    … conf.set("spark.driver.allowMultipleContexts",True)</strong></span>
<span class="strong"><strong>    …conf.set("spark.mongodb.input.uri",jobparameters.get('inputCollection',\</strong></span>
<span class="strong"><strong>   …     "mongodb://127.0.0.1/datasets.bank?readPreference=primaryPreferred"))</strong></span>
<span class="strong"><strong>    … conf.set("spark.mongodb.output.uri",jobparameters.get('outputCollection',\</strong></span>
<span class="strong"><strong>    …    "mongodb://127.0.0.1/datasets.bankResults"))</strong></span>
<span class="strong"><strong>    …return conf</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>Note that the arguments to the <code class="literal">SparkConfiguration</code> are used by the Spark mongo connector. This connector is an external dependency that needs to be downloaded and added at runtime to the system path of our Spark application, which can be accomplished by adding the following to your system parameters (assuming a Linux command line environment):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export PYSPARK_SUBMIT_ARGS="--packages org.mongodb.spark:mongo-spark-connector_2.10:1.0.0 pyspark-shell"</strong></span>
</pre></div></div></div><p>Here we set the<a id="id550" class="indexterm"/> application name by which we will identify the train task in the Spark UI on port 4040, and allow multiple contexts through <code class="literal">"spark.driver.allowMultipleContexts"</code> such that several Spark applications could be potentially run in parallel. Finally, we provide the <code class="literal">mongodb</code> input and output locations where Spark will read the data for training and store scored results. Note that these are both given as defaults, but could be changed by modifying parameters in the <code class="literal">job.json</code> file, allowing our application to operate on different inputs and store to different output locations by only changing the arguments to the POST request.</p><p>Now that we have the configuration to pass to the Spark job, let us look at the model object which will receive these parameters. We construct it as a global object at the beginning of the <code class="literal">modelservice</code> file in the line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; model = ModelFactory()</strong></span>
</pre></div><p>If you examine the definition of the <code class="literal">ModelFactory</code> class in the <code class="literal">modelfactory.py</code> file supplied with the code example for this chapter, you see can see that it provides a generic interface for wrapping the training and prediction functions of different machine learning algorithms:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; class ModelFactory:</strong></span>

<span class="strong"><strong>...  def __init__(self):</strong></span>
<span class="strong"><strong>…    self._model = None</strong></span>

<span class="strong"><strong>…  def set_model(self,modelparameters):</strong></span>
<span class="strong"><strong>…    module = importlib.import_module(modelparameters.get('name'))</strong></span>
<span class="strong"><strong>…    model_class = getattr(module, modelparameters.get('name'))</strong></span>
<span class="strong"><strong>…    self._model = model_class(modelparameters)</strong></span>

<span class="strong"><strong>…  def get_model(self,modelparameters,modelkey):</strong></span>
<span class="strong"><strong>…    module = importlib.import_module(modelparameters.get('name'))</strong></span>
<span class="strong"><strong>…    model_class = getattr(module, modelparameters.get('name'))</strong></span>
<span class="strong"><strong>…    self._model = model_class(modelparameters)</strong></span>
<span class="strong"><strong>…    self._model.get_model(modelkey)</strong></span>

<span class="strong"><strong>…  def train(self,parameters,messagehandler,sc):</strong></span>
<span class="strong"><strong>…    self._model.train(parameters,messagehandler,sc)</strong></span>

<span class="strong"><strong>…  def predict(self,parameters,input_data):</strong></span>
<span class="strong"><strong>…    return self._model.predict(parameters,input_data)</strong></span>

<span class="strong"><strong>…  def predict_all(self,parameters,messagehandler,sc):</strong></span>
<span class="strong"><strong>…    self._model.predict_all(parameters,messagehandler,sc)</strong></span>
</pre></div><p>As you can <a id="id551" class="indexterm"/>see, nowhere in this class do we specify the particular implementation of train or prediction tasks. Rather, we create an object with an internal member (<code class="literal">self_model</code>) that we can set using <code class="literal">set_model</code>, by dynamically retrieving code associated with a particular algorithm using <code class="literal">importlib</code>. The <code class="literal">"name"</code> argument also comes from <code class="literal">job.json</code>, meaning we could load different algorithms in our application and run training tasks simply by changing the parameters of our POST request. In this example, we specify the model as <code class="literal">LogisticRegressionWrapper</code>, which will cause this model (and the class of the same name) to be loaded and inserted into the <code class="literal">self_model</code> of the <code class="literal">ModelFactory</code> when we call <code class="literal">train_task</code>. ModelFactory also has a generic method for loading an existing model, <code class="literal">get_model</code>, which takes as input a task ID such as the one generated in response to our train request and sets <code class="literal">self_model</code> to be a previously trained model object which is retrieved using this task ID as a reference. In addition, this class has methods for predict (to give the predicted response for a single row of data) or <code class="literal">predict_all</code> (to perform bulk scoring using Spark).</p><p>To recap, now we see that in response to our POST request, the CherryPy server hands off the information in <code class="literal">data.json</code> to the <code class="literal">train</code> function of our Flask service, which starts a background process on a Celery worker. This worker process sets the generic model object of our Flask app to a Logistic Regression, creates a Spark configuration to run the training task, and returns a task ID that we can use to monitor the progress of the model training. In the final step in the journey of this POST request, let us see how the Logistic Regression model implements the training task.</p><p>In the <code class="literal">LogisticRegressionWrapper.py</code> file, you can see the specifications of the train <a id="id552" class="indexterm"/>task:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def train(self,parameters,messagehandler,spark_conf):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            sc = SparkContext(conf=spark_conf, pyFiles=['modelfactory.py', 'modelservice.py'])</strong></span>
<span class="strong"><strong>…            sqlContext = SQLContext(sc)</strong></span>
<span class="strong"><strong>…            iterations = parameters.get('iterations',None)</strong></span>
<span class="strong"><strong>…            weights = parameters.get('weights',None)</strong></span>
<span class="strong"><strong>…           intercept = parameters.get('intercept',False)</strong></span>
<span class="strong"><strong>…            regType = parameters.get('regType',None)</strong></span>
<span class="strong"><strong> …           data = sqlContext.\</strong></span>
<span class="strong"><strong> …               createDataFrame(\</strong></span>
<span class="strong"><strong> …               sqlContext.read.format("com.mongodb.spark.sql.DefaultSource").\</strong></span>
<span class="strong"><strong> …               load().\</strong></span>
<span class="strong"><strong> …               map(lambda x: DataParser(parameters).parse_line(x)))</strong></span>
<span class="strong"><strong> …           lr = LogisticRegression()</strong></span>
<span class="strong"><strong> …           pipeline = Pipeline(stages=[lr])</strong></span>
<span class="strong"><strong> …           paramGrid = ParamGridBuilder()\</strong></span>
<span class="strong"><strong> …               .addGrid(lr.regParam, [0.1]) \</strong></span>
<span class="strong"><strong> …               .build()</strong></span>
<span class="strong"><strong>            </strong></span>
<span class="strong"><strong> …           crossval = CrossValidator(estimator=pipeline,\</strong></span>
<span class="strong"><strong> …                 estimatorParamMaps=paramGrid,\</strong></span>
<span class="strong"><strong> …                 evaluator=BinaryClassificationEvaluator(),\</strong></span>
<span class="strong"><strong> …                 numFolds=2)</strong></span>
<span class="strong"><strong> …           messagehandler.update("SUBMITTED","submitting training job")</strong></span>
<span class="strong"><strong> …           crossvalModel = crossval.fit(data)</strong></span>
<span class="strong"><strong> …           self._model = crossvalModel.bestModel.stages[-1]</strong></span>
<span class="strong"><strong> …           self._model.numFeatures = len(data.take(1)[0]['features'])</strong></span>
<span class="strong"><strong> …           self._model.numClasses = len(data.select('label').distinct().collect())</strong></span>
<span class="strong"><strong>  …          r = redis.StrictRedis(host='localhost', port=6379, db=1)</strong></span>
<span class="strong"><strong>  …          r.set( messagehandler.get_id(), self.serialize(self._model) )</strong></span>
<span class="strong"><strong>  …          messagehandler.update("COMPLETED","completed training job")</strong></span>
<span class="strong"><strong>  …          sc.stop()</strong></span>
<span class="strong"><strong>  …      except:</strong></span>
<span class="strong"><strong>  …          print(traceback.format_exc())</strong></span>
<span class="strong"><strong>  ….          messagehandler.update("FAILED",traceback.format_exc())</strong></span>
</pre></div><p>First of all, we <a id="id553" class="indexterm"/>start a SparkContext using the parameters we defined in the SparkConfiguration we passed to this function. The parameters in our <code class="literal">job.json</code> file also include the algorithm parameters, which we parse. We then read the input data which we specified in the SparkConfiguration in a distributed fashion from mongodb into a Spark DataFrame, using a lambda function to parse the input. The parsing logic is defined in <code class="literal">dataparser.py</code>, in the <code class="literal">parse_line</code> function of the <code class="literal">DataParser</code> class:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def parse_line(self,input,train=True):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            if train:</strong></span>
<span class="strong"><strong>…               if self.schema_dict.get('label').get('values',None) is not None:</strong></span>
<span class="strong"><strong> …                   label = self.schema_dict.\</strong></span>
<span class="strong"><strong> …                   get('label').\</strong></span>
<span class="strong"><strong> …                   get('values').\</strong></span>
<span class="strong"><strong> …                   get(input[self.schema_dict.\</strong></span>
<span class="strong"><strong> …                   get('label').\</strong></span>
<span class="strong"><strong>…                    get('key')])</strong></span>
<span class="strong"><strong> …               else:</strong></span>
<span class="strong"><strong> …                   label = input[self.schema_dict.\</strong></span>
<span class="strong"><strong> …                   get('label').\</strong></span>
<span class="strong"><strong> …                   get('key')]</strong></span>
<span class="strong"><strong> …           features = []</strong></span>
<span class="strong"><strong> …           for f in self.schema_dict['features']:</strong></span>
<span class="strong"><strong> …               if f.get('values',None) is not None:</strong></span>
<span class="strong"><strong> …                   cat_feature = [ 0 ] * len(f['values'].keys())</strong></span>
<span class="strong"><strong> …                  if len(f['values'].keys()) &gt; 1: # 1 hot encoding</strong></span>
<span class="strong"><strong> …                       cat_feature[f['values'][str(input[f.get('key')])]] = 1</strong></span>
<span class="strong"><strong> …                   features += cat_feature # numerical</strong></span>
<span class="strong"><strong> …               else:</strong></span>
<span class="strong"><strong> …                   features += [ input[f.get('key')] ]</strong></span>

<span class="strong"><strong> …           if train:</strong></span>
<span class="strong"><strong> …               Record = Row("features", "label")</strong></span>
<span class="strong"><strong> …               return Record(Vectors.dense(features),label)</strong></span>
<span class="strong"><strong> …           else:</strong></span>
<span class="strong"><strong> …               return Vectors.dense(features)</strong></span>

<span class="strong"><strong>…        except:</strong></span>
<span class="strong"><strong>…            print(traceback.format_exc())</strong></span>
<span class="strong"><strong>…            pass</strong></span>
</pre></div><p>The <code class="literal">DataParser</code> class <a id="id554" class="indexterm"/>takes as input a parameters dictionary containing the schema of the data that—once again—we specified in our <code class="literal">job.json</code> data we included in our POST request. This information is stored in the <code class="literal">self._schema</code> property of the parser. Using this information, the parse_line function extracts the label (the response column) and encodes it as a numeric value if necessary. Similarly, the features of each record are parsed and, if necessary, one-hot encoded using information in the POST request. If the data is to be used in training (<code class="literal">train=True</code>), the parser returns the label and a vector of features. Otherwise, it just returns the features to be used in scoring new records. In either case, the features are encoded as a dense Vector from the Spark ml library (which is required for the logistic regression algorithm), and the row is returned as a Row object to be compatible with the Spark DataFrame needed for the training code. Because the fields we use as features are specified in our <code class="literal">job.json</code> data, we could train models using different columns from the same dataset without changing the underlying code.</p><p>Once the data is parsed, we construct a Spark Pipeline object to handle the stages of the model training. In our example, the only step is the model training itself, but we could potentially have transformations like the Vectorizers we examined in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span> in the context of text data as part of such as pipeline. We then create a ParamGrid to perform a grid search of the regularization parameter of our model, and pass it to a CrossValidator, which will peform n-fold validation to determine the best model. Once we have fit this model, we retrieve the optimal model from the CrossValidator results and determine the number of features and classes used in the model. Finally, we open a connection to the Redis database and store the parameters of this model after serializing it with the function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def serialize(self,model):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            model_dict = {}</strong></span>
<span class="strong"><strong> …           model_dict['weights'] = model.weights.tolist()</strong></span>
<span class="strong"><strong>…            model_dict['intercept'] = model.intercept</strong></span>
<span class="strong"><strong> …           model_dict['numFeatures'] = model.numFeatures</strong></span>
<span class="strong"><strong>…            model_dict['numClasses'] = model.numClasses</strong></span>
<span class="strong"><strong> …           return json.dumps(model_dict)</strong></span>
<span class="strong"><strong>…        except:</strong></span>
<span class="strong"><strong> …           raise Exception("failed serializing model: {0}".format(traceback.format_exc()))</strong></span>
</pre></div><p>Notice that we <a id="id555" class="indexterm"/>use the MessageHandler attached to this task to retrieve the task ID, which is used as the key to store the serialized model in Redis. Also, though we store the result in the same Redis instance listening on port 6379 that is used by Celery to queue tasks and update the status of background tasks, we save to db 1 instead of the default 0 to separate the information.</p><p>By tracing through the steps above, you should now be able to see how a POST request can be translated into a series of commands that parse data, perform cross-validated grid-search to train a model, and then serialize that model for later use. You should also appreciate how the parameterizations at each layer allow us to modify the behavior of this training task purely by modifying the contents of the POST request, and how the modularity of the application will make it easy to extend to other models. We also have utilized Spark, which will allow us to easily scale our calculations to larger datasets over time.</p><p>Now that we have illustrated the logical flow of data in our prediction service, let us finish by examining the prediction functions, whose output we will use in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span>.</p></div><div class="section" title="On-demand and bulk prediction"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec02"/>On-demand and bulk prediction</h3></div></div></div><p>Now that we have a trained model saved in our system, how can we utilize it to score new data? Our Flask app has two endpoints for this service. In the first, we make a POST request <a id="id556" class="indexterm"/>giving a row of data as a json, along with a model ID, and ask for a score from the logistic regression model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route("/predict/",methods=['POST'])</strong></span>
<span class="strong"><strong>… def predict():</strong></span>
<span class="strong"><strong>…    try:</strong></span>
<span class="strong"><strong>…        parsed_parameters = request.json</strong></span>
<span class="strong"><strong> …       model.get_model(parsed_parameters,parsed_parameters.get('modelkey'))</strong></span>
<span class="strong"><strong>…        score = model.predict(parsed_parameters,parsed_parameters.get('record'))</strong></span>
<span class="strong"><strong>…        return json.dumps(score)</strong></span>
<span class="strong"><strong>…    except:</strong></span>
<span class="strong"><strong> …       print(traceback.format_exc())</strong></span>
</pre></div><p>This time, instead of calling the <code class="literal">set_model</code> method of ModelFactory, we use <code class="literal">get_model</code> to load a previously trained model, then use it to predict the label of the input record and return the value. In the case of Logistic Regression, this will be a 0 or 1 value. While we do not provide a user interface in this example, we could imagine a simple form in which the user specifies a number of features of a record and submits them through a POST request, receiving back a prediction in realtime.</p><p>Looking at the<a id="id557" class="indexterm"/> implementation of <code class="literal">get_model</code> in LogisticRegressionWrapper, we see that we can retrieve and de-serialize the model we generated in the train task, and assign it to the <code class="literal">self._model</code> member of ModelFactory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def get_model(self,modelkey):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            r = redis.StrictRedis(host='localhost', port=6379, db=1)</strong></span>
<span class="strong"><strong>…            model_dict = json.loads(r.get(modelkey))</strong></span>
<span class="strong"><strong> …           self._model = LogisticRegressionModel(weights=Vectors.dense(model_dict['weights']),\</strong></span>
<span class="strong"><strong> …               intercept=model_dict['intercept'],\</strong></span>
<span class="strong"><strong> …               numFeatures=model_dict['numFeatures'],\</strong></span>
<span class="strong"><strong> …               numClasses=model_dict['numClasses']</strong></span>
<span class="strong"><strong> …               )</strong></span>
<span class="strong"><strong> …       except:</strong></span>
<span class="strong"><strong>  …          raise Exception("couldn't load model {0}: {1}".format(modelkey,traceback.format_exc()))</strong></span>
</pre></div><p>Subsequently, when we score a new record, we call the <code class="literal">predict</code> function to parse this record and use the de-serialized model to generate a prediction:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def predict(self,parameters,input_data):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            if self._model is not None:</strong></span>
<span class="strong"><strong> …               return self._model.predict(DataParser(parameters).parse_line(input_data,train=False))</strong></span>
<span class="strong"><strong> …           else:</strong></span>
<span class="strong"><strong> …               return "Error, no model is trained to give predictions"</strong></span>
<span class="strong"><strong> …       except:</strong></span>
<span class="strong"><strong> …           print(traceback.format_exc())</strong></span>
</pre></div><p>This sort of functionality will be useful for interactive applications, such as a human user submitting a few records of interest to obtain predictions, or for real time applications in which we might receive streaming input and provide predictions for immediate use. Note that thought we do not use Spark in this particular instance, we still have a nice opportunity for horizontal scaling. Once we have trained the model, we could de-serialize the resulting parameters in several copies of the modelservice, which will allow<a id="id558" class="indexterm"/> use to potentially avoid timeouts if we receive many requests. However, in cases where the volume of predictions required is large and the necessary latency is <span class="emphasis"><em>not</em></span> realtime, it may be more effective to utilize Spark to perform bulk-scoring of records in our database. We implement this bulk-scoring capability using a Celery task in a manner similar to the <code class="literal">train_task</code>, specifying a <code class="literal">predictall</code> endpoint in the Flask app:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route("/predictall/",methods=["POST"])</strong></span>
<span class="strong"><strong>… def predictall():</strong></span>
<span class="strong"><strong>…    try:</strong></span>
<span class="strong"><strong>…       parsed_parameters = request.json</strong></span>

<span class="strong"><strong>…        predictTask = predict_task.apply_async(args=[parsed_parameters])</strong></span>
<span class="strong"><strong>…        return json.dumps( {"job_id": predictTask.id } )</strong></span>
<span class="strong"><strong>…    except:</strong></span>
<span class="strong"><strong>…        print(traceback.format_exc())</strong></span>
</pre></div><p>The associated Celery task is show below:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @celery.task(bind=True)</strong></span>
<span class="strong"><strong>… def predict_task(self,parameters):</strong></span>
<span class="strong"><strong>…    try: </strong></span>
<span class="strong"><strong>…        spark_conf = start_conf(parameters)</strong></span>
<span class="strong"><strong> …       messagehandler = MessageHandler(self)</strong></span>
<span class="strong"><strong>…        model.get_model(parameters,parameters.get('modelkey'))</strong></span>
<span class="strong"><strong>…        print(model._model._model)</strong></span>
<span class="strong"><strong> …       model.predict_all(parameters,messagehandler=messagehandler,sc=spark_conf)</strong></span>
<span class="strong"><strong>…    except:</strong></span>
<span class="strong"><strong>…        messagehandler.update('FAILURE',traceback.format_exc())</strong></span>
</pre></div><p>Again, we create a SparkConfiguration and MessageHandler, and like the predict method, we use a prior model ID specified in <code class="literal">job.json</code> to load a previous train model. We then call the <code class="literal">predict_all</code> method of this model to start a bulk scoring routine that will generate predictions for a large collection of data, and store the resulting in the <code class="literal">mongodb</code> collection specified by the output location parameter of the SparkConfiguration. For the <code class="literal">LogisticRegressionWrapper</code>, the <code class="literal">predict_all</code> method is <a id="id559" class="indexterm"/>shown below:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def predict_all(self,parameters,messagehandler,spark_conf):</strong></span>
<span class="strong"><strong>…        try:</strong></span>
<span class="strong"><strong>…            sc = SparkContext(conf=spark_conf, pyFiles=['modelfactory.py', 'modelservice.py'])</strong></span>
<span class="strong"><strong>…            sqlContext = SQLContext(sc)</strong></span>
<span class="strong"><strong>…            Record = Row("score","value")</strong></span>
<span class="strong"><strong>…           scored_data = sqlContext.\</strong></span>
<span class="strong"><strong>…                createDataFrame(\</strong></span>
<span class="strong"><strong>…                sqlContext.read.format("com.mongodb.spark.sql.DefaultSource").\</strong></span>
<span class="strong"><strong>…                load().\</strong></span>
<span class="strong"><strong>…                map(lambda x: Record(self._model.predict(DataParser(parameters).parse_line(x,train=False)),x)))</strong></span>
<span class="strong"><strong>…           messagehandler.update("SUBMITTED","submitting scoring job")</strong></span>
<span class="strong"><strong>… scored_data.write.format("com.mongodb.spark.sql.DefaultSource").mode("overwrite").save()</strong></span>
<span class="strong"><strong>…            sc.stop()</strong></span>
<span class="strong"><strong>…        except:</strong></span>
<span class="strong"><strong>…         messagehander.update("FAILED",traceback.format_exc())</strong></span>
</pre></div><p>As with the training task, we start a SparkContext using the SparkConfiguration we defined in the Celery task, and load the input from mongodb using the Spark connector. Instead of simply parsing the data, we score the parsed records using the de-serialized model we loaded using the <code class="literal">get_model</code> command, and pass both it and the original record into a new Row object, which now has two columns: the score and the input. We then save this data back to mongodb.</p><p>If you open the mongo client and examine the <code class="literal">bankResults</code> collection, you can verify that it now contains the bulk-scored input data. We will utilize these results in <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span> where we will expose these scores in a reporting application to visualize the ongoing performance of our model and diagnose potential issues in model performance.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec47"/>Summary</h1></div></div></div><p>In this chapter, we described the three components of a basic prediction service: a client, the server, and the web application. We discussed how this design allows us to share the results of predictive modelling with other users or software systems, and scale our modeling horizontally and modularly to meet the demands of various use cases. Our code examples illustrate how to create a prediction service with generic model and data parsing functions that can be reused as we try different algorithms for a particular business use case. By utilizing background tasks through Celery worker threads and distributed training and scoring on Spark, we showed how to potentially scale this application to large datasets while providing intermediate feedback to the client on task status. We also showed how an on-demand prediction utility could be used to generate real-time scores for streams of data through a REST API.</p><p>Using this prediction service framework, in the next chapter we will extend this application to provide ongoing monitoring and reporting about the performance and health of our predictive models.</p></div></div>
</body></html>