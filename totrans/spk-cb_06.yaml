- en: Chapter 6. Getting Started with Machine Learning Using MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a labeled point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating machine learning pipelines using ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is Wikipedia''s definition of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Machine learning is a scientific discipline that explores the construction
    and study of algorithms that can learn from data."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Essentially, machine learning is making use of past data to make predictions
    about the future. Machine learning heavily depends upon statistical analysis and
    methodology.
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, there are four types of measurement scales:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scale type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Nominal Scale | =, ≠Identifies categoriesCan''t be numericExample: male,
    female |'
  prefs: []
  type: TYPE_TB
- en: '| Ordinal Scale | =, ≠, <, >Nominal scale +Ranks from least important to most
    importantExample: corporate hierarchy |'
  prefs: []
  type: TYPE_TB
- en: '| Interval Scale | =, ≠, <, >, +, -Ordinal scale + distance between observationsNumbers
    assigned to observations indicate orderDifference between any consecutive values
    is same as others60° temperature is not the double of 30° |'
  prefs: []
  type: TYPE_TB
- en: '| Ratio Scale | =, ≠, <, >, +, ×, ÷Interval scale +ratios of observations$20
    is twice as costly as $10 |'
  prefs: []
  type: TYPE_TB
- en: Another distinction that can be made among the data is between the continuous
    and discrete data. Continuous data can take any value. Most data belonging to
    the interval and ratio scale is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete variables can take on only particular values and there are clear boundaries
    between the values. For example, a house can have two or three rooms but not 2.75
    rooms. Data belonging to nominal and ordinal scale is always discrete.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib is the Spark's library for machine learning. In this chapter, we will
    focus on the fundamentals of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Creating vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before understanding Vectors, let's focus on what is a point. A point is just
    a set of numbers. This set of numbers or coordinates defines the point's position
    in space. The numbers of coordinates determine dimensions of the space.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize space with up to three dimensions. Space with more than three
    dimensions is called **hyperspace**. Let's put this spatial metaphor to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a person. A person has the following dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Height
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are working in three-dimensional space here. Thus, the interpretation of
    point (160,69,24) would be 160 lb weight, 69 inches height, and 24 years age.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Points and vectors are same thing. Dimensions in vectors are called **features**.
    In another way, we can define a feature as an individual measurable property of
    a phenomenon being observed.
  prefs: []
  type: TYPE_NORMAL
- en: Spark has local vectors and matrices and also distributed matrices. Distributed
    matrix is backed by one or more RDDs. A local vector has numeric indices and double
    values, and is stored on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of local vectors in MLlib: dense and sparse. A dense vector
    is backed by an array of its values, while a sparse vector is backed by two parallel
    arrays, one for indices and another for values.'
  prefs: []
  type: TYPE_NORMAL
- en: So, person data (160,69,24) will be represented as [160.0,69.0,24.0] using dense
    vector and as (3,[0,1,2],[160.0,69.0,24.0]) using sparse vector format.
  prefs: []
  type: TYPE_NORMAL
- en: Whether to make a vector sparse or dense depends upon how many null values or
    0s it has. Let's take a case of a vector with 10,000 values with 9,000 of them
    being 0\. If we use dense vector format, it would be a simple structure, but 90
    percent of space would be wasted. Sparse vector format would work out better here
    as it would only keep indices, which are non-zero.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse data is very common and Spark supports the `libsvm` format for it which
    stores one feature vector per line.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the MLlib vector explicitly (not to confuse with other vector classes):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dense vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a sparse vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the method signature of `vectors.dense`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, values represent double array of elements in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the method signature of `Vectors.sparse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, `size` represents the size of the vector, `indices` is an array of indices,
    and `values` is an array of values as doubles. Do make sure you specify `double`
    as datatype or use decimal in at least one value; otherwise it will throw an exception
    for the dataset, which has only integer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a labeled point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeled point is a local vector (sparse/dense), which has an associated label
    with it. Labeled data is used in supervised learning to help train algorithms.
    You will get to know more about it in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Label is stored as a double value in `LabeledPoint`. It means that when you
    have categorical labels, they need to be mapped to double values. What value you
    assign to a category is immaterial and is only a matter of convenience.
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Label values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Binary classification | 0 or 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiclass classification | 0, 1, 2… |'
  prefs: []
  type: TYPE_TB
- en: '| Regression | Decimal values |'
  prefs: []
  type: TYPE_TB
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the MLlib vector explicitly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `LabeledPoint`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point with a positive label and dense vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point with a negative label and dense vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point with a positive label and sparse vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point with a negative label and sparse vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `libsvm` file with the same data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload `person_libsvm.txt` to `hdfs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do a few more imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load data from `libsvm` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrix is simply a table to represent multiple feature vectors. A matrix that
    can be stored on one machine is called **local matrix** and the one that can be
    distributed across the cluster is called **distributed matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: Local matrices have integer-based indices, while distributed matrices have long-based
    indices. Both have values as doubles.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of distributed matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RowMatrix`: This has each row as a feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndexedRowMatrix`: This also has row indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CoordinateMatrix`: This is simply a matrix of `MatrixEntry`. A `MatrixEntry`
    represents an entry in the matrix represented by its row and column index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the matrix-related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dense local matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `personRDD` as RDD of vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `RowMatrix` and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a row matrix of `personRDD`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an RDD of indexed rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an indexed row matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the indexed row matrix back to row matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an RDD of matrix entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a coordinate matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calculating summary statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Summary statistics is used to summarize observations to get a collective sense
    of the data. The summary includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Central tendency of data—mean, mode, median
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spread of data—variance, standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boundary conditions—min, max
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This recipe covers how to produce summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the matrix-related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `personRDD` as RDD of vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the column summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the mean of this summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the non-zero values in each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the sample size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the max value of each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calculating correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correlation is a statistical relationship between two variables such that when
    one variable changes, it leads to a change in the other variable. Correlation
    analysis measures the extent to which the two variables are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: If an increase in one variable leads to an increase in another, it is called
    a **positive correlation**. If an increase in one variable leads to a decrease
    in the other, it is a **negative correlation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark supports two correlation algorithms: Pearson and Spearman. Pearson algorithm
    works with two continuous variables, such as a person''s height and weight or
    house size and house price. Spearman deals with one continuous and one categorical
    variable, for example, zip code and house price.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use some real data so that we can calculate correlation more meaningfully.
    The following are the size and price of houses in the City of Saratoga, California,
    in early 2014:'
  prefs: []
  type: TYPE_NORMAL
- en: '| House size (sq ft) | Price |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2100 | $1,620,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2300 | $1,690,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2046 | $1,400,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4314 | $2,000,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1244 | $1,060,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4608 | $3,830,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2173 | $1,230,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2750 | $2,400,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4010 | $3,380,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1959 | $1,480,000 |'
  prefs: []
  type: TYPE_TB
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an RDD of house sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an RDD of house prices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the correlation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`0.85` means a very strong positive correlation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since we do not have a specific algorithm here, it is, by default, Pearson.
    The `corr` method is overloaded to take the algorithm name as the third parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the correlation with Pearson:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the correlation with Spearman:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Both the variables in the preceding example are continuous, so Spearman assumes
    the size to be discrete. A better example of Spearman's use would be zip code
    versus price.
  prefs: []
  type: TYPE_NORMAL
- en: Doing hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hypothesis testing is a way of determining probability that a given hypothesis
    is true. Let's say a sample data suggests that females tend to vote more for the
    Democratic Party. This may or may not be true for the larger population. What
    if this pattern is there in the sample data just by chance?
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to look at the goal of hypothesis testing is to answer this question:
    If a sample has a pattern in it, what are the chances of the pattern being there
    just by chance?'
  prefs: []
  type: TYPE_NORMAL
- en: How do we do it? There is a saying that the best way to prove something is to
    try to disprove it.
  prefs: []
  type: TYPE_NORMAL
- en: The hypothesis to disprove is called **null hypothesis**. Hypothesis testing
    works with categorical data. Let's look at the example of a gallop poll of party
    affiliations.
  prefs: []
  type: TYPE_NORMAL
- en: '| Party | Male | Female |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Democratic Party | 32 | 41 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican Party | 28 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| Independent | 34 | 26 |'
  prefs: []
  type: TYPE_TB
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the relevant classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector for the Democratic Party:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector for the Republican Party:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector for the Independents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the chi-square goodness of fit test of the observed data against uniform
    distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the goodness of fit results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the input matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the chi-square independence test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the independence test results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating machine learning pipelines using ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark ML is a new library in Spark to build machine learning pipelines. This
    library is being developed along with MLlib. It helps to combine multiple machine
    learning algorithms into a single pipeline, and uses DataFrame as dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first understand some of the basic concepts in Spark ML. It uses transformers
    to transform one DataFrame into another DataFrame. One example of simple transformations
    can be to append a column. You can think of it as being equivalent to "alter table"
    in relational world.
  prefs: []
  type: TYPE_NORMAL
- en: Estimator, on the other hand, represents a machine learning algorithm, which
    learns from the data. Input to an estimator is a DataFrame and output is a transformer.
    Every Estimator has a `fit()` method, which does the job of training the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning pipeline is defined as a sequence of stages; each stage can
    be either an estimator or a transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The example we are going to use in this recipe is whether someone is a basketball
    player or not a basketball player. For this, we are going to have a pipeline of
    one estimator and one transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Estimator gets training data to train the algorithms and then transformer makes
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For now, assume `LogisticRegression` to be the machine learning algorithm we
    are using. We will explain the details about `LogisticRegression` along with other
    algorithms in the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point for Lebron who is a basketball player, is 80 inches
    tall height and weighs 250 lbs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point for Tim who is not a basketball player, is 70 inches
    tall height and weighs 150 lbs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point for Brittany who is a basketball player, is 80 inches
    tall height and weighs 207 lbs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a labeled point for Stacey who is not a basketball player, is 65 inches
    tall, and weighs 120 lbs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `LogisticRegression` estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a transformer by fitting the estimator with training DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s create a test data—John is 90 inches tall and weighs 270 lbs, and
    is a basketball player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create another test data—Tom is 62 inches tall and weighs 150 lbs, and is not
    a basketball player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `Features` case class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Map the `testRDD` to an RDD for `Features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `featuresRDD` into a DataFrame with column name `"features"`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform `featuresDF` by adding the `predictions` column to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the `predictionsDF`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`PredictionDF`, as you can see, creates three columns—`rawPrediction`, `probability`,
    and `prediction`—besides keeping features. Let''s select only `features` and `prediction`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rename the prediction to `isBasketBallPlayer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the schema for `playerDF`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
