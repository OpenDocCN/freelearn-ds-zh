<html><head></head><body>
		<div><h1 id="_idParaDest-48"><em class="italic"><a id="_idTextAnchor049"/>Chapter 4</em>: Working with Databases</h1>
			<p>In the previous chapter, you learned how to read and write text files. Reading log files or other text files from a data lake and moving them into a database or data warehouse is a common task for data engineers. In this chapter, you will use the skills you gained working with text files and learn how to move that data into a database. This chapter will also teach you how to extract data from relational and NoSQL databases. By the end of this chapter, you will have the skills needed to work with databases using Python, NiFi, and Airflow. It is more than likely that most of your data pipelines will end with a database and very likely that they will start with one as well. With these skills, you will be able to build data pipelines that can extract and load, as well as start and finish, with both relational and NoSQL databases.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Inserting and extracting relational data in Python</li>
				<li>Inserting and extracting NoSQL database data in Python</li>
				<li>Building database pipelines in Airflow</li>
				<li>Building database pipelines in NiFi</li>
			</ul>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor050"/>Inserting and extracting relational data in Python</h1>
			<p>When<a id="_idIndexMarker234"/> you hear the word <strong class="bold">database</strong>, you probably<a id="_idIndexMarker235"/> picture a relational database – that is, a database <a id="_idIndexMarker236"/>made up of tables containing columns and<a id="_idIndexMarker237"/> rows with relationships between the tables; for example, a purchase order system that has inventory, purchases, and customer information. Relational databases have been around for over 40 years and come from the relational data model developed by E. F. Codd in the late 1970s. There are several vendors of relational databases – including IBM, Oracle, and Microsoft – but all of these databases use a similar dialect of <strong class="bold">SQL</strong>, which stands<a id="_idIndexMarker238"/> for <strong class="bold">Structured Query Language</strong>. In this book, you will work with a popular open<a id="_idIndexMarker239"/> source database – <strong class="bold">PostgreSQL</strong>. In the next section, you will learn how to create a database and tables.</p>
			<h3>Creating a PostgreSQL database and tables</h3>
			<p>In <a href="B15739_02_ePub_AM.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Building Our Data Engineering Infrastructure</em>, you created a database in PostgreSQL <a id="_idIndexMarker240"/>using pgAdmin 4. The database was named <code>dataengineering</code> and you <a id="_idIndexMarker241"/>created a table named <code>users</code> with columns for name, street, city, ZIP, and ID. The database is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.1_B15739.jpg" alt="Figure 4.1 – The dataengineering database&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – The dataengineering database</p>
			<p>If you have the database created, you can skip this section, but if you do not, this section will quickly walk you through creating one.</p>
			<p>To create a <a id="_idIndexMarker242"/>database in PostgreSQL with pgAdmin 4, take the following steps:</p>
			<ol>
				<li>Browse to <code>http://localhost/pgadmin4</code> and log in using the account you created during the installation of <code>pgAdmin</code> in <a href="B15739_02_ePub_AM.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Building Our Data Engineering Infrastructure</em>.</li>
				<li>Expand the server icon in the <strong class="bold">Browser</strong> pane. Right-click on the <strong class="bold">MyPostgreSQL</strong> icon and select <strong class="bold">Create</strong> | <strong class="bold">Database</strong>.</li>
				<li>Name the database <code>dataengineering</code>. You can leave the user as <code>postgres</code>.</li>
				<li>Expand the <code>dataengineering</code> icon, then expand <strong class="bold">Schemas</strong>, then <strong class="bold">public</strong>, then <strong class="bold">Tables</strong>. Right-click on <strong class="bold">Tables</strong>, then click <strong class="bold">Create</strong> | <strong class="bold">Table</strong>.</li>
				<li>Name the <a id="_idIndexMarker243"/>table <code>users</code>. Click the <code>name</code>: <code>text</code></p><p>b) <code>id</code>: <code>integer</code></p><p>c) <code>street</code>: <code>text</code></p><p>d) <code>city</code>: <code>text</code></p><p>e) <code>zip</code>: <code>text</code> </p></li>
			</ol>
			<p>Now you have a database and a table created in PostgreSQL and can load data using Python. You will populate the table in the next section.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/>Inserting data into PostgreSQL</h2>
			<p>There<a id="_idIndexMarker244"/> are several libraries<a id="_idIndexMarker245"/> and ways to connect to a database in Python – <code>pyodbc</code>, <code>sqlalchemy</code>, <code>psycopg2</code>, and using an API and requests. In this book, we will use the <code>psycopg2</code> library to connect to PostgreSQL because it is built specifically to connect to PostgreSQL. As your skills progress, you may want to look into tools such as <strong class="bold">SQLAlchemy</strong>. SQLAlchemy is a <a id="_idIndexMarker246"/>toolkit and an object-relational mapper for Python. It allows you to perform queries in a more Pythonic way – without SQL – and to map Python classes to database tables. </p>
			<h3>Installing psycopg2</h3>
			<p>You can check <a id="_idIndexMarker247"/>whether you have <code>psycopg2</code> installed by running the following command:</p>
			<pre>python3 -c "import psycopg2; print(psycopg2.__version__)"</pre>
			<p>The preceding command runs <code>python3</code> with the command flag. The flag tells Python to run the commands as a Python program. The quoted text imports <code>psycopg2</code> and then prints the version. If you receive an error, it is not installed. You should see a version such as 2.8.4 followed by some text in parentheses. The library should have been installed during the installation of Apache Airflow because you used all the additional libraries in <a href="B15739_02_ePub_AM.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Building Our Data Engineering Infrastructure</em>.</p>
			<p>If it is not installed, you can add it with the following command:</p>
			<pre>pip3 install psycopg2</pre>
			<p>Using <code>pip</code> requires that there are additional dependencies present for it to work. If you run into problems, you can also install a precompiled binary version using the following command:</p>
			<pre>pip3 install psycopg2-binary</pre>
			<p>One of these two methods will get the library installed and ready for us to start the next section.</p>
			<h3>Connecting to PostgreSQL with Python</h3>
			<p>To connect to<a id="_idIndexMarker248"/> your database using <code>psycopg2</code>, you will need to create a connection, create a cursor, execute a command, and get the results. You will take these same steps whether you are querying or inserting data. Let's walk through the steps as follows: </p>
			<ol>
				<li value="1">Import the library and reference it as <code>db</code>:<pre>import psycopg2 as db</pre></li>
				<li>Create a connection string that contains the host, database, username, and password:<pre>conn_string="dbname='dataengineering' host='localhost' user='postgres' password='postgres'"</pre></li>
				<li>Create the connection object by passing the connection string to the <code>connect()</code> method:<pre>conn=db.connect(conn_string)</pre></li>
				<li>Next, create the cursor from the connection:<pre>cur=conn.cursor()</pre></li>
			</ol>
			<p>You are now connected<a id="_idIndexMarker249"/> to the database. From here, you can issue any SQL commands. In the next section, you will learn how to insert data into PostgreSQL</p>
			<h3>Inserting data</h3>
			<p>Now that you<a id="_idIndexMarker250"/> have a connection open, you can insert data using SQL. To insert a single person, you need to format a SQL <code>insert</code> statement, as shown:</p>
			<pre>query = "insert into users (id,name,street,city,zip) values({},'{}','{}','{}','{}')".format(1,'Big Bird','Sesame Street','Fakeville','12345')</pre>
			<p>To see what this query will look like, you can use the <code>mogrify()</code> method.</p>
			<p class="callout-heading">What is mogrify?</p>
			<p class="callout">According to the <code>psycopg2</code> docs, the <code>mogrify</code> method will return a query string after arguments binding. The string returned is <a id="_idIndexMarker251"/>exactly the one that would be sent to the database running the <code>execute()</code> method or similar. In short, it returns the formatted query. This is helpful as you can see what you are sending to the database, because your SQL query can often be a source of errors.</p>
			<p>Pass your query to the <code>mogrify</code> method:</p>
			<pre>cur.mogrify(query)</pre>
			<p>The preceding code will create a proper SQL <code>insert</code> statement; however, as you progress, you will add multiple records in a single statement. To do so, you will create a tuple of tuples. To create the same SQL statement, you can use the following code:</p>
			<pre>query2 = "insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)"
data=(1,'Big Bird','Sesame Street','Fakeville','12345')</pre>
			<p>Notice that in <code>query2</code>, you did not need to add quotes around strings that would be passed in as you did in <code>query</code> when you used <code>{}</code>. Using the preceding formatting, <code>psycopg2</code> will handle the mapping of types in the query string. To see what the query will look like when you execute it, you can use <code>mogrify</code> and pass the data along with the query:</p>
			<pre>cur.mogrify(query2,data)</pre>
			<p>The results of <code>mogrify</code> on <code>query</code> and <code>query2</code> should be identical. Now, you can execute the query to add it to the database:</p>
			<pre>cur.execute(query2,data)</pre>
			<p>If you go back to<a id="_idIndexMarker252"/> pgAdmin 4, right-click on the <code>insert</code> statement, you need to make it permanent by committing the transaction using the following code:</p>
			<pre>conn.commit()</pre>
			<p>Now, in pgAdmin 4, you should be able to see the record, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.2_B15739.jpg" alt="Figure 4.2 – Record added to the database&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Record added to the database</p>
			<p>The record is <a id="_idIndexMarker253"/>now added to the database and visible in pgAdmin 4. Now that you have entered a single record, the next section will show you how to enter multiple records.</p>
			<h3>Inserting multiple records</h3>
			<p>To insert <a id="_idIndexMarker254"/>multiple records, you could loop through data and use the same code shown in the preceding section, but this would require a transaction per record in the database. A better way would be to use a single transaction and send all the data, letting <code>psycopg2</code> handle the bulk insert. You can accomplish this by using the <code>executemany</code> method. The following code will use <code>Faker</code> to create the records and then <code>executemany()</code> to insert them:</p>
			<ol>
				<li value="1">Import the needed libraries:<pre>import psycopg2 as db
from faker import Faker</pre></li>
				<li>Create the <code>faker</code> object and an array to hold all the data. You will initialize a variable, <code>i</code>, to hold an ID:<pre>fake=Faker()
data=[]
i=2</pre></li>
				<li>Now, you can look, iterate, and append a fake tuple to the array you created in the previous step. Increment <code>i</code> for the next record. Remember that in the previous section, you created a record for <code>Big Bird</code> with an ID of <code>1</code>. That is why you will start with <code>2</code> in this example. We cannot have the same primary key in the <a id="_idIndexMarker255"/>database table:<pre>for r in range(1000):
    data.append((i,fake.name(),fake.street_address(),
               fake.city(),fake.zipcode()))
    i+=1</pre></li>
				<li>Convert the array into a tuple of tuples:<pre>data_for_db=tuple(data)</pre></li>
				<li>Now, you are back to the <code>psycopg</code> code, which will be similar to the example from the previous section:<pre>conn_string="dbname='dataengineering' host='localhost' user='postgres' password='postgres'"
conn=db.connect(conn_string)
cur=conn.cursor()
query = "insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)"</pre></li>
				<li>You can print out what the code will send to the database using a single record from the <code>data_for_db</code> variable:<pre>print(cur.mogrify(query,data_for_db[1]))</pre></li>
				<li>Lastly, use <code>executemany()</code> instead of <code>execute()</code> to let the library handle the <a id="_idIndexMarker256"/>multiple inserts. Then, commit the transaction:<pre>cur.executemany(query,data_for_db)
conn.commit()</pre></li>
			</ol>
			<p>Now, you can look at pgAdmin 4 and see the 1,000 records. You will have data similar to what is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.3_B15739.jpg" alt="Figure 4.3 – 1,000 records added to the database&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – 1,000 records added to the database</p>
			<p>Your table should now have 1,001 records. Now that you can insert data into PostgreSQL, the next section will show you how to query it in Python.</p>
			<h3>Extracting data from PostgreSQL</h3>
			<p>Extracting <a id="_idIndexMarker257"/>data using <code>psycopgs</code> follows the exact same <a id="_idIndexMarker258"/>procedure as inserting, the only difference being that you will use a <code>select</code> statement instead of <code>insert</code>. The following steps show you how to extract data:</p>
			<ol>
				<li value="1">Import the library, then set up your connection and cursor:<pre>import psycopg2 as db
conn_string="dbname='dataengineering' host='localhost' user='postgres' password='postgres'"
conn=db.connect(conn_string)
cur=conn.cursor()</pre></li>
				<li>Now, you can execute a query. In this example, you will select all records from the <code>users</code> table:<pre>query = "select * from users"
cur.execute(query)</pre></li>
				<li>Now, you have an iterable object with the results. You can iterate over the cursor, as shown:<pre>for record in cur:
    print(record)</pre></li>
				<li>Alternatively, you could use one of the <code>fetch</code> methods:<pre>cur.fetchall()
cur.fetchmany(howmany)  # where howmany equals the number of records you want returned 
cur.fetchone()</pre></li>
				<li>To grab a single record, you can assign it to a variable and look at it. Note that even when you select one record, the cursor returns an array:<pre>data=cur.fetchone()
print(data[0])</pre></li>
				<li>Regardless of whether you are fetching one or many, you need to know where you are and how many records there are. You can get the row count of the query using the following code:<pre>cur.rowcount
# 1001</pre></li>
				<li>You can get the<a id="_idIndexMarker259"/> current row number using <code>rownumber</code>. If you use <code>fetchone() </code>and then call <code>rownumber</code> again, it should increment with your new position:<pre>cur.rownumber</pre><p>The last thing <a id="_idIndexMarker260"/>to mention is that you can also query a table and write it out to a CSV file using the <code>copy_to()</code> method.</p></li>
				<li>Create the connection and the cursor:<pre>conn=db.connect(conn_string)
cur=conn.cursor()</pre></li>
				<li>Open a file to write the table to:<pre>f=open('fromdb.csv','w')</pre></li>
				<li>Then, call <code>copy_to</code> and pass the file, the table name, and the separator (which will default to tabs if you do not include it). Close the file, and you will have all the rows as a CSV:<pre>cur.copy_to(f,'users',sep=',')
f.close()</pre></li>
				<li>You can verify the results by opening the file and printing the contents:<pre>f=open('fromdb.csv','r')
f.read()</pre></li>
			</ol>
			<p>Now that you know how to read and write to a database using the <code>psycopg2</code> library, you can also read and write data using DataFrames, which you will learn about in the next section.</p>
			<h4>Extracting data with DataFrames</h4>
			<p>You can<a id="_idIndexMarker261"/> also query data using <code>pandas</code> DataFrames. To do so, you need to establish a connection using <code>psycopg2</code>, and then you can skip the cursor and go straight to the query. DataFrames give you a lot of power in filtering, analyzing, and transforming data. The following steps will walk you through using DataFrames:</p>
			<ol>
				<li value="1">Set up the connection:<pre>import psycopg2 as db
import pandas as pd
conn_string="dbname='dataengineering' host='localhost' user='postgres' password='postgres'"
conn=db.connect(conn_string)</pre></li>
				<li>Now, you can execute the query in a DataFrame using the <code>pandas</code> <code>read_sql()</code> method. The method takes a query and a connection:<pre>df=pd.read_sql("select * from users", conn)</pre></li>
				<li>The result is a DataFrame, <code>df</code>, with the full table users. You now have full access to all the DataFrame tools for working with the data – for example, you can export it to JSON using the following code:<pre>df.to_json(orient='records')</pre></li>
			</ol>
			<p>Now that you know how to work with data in a relational database, it is time to learn about NoSQL databases. The next section will show you how to use Python with Elasticsearch.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor052"/>Inserting and extracting NoSQL database data in Python</h1>
			<p>Relational <a id="_idIndexMarker262"/>databases may be what you think of <a id="_idIndexMarker263"/>when you hear the term database, but<a id="_idIndexMarker264"/> there are several other types of databases, such<a id="_idIndexMarker265"/> as columnar, key-value, and time-series. In this section, you will learn how to work with Elasticsearch, which is a NoSQL database. NoSQL is a generic term referring to databases that do not store data in rows and columns. NoSQL databases often store their data as JSON documents and use a query language other than SQL. The next section will teach you how to load data into Elasticsearch.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>Installing Elasticsearch</h2>
			<p>To<a id="_idIndexMarker266"/> install the <code>elasticsearch</code> library, you can use <code>pip3</code>, as shown:</p>
			<pre>pip3 install elasticsearch</pre>
			<p>Using <code>pip</code> will install the newest version, which, if you installed Elasticsearch according to the instructions in <a href="B15739_02_ePub_AM.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Building Our Data Engineering Infrastructure</em>, is what you will need. You can get the library for Elasticsearch versions 2, 5, 6, and 7. To verify the installation and check the version, you can use the following code:</p>
			<pre>import elasticsearch
elasticsearch.__version__</pre>
			<p>The preceding code should print something like the following:</p>
			<pre>(7.6.0)</pre>
			<p>If you have the right version for your Elasticsearch version, you are ready to start importing data.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor054"/>Inserting data into Elasticsearch</h2>
			<p>Before <a id="_idIndexMarker267"/>you can<a id="_idIndexMarker268"/> query Elasticsearch, you will need to load some data into an index. In the previous section, you used a library, <code>psycopg2</code>, to access PostgreSQL. To access Elasticsearch, you will use the <code>elasticsearch</code> library. To load data, you need to create the connection, then you can issue commands to Elasticsearch. Follow the given steps to add a record to Elasticsearch:</p>
			<ol>
				<li value="1">Import the libraries. You can also create the <code>Faker</code> object to generate random data:<pre>from elasticsearch import Elasticsearch
from faker import Faker
fake=Faker()</pre></li>
				<li>Create a connection to Elasticsearch:<pre>es = Elasticsearch()</pre></li>
				<li>The preceding code assumes that your <code>Elasticsearch</code> instance is running on <code>localhost</code>. If it is not, you can specify the IP address, as shown:<pre>es=Elasticsearch({'127.0.0.1'})</pre></li>
			</ol>
			<p>Now, you can<a id="_idIndexMarker269"/> issue commands to your <code>Elasticsearch</code> instance. The <code>index</code> method will allow you to add data. The method takes an <a id="_idIndexMarker270"/>index name, the document type, and a body. The body is what is sent to Elasticsearch and is a JSON object. The following code creates a JSON object to add to the database, then uses <code>index</code> to send it to the <code>users</code> index (which will be created automatically during the index operation):</p>
			<pre>doc={"name": fake.name(),"street": fake.street_address(), "city": fake.city(),"zip":fake.zipcode()}
res=es.index(index="users",doc_type="doc",body=doc)
print(res['result']) #created</pre>
			<p>The preceding code should print the word <code>created</code> to the console, meaning the document has been added. Elasticsearch returns an object with a result key that will let you know whether the operation failed or succeeded. <code>created</code>, in this case, means the index operation succeeded and created the document in the index. Just as with the PostgreSQL example earlier in this chapter, you could iterate and run the <code>index</code> command, or you can use a bulk operation to let the library handle all the inserts for you.</p>
			<h3>Inserting data using helpers</h3>
			<p>Using the <code>bulk</code> method, you can insert many documents at a time. The process is similar to inserting<a id="_idIndexMarker271"/> a single record, except that you will generate all the <a id="_idIndexMarker272"/>data, then insert it. The steps are as follows:</p>
			<ol>
				<li value="1">You need to import the <code>helpers</code> library to access the <code>bulk</code> method:<pre>from elasticsearch import helpers</pre></li>
				<li>The data needs to be an array of JSON objects. In the previous example, you created a JSON object with attributes. In this example, the object needs to have some additional information. You must specify the index and the type. Underscores in the names are used for Elasticsearch fields. The <code>_source</code> field is where you would put the JSON document you want to insert in the database. Outside the JSON is a <code>for</code> loop. This loop creates the 999 (you already added one and you index from 0 – to 998) documents:<pre>actions = [
  {
    "_index": "users",
    "_type": "doc",
    "_source": {
	"name": fake.name(),
	"street": fake.street_address(), 
	"city": fake.city(),
	"zip":fake.zipcode()}
  }
  for x in range(998) # or for i,r in df.iterrows()
]</pre></li>
				<li>Now, you can call the <code>bulk</code> method and pass it the <code>elasticsearch</code> instance and the array of data. You can print the results to check that it worked:<pre>res = helpers.bulk(es, actions)
print(res['result'])</pre></li>
			</ol>
			<p>You should now have<a id="_idIndexMarker273"/> 1,000 records in an Elasticsearch index <a id="_idIndexMarker274"/>named <code>users</code>. We can verify this in Kibana. To add the new index to Kibana, browse to your Kibana dashboard at <code>http://localhost:5601</code>. Selecting <strong class="bold">Management</strong> at the bottom left of the toolbar, you can then create an index pattern by clicking the blue <strong class="bold">+ Create index pattern</strong> button, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.4_B15739.jpg" alt="Figure 4.4 – Creating an index pattern&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Creating an index pattern</p>
			<p>Add an Elasticsearch index pattern to Kibana. On the next screen, enter the name of the index – <code>users</code>. Kibana will start pattern matching to find the index. Select the <code>users</code> index from the dropdown and click the <code>users</code>), as shown in the following screenshot; you should see your documents:</p>
			<div><div><img src="img/Figure_4.5_B15739.jpg" alt="Figure 4.5 – All of your documents in the Discover tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – All of your documents in the Discover tab</p>
			<p>Now that you<a id="_idIndexMarker275"/> can create a record individually or using the <code>bulk</code> method, the <a id="_idIndexMarker276"/>next section will teach you how you can query your data.</p>
			<h3>Querying Elasticsearch</h3>
			<p>Querying <a id="_idIndexMarker277"/>Elasticsearch follows the exact same steps as inserting data. The only difference is you use a different method – <code>search</code> – to send a different body object. Let's walk through a simple query on all the data:</p>
			<ol>
				<li value="1">Import the library and create your <code>elasticsearch</code> instance:<pre>from elasticsearch import Elasticsearch
es = Elasticsearch() </pre></li>
				<li>Create the JSON object to send to Elasticsearch. The object is a query, using the <code>match_all</code> search:<pre>doc={"query":{"match_all":{}}}</pre></li>
				<li>Pass the object to <a id="_idIndexMarker278"/>Elasticsearch using the <code>search</code> method. Pass the index and the return size. In this case, you will only return 10 records. The maximum return size is 10,000 documents:<pre>res=es.search(index="users",body=doc,size=10)</pre></li>
				<li>Lastly, you can print the documents:<pre>print(res['hits']['hits'])</pre><p>Or you can iterate through grabbing <code>_source</code> only:</p><pre>for doc in res['hits']['hits']:
    print(doc['_source'])</pre></li>
			</ol>
			<p>You can load the results of the query into a <code>pandas</code> DataFrame – it is JSON, and you learned how to read JSON in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a>, <em class="italic">Reading and Writing Files</em>. To load the results into a DataFrame, import <code>json_normalize</code> from the <code>pandas</code> <code>json</code> library, and use it (<code>json_normalize</code>) on the JSON results, as shown in the following code:</p>
			<pre>from pandas.io.json import json_normalize
df=json_normalize(res['hits']['hits'])</pre>
			<p>Now you will have the results of the search in a DataFrame. In this example, you just grabbed all the records, but there are other queries available besides <code>match_all</code>. </p>
			<p>Using the <code>match_all</code> query, I know I have a document with the name <code>Ronald Goodman</code>. You can query on a field using the <code>match</code> query:</p>
			<pre>doc={"query":{"match":{"name":"Ronald Goodman"}}}
res=es.search(index="users",body=doc,size=10)
print(res['hits']['hits'][0]['_source'])</pre>
			<p>You can also use a Lucene syntax for queries. In Lucene, you can specify <code>field:value</code>. When performing this kind of search, you do not need a document to send. You can pass the <code>q</code> parameter to the <code>search</code> method:</p>
			<pre>res=es.search(index="users",q="name:Ronald Goodman",size=10)
print(res['hits']['hits'][0]['_source'])</pre>
			<p>Using the <code>City</code> field, you can search for <code>Jamesberg</code>. It will return two records: one for <code>Jamesberg</code> and one for <code>Lake Jamesberg</code>. Elasticsearch will tokenize strings with spaces in them, splitting them into multiple strings to search:</p>
			<pre># Get City Jamesberg - Returns Jamesberg and Lake Jamesberg
doc={"query":{"match":{"city":"Jamesberg"}}}
res=es.search(index="users",body=doc,size=10)
print(res['hits']['hits'])</pre>
			<p>The results are the two <a id="_idIndexMarker279"/>records in the following code block:</p>
			<pre>[{'_index': 'users', '_type': 'doc', '_id': 'qDYoOHEBxMEH3Xr-PgMT', '_score': 6.929674, '_source': {'name': 'Tricia Mcmillan', 'street': '8077 Nancy #Mills Apt. 810', 'city': 'Jamesberg', 'zip': '63792'}}, {'_index': 'users', '_type': 'doc', '_id': 'pTYoOHEBxMEH3Xr-PgMT', '_score': 5.261652, '_source': {'name': 'Ryan Lowe', 'street': '740 Smith Pine Suite 065', 'city': 'Lake Jamesberg', 'zip': '38837'}}]</pre>
			<p>You can use Boolean queries to specify multiple search criteria. For example, you can use <code>must</code>, <code>must not</code>, and <code>should</code> before your queries. Using a Boolean query, you can filter out <code>Lake Jamesberg</code>. Using a <code>must</code> match on <code>Jamesberg</code> as the city (which will return two records), and adding a filter on the ZIP, you can make sure only <code>Jamesberg</code> with the ZIP <code>63792</code> is returned. You could also use a <code>must not</code> query on the <code>Lake Jameson</code> ZIP:</p>
			<pre># Get Jamesberg and filter on zip so Lake Jamesberg is removed
doc={"query":{"bool":{"must":{"match":{"city":"Jamesberg"}},
"filter":{"term":{"zip":"63792"}}}}}
res=es.search(index="users",body=doc,size=10)
print(res['hits']['hits'])</pre>
			<p>Now, you only get the single record that you wanted:</p>
			<pre>[{'_index': 'users', '_type': 'doc', '_id': 'qDYoOHEBxMEH3Xr-
PgMT', '_score': 6.929674, '_source': {'name': 'Tricia 
Mcmillan', 'street': '8077 Nancy #Mills Apt. 810', 'city': 
'Jamesberg', 'zip': '63792'}}]</pre>
			<p>Your queries only <a id="_idIndexMarker280"/>returned a few documents, but in production, you will probably have large queries with tens of thousands of documents being returned. The next section will show you how to handle all that data.</p>
			<h3>Using scroll to handle larger results</h3>
			<p>In the first <a id="_idIndexMarker281"/>example, you used a size of 10 for your search. You could have grabbed all 1,000 records, but what do you do when you have more than 10,000 and you need all of them? Elasticsearch has a scroll method that will allow you to iterate over the results until you get them all. To scroll through the data, follow the given steps:</p>
			<ol>
				<li value="1">Import the library and create your <code>Elasticsearch</code> instance:<pre>from elasticsearch import Elasticsearch
es = Elasticsearch() </pre></li>
				<li>Search your data. Since you do not have over 10,000 records, you will set the size to <code>500</code>. This means you will be missing 500 records from your initial search. You will pass a new parameter to the search method – <code>scroll</code>. This parameter specifies how long you want to make the results available for. I am using 20 milliseconds. Adjust this number to make sure you have enough time to get the data – it will depend on the document size and network speed:<pre>res = es.search(
  index = 'users',
  doc_type = 'doc',
  scroll = '20m',
  size = 500,
  body = {"query":{"match_all":{}}}
)</pre></li>
				<li>The results will include <code>_scroll_id</code>, which you will need to pass to the <code>scroll</code> method later. Save the scroll ID and the size of the result set:<pre>sid = res['_scroll_id']
size = res['hits']['total']['value']</pre></li>
				<li>To start<a id="_idIndexMarker282"/> scrolling, use a <code>while</code> loop to get records until the size is 0, meaning there is no more data. Inside the loop, you will call the <code>scroll</code> method and pass <code>_scroll_id</code> and how long to scroll. This will grab more of the results from the original query:<pre>while (size &gt; 0):
    res = es.scroll(scroll_id = sid, scroll = '20m')</pre></li>
				<li>Next, get the new scroll ID and the size so that you can loop through again if the data still exists:<pre>    sid = res['_scroll_id']
    size = len(res['hits']['hits'])</pre></li>
				<li>Lastly, you can do something with the results of the scrolls. In the following code, you will print the source for every record:<pre>    for doc in res['hits']['hits']:
        print(doc['_source'])</pre></li>
			</ol>
			<p>Now you know how to create documents in Elasticsearch and how to query them, even when there is more than the maximum return value of 10,000. You can do the same using relational databases. It is now time to start putting these skills to use in building data pipelines. The next two sections will teach you how to use databases in your data pipelines using Apache Airflow and NiFi.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor055"/>Building data pipelines in Apache Airflow</h1>
			<p>In <a id="_idIndexMarker283"/>the previous chapter, you built your first Airflow<a id="_idIndexMarker284"/> data pipeline using a Bash and Python operator. This time, you will combine two Python operators to extract data from PostgreSQL, save it as a CSV file, then read it in and write it to an Elasticsearch index. The complete pipeline is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.6_B15739.jpg" alt="Figure 4.6 – Airflow DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Airflow DAG</p>
			<p>The pre<a id="_idTextAnchor056"/>ceding <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) looks very simple; it is only two tasks, and you could combine the<a id="_idIndexMarker285"/> tasks into a single function. This is not a good idea. In <em class="italic">Section 2</em>, <em class="italic">Deploying Pipelines into Production</em>, you will learn about modifying your data pipelines for production. A key tenant of production pipelines is that each task should be atomic; that is, each task should be able to stand on its own. If you had a single function that read a database and inserted the results, when it<a id="_idIndexMarker286"/> fails, you have to track down whether the <a id="_idIndexMarker287"/>query failed or the insert failed. As your tasks get more complicated, it will take much more work to debug. The next section will walk you through building the data pipeline.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor057"/>Setting up the Airflow boilerplate</h2>
			<p>Every DAG is<a id="_idIndexMarker288"/> going to have some standard, boilerplate code to make it run in Airflow. You will always import the needed libraries, and then any other libraries you need for your tasks. In the following code block, you import the operator, <code>DAG</code>, and the time libraries for Airflow. For your tasks, you import the <code>pandas</code>, <code>psycopg2</code>, and <code>elasticsearch</code> libraries:</p>
			<pre>import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import pandas as pd
import psycopg2 as db
from elasticsearch import Elasticsearch</pre>
			<p>Next, you will specify the arguments for your DAG. Remember that the start time should be a day behind if you schedule the task to run daily:</p>
			<pre>default_args = {
    'owner': 'paulcrickard',
    'start_date': dt.datetime(2020, 4, 2),
    'retries': 1,
    'retry_delay': dt.timedelta(minutes=5),
}</pre>
			<p>Now, you can pass the arguments to the DAG, name it, and set the run interval. You will define your operators here as well. In this example, you will create two Python operators – one to get data from PostgreSQL and one to insert data in to Elasticsearch. The <code>getData</code> task will be upstream and the <code>insertData</code> task downstream, so you will use the <code>&gt;&gt;</code> bit<a id="_idIndexMarker289"/> shift operator to specify this:</p>
			<pre>with DAG('MyDBdag',
         default_args=default_args,
         schedule_interval=timedelta(minutes=5),      
                           # '0 * * * *',
         ) as dag:
    getData = PythonOperator(task_id='QueryPostgreSQL',
         python_callable=queryPostgresql)
    
    insertData = PythonOperator
    (task_id='InsertDataElasticsearch',
         python_callable=insertElasticsearch)
getData &gt;&gt; insertData</pre>
			<p>Lastly, you will define the tasks. In the preceding operators, you named them <code>queryPostgresql</code> and <code>insertElasticsearch</code>. The code in these tasks should look very familiar; it is almost identical to the code from the previous sections in this chapter.</p>
			<p>To query PostgreSQL, you create the connection, execute the <code>sql</code> query using the <code>pandas</code> <code>read_sql()</code> method, and then use the <code>pandas</code> <code>to_csv()</code> method to write the data to disk:</p>
			<pre>def queryPostgresql():
    conn_string="dbname='dataengineering' host='localhost'
    user='postgres' password='postgres'"
    conn=db.connect(conn_string)
    df=pd.read_sql("select name,city from users",conn)
    df.to_csv('postgresqldata.csv')
    print("-------Data Saved------")</pre>
			<p>To insert the data into <a id="_idIndexMarker290"/>Elasticsearch, you create the Elasticsearch object connecting to <code>localhost</code>. Then, read the CSV from the previous task into a DataFrame, iterate through the DataFrame, converting each row into JSON, and insert the data using the <code>index</code> method:</p>
			<pre>def insertElasticsearch():
    es = Elasticsearch() 
    df=pd.read_csv('postgresqldata.csv')
    for i,r in df.iterrows():
        doc=r.to_json()
        res=es.index(index="frompostgresql",
                    doc_type="doc",body=doc)
        print(res)	</pre>
			<p>Now you have a complete data pipeline in Airflow. In the next section, you will run it and view the results.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor058"/>Running the DAG</h2>
			<p>To run the DAG, you <a id="_idIndexMarker291"/>need to copy your code to your <code>$AIRFLOW_HOME/dags</code> folder. After moving the file, you can run the following commands:</p>
			<pre>airflow webserver
airflow scheduler</pre>
			<p>When these commands complete, browse to <code>http://localhost:8080</code> to see the Airflow GUI. Select <strong class="bold">MyDBdag</strong>, and then select <strong class="bold">Tree View</strong>. You can schedule five runs of the DAG and click <strong class="bold">Go</strong>. As it runs, you should see the results underneath, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.7_B15739.jpg" alt="Figure 4.7 – Task showing successful runs and queued runs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Task showing successful runs and queued runs</p>
			<p>To verify that the data pipeline was successful, you can view the data in Elasticsearch using Kibana. To see the results, browse to Kibana at <code>http://localhost:5601</code>. You will need to create a new index in Kibana. You performed this task in the <em class="italic">Inserting data using helpers</em> section of this chapter. But to recap, you will select <strong class="bold">Management</strong> in Kibana from the bottom of the left-hand toolbar in Kibana, then create the index pattern by clicking the <strong class="bold">Create index pattern</strong> button. Start typing the name of the index and Kibana will find it, then click <strong class="bold">Create</strong>. Then, you can go to the <strong class="bold">Discover</strong> tab on the toolbar and view the data. You <a id="_idIndexMarker292"/>should see records as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.8_B15739.jpg" alt="Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch</p>
			<p>You will see that there are documents containing only names and cities, as specified in your data pipeline task. One thing to note is that we now have over 2,000 records. There were only 1,000 records in the PostgreSQL database, so what happened? The data pipeline ran multiple times, and each time, it inserted the records from PostgreSQL. A second tenant of data pipelines is that they should be idempotent. That means that no matter how many times you run it, the results are the same. In this case, they are not. You will learn how to fix this in <em class="italic">Section 2</em>, <em class="italic">Deploying Pipelines into Production</em>, in <a href="B15739_07_ePub_AM.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a>, <em class="italic">Features of a Production Pipeline</em>. For now, the next section of this chapter will teach you how to build the same data pipeline in Apache NiFi.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor059"/>Handling databases with NiFi processors</h1>
			<p>In the <a id="_idIndexMarker293"/>previous sections, you learned how to read and <a id="_idIndexMarker294"/>write CSV and JSON files using Python. Reading files is such a common task that tools such as NiFi have prebuilt processors to handle it. In this section, you will build the same data pipeline as in the previous section. In NiFi, the data pipeline will look as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.9_B15739.jpg" alt="Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch</p>
			<p>The data pipeline contains one more task than the Airflow version, but otherwise, it should look straightforward. The following sections will walk you through building the data pipeline.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor060"/>Extracting data from PostgreSQL</h2>
			<p>The<a id="_idIndexMarker295"/> processor <a id="_idIndexMarker296"/>most used for handling relational databases in NiFi is the <code>ExecuteSQLRecord</code> processor. Drag the <code>ExecuteSQLRecord</code> processor. Once it has been added to the canvas, you need to configure it.</p>
			<h3>Configuring the ExecuteSQLCommand processor</h3>
			<p>To configure<a id="_idIndexMarker297"/> the processor, you need to create a database connection pool, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.10_B15739.jpg" alt="Figure 4.10 – Creating a database connection pooling service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Creating a database connection pooling service</p>
			<p>After selecting <code>dataengineering</code>. Notice how I did not name it <code>PostgreSQL</code>. As you add more services, you will add more PostgreSQL connections for different databases. It would then be hard to remember which PostgreSQL database the service was for. </p>
			<p>To configure the service, select the arrow in the processor configuration. The configuration for the service should look as in the following screenshot:</p>
			<div><div><img src="img/Figure_4.11_B15739.jpg" alt="Figure 4.11 – Configuring the database service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Configuring the database service</p>
			<p>The configuration <a id="_idIndexMarker298"/>requires you to specify the connection URL, which is a Java database connection string. The string specifies <code>PostgreSQL</code>. It then names the host, <code>localhost</code>, and the database name, <code>dataengineering</code>. The driver class specifies the <code>postgresql</code> driver. The location of the driver is where you downloaded it in <a href="B15739_02_ePub_AM.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Building Our Data Engineering Infrastructure</em>. It should be in your home directory in the <code>nifi</code> folder, in a subdirectory named <code>drivers</code>. Lastly, you need to enter the username and password for the database.</p>
			<p>Next, you need to create a record writer service. Select <strong class="bold">Create new service…</strong>, choose <strong class="bold">JSONRecordSetWriter</strong>, and click the arrow to configure it. There is one important configuration setting that you cannot skip – <strong class="bold">Output Grouping</strong>. You must set this property to <strong class="bold">One Line Per Object</strong>. The finished configuration will look as in the following screenshot:</p>
			<div><div><img src="img/Figure_4.12_B15739.jpg" alt="Figure 4.13 – The JSONRecordSetWriter configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – The JSONRecordSetWriter configuration</p>
			<p>Now that you have<a id="_idIndexMarker299"/> set up the services for the processor, you need to finish configuring the process. The last parameter you need to configure is <strong class="bold">SQL Select Query</strong>. This is where you can specify the SQL command to run against the database. For example, you can enter the following:</p>
			<pre>select name, city from users</pre>
			<p>This will grab all the records in the PostgreSQL database, but only the <code>name</code> and <code>city</code> fields. You can now move on to the next processor in the pipeline.</p>
			<h3>Configuring the SplitText processor</h3>
			<p>Now that you <a id="_idIndexMarker300"/>have configured the <strong class="bold">ExecuteSQLRecord</strong> processor, you will receive an array of records. To process this data, you need to have a flowfile per record. To do that, you can use the <strong class="bold">SplitText</strong> processor. Drag it to the canvas and open the <strong class="bold">Properties</strong> tab by double-clicking on the processor – or right-click and select <strong class="bold">Properties</strong>. The processor defaults work, but make sure that <strong class="bold">Line Split Count</strong> is set to <strong class="bold">1</strong>, <strong class="bold">Header Line Count</strong> is <strong class="bold">0</strong> – your data does not have a header when it comes from the <strong class="bold">ExecuteSQLRecord</strong> processor – and <strong class="bold">Remove Trailing Newlines</strong> is <strong class="bold">true</strong>. </p>
			<p>These settings will allow the<a id="_idIndexMarker301"/> processor to take each line of the flowfile and split it into a new flowfile. So, your one incoming flowfile will come out of this processor as 1,000 flow files.</p>
			<h3>Configuring the PutElasticsearchHttp processor</h3>
			<p>The last step in the<a id="_idIndexMarker302"/> data pipeline is to insert the flowfiles into Elasticsearch. You can do that using the <strong class="bold">PutElasticsearchHttp</strong> processor. There are four different <strong class="bold">PutElasticsearch</strong> processors. Only two will be relevant in this book – <strong class="bold">PutElasticsearchHttp</strong> and <strong class="bold">PutelasticsearchHttpRecord</strong>. These are the processors to insert a single record or to use the bulk API. The other two processors – <strong class="bold">Putelasticsearch</strong> and <strong class="bold">Putelasticsearch5</strong> – are for older versions of Elasticsearch (2 and 5).</p>
			<p>To configure the processor, you must specify the URL and the port. In this example, you will use <code>http://localhost:9200</code>. The index will be <code>fromnifi</code>, but you can name it anything you would like. The type is <code>doc</code> and the index operation will be <code>index</code>. In <em class="italic">Section 2</em>, <em class="italic">Deploying Pipelines into Production</em>, you will use other index operations and you will specify the ID of the records that you insert. </p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor061"/>Running the data pipeline</h2>
			<p>Now that you have configured all of<a id="_idIndexMarker303"/> the processors, you can connect them by dragging the arrow from <strong class="bold">ExecuteSQLRecord</strong> to the <strong class="bold">SplitRecord</strong> processor for success. Then, connect the <strong class="bold">SplitRecord</strong> processor to the <strong class="bold">PutElasticsearchHttp</strong> processor for splits. Lastly, terminate the <strong class="bold">PutElasticsearchHttp</strong> processor for all relationships.</p>
			<p>Run each of the processors, or in the <strong class="bold">Operations</strong> pane, select <strong class="bold">Run</strong> to start them all. You will see one flowfile in the first queue, then it will split into 1,000 flowfiles in the second queue. The queue will empty in batches of 100 as they are inserted into Elasticsearch. </p>
			<p>To verify the results, you can use the <code>elasticsearch</code> API, and not Kibana. In your browser, go to <code>http://localhost:9200/_cat/indices</code>. This is the REST endpoint to view the indices in your Elasticsearch database. You should see your new index, <code>fromnifi</code>, and the total number of documents, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_4.13_B15739.jpg" alt="Figure 4.13 – The index contains all the records from PostgreSQL&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – The index contains all the records from PostgreSQL</p>
			<p>The number of documents in the<a id="_idIndexMarker304"/> index will vary depending on whether you left the pipeline running or not. Just as in the Airflow example, this pipeline is not idempotent. As it runs, it will keep adding the same records with a different ID into Elasticsearch. This is not the behavior you will want in production and we will fix this in <em class="italic">Section 2</em>, <em class="italic">Deploying Pipelines into Production</em>.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor062"/>Summary</h1>
			<p>In this chapter, you learned how to use Python to query and insert data into both relational and NoSQL databases. You also learned how to use both Airflow and NiFi to create data pipelines. Database skills are some of the most important for a data engineer. There will be very few data pipelines that do not touch on them in some way. The skills you learned in this chapter provide the foundation for the other skills you will need to learn – primarily SQL. Combining strong SQL skills with the data pipeline skills you learned in this chapter will allow you to accomplish most of the data engineering tasks you will encounter. </p>
			<p>In the examples, the data pipelines were not idempotent. Every time they ran, you got new results, and results you did not want. We will fix that in <em class="italic">Section 2</em>, <em class="italic">Deploying Pipelines into Production</em>. But before you get to that, you will need to learn how to handle common data issues, and how to enrich and transform your data.</p>
			<p>The next chapter will teach you how to use Python to work with your data in between the extract and load phases of your data pipelines.</p>
		</div>
	</body></html>