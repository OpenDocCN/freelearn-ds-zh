- en: Chapter 6. SciPy for Data Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers those branches of mathematics and statistics that treat
    the collection, organization, analysis, and interpretation of data. There are
    different applications and operations that spread over several modules and submodules:
    `scipy.stats` (for purely statistical tools), `scipy.ndimage.measurements` (for
    analysis and organization of data), `scipy.spatial` (for spatial algorithms and
    data structures), and finally the clustering package `scipy.cluster`. The `scipy.cluster`
    clustering package consists of two submodules: `scipy.cluster.vq` (vector quantization)
    and `scipy.cluster.hierarchy` (for hierarchical and **agglomerative** clustering).'
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous chapters, fluency with the subject matter is assumed. Our
    emphasis is to show you some of the SciPy functions available to perform statistical
    computations, not to teach it. Accordingly, you are welcome to read this chapter
    along side your preferred book(s) on the subject so that you can fully explore
    the examples provided in this chapter on additional data sets.
  prefs: []
  type: TYPE_NORMAL
- en: We should mention, however, that there are other specialized modules in Python
    that can be used to explore this subject from different perspectives. Some of
    them (not covered by any means in this book) are the **Modular Toolkit for Data
    Processing** (**MDP**) ([http://mdp-toolkit.sourceforge.net/install.html](http://mdp-toolkit.sourceforge.net/install.html)),
    **scikit-learn** ([http://scikit-learn.org/](http://scikit-learn.org/)), and **Statsmodels**
    ([http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: The standard descriptive statistics measures computed via SciPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The built-in functions in SciPy that deal with statistical distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Scipy functionality to find interval estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing computations of statistical correlations and some statistical tests,
    the fitting of distributions, and statistical distances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clustering example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often require the analysis of data in which certain features are grouped
    in different regions, each with different sizes, values, shapes, and so on. The
    `scipy.ndimage.measurements` submodule has the right tools for this task, and
    the best way to illustrate the capabilities of the module is by means of exhaustive
    examples. For example, for binary images of zeros and ones, it is possible to
    label each blob (areas of contiguous pixels with value one) and obtain the number
    of these with the `label` command. If we desire to obtain the center of mass of
    the blobs, we may do so with the `center_of_mass command`. We may see these operations
    in action once again in the application to obtain the structural model of oxides
    in [Chapter 7](ch07.html "Chapter 7. SciPy for Computational Geometry"), *SciPy
    for Computational Geometry*.
  prefs: []
  type: TYPE_NORMAL
- en: For nonbinary data, the `scipy.ndimage.measurements` submodule provides the
    usual basic statistical measurements (value and location of extreme values, mean,
    standard deviation, sum, variance, histogram, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: For more advanced statistical measurements, we must access functions from the
    `scipy.stats` module. We may now use geometric and harmonic means (`gmean`, `hmean`),
    median, mode, skewness, various moments, or kurtosis (`median`, `mode`, `skew`,
    `moment`, `kurtosis`). For an overview of the most significant statistical properties
    of the dataset, we prefer to use the `describe` routine. We may also compute item
    frequencies (`itemfreq`), percentiles (`scoreatpercentile`, `percentileofscore`),
    histograms (`histogram`, `histogram2`), cumulative and relative frequencies (`cumfreq`,
    `relfreq`), standard error (`sem`), and the signal-to-noise ratio (`signaltonoise`),
    which is always useful.
  prefs: []
  type: TYPE_NORMAL
- en: Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main strengths of the `scipy.stats` module is the great number of
    distributions coded, both continuous and discrete. The list is impressively large
    and has at least 80 continuous distributions and 10 discrete distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common ways to employ these distributions is the generation
    of random numbers. We have been employing this technique to *contaminate* our
    images with noise, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the SciPy way of handling distributions. First, a random variable
    class is created (in SciPy there is the `rv_continuous` class for continuous random
    variables and the `rv_discrete` class for the discrete case). Each continuous
    random variable has an associated probability density function (`pdf`), a cumulative
    distribution function (`cdf`), a survival function along with its inverse (`sf`,
    `isf`), and all possible descriptive statistics. They also have associated the
    random variable, `rvs`, which is what we used to actually generate the random
    instances. For example, with a Pareto continuous random variable with parameter
    *b = 5*, to check these properties, we could issue the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following graphs, showing probability density function (left),
    cumulative distribution function (center), and random generation (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributions](img/7702OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interval estimation, correlation measures, and statistical tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We briefly covered interval estimation as an introductory example of SciPy:
    `bayes_mvs`, in [Chapter 1](ch01.html "Chapter 1. Introduction to SciPy"), *Introduction
    to SciPy*, with very simple syntax, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It returns a tuple of three arguments in which each argument has the form `(center,
    (lower, upper))`. The first argument refers to the mean; the second refers to
    the variance; and the third to the standard deviation. All intervals are computed
    according to the probability given by `alpha`, which is `0.9` by default.
  prefs: []
  type: TYPE_NORMAL
- en: We may use the `linregress` routine to compute the regression line of some two-dimensional
    data *x*, or two sets of one-dimensional data, *x* and *y*. We may compute different
    correlation coefficients, with their corresponding p-values, as well. We have
    the **Pearson correlation coefficient** (`pearsonr`), **Spearman's rank-order
    correlation** (`spearmanr`), **point biserial correlation** (`pointbiserialr`),
    and **Kendall's tau** for ordinal data (`kendalltau`). In all cases, the syntax
    is the same, as it is only required either a two-dimensional array of data, or
    two one-dimensional arrays of data with the same length.
  prefs: []
  type: TYPE_NORMAL
- en: 'SciPy also has most of the best-known statistical tests and procedures: **t-tests**
    (`ttest_1samp` for one group of scores, `ttest_ind` for two independent samples
    of scores, or `ttest_rel` for two related samples of scores), **Kolmogorov-Smirnov
    tests** for goodness of fit (`kstest`, `ks_2samp`), one-way **Chi-square test**
    (`chisquare`), and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us illustrate some of the routines of this module with a textbook example,
    based on Timothy Sturm's studies on control design.
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn a knob that moved an indicator by the screw action, 25 right-handed
    individuals were asked to use their right hands. There were two identical instruments,
    one with a right-handed thread where the knob turned clockwise, and the other
    with a left-hand thread where the knob turned counter-clockwise. The following
    table gives the times in seconds each subject took to move the indicator to a
    fixed distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Subject** | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| **Right thread** | 113 | 105 | 130 | 101 | 138 | 118 | 87 | 116 | 75 | 96
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Left thread** | 137 | 105 | 133 | 108 | 115 | 170 | 103 | 145 | 78 | 107
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Subject** | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| **Right thread** | 122 | 103 | 116 | 107 | 118 | 103 | 111 | 104 | 111 |
    89 |'
  prefs: []
  type: TYPE_TB
- en: '| **Left thread** | 84 | 148 | 147 | 87 | 166 | 146 | 123 | 135 | 112 | 93
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Subject** | 21 | 22 | 23 | 24 | 25 |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| **Right thread** | 78 | 100 | 89 | 85 | 88 |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| **Left thread** | 76 | 116 | 78 | 101 | 123 |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: 'We may perform an analysis that leads to a conclusion about right-handed people
    finding right-hand threads easier to use, by a simple one-sample t-statistic.
    We will load the data in memory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference of each row indicates which knob was faster, and for how much
    time. We can obtain that information easily and perform some basic statistical
    analysis on it. We will start by computing the mean, standard deviation, and a
    histogram with 10 bins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the histogram by issuing the following set of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Interval estimation, correlation measures, and statistical tests](img/7702OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In light of this histogram, it is not far-fetched to assume a normal distribution.
    If we assume that this is a proper simple random sample, the use of t-statistics
    is justified. We would like to prove that it takes longer to turn the left thread
    than the right, so we set the mean of `dataDiff` to be contrasted against the
    zero mean (which would indicate that it takes the same time for both threads).
  prefs: []
  type: TYPE_NORMAL
- en: 'The two-sample t-statistics and p-value for the two-sided test are computed
    by the simple command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The p-value for the one-sided test is then calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that this p-value is much smaller than either of the usual thresholds `alpha
    = 0.05` or `alpha = 0.1`. We can thus guarantee that we have enough evidence to
    support the claim that right-handed threads take less time to turn than left-handed
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Timothy Sturm''s example, we claim that the histogram of some data seemed
    to fit a normal distribution. SciPy has a few routines to help us approximate
    the best distribution to a random variable, together with the parameters that
    best approximate this fit. For example, for the data in that problem, the mean
    and standard deviation of the normal distribution that realizes the best fit can
    be found in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot the (`normed`) histogram of the data, together with the computed
    probability density function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will obtain the following graph showing the maximum likelihood estimate
    to the normal distribution that best fits `dataDiff`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution fitting](img/7702OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We may even fit the best probability density function without specifying any
    particular distribution, thanks to a non-parametric technique, **kernel density
    estimation**. We can find an algorithm to perform Gaussian kernel density estimation
    in the `scipy.stats.kde` submodule. Let us show by example with the same data
    as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A slightly different plotting session as given before, offers us the following
    graph, showing probability density function obtained by kernel density estimation
    on `dataDiff`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution fitting](img/7702OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The full piece of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparative purposes, the last two plots can be combined into one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the combined plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution fitting](img/7702OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the field of data mining, it is often required to determine which members
    of a training set are closest to unknown test instances. It is imperative to have
    a good set of different distance functions for any of the algorithms that perform
    the search, and SciPy has, for this purpose, a huge collection of optimally coded
    functions in the distance submodule of the scipy.spatial module. The list is long.
    Besides Euclidean, squared Euclidean, or standardized Euclidean, we have many
    more—**Bray-Curtis**, **Canberra**, **Chebyshev**, **Manhattan**, correlation
    distance, cosine distance, **dice dissimilarity**, **Hamming**, **Jaccard-Needham**,
    **Kulsinski**, **Mahalanobis**, and so on. The syntax in most cases is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The only three cases in which the syntax is different are the Minkowski, Mahalanobis,
    and standardized Euclidean distances, in which the distance function requires
    either an integer number (for the order of the norm in the definition of Minkowski
    distance), a covariance for the Mahalanobis case (but this is an optional requirement),
    or a variance matrix to standardize the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see now a fun exercise to visualize the unit balls in Minkowski metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created a function, `Ball`, which creates a grid of 512 x 512 Boolean
    values. The grid represents a square of length 2.2 centered at the origin, with
    sides parallel to the coordinate axis, and the true values on it represent all
    those points of the grid inside of the unit ball for the Minkowksi metric, for
    the parameter `p`. All we have to do is show it graphically, as in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following, where `Ball(3)` is a unit ball in the Minkowski
    metric with parameter `p = 3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distances](img/7702OS_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We feel the need to issue the following four important warnings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First warning**: We must use these routines instead of creating our own definitions
    of the corresponding distance functions whenever possible. They guarantee a faster
    result and optimal coding to take care of situations in which the inputs are either
    too large or too small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second warning**: These functions work great when comparing two vectors;
    however, for the pairwise computation of many vectors, we must resort to the `pdist`
    routine. This command takes an *m x n* array representing *m* vectors of dimension
    *n*, and computes the distance of each of them to each other. We indicate the
    distance function to be used with the option metric and additional parameters
    as needed. For example, for the Manhattan (`cityblock`) distance for five randomly
    selected randomly selected four-dimensional vectors with integer values `1`, `0`,
    or `-1`, we could issue the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the following `pdist` command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means, if `v1 = [1,0,1,-1]`, `v2 = [-1,0,-1,0]`, `v3 = [1,1,1,-1]`, `v4
    = [1,1,-1,0]`, and `v5 = [0,0,1,-1]`, then the Manhattan distance of `v1` from
    `v2` is 5\. The distance from `v1` to `v3` is 1; from `v1` to `v4`, 4; and from
    `v1` to `v5`, 1\. From `v2` to `v3` the distance is 6; from `v2` to `v4`, 3; and
    from `v2` to `v5`, 4\. From `v3` to `v4` the distance is 3; and from `v3` to `v5`,
    2\. And finally, the distance from `v4` to `v5` is 5, which is the last entry
    of the output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Third warning**: When computing the distance between each pair of two collections
    of inputs, we should use the `cdist` routine, which has a similar syntax. For
    instance, for the two collections of three randomly selected four-dimensional
    Boolean vectors, the corresponding Jaccard-Needham dissimilarities are computed,
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That is, if the three vectors in `V` are labeled `v1`, `v2`, and `v3` and if
    the two vectors in `W` are labeled as `w1` and `w2`, then the dissimilarity between
    `v1` and `w1` is 0.75; between `v1` and `w2`, 1; and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Fourth warning**: When we have a large amount of data points and we need
    to address the problem of nearest neighbors (for example, to locate the closest
    element of the data to a new instance point), we seldom do it by brute force.
    The optimal algorithm to perform this search is based on the idea of k-dimensional
    trees. SciPy has two classes to handle these objects – `KDTree` and `cKDTree`.
    The latter is a subset of the former, a little faster since it is wrapped from
    C code, but with very limited use. It only has the `query` method to find the
    nearest neighbors of the input. The syntax is simple, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This creates a structure containing a binary tree, very apt for the design of
    fast search algorithms. The `leafsize` option indicates at what level the search
    based on the structure of the binary tree must be abandoned in favor of brute
    force.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The other methods associated with the `KDTree` class are—`count_neighbors`,
    to compute the number of nearby pairs that can be formed with another `KDTree`;
    `query_ball_point`, to find all points at a given distance from the input; `query_ball_tree`
    and `query_pairs`, to find all pairs of points within certain distance; and `sparse_distance_matrix`,
    that computes a sparse matrix with the distances between two `KDTree` classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let us see it in action, with a small dataset of 10 randomly generated four-dimensional
    points with integer entries:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means, among all the points in the dataset, the closest one in the Euclidean
    distance to the origin is the fifth one (index 4), and the distance is precisely
    about 4.6 units.
  prefs: []
  type: TYPE_NORMAL
- en: We can have an input of more than one point; the output will still be a tuple,
    where the first entry is an array that indicates the smallest distance to each
    of the input points. The second entry is another array that indicates the indices
    of the nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another technique used in data mining is clustering. SciPy has two modules to
    deal with any problem in this field, each of them addressing a different clustering
    tool—`scipy.cluster.vq` for k-means and `scipy.cluster.hierarchy` for hierarchical
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Vector quantization and k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have two routines to divide data into clusters using the k-means technique—`kmeans`
    and `kmeans2`. They correspond to two different implementations. The former has
    a very simple syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `obs` parameter is an `ndarray` with the data we wish to cluster. If the
    dimensions of the array are *m* x *n*, the algorithm interprets this data as *m*
    points in the n-dimensional Euclidean space. If we know the number of clusters
    in which this data should be divided, we enter so with the `k_or_guess` option.
    The output is a tuple with two elements. The first is an `ndarray` of dimension
    *k* x *n*, representing a collection of points—as many as clusters were indicated.
    Each of these locations indicates the centroid of the found clusters. The second
    entry of the tuple is a floating-point value indicating the distortion between
    the passed points, and the centroids generated previously.
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to impose an initial guess for the centroids of the clusters, we
    may do so with the `k_or_guess` parameter again, by sending a *k* x *n* `ndarray`.
  prefs: []
  type: TYPE_NORMAL
- en: The data we pass to `kmeans` need to be normalized with the `whiten` routine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is much more flexible, as its syntax indicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `data` and `k` parameters are the same as `obs` and `k_or_guess`, respectively.
    The difference in this routine is the possibility of choosing among different
    initialization algorithms, hence providing us with the possibility to speed up
    the process and use fewer resources if we know some properties of our data. We
    do so by passing to the `minit` parameter, one of the strings such as `'random'`
    (initialization centroids are constructed randomly using a Gaussian), `'points'`
    (initialization is done by choosing points belonging to our data), or `'uniform'`
    (if we prefer uniform distribution to Gaussian).
  prefs: []
  type: TYPE_NORMAL
- en: In case we would like to provide the initialization centroids ourselves with
    the `k` parameter, we must indicate our choice to the algorithm by passing `'matrix'`
    to the `minit` option as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, if we wish to classify the original data by assigning to each
    point the cluster to which it belongs; we do so with the `vq` routine (for vector
    quantization). The syntax is pretty simple as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output is a tuple with two entries. The first entry is a one-dimensional
    `ndarray` of size *n* holding for each point in `obs`, the cluster to which it
    belongs. The second entry is another one-dimensional `ndarray` of the same size,
    but containing floating-point values indicating the distance from each point to
    the centroid of its cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us illustrate with a classical example, the mouse dataset. We will create
    a big dataset with randomly generated points in three disks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once created, we will request the data to be separated into three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us present the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following plot showing the mouse dataset with three clusters
    from left to right—red (squares), blue (pluses), and black (dots):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vector quantization and k-means](img/7702OS_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several different algorithms to perform hierarchical clustering.
    SciPy has routines for the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single/min/nearest method**: `single`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete/max/farthest method**: `complete`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average/UPGMA method**: `average`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted/WPGMA method**: `weighted`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid/UPGMC method**: `centroid`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median/WPGMC method**: `median`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward''s linkage method**: `ward`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In any of the previous cases, the syntax is the same; the only input is the
    dataset, which can be either an *m* x *n* `ndarray` representing *m* points in
    the n-dimensional Euclidean space, or a condensed distance matrix obtained from
    the previous data using the `pdist` routine from `scipy.spatial`. The output is
    always an `ndarray` representing the corresponding linkage matrix of the clustering
    obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we may call the clustering with the generic routine `linkage`.
    This routine accepts a dataset/distance matrix, and a string indicating the method
    to use. The strings coincide with the names introduced. The advantage of `linkage`
    over the previous routines is that we are also allowed to indicate a different
    metric than the usual Euclidean distance. The complete syntax for `linkage` is
    then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Different statistics on the resulting linkage matrices may be performed with
    the routines such as Cophenetic distances between observations (`cophenet`); inconsistency
    statistics (`inconsistent`); maximum inconsistency coefficient for each non-singleton
    cluster with its descendants (`maxdists`); and maximum statistic for each non-singleton
    cluster with its descendants (`maxRstat`).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is customary to use binary trees to represent linkage matrices, and the
    `scipy.cluster.hierachy` submodule has a large number of different routines to
    manipulate and extract information from these trees. The most useful of these
    routines is the visualization of these trees, often called dendrograms. The corresponding
    routine in SciPy is dendrogram, and has the following imposing syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The first obvious parameter, `Z`, is a linkage matrix. This is the only non-optional
    variable. The other options control the style of the output (colors, labels, rotation,
    and so on), and since they are technically nonmathematical in nature, we will
    not explore them in detail in this monograph, other than through the simple application
    to animal clustering shown next.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering mammals by their dentition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mammals' teeth are divided into four groups such as incisors, canines, premolars,
    and molars. The dentition of several mammals has been collected, and is available
    for download at [http://www.uni-koeln.de/themen/statistik/data/cluster/dentitio.dat](http://www.uni-koeln.de/themen/statistik/data/cluster/dentitio.dat).
  prefs: []
  type: TYPE_NORMAL
- en: This file presents the name of the mammal, together with the number of top incisors,
    bottom incisors, top canines, bottom canines, top premolars, bottom premolars,
    top molars, and bottom molars.
  prefs: []
  type: TYPE_NORMAL
- en: We wish to use hierarchical clustering on that dataset to assess which species
    are closer to each other by these features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by preparing the dataset and store the relevant data in ndarrays.
    The original data is given as a text file, where each line represents a different
    mammal. The first four lines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 27 characters of each line hold the name of the animal. The characters
    in positions 28 to 35 are the number of respective kinds of dentures. We need
    to prepare this data into something that SciPy can handle. We will collect the
    names apart, since we will be using them as labels in the dendrogram. The rest
    of the data will be forced into an array of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We will proceed to compute the linkage matrix and its posterior dendrogram,
    making sure to use the Python list, mammals, as labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following dendrogram showing clustering of mammals according
    to their dentition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering mammals by their dentition](img/7702OS_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note how all the bats are clustered together. The mice are also clustered together,
    but far from the bats. Sheep, goats, antelopes, deer, and moose have similar dentures
    too, and they appear clustered at the bottom of the tree, next to the opossum
    and the armadillo. Note how all felines are also clustered together, on the top
    of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Experts in data analysis can obtain more information from dendrograms; they
    are able to interpret the lengths of the branches or the different colors used
    in the composition, and give us more insightful explanations about the way the
    clusters differ from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dealt with tools appropriate for data mining and explored modules
    such as `stats` (for statistics), `spatial` (for data structures), and `cluster`
    (for clustering and vector quantization). In the next chapter, additional functionalities
    included in the SciPy module, `scipy.spatial`, will be studied, complementing
    the ones already explored in previous chapters. As usual, each function introduced
    will be illustrated via non-trivial examples which can be enriched modifying the
    IPython Notebook corresponding to this chapter.
  prefs: []
  type: TYPE_NORMAL
