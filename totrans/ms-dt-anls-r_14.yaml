- en: Chapter 14. Analyzing the R Community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, I will try to summarize what you have learned in the
    past 13 chapters. To this end, we will create an actual case study, independent
    from the previously used `hflights` and `mtcars` datasets, and will now try to
    estimate the size of the R community. This is a rather difficult task as there
    is no list of R users around the world; thus, we will have to build some predicting
    models on a number of partial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we will do the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect live data from different data sources on the Internet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleanse the data and transform it to a standard format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run some quick descriptive, exploratory analysis methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the extracted data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build some log-linear models on the number of R users based on an independent
    list of names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R Foundation members
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the easiest things we can do is count the members of the R Foundation—the
    organization coordinating the development of the core R program. As the ordinary
    members of the Foundation include only the *R Development Core Team*, we had better
    check the supporting members. Anyone can become a supporting member of the Foundation
    by paying a nominal yearly fee— I highly suggest you do this, by the way. The
    list is available on the [http://r-project.org](http://r-project.org) site, and
    we will use the `XML` package (for more detail, see [Chapter 2](ch02.html "Chapter 2. Getting
    Data from the Web"), *Getting Data from the Web*) to parse the HTML page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the HTML page loaded into R, we can use the XML Path Language
    to extract the list of the supporting members of the Foundation, by reading the
    list after the `Supporting members` header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Form this character vector of 279 names and countries, let''s extract the list
    of supporting members and the countries separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So we first extracted the names by removing everything starting from the opening
    parenthesis in the strings, and then we matched the countries by the character
    positions computed from the number of characters in the names and the original
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the name list of 279 supporting members of the R Foundation, we also
    know the proportion of the citizenship or residence of the members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing supporting members around the world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probably it''s not that surprising that most supporting members are from the
    USA, and some European countries are also at the top of this list. Let''s save
    this table so that we can generate a map on this count data after some quick data
    transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in [Chapter 13](ch13.html "Chapter 13. Data Around Us"), *Data
    Around Us*, the `rworldmap` package can render country-level maps in a very easy
    way; we just have to map the values with some polygons. Here, we will use the
    `joinCountryData2Map` function, first enabling the `verbose` option to see what
    country names have been missed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So we tried to match the country names stored in the countries data frame,
    but failed for the previously listed four strings. Although we could manually
    fix this, in most cases it''s better to automate what we can, so let''s pass all
    the failed strings to the Google Maps geocoding API and see what it returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have fixed the country names with the help of the Google geocoding
    service, let''s regenerate the frequency table and map those values to the polygon
    names with the `rworldmap` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These results are much more satisfying! Now we have the number of supporting
    members of the R Foundation mapped to the countries, so we can easily plot this
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualizing supporting members around the world](img/2028OS_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Well, it's clear that most supporting members of the R Foundation are based
    in the USA, Europe, Australia, and New Zealand (where R was born more than 20
    years ago).
  prefs: []
  type: TYPE_NORMAL
- en: But the number of supporters is unfortunately really low, so let's see what
    other data sources we can find and utilize in order to estimate the number of
    R users around the world.
  prefs: []
  type: TYPE_NORMAL
- en: R package maintainers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another similarly straightforward data source might be the list of R package
    maintainers. We can download the names and e-mail addresses of the package maintainers
    from a public page of CRAN, where this data is stored in a nicely structured HTML
    table that is extremely easy to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting the names from the `Maintainer` column can be done via some quick
    data cleansing and transformations, mainly using regular expressions. Please note
    that the column name starts with a space—that''s why we quoted the column name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This list of almost 7,000 package maintainers includes some duplicated names
    (they maintain multiple packages). Let''s see the list of the top, most prolific
    R package developers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Although there's an odd name in the preceding list (orphaned packages do not
    have a maintainer—it's worth mentioning that having only 26 packages out of the
    6,994 no longer actively maintained is a pretty good ratio), but the other names
    are indeed well known in the R community and work on a number of useful packages.
  prefs: []
  type: TYPE_NORMAL
- en: The number of packages per maintainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the other hand, there are a lot more names in the list associated with only
    one or a few R packages. Instead of visualizing the number of packages per maintainer
    on a simple bar chart or histogram, let''s load the `fitdistrplus` package, which
    we will use on the forthcoming pages to fit various theoretical distributions
    on this analyzed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![The number of packages per maintainer](img/2028OS_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding plots also show that most people in the list maintain only one,
    but no more than two or three, packages. If we are interested in how long/heavy
    tailed this distribution is, we might want to call the `descdist` function, which
    returns some important descriptive statistics on the empirical distribution and
    also plots how different theoretical distributions fit our data on a skewness-kurtosis
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![The number of packages per maintainer](img/2028OS_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our empirical distribution seems to be rather long-tailed with a very high
    kurtosis, and it seems that the gamma distribution is the best fit for this dataset.
    Let''s see the estimate parameters of this gamma distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these parameters to simulate a lot more R package maintainers with
    the `rgamma` function. Let''s see how many R packages would be available on CRAN
    with, for example, 100,000 package maintainers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![The number of packages per maintainer](img/2028OS_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s rather clear that this distribution is not as long-tailed as our real
    dataset: even with 100,000 simulations, the largest number was below 10, as we
    can see in the preceding plot; in reality, though, the R package maintainers are
    a lot more productive with up to 20 or 30 packages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s verify this by estimating the proportion of R package maintainers with
    no more than two packages based on the preceding gamma distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'But this percentage is a lot higher in the real dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This may suggest trying to fit a longer-tailed distribution. Let''s see for
    example how Pareto distribution would fit our data. To this end, let''s follow
    the analytical approach by using the lowest value as the location of the distribution,
    and the number of values divided by the sum of the logarithmic difference of all
    these values from the location as the shape parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, there is no `ppareto` function in the base `stats` package,
    so we have to first load the `actuar` or `VGAM` package to compute the distribution
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, now this is even higher than the real proportion! It seems that none
    of the preceding theoretical distributions fit our data perfectly—which is pretty
    normal by the way. But let''s see how these distributions fit our original data
    set on a joint plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![The number of packages per maintainer](img/2028OS_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After all, it seems that the Pareto distribution is the closest fit to our
    long-tailed data. But more importantly, we know about more than 4,000 R users
    besides the previously identified 279 R Foundation supporting members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: What other data sources can we use to find information on the (number of) R
    users?
  prefs: []
  type: TYPE_NORMAL
- en: The R-help mailing list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'R-help is the official, main mailing list providing general discussion about
    problems and solutions using R, with many active users and several dozen e-mails
    every day. Fortunately, this public mailing list is archived on several sites,
    and we can easily download the compressed monthly files from, for example, ETH
    Zurich''s R-help archives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s extract the URL of the monthly compressed archives from this page
    via an XPath query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let''s download these files to our computer for future parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on your operating system and R version, the `curl` option that we
    used to download files via the HTTPS protocol might not be available. In such
    cases, you can try other another method or update the query to use the `RCurl`,
    `curl`, or `httr` packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading these ~200 files takes some time and you might also want to add
    a `Sys.sleep` call in the loop so as not to overload the server. Anyway, after
    some time, you will have a local copy of the `R-help` mailing list in the `r-help`
    folder, ready to be parsed for some interesting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of loading all the text files into R and using `grep` there, I pre-filtered
    the files via the Linux command line `zgrep` utility, which can search in `gzipped`
    (compressed) text files efficiently. If you do not have `zgrep` installed (it
    is available on both Windows and the Mac), you can extract the files first and
    use the standard `grep` approach with the very same regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we filtered for all lines of the e-mails and headers, starting with the
    `From` string, that hold information on the senders in the e-mail address and
    name. Out of the ~387,000 e-mails, we have found around ~110,000 unique e-mail
    sources. To understand the following regular expressions, let''s see how one of
    these lines looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s process these lines by removing the static prefix and extracting
    the names found between parentheses after the e-mail address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can see the list of the most active `R-help` posters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This list seems to be legitimate, right? Although my first guess was that Professor
    Brian Ripley with his brief messages will be the first one in this list. As a
    result of some earlier experiences, I know that matching names can be tricky and
    cumbersome, so let''s verify that our data is clean enough and there''s only one
    version of the Professor''s name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, it seems that the Professor used some alternative `From` addresses as
    well, so a more valid estimate of the number of his messages should be something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'So using quick, regular expressions to extract the names from the e-mails returned
    most of the information we were interested in, but it seems that we have to spend
    a lot more time to get the whole information set. As usual, the Pareto rule applies:
    we can spend around 80 percent of our time on preparing data, and we can get 80
    percent of the data in around 20 percent of the whole project timeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to page limitations, we will not cover data cleansing on this dataset in
    greater detail at this point, but I highly suggest checking Mark van der Loo's
    `stringdist` package, which can compute string distances and similarities to,
    for example, merge similar names in cases like this.
  prefs: []
  type: TYPE_NORMAL
- en: Volume of the R-help mailing list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: But besides the sender, these e-mails also include some other really interesting
    data as well. For example, we can extract the date and time when the e-mail was
    sent—to model the frequency and temporal pattern of the mailing list.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, let''s filter for some other lines in the compressed text files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns fewer lines when compared to the previously extracted `From` lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This is due to the various date and time formats used in the e-mail headers,
    as sometimes the day of the week was not included in the string or the order of
    year, month, and day was off compared to the vast majority of other mails. Anyway,
    we will only concentrate on this significant portion of mails with the standard
    date and time format but, if you are interested in transforming these other time
    formats, you might want to check Hadley Wickham's `lubridate` package to help
    your workflow. But please note that there's no general algorithm to guess the
    order of decimal year, month, and day—so you will end up with some manual data
    cleansing for sure!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how these (subset of) lines look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can simply get rid of the `Date` prefix and parse the time stamps via
    `strptime`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is in a parsed format (even the local time-zones were converted
    to UTC), it''s relatively easy to see, for example, the number of e-mails on the
    mailing list per year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Volume of the R-help mailing list](img/2028OS_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although the volume on the `R-help` mailing list seems to have decreased in
    the past few years, it''s not due to the lower R activity: R users, okay as is
    or no/. others on the Internet, nowadays tend to use other information channels
    more often than e-mail—for example: StackOverflow and GitHub (or even Facebook
    and LinkedIn). For a related research, please see the paper of Bogdan Vasilescu
    at al at [http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf](http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we can do a lot better than this, right? Let''s massage our data a bit
    and visualize the frequency of mails based on the day of week and hour of the
    day via a more elegant graph—inspired by GitHub''s punch card plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualizing this dataset is relatively straightforward with `ggplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Volume of the R-help mailing list](img/2028OS_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the times are by UTC, the early morning mails might suggest that where most
    `R-help` posters live has a positive GMT offset—if we suppose that most e-mails
    were written in business hours. Well, at least the lower number of e-mails on
    the weekends seems to suggest this statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'And it seems that the UTC, UTC+1, and UTC+2 time zones are indeed rather frequent,
    but the US time zones are also pretty common for the `R-help` posters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Forecasting the e-mail volume in the future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'And we can also use this relatively clean dataset to forecast the future volume
    of the `R-help` mailing list. To this end, let''s aggregate the original dataset
    to count data daily, as we saw in [Chapter 3](ch03.html "Chapter 3. Filtering
    and Summarizing Data"), *Filtering and Summarizing Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s transform this `data.table` object into a time-series object by
    referencing the actual mail counts as values and the dates as the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, this daily dataset is a lot spikier than the previously rendered yearly
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Forecasting the e-mail volume in the future](img/2028OS_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But instead of smoothing or trying to decompose this time-series, like we did
    in [Chapter 12](ch12.html "Chapter 12. Analyzing Time-series"), *Analyzing Time-series*,
    let''s rather see how we can provide some quick estimates (based on historical
    data) on the forthcoming number of mails on this mailing list with some automatic
    models. To this end, we will use the `forecast` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ets` function implements a fully automatic method that can select the
    optimal trend, season, and error type for the given time-series. Then we can simply
    call the `predict` or `forecast` function to see the specified number of estimates,
    only for the next day in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'So it seems that, for the next day, our model estimated around 28 e-mails with
    a confidence interval of 80 percent being somewhere between 10 and 47\. Visualizing
    predictions for a slightly longer period of time with some historical data can
    be done via the standard `plot` function with some useful new parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Forecasting the e-mail volume in the future](img/2028OS_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing overlaps between our lists of R users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But our original idea was to predict the number of R users around the world
    and not to focus on some minor segments, right? Now that we have multiple data
    sources, we can start building some models combining those to provide estimates
    on the global number of R users.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind this approach is the capture-recapture method, which is
    well known in ecology, where we first try to identify the probability of capturing
    a unit from the population, and then we use this probability to estimate the number
    of not captured units.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our current study, units will be R users and the samples are the previously
    captured name lists on the:'
  prefs: []
  type: TYPE_NORMAL
- en: Supporters of the *R Foundation*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R package maintainers who submitted at least one package to *CRAN*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R-help* mailing list e-mail senders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s merge these lists with a tag referencing the data source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next let''s see the number of names we can find in one, two or all three groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: So there are (at least) 40 persons who support the R Foundation, maintain at
    least one R package on CRAN, and have posted at least one mail to `R-help` since
    1997! I am happy and proud to be one of these guys -- especially with an accent
    in my name, which often makes matching of strings more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we suppose these lists refer to the same population, namely R users
    around the world, then we can use these common occurrences to predict the number
    of R users who somehow missed supporting the R Foundation, maintaining a package
    on CRAN, and writing a mail to the R-help mailing list. Although this assumption
    is obviously off, let's run this quick experiment and get back to these outstanding
    questions later.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the best things in R is that we have a package for almost any problem.
    Let''s load the `Rcapture` package, which provides some sophisticated, yet easily
    accessible, methods for capture-recapture models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'These numbers from the first `fi` column are familiar from the previous table,
    and represent the number of R users identified on one, two, or all three lists.
    It''s a lot more interesting to fit some models on this data with a simple call
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, I have to emphasize that these estimates are not actually on the
    abundance of all R users around the world, because:'
  prefs: []
  type: TYPE_NORMAL
- en: Our non-independent lists refer to far more specific groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model assumptions do not stand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The R community is definitely not a closed population and some open-population
    models would be more reliable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We missed some very important data-cleansing steps, as noted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further ideas on extending the capture-recapture models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although this playful example did not really help us to find out the number
    of R users around the world, with some extensions the basic idea is definitely
    viable. First of all, we might consider analyzing the source data in smaller chunks—for
    example, looking for the same e-mail addresses or names in different years of
    the R-help archives. This might help with estimating the number of persons who
    were thinking about submitting a question to `R-help`, but did not actually send
    the e-mail after all (for example, because another poster's question had already
    been answered or she/he resolved the problem without external help).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we could also add a number of other data sources to the models,
    so that we can do more reliable estimates on some other R users who do not contribute
    to the R Foundation, CRAN, or R-help.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have been working on a similar study over the past 2 years, collecting data
    on the number of:'
  prefs: []
  type: TYPE_NORMAL
- en: R Foundation ordinary and supporting members, donators and benefactors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attendees at the annual R conference between 2004 and 2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRAN downloads per package and country in 2013 and 2014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R User Groups and meet-ups with the number of members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [http://www.r-bloggers.com](http://www.r-bloggers.com) visitors in 2013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub users with at least one repository with R source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google search trends on R-related terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the results on an interactive map and the country-level aggregated
    data in a CSV file at [http://rapporter.net/custom/R-activity](http://rapporter.net/custom/R-activity)
    and an offline data visualization presented in the past two *useR!* conferences
    at [http://bit.ly/useRs2015](http://bit.ly/useRs2015).
  prefs: []
  type: TYPE_NORMAL
- en: The number of R users in social media
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative way to try to estimate the number of R users could be to analyze
    the occurrence of the related terms on social media. This is relatively easy on
    Facebook, where the marketing API allows us to query the size of the so-called
    target audiences, which we can use to define targets for some paid ads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we are not actually interested in creating a paid advertisement on Facebook
    right now, although this can be easily done with the `fbRads` package, but we
    can use this feature to see the estimated size of the *target* group of persons
    interested in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, to run this quick example you will need to have a (free) Facebook
    developer account, a registered application, and a generated token (please see
    the package docs for more details), but it is definitely worth it: we have just
    found out that there are more than 1.3M users around the world interested in R!
    That''s really impressive, although it seems to be rather high to me, especially
    when compared with some other statistical software, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Having said this, comparing R with other programming languages suggests that
    the audience size might actually be correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: There are many programmers around the world, it seems! But what are they talking
    about and what are the trending topics? We will cover these questions in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: R-related posts in social media
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One option to collect posts from the past few days of social media is processing
    Twitter's global stream of Tweet data. This stream data and API provides access
    to around 1 percent of all tweets. If you are interested in all this data, then
    a commercial Twitter Firehouse account is needed. In the following examples, we
    will use the free Twitter search API, which provides access to no more than 3,200
    tweets based on any search query—but this will be more than enough to do some
    quick analysis on the trending topics among R users.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s load the `twitteR` package and initialize the connection to the API
    by providing our application tokens and secrets, generated at [https://apps.twitter.com](https://apps.twitter.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Now we can start using the `searchTwitter` function to search tweets for any
    keywords, including hashtags and mentions. This query can be fine-tuned with a
    couple of arguments. `Since`, `until`, and *n* set the beginning and end date,
    also the number of tweets to return respectively. Language can be set with the
    `lang` attribute by the ISO 639-1 format—for example, use `en` for English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s search for the most recent tweet with the official R hashtag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This is quite an impressive amount of information for a character string with
    no more than 140 characters, isn't it? Besides the text including the actual tweet,
    we got some meta-information as well—for example, the author, post time, the number
    of times other users favorited or retweeted the post, the Twitter client name,
    and the URLs in the post along with the shortened, expanded, and displayed format.
    The location of the tweet is also available in some cases, if the user enabled
    that feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this piece of information, we could focus on the Twitter R community
    in very different ways. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Counting the users mentioning R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing social network or Twitter interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-series analysis on the time of posts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial analysis on the location of tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text mining of the tweet contents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probably a mixture of these (and other) methods would be the best approach,
    and I highly suggest you do that as an exercise to practice what you have learned
    in this book. However, in the following pages we will only concentrate on the
    last item.
  prefs: []
  type: TYPE_NORMAL
- en: 'So first, we need some recent tweets on the R programming language. To search
    for `#rstats` posts, instead of providing the related hashtag (like we did previously),
    we can use the `Rtweets` wrapper function as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This function returned 500 reference classes similar to those we saw previously.
    We can count the number of original tweets excluding retweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'But, as we are looking for the trending topics, we are interested in the original
    list of tweets, where the retweets are also important as they give a natural weight
    to the trending posts. So let''s transform the list of reference classes to a
    `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset consists of 500 rows (tweets) and 16 variables on the content,
    author, and location of the posts, as described previously. Now, as we are only
    interested in the actual text of the tweets, let''s load the `tm` package and
    import our corpus as seen in [Chapter 7](ch07.html "Chapter 7. Unstructured Data"),
    *Unstructured Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'As the data is in the right format, we can start to clean the data from the
    common English words and transform everything into lowercase format; we might
    also want to remove any extra whitespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s also wise to remove the R hashtag, as this is part of all tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can use the `wordcloud` package to plot the most important words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![R-related posts in social media](img/2028OS_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the past few pages, I have tried to cover a variety of data science and
    R programming topics, although many important methods and questions were not addressed
    due to page limitation. To this end, I''ve compiled a short reading list in the
    *References* chapter of the book. And don''t forget: now it''s your turn to practice
    everything you learned in the previous chapters. I wish you a lot of fun and success
    in this journey!'
  prefs: []
  type: TYPE_NORMAL
- en: And once again, thanks for reading this book; I hope you found it useful. If
    you have any questions, comments, or any kind of feedback, please feel free to
    get in touch, I'm looking forward to hearing from you!
  prefs: []
  type: TYPE_NORMAL
