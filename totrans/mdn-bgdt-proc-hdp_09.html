<html><head></head><body>
        

                            
                    <h1 class="header-title">Designing Data Visualization Solutions</h1>
                
            
            
                
<p>Once we have the data living in the Hadoop ecosystem and it's been processed, the next logical step is to build the analytics that drive the business decisions.</p>
<p>In this chapter, we will learn the following topics:</p>
<ul>
<li>Data visualization</li>
<li>Apache Druid</li>
<li>Apache Superset</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data visualization</h1>
                
            
            
                
<p>Data visualization is the process of understanding the relationships between various entities in the raw data via graphical means. This is a very powerful technique because it enables end users to get the message in a very easy form without even knowing anything about the underlying data.</p>
<p>Data visualization plays a very important role in visual communication of insights from big data. Its both an art and a science, and requires some effort in terms of understanding the data; at the same time we need some understanding of the target audience as well.</p>
<p>So far, we have seen that any type of data can be stored in the <strong>Hadoop filesystem</strong> (<strong>HDFS</strong>). In order to convert complex data structures into a visual form, we need to understand the standard techniques that are used to represent the data.</p>
<p>In data visualization, the message is conveyed to the end users in the form of graphics which can be in 1D, 2D, 3D, or even higher dimensions. This purely depends on the meaning we are trying to convey.</p>
<p>Let's take a look at the standard graphics that are used to convey visual messages to users:</p>
<ul>
<li style="font-weight: 400">Bar/column chart</li>
<li style="font-weight: 400">Line/area chart</li>
<li style="font-weight: 400">Pie chart</li>
<li style="font-weight: 400">Radar chart</li>
<li style="font-weight: 400">Scatter/bubble chart</li>
<li style="font-weight: 400">Tag Cloud</li>
<li style="font-weight: 400">Bubble chart</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Bar/column chart</h1>
                
            
            
                
<p>This is a 2D graphical representation of data where the data points are shown as vertical/horizontal bars. Each bar represents one data point. When there is no time dimension involved with reference to the data points, the order in which these points are shown might not make any difference. When we deal with time series data for representing bar charts, we generally follow the chronological order of display along the X (horizontal) axis.</p>
<p>Let's take a look at a sample chart that is generated with four data points. The data represents the amount each user has:</p>
<div><img src="img/9a5e0f63-0700-430c-873e-dbeeb2c09d0b.png" style="width:31.67em;height:22.83em;"/></div>
<p><strong>Interpretation</strong>: The graph has both text data in rows and columns, and also visuals. If you observe carefully, the textual data is smaller in size and has only four records. But the visual graphic conveys the message straightaway without knowing anything about the data.</p>
<p>The message the graph conveys is that:</p>
<ul>
<li style="font-weight: 400">Sita has more money than everyone</li>
<li style="font-weight: 400">Gita has the least money</li>
</ul>
<p>Other interpretations are also possible. They are left to the reader.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Line/area chart</h1>
                
            
            
                
<p>This is also typically a 2D chart where each data point is represented as a point on canvas and all these points belonging to the same dataset are connected using a line. This chart becomes an area chart when the region from the horizontal/vertical axis is completely covered up to the line.</p>
<p>There can be more than one line in the same graph, which indicates multiple series of data for the same entities.</p>
<p>Let's take a look at the sample of this area chart based on the same data as before:</p>
<div><img src="img/b9849529-fcc2-488a-8c2d-02fadaba8876.png" style="width:33.50em;height:23.42em;"/></div>
<p>These are the properties of the chart:</p>
<ul>
<li style="font-weight: 400">The <em>x</em> axis has the list of all the people</li>
<li style="font-weight: 400">The <em>y</em> axis indicates the amount from <strong>0</strong> to <strong>100</strong></li>
<li style="font-weight: 400">Points are drawn on the graph at four places, corresponding to the values in tabular form</li>
<li style="font-weight: 400">Points are connected with straight lines</li>
<li style="font-weight: 400">The area is filled below the line to make it an area chart</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Pie chart</h1>
                
            
            
                
<p>This is also a 2D chart drawn as multiple sectors in a circle. This chart is useful when we want to highlight the relative importance of all the data points.</p>
<p>Let's take a look at the example chart that is drawn with the same dataset as before to understand it better:</p>
<div><img src="img/ffa15300-a402-41d1-9baf-d4caae0188a9.png" style="width:37.58em;height:26.67em;"/></div>
<p>As you can see, it's easy to understand the relative importance of the amounts owned by each of the persons using this chart.</p>
<p>The conclusions that are drawn are similar to the previous charts. But the graph is a simple circle and there are no multiple dimensions here to burden the user.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Radar chart</h1>
                
            
            
                
<p>This is also a 2D graphic where the data axes are the edges of equidistant sectors (like a pie chart's edges). This graph is useful when there are multiple dimensions in which we want to understand the relative significance of each data point.</p>
<p>To understand this graph better, let's take a look at this sample data and the graphic:</p>
<div><img src="img/436900d2-ca59-4ff0-a579-c8a69ab9b5a6.png" style="width:41.92em;height:27.58em;"/></div>
<p>The data consists of eight columns:</p>
<ul>
<li style="font-weight: 400"><strong>First column</strong>: List of all users</li>
<li style="font-weight: 400"><strong>Second to Eighth column</strong>: Days in a week and the dollars owned by each person on that day</li>
</ul>
<p>We want to draw a graph that shows us the following things:</p>
<ul>
<li style="font-weight: 400">Total dollars per day</li>
<li style="font-weight: 400">Dollars owned by every person every day</li>
</ul>
<p>We have drawn all this information in the radar chart, where the axes are the sectors (days) and are capped at a maximum value of <strong>400</strong>. Each user's worth is drawn one on top of another so that we will know the total worth instead of relative worth (this is similar to area stacking).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scatter/bubble chart</h1>
                
            
            
                
<p>A scatter chart can be a multi-dimensional graphic. This is one of the simpler graphics to understand as we render each data point on the canvas corresponding to the numeric value along the axis. This graph is useful to understand the relative importance of each point along the axes.</p>
<p>A bubble chart is a variant of a scatter chart, where the points on the canvas show the values as big bubbles (to signify their importance).</p>
<p>Let's take a look at both these graphics with this example:</p>
<div><img src="img/15f06c51-ebce-4863-935b-8485c89f33e4.png"/></div>
<p>The graphic on the left-hand side is a bubble chart and the right one is a scatter plot.</p>
<p>Let's take a look at the data and the charts that are generated.</p>
<p>The input data:</p>
<ul>
<li style="font-weight: 400">Consists of five rows, whereas we have <strong>Sales</strong> and <strong>Number of Products</strong> in columns</li>
</ul>
<p>With a bubble chart:</p>
<ul>
<li style="font-weight: 400">The <em>y</em> axis shows the number of products</li>
<li style="font-weight: 400">The <em>x</em> axis is just positional and doesn't reflect the value from the input data</li>
<li style="font-weight: 400">Each point on the canvas shows the sales corresponding to the number of products</li>
</ul>
<p>With the scatter chart:</p>
<ul>
<li style="font-weight: 400">The <em>y</em> axis shows the sales done</li>
<li style="font-weight: 400">The <em>x</em> axis shows the products sold</li>
<li style="font-weight: 400">Each point on the canvas shows each row in the input</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Other charts</h1>
                
            
            
                
<p>There are many other types of graphics possible that are not covered in this section but are worth exploring on the <a href="https://d3js.org">https://d3js.org</a> website. This will give you an understanding of how data can be represented to convey a very good message to the users.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Practical data visualization in Hadoop</h1>
                
            
            
                
<p>Hadoop has a rich ecosystem of data sources and applications that help us build rich visualizations. In the coming sections, we will understand two such applications:</p>
<ul>
<li>Apache Druid</li>
<li>Apache Superset</li>
</ul>
<p>We will also learn how to use Apache Superset with data in RDBMSes such as MySQL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache Druid</h1>
                
            
            
                
<p>Apache Druid is a distributed, high-performance columnar store. Its official website is <a href="https://druid.io">https://druid.io</a>.</p>
<p>Druid allows us to store both real-time and historical data that is time series in nature. It also provides fast data aggregation and flexible data exploration. The architecture supports storing trillions of data points on petabyte sizes.</p>
<p>In order to understand more about the Druid architecture, please refer to this white paper at <a href="http://static.druid.io/docs/druid.pdf">http://static.druid.io/docs/druid.pdf</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Druid components</h1>
                
            
            
                
<p>Let's take a quick look at the different components of the Druid cluster:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Component</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p>Druid Broker</p>
</td>
<td>
<p>These are the nodes that are aware of where the data lies in the cluster. These nodes are contacted by the applications/clients to get the data within Druid.</p>
</td>
</tr>
<tr>
<td>
<p>Druid Coordinator</p>
</td>
<td>
<p>These nodes manage the data (they load, drop, and load-balance it) on the historical nodes.</p>
</td>
</tr>
<tr>
<td>
<p>Druid Overlord</p>
</td>
<td>
<p>This component is responsible for accepting tasks and returning the statuses of the tasks.</p>
</td>
</tr>
<tr>
<td>
<p>Druid Router</p>
</td>
<td>
<p>These nodes are needed when the data volume is in terabytes or higher range. These nodes route the requests to the brokers.</p>
</td>
</tr>
<tr>
<td>
<p>Druid Historical</p>
</td>
<td>
<p>These nodes store immutable segments and are the backbone of the Druid cluster. They serve load segments, drop segments, and serve queries on segments' requests.</p>
</td>
</tr>
</tbody>
</table>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Other required components</h1>
                
            
            
                
<p>The following table presents a couple of other required components:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Component</strong> </p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p>Zookeeper</p>
</td>
<td>
<p>Apache Zookeeper is a highly reliable distributed coordination service</p>
</td>
</tr>
<tr>
<td>
<p>Metadata Storage</p>
</td>
<td>
<p>MySQL and PostgreSQL are the popular RDBMSes used to keep track of all segments, supervisors, tasks, and configurations</p>
</td>
</tr>
</tbody>
</table>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache Druid installation</h1>
                
            
            
                
<p>Apache Druid can be installed either in standalone mode or as part of a Hadoop cluster. In this section, we will see how to install Druid via Apache Ambari.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Add service</h1>
                
            
            
                
<p>First, we invoke the Actions drop-down below the list of services in the Hadoop cluster.</p>
<p>The screen looks like this:</p>
<div><img src="img/56f10c5f-97bf-40c1-b098-83a1f671e5ee.png" style="width:20.25em;height:23.67em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Select Druid and Superset</h1>
                
            
            
                
<p>In this setup, we will install both Druid and Superset at the same time. Superset is the visualization application that we will learn about in the next step.</p>
<p>The selection screen looks like this:</p>
<div><img src="img/f0086078-bf64-45a7-af90-00f00093b19f.png" style="width:57.33em;height:30.25em;"/></div>
<p>Click on Next when both the services are selected.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service placement on servers</h1>
                
            
            
                
<p>In this step, we will be given a choice to select the servers on which the application has to be installed. I have selected node 3 for this purpose. You can select any node you wish.</p>
<p>The screen looks something like this:</p>
<div><img src="img/e573a6f3-105c-45a7-a56b-b903cd89c4a9.png" style="width:63.67em;height:33.83em;"/></div>
<p>Click on Next when when the changes are done.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Choose Slaves and Clients</h1>
                
            
            
                
<p>Here, we are given a choice to select the nodes on which we need the Slaves and Clients for the installed components. I have left the options that are already selected for me:</p>
<div><img src="img/23b2b7f5-6540-421e-8a95-2a3a0a3dcf83.png" style="width:59.58em;height:25.33em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Service configurations</h1>
                
            
            
                
<p>In this step, we need to select the databases, usernames, and passwords for the metadata store used by the Druid and Superset applications. Feel free to choose the default ones. I have given MySQL as the backend store for both of them.</p>
<p>The screen looks like this:</p>
<div><img src="img/72b59851-5564-4c73-8143-b34a002282ee.png" style="width:60.00em;height:31.83em;"/></div>
<p>Once the changes look good, click on the Next button at the bottom of the screen.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service installation</h1>
                
            
            
                
<p>In this step, the applications will be installed automatically and the status will be shown at the end of the plan.</p>
<p>Click on Next once the installation is complete. Changes to the current screen look like this:</p>
<div><img src="img/1d03751d-b4f0-4879-bbc8-ef7ba50c9a27.png" style="width:47.50em;height:21.33em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Installation summary</h1>
                
            
            
                
<p>Once everything is successfully completed, we are shown a summary of what has been done. Click on Complete when done:</p>
<div><img src="img/02790068-b2da-4efd-a4b5-2afc1dfdb055.png" style="width:47.67em;height:19.42em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Sample data ingestion into Druid</h1>
                
            
            
                
<p>Once we have all the Druid-related applications running in our Hadoop cluster, we need a sample dataset that we must load in order to run some analytics tasks.</p>
<p>Let's see how to load sample data. Download the Druid archive from the internet:</p>
<pre>[druid@node-3 ~$ curl -O http://static.druid.io/artifacts/releases/druid-0.12.0-bin.tar.gz<br/>% Total % Received % Xferd Average Speed Time Time Time Current<br/>                               Dload Upload Total Spent Left Speed<br/>100 222M 100 222M 0 0 1500k 0 0:02:32 0:02:32 --:--:-- 594k</pre>
<p>Extract the archive:</p>
<pre>[druid@node-3 ~$ tar -xzf druid-0.12.0-bin.tar.gz</pre>
<p>Copy the sample Wikipedia data to Hadoop:</p>
<pre>[druid@node-3 ~]$ cd druid-0.12.0<br/>[druid@node-3 ~/druid-0.12.0]$ hadoop fs -mkdir /user/druid/quickstart<br/>[druid@node-3 ~/druid-0.12.0]$ hadoop fs -put quickstart/wikiticker-2015-09-12-sampled.json.gz /user/druid/quickstart/</pre>
<p>Submit the import request:</p>
<pre>[druid@node-3 druid-0.12.0]$ curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/wikiticker-index.json localhost:8090/druid/indexer/v1/task;echo<br/>{"task":"index_hadoop_wikiticker_2018-03-16T04:54:38.979Z"}</pre>
<p>After this step, Druid will automatically import the data into the Druid cluster and the progress can be seen in the overlord console.</p>
<p>The interface is accessible via <kbd>http://&lt;overlord-ip&gt;:8090/console.html</kbd>. The screen looks like this:</p>
<div><img src="img/8df1b469-ba70-4646-a81f-bfe2cce4267c.png"/></div>
<p>Once the ingestion is complete, we will see the status of the job as SUCCESS.</p>
<p>In case of <kbd>FAILED</kbd> imports, please make sure that the backend that is configured to store the Metadata for the Druid cluster is up and running.</p>
<p>Even though Druid works well with the OpenJDK installation, I have faced a problem with a few classes not being available at runtime. In order to overcome this, I have had to use Oracle Java version 1.8 to run all Druid applications.</p>
<p>Now we are ready to start using Druid for our visualization tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">MySQL database</h1>
                
            
            
                
<p>Apache Superset also allows us to read the data present in an RDBMS system such as MySQL. We will also create a sample database in this section, which we can use later with Superset to create visualizations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sample database</h1>
                
            
            
                
<p>The employees database is a standard dataset that has a sample organization and their employee, salary, and department data. We will see how to set it up for our tasks.</p>
<p>This section assumes that the MySQL database is already configured and running.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Download the sample dataset</h1>
                
            
            
                
<p>Download the sample dataset from GitHub with the following command on any server that has access to the MySQL database:</p>
<pre>[user@master ~]$ sudo yum install git -y
[user@master ~]$ git clone https://github.com/datacharmer/test_db<br/>Cloning into 'test_db'...<br/>remote: Counting objects: 98, done.<br/>remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98<br/>Unpacking objects: 100% (98/98), done.</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Copy the data to MySQL</h1>
                
            
            
                
<p>In this step, we will import the contents of the data in the files to the MySQL database:</p>
<pre>[user@master test_db]$ mysql -u root &lt; employees.sql<br/>INFO<br/>CREATING DATABASE STRUCTURE<br/>INFO<br/>storage engine: InnoDB<br/>INFO<br/>LOADING departments<br/>INFO<br/>LOADING employees<br/>INFO<br/>LOADING dept_emp<br/>INFO<br/>LOADING dept_manager<br/>INFO<br/>LOADING titles<br/>INFO<br/>LOADING salaries<br/>data_load_time_diff<br/>NULL<br/><br/></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Verify integrity of the tables</h1>
                
            
            
                
<p>This is an important step, just to make sure that all of the data we have imported is correctly stored in the database. The summary of the integrity check is shown as the verification happens:</p>
<pre>[user@master test_db]$ mysql -u root -t &lt; test_employees_sha.sql<br/>+----------------------+<br/>| INFO                 |<br/>+----------------------+<br/>| TESTING INSTALLATION |<br/>+----------------------+<br/>+--------------+------------------+------------------------------------------+<br/>| table_name   | expected_records | expected_crc                             |<br/>+--------------+------------------+------------------------------------------+<br/>| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |<br/>| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |<br/>| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |<br/>| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |<br/>| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |<br/>| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |<br/>+--------------+------------------+------------------------------------------+<br/>+--------------+------------------+------------------------------------------+<br/>| table_name   | found_records    | found_crc                        |<br/>+--------------+------------------+------------------------------------------+<br/>| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |<br/>| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |<br/>| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |<br/>| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |<br/>| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |<br/>| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |<br/>+--------------+------------------+------------------------------------------+
+--------------+---------------+-----------+<br/>| table_name   | records_match | crc_match |<br/>+--------------+---------------+-----------+<br/>| employees    | OK | ok        |<br/>| departments  | OK | ok        |<br/>| dept_manager | OK            | ok |<br/>| dept_emp     | OK | ok        |<br/>| titles       | OK | ok        |<br/>| salaries     | OK | ok        |<br/>+--------------+---------------+-----------+<br/>+------------------+<br/>| computation_time |<br/>+------------------+<br/>| 00:00:11         |<br/>+------------------+<br/>+---------+--------+<br/>| summary | result |<br/>+---------+--------+<br/>| CRC     | OK |<br/>| count   | OK |<br/>+---------+--------+</pre>
<p>Now the data is correctly loaded in the MySQL database called <strong>employees</strong>. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Single Normalized Table</h1>
                
            
            
                
<p>In data warehouses, its a standard practice to have normalized tables when compared to many small related tables. Lets create a single normalized table that contains details of employees, salaries, departments</p>
<pre>MariaDB [employees]&gt; create table employee_norm as select e.emp_no, e.birth_date, CONCAT_WS(' ', e.first_name, e.last_name) full_name , e.gender, e.hire_date, s.salary, s.from_date, s.to_date, d.dept_name, t.title from employees e, salaries s, departments d, dept_emp de, titles t where e.emp_no = t.emp_no and e.emp_no = s.emp_no and d.dept_no = de.dept_no and e.emp_no = de.emp_no and s.to_date &lt; de.to_date and s.to_date &lt; t.to_date order by emp_no, s.from_date;
Query OK, 3721923 rows affected (1 min 7.14 sec)
Records: 3721923  Duplicates: 0  Warnings: 0

MariaDB [employees]&gt; select * from employee_norm limit 1\G
*************************** 1. row ***************************
    emp_no: 10001
birth_date: 1953-09-02
 full_name: Georgi Facello
    gender: M
 hire_date: 1986-06-26
    salary: 60117
 from_date: 1986-06-26
   to_date: 1987-06-26
 dept_name: Development
     title: Senior Engineer
1 row in set (0.00 sec)

MariaDB [employees]&gt; </pre>
<p>Once we have normalized data, we will see how to use the data from this table to generate rich visualisations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache Superset</h1>
                
            
            
                
<p>Superset is a modern, enterprise-grade business intelligence application. The important feature of this application is that we can run all analyses directly from the browser. There is no need to install any special software for this.</p>
<p>If you remember, we have already installed Superset along with Druid in the previous sections. Now we need to learn how to use Superset to build rich visualizations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Accessing the Superset application</h1>
                
            
            
                
<p>Open <kbd>http://&lt;SERVER-IP&gt;:9088/</kbd> in your web browser. If everything is running fine, we will see a login screen like this:</p>
<div><img src="img/cb4e68e7-e99a-4f3d-956f-ac74a506fe67.png" style="width:42.33em;height:23.42em;"/></div>
<p>Enter <kbd>admin</kbd> as the username and the password as chosen during the installation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Superset dashboards</h1>
                
            
            
                
<p>Dashboards are important pieces of the Superset application. They let us showcase the results of the analytics computation in a graphical form. Dashboards are created from Slices, which in turn are built from the various data sources configured in the Superset application.</p>
<p>After successful login, there won’t be any dashboards created automatically for us. We will see a blank list of dashboards, like this:</p>
<div><img src="img/1cfcf69f-da70-4dc1-a4ff-b74154a0cfdc.png"/></div>
<p>In order to build dashboards, we first need to configure the data sources. So, let's click on the Sources menu from the top navigation and click on Refresh Druid Metadata:</p>
<div><img src="img/2ca5bc92-fdeb-48ec-90cb-1e1b41350655.png"/></div>
<p>After this step, we are taken to the data sources page and a new data source automatically appears here. Remember we uploaded this dataset to Druid before?</p>
<div><img src="img/186e1e1f-b1d8-4348-9b89-d65bc8e27621.png"/></div>
<p>Now we can click on the data source name (in green), which will take us to the data source exploration page:</p>
<div><img src="img/721f4f3e-09d5-4153-aa95-0e04aec73401.png"/></div>
<p>As we can see, this page is divided into multiple sections.</p>
<ul>
<li style="font-weight: 400"><strong>Left Side UI</strong>:
<ul style="padding-left: 1px">
<li style="font-weight: 400"><strong>Datasource and Chart Type</strong>: In this column, we can select the data source that we need to use and also the type of graphic we want to see on the right.</li>
<li style="font-weight: 400"><strong>Time</strong>: This is the column where we can restrict the data from the data source to a given time range. Beginners tend to make a mistake with this column as they won’t see any data on the right side. So, choose a start time value (a relative value like 100 years ago is recommended for better results).</li>
<li style="font-weight: 400"><strong>Group By</strong>: This column is used to group data based on the dimensions of the input data.</li>
<li style="font-weight: 400"><strong>Other Options</strong>: There are other options that are available below Group By, which we will explore in the coming steps.</li>
</ul>
</li>
<li style="font-weight: 400"><strong>Right Side UI</strong>:
<ul style="padding-left: 1px">
<li style="font-weight: 400">This UI contains the results of the options that we have selected on the left-hand side.</li>
</ul>
</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding Wikipedia edits data</h1>
                
            
            
                
<p>Before we jump into building visualizations. Let's take a closer look at the data we have ingested into Druid and what types of graphics we can render from that data:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Metric/Dimension</strong></p>
</td>
<td>
<p><strong>Datatype</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>delta</kbd></p>
</td>
<td>
<p><kbd>LONG</kbd></p>
</td>
<td>
<p>Change represented in numeric form</p>
</td>
</tr>
<tr>
<td>
<p><kbd>deleted</kbd></p>
</td>
<td>
<p><kbd>LONG</kbd></p>
</td>
<td>
<p>Deleted data from the article in numeric form</p>
</td>
</tr>
<tr>
<td>
<p><kbd>added</kbd></p>
</td>
<td>
<p><kbd>LONG</kbd></p>
</td>
<td>
<p>Added data, measured in numeric form</p>
</td>
</tr>
<tr>
<td>
<p><kbd>isMinor</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Boolean, indicating whether this is a minor edit or not</p>
</td>
</tr>
<tr>
<td>
<p><kbd>page</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>The page where the change has happened in Wikipedia</p>
</td>
</tr>
<tr>
<td>
<p><kbd>isRobot</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Is the change done by a robot (not a human but some form of program)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>channel</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Wikipedia channel where the change has happened</p>
</td>
</tr>
<tr>
<td>
<p><kbd>regionName</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Geographical region name from which the change has been done</p>
</td>
</tr>
<tr>
<td>
<p><kbd>cityName</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>City name from which the change has been done</p>
</td>
</tr>
<tr>
<td>
<p><kbd>countryIsoCode</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>ISO code of the country from which the change has been done</p>
</td>
</tr>
<tr>
<td>
<p><kbd>user</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Wikipedia user or IP address that has made the change</p>
</td>
</tr>
<tr>
<td>
<p><kbd>countryName</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Name of the country from which the change has been made</p>
</td>
</tr>
<tr>
<td>
<p><kbd>isAnonymous</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Has the change been done by a anonymous user (not logged-in state)?</p>
</td>
</tr>
<tr>
<td>
<p><kbd>regionIsoCode</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>ISO code of the geographical region from which the change has been done</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metroCode</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>This is similar to ZIP code in the United States (see <a href="http://www.nlsinfo.org/usersvc/NLSY97/NLSY97Rnd9geocodeCodebookSupplement/gatt101.html" target="_blank">http://www.nlsinfo.org/usersvc/NLSY97/NLSY97Rnd9geocodeCodebookSupplement/gatt101.html</a>)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>namespace</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Wikipedia article/page namespace</p>
</td>
</tr>
<tr>
<td>
<p><kbd>comment</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p>Comment that was added for this change</p>
</td>
</tr>
<tr>
<td>
<p><kbd>isNew</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p><kbd>true</kbd> if this is a new page (see <a href="https://en.wikipedia.org/wiki/Wikipedia:Glossary#N" target="_blank">https://en.wikipedia.org/wiki/Wikipedia:Glossary#N</a>)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>isUnpatrolled</kbd></p>
</td>
<td>
<p><kbd>STRING</kbd></p>
</td>
<td>
<p><kbd>true</kbd> if the change is not a patrolled one (see <a href="https://en.wikipedia.org/wiki/Wikipedia:New_pages_patrol" target="_blank">https://en.wikipedia.org/wiki/Wikipedia:New_pages_patrol</a>)</p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>So, we have listed all the attributes of the data. Let's take a look at the sample one to get a better understanding of what we are talking about:</p>
<pre>{<br/>  "time": "2015-09-12T00:47:21.578Z",<br/>  "channel": "#en.wikipedia",<br/>  "cityName": null,<br/>  "comment": "Copying assessment table to wiki",<br/>  "countryIsoCode": null,<br/>  "countryName": null,<br/>  "isAnonymous": false,<br/>  "isMinor": false,<br/>  "isNew": false,<br/>  "isRobot": true,<br/>  "isUnpatrolled": false,<br/>  "metroCode": null,<br/>  "namespace": "User",<br/>  "page": "User:WP 1.0 bot/Tables/Project/Pubs",<br/>  "regionIsoCode": null,<br/>  "regionName": null,<br/>  "user": "WP 1.0 bot",<br/>  "delta": 121,<br/>  "added": 121,<br/>  "deleted": 0<br/>}</pre>
<p>Once we have some understanding of the data dimensions, we need to see what types of questions we can answer from this data. These questions are the insights that are readily available to us. Later, we can represent these in the graphical form that best suits us.</p>
<p>So let's see some of the questions we can answer from this data.</p>
<p><strong>Uni-dimensional insights</strong>:</p>
<ul>
<li style="font-weight: 400">Which are the cities from which changes were made?</li>
<li style="font-weight: 400">Which pages were changed?</li>
<li style="font-weight: 400">Which are the countries from which changes were made?</li>
<li style="font-weight: 400">How many new pages were created?</li>
</ul>
<p><strong>Counts along the dimension</strong>:</p>
<ul>
<li style="font-weight: 400">How many changes were made from each city?</li>
<li style="font-weight: 400">Which are the top cities from which changes were made?</li>
<li style="font-weight: 400">Which are the top users who have contributed to the changes?</li>
<li style="font-weight: 400">What are the namespaces that were changed frequently?</li>
</ul>
<p><strong>Multi-dimensional insights</strong>:</p>
<ul>
<li style="font-weight: 400">How many changes were made between 9.00 am to 10.00 am across all countries?</li>
<li style="font-weight: 400">What are the wall clock hours when the edits are made by robots?</li>
<li style="font-weight: 400">Which country has the most origin of changes that are targeted by robots and at what times?</li>
</ul>
<p>Looks interesting, right? Why don't we try to use Apache Superset to create a dashboard with some of these insights?</p>
<p>In order to do this we need to follow this simple workflow in the Superset application:</p>
<ol>
<li style="font-weight: 400">Data sources:
<ul style="padding-left: 1px">
<li style="font-weight: 400">Define new data sources from supported databases</li>
<li style="font-weight: 400">Refresh the Apache Druid data sources</li>
</ul>
</li>
<li style="font-weight: 400">Create Slices</li>
<li style="font-weight: 400">Use the Slices to make a dashboard</li>
</ol>
<p>If we recollect, we have already done <em>Step 1</em> in previous sections. So, we can go right away to the second and third steps.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Create Superset Slices using Wikipedia data</h1>
                
            
            
                
<p>Let's see what types of graphics we can generate using the Slices feature in the Superset application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Unique users count</h1>
                
            
            
                
<p>In this Slice, we see how to generate a graphic to find unique users who have contributed to the edits in the dataset.</p>
<p>First, we need to go to the Slices page from the top navigation. After this, the screen looks like this:</p>
<div><img src="img/178cfd0e-a1c9-4260-80b8-b8e5e6b76a58.png" style="width:42.08em;height:20.83em;"/></div>
<p>From this page, click on the plus icon (+) to add a new slice:</p>
<div><img src="img/35366e2f-997f-4366-84b8-3755677d28fc.png" style="width:16.83em;height:7.67em;"/></div>
<p>After this, we see a list of data sources that are configured in the system. We have to click on the data source name:</p>
<div><img src="img/9667e719-7831-48fc-9d87-4667320f76de.png" style="width:24.83em;height:10.92em;"/></div>
<p>After we click on wikiticker, we are taken to the visualization page, where we define the dimensions that we want to render as a graphic.</p>
<p>For the current use case, let's choose the following options from the UI:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>UI Location</strong></p>
</td>
<td>
<p><strong>Graphic</strong></p>
</td>
<td>
<p><strong>Explanation</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sidebar</p>
</td>
<td><img src="img/dfd162c5-4692-4ee4-865a-b17419505a33.png" style="width:9.17em;height:16.75em;"/></td>
<td>
<p>Choose the <strong>Datasource</strong> as [druid-ambari].[wikiticker] and the graphic type as Big Number<em>. </em>In the Time section, choose the value for since as 5 years ago and leave the rest of the values to their defaults. In the Metric section. Choose COUNT(DISTINCT user_unique) from the autocomplete. In the Subheader Section, add Unique User Count, which is displayed on the screen. After this, click on the Query button at the top.</p>
</td>
</tr>
<tr>
<td>
<p>Graphic Output</p>
</td>
<td><img src="img/800cad03-8066-4307-9252-a685b878591e.png" style="width:6.92em;height:4.08em;"/></td>
<td>
<p>We see the result of the query in this graphic.</p>
</td>
</tr>
<tr>
<td>
<p>Save Slice</p>
</td>
<td><img src="img/798a6ed3-139e-4199-9f06-951316752e77.png" style="width:8.58em;height:5.33em;"/></td>
<td>
<p>Clicking on the Save As button on top will show a pop-up window like this, where we need to add the corresponding values. Save the slice as <kbd>Unique Users</kbd> and add it to a new dashboard with the name <kbd>My Dashboard 1</kbd>.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Sounds so simple, right? Let's not hurry to see the dashboard yet. Let's create some more analytics from the data in the coming sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Word Cloud for top US regions</h1>
                
            
            
                
<p>In this section, we will learn how to build a word cloud for the top US regions that have contributed to the Wikipedia edits in the datasource we have in Druid. We can continue editing the same Slice from the previous section or go to a blank Slice, as mentioned in the previous section.</p>
<p>Let's concentrate on the values that we need to select for generating a word cloud:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>UI Location</strong></p>
</td>
<td>
<p><strong>Graphic</strong></p>
</td>
<td>
<p><strong>Explanation</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sidebar</p>
</td>
<td><img src="img/0ad3f384-ff61-47a6-aa8e-db2a4eae0ef6.png" style="width:10.83em;height:13.00em;"/></td>
<td>
<p>Choose the Datasource as [druid-ambari].[wikiticker] and the graphic type as Word Cloud. In the Time section, choose the value for Since as 5 years ago and leave the rest of the values to their defaults.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/4c1ceee1-5973-450a-972b-4988d1920fc8.png" style="width:10.92em;height:9.92em;"/></td>
<td>
<p>In the Series section. Choose the regionName from the dropdown. In Metric, choose COUNT(*), which is the total edit count.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/c8882e1f-fd08-4647-b59d-91b4b4463747.png" style="width:10.50em;height:8.42em;"/></td>
<td>
<p>In the Filters section, choose countryIsoCode; it should be in US. Add another filter to select only valid regions (skip null codes). Add the values as shown here in the graphic.</p>
</td>
</tr>
<tr>
<td>
<p>Graphic Output</p>
</td>
<td><img src="img/92a36d97-f891-40f6-b609-7846c7ede83b.png" style="width:9.83em;height:6.50em;"/></td>
<td>
<p>After clicking on Query, we see this beautiful word cloud.</p>
</td>
</tr>
<tr>
<td>
<p>Save Slice</p>
</td>
<td><img src="img/690e8348-71ec-4532-9bf3-586060cd9bbf.png" style="width:11.25em;height:7.75em;"/></td>
<td>
<p>Clicking on the Save As button at the top will show a pop-up window like this, where we need to add the corresponding values. Save the Slice as <kbd>Word Cloud - Top US Regions</kbd> and add it to a new dashboard named <kbd>My Dashboard 1</kbd><em>.</em></p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>The significance of the word cloud is that we can see the top words according to their relative sizes. This type of visualization is helpful when there are fewer words for which we want to see the relative significance.</p>
<p>Let's try to generate another graphic from the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sunburst chart – top 10 cities</h1>
                
            
            
                
<p>In this section, we will learn about a different type of chart that we have not seen so far in this chapter. But first, let's put forward the use case.</p>
<p>We want to find the unique users per channel, city name, and namespace at all three levels; that is, the graphic should be able to show us the:</p>
<ul>
<li style="font-weight: 400">Unique users per channel</li>
<li style="font-weight: 400">Unique users per channel/city name</li>
<li style="font-weight: 400">Unique users per channel/city name/namespace</li>
</ul>
<p>In order to show this kind of hierarchical data, we can use a sunburst chart.</p>
<p>Let's check out what type of values we need to select to render this type of chart:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>UI Location</strong></p>
</td>
<td>
<p><strong>Graphic</strong></p>
</td>
<td>
<p><strong>Explanation</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sidebar</p>
</td>
<td><img src="img/0329cb06-5945-445b-8893-b5e9f2f69255.png" style="width:7.83em;height:9.42em;"/></td>
<td>
<p>Choose the Datasource as [druid-ambari].[wikiticker] and the graphic type as Sunburst. In the Time section, choose the value for Since as 5 years ago and leave the rest of the values to their defaults.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/c4970db4-b189-470b-9e03-fa0488452a60.png" style="width:12.92em;height:9.75em;"/></td>
<td>
<p>In the Hierarchy section, choose the <kbd>channel</kbd>, <kbd>cityName</kbd>, and <kbd>namespace</kbd> from the dropdown. In the Primary Metric and Secondary Metric, choose COUNT(DISTINCT user_unique), which is the total user count.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/11a6a4bc-0a09-4e22-b301-2d32bab56027.png" style="width:12.00em;height:6.67em;"/></td>
<td>
<p>In the <strong>Filters</strong> section, choose cityName and add the not null condition using regex matching</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/e6b4b21e-6e88-43fa-a7b8-d49c731e309e.png" style="width:11.58em;height:7.92em;"/></td>
<td>
<p>Clicking on the Save As button at the top will show a pop-up window like this. We need to add the corresponding values here. Save the Slice as <kbd>Sunburst - Top 10 Cities</kbd> and add it to a new dashboard named <kbd>My Dashboard 1</kbd><em>.</em></p>
</td>
</tr>
<tr>
<td>
<p>Graphic Output</p>
</td>
<td><img src="img/fa393586-c43c-442b-8b17-0a69a61a1835.png" style="width:7.17em;height:4.92em;"/></td>
<td>
<p>After clicking on Query, we see this beautiful graphic.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>As we can see there are three concentric rings in the graphic:</p>
<ul>
<li style="font-weight: 400">The innermost ring is the <kbd>channel</kbd> dimension</li>
<li style="font-weight: 400">The middle ring shows the <kbd>cityName</kbd> dimension</li>
<li style="font-weight: 400">The outermost ring is the <kbd>namespace</kbd> dimension</li>
</ul>
<p>When we hover over the innermost ring, we can see how it spreads out into the outermost circles. The same thing happens with the other rings as well.</p>
<p>This type of graphic is very helpful when we want to do funnel analysis on our data. Let's take a look at another type of analysis in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Top 50 channels and namespaces via directed force layout</h1>
                
            
            
                
<p><strong>Directed force layout</strong> (<strong>DFL</strong>) is a network layout with points that are interconnected to each other. Since it's a force layout, we can see the points moving on the screen as <kbd>d3.js</kbd> applies the physics engine.</p>
<p>In this network graph, we want to understand the connectivity between the namespace and channel over the unique users count metric. Since this is a network graph, we will see the nodes getting repeated in different paths.</p>
<p>Let's see how we can arrive at this graph:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>UI Location</strong></p>
</td>
<td>
<p><strong>Graphic</strong></p>
</td>
<td>
<p><strong>Explanation</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sidebar</p>
</td>
<td><img src="img/d0bacbf9-fbe8-4b9a-868d-afb3fead3ed0.png" style="width:10.50em;height:12.58em;"/></td>
<td>
<p>Choose the Datasource as [druid-ambari].[wikiticker] and the Graphic type as Directed Force Layout<em>. </em>In the Time section, choose the value for since as 5 years ago and leave the rest of the values to their defaults.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/e5b3cdcf-eebf-45d9-a9b0-89d3ed63f1e8.png" style="width:8.42em;height:5.00em;"/></td>
<td>
<p>In the Source / Target section, choose the <kbd>channel</kbd> and <kbd>namespace</kbd> from the dropdown. In the Metric section, choose COUNT(DISTINCT user_unique) which is the total user count. We keep the Row limit at 50 so that we will see only the top 50<em>.</em></p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/540474de-0c35-4f68-b0f7-3004dfc034c1.png" style="width:11.08em;height:7.58em;"/></td>
<td>
<p>Clicking on the Save As button at the top will show a pop up window like this, where we need to add the corresponding values. Save the Slice as <kbd>DFL - Top 50 Channels &amp; Namespaces</kbd>. Add it to a new dashboard with the name <kbd>My Dashboard 1</kbd>.</p>
</td>
</tr>
<tr>
<td>
<p>Graphic Output</p>
</td>
<td><img src="img/a2c1210d-ffd7-48ac-896f-a7f1ed5dc218.png" style="width:10.33em;height:6.92em;"/></td>
<td>
<p>After clicking on Query, we see this beautiful graphic.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>Feel free to drag the nodes in the graphic to learn more about how they are interconnected to each other. The size of the nodes indicates the unique user count and its breakdown (similar to a sunburst chart).</p>
<p>Let's spend some time learning another visualization and business use case in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Top 25 countries/channels distribution</h1>
                
            
            
                
<p>Now we will learn the Sankey chart, a waterfall-like way of representing of breakdown and interconnectivity between data. In this case, we want to find out how the channelName and countryName dimensions are related when it comes to the unique users metric:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>UI Location</strong></p>
</td>
<td>
<p><strong>Graphic</strong></p>
</td>
<td>
<p><strong>Explanation</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sidebar</p>
</td>
<td><img src="img/b7bc7cbf-4167-49e8-b6e4-758a9813b6e1.png" style="width:11.92em;height:14.33em;"/></td>
<td>
<p>Choose the <strong>Datasource</strong> as [druid-ambari].[wikiticker] and the <strong>Graphic Type</strong> as Sankey.<em> </em>In the <strong>Time</strong> section, choose the value for Since as 5 years ago and leave the rest as default.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/c11d0b06-6547-4392-b8d3-a1aa54e98eb0.png" style="width:12.33em;height:7.42em;"/></td>
<td>
<p>In the <strong>Source / Target</strong> section, choose <kbd>channel</kbd> and <kbd>countryName</kbd> from the drop-down. In the Metric, choose COUNT(*), which is the total edit count. Keep the row limit at 25; so we will see only the top 25 items<em>.</em></p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/1eaf35b3-357d-4876-a6b9-cb61601ac660.png" style="width:9.50em;height:5.33em;"/></td>
<td>
<p>In the <strong>Filters</strong> section, choose countryName and enable the regex filter so as to choose only those records that have a valid country name.</p>
</td>
</tr>
<tr>
<td/>
<td><img src="img/42a3d617-7a02-4545-8ef9-674f23fce62b.png" style="width:12.33em;height:7.58em;"/></td>
<td>
<p>Clicking on the <strong>Save As</strong> button at the top will show a pop-up window. We need to add the corresponding values here. Save the Slice as <kbd>Top 25 Countries/Channels Distribution</kbd> and add it to a new dashboard with the name <kbd>My Dashboard 1</kbd>.</p>
</td>
</tr>
<tr>
<td>
<p>Graphic Output</p>
</td>
<td><img src="img/0efcc190-641e-49dc-946c-0bddf1cba3fe.png" style="width:11.75em;height:7.92em;"/></td>
<td>
<p>After clicking on Query, we see this beautiful graphic.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p>This completes the list of all the analytics that we can generate so far. Now in the next section, we will see how to use this in the dashboard (which was our original goal anyway).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating wikipedia edits dashboard from Slices</h1>
                
            
            
                
<p>So far we have seen how to create slices in the Apache Superset application for the wikipedia edits data, that is stored in the Apache Druid database. Its now time for us to see how to create a dashboard so that we can share it with the Business Teams or any other teams for which we want to share the insights.</p>
<p>In this process, the first step would be to click on the Dashboard menu on the top navigation bar. Which will take us to Add New Dashboard Page, where we need to fill the following details.</p>
<table>
<tbody>
<tr>
<td><strong>Element</strong></td>
<td><strong>Description</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td>Title</td>
<td>This is the name of the dashboard that we want to create</td>
<td>My Dashboard 1</td>
</tr>
<tr>
<td>Slug</td>
<td>Short alias for the dashboard</td>
<td>dash1</td>
</tr>
<tr>
<td>Slices</td>
<td>List of Slices that we want to add to the dashboard.</td>
<td>
<ol>
<li>Sunburst - Top 10 Cities</li>
<li>DFL - Top 50 Channels &amp; Namespaces</li>
<li>Top 25 Countries / Channels Contribution</li>
<li>Word Cloud - Top US Regions</li>
<li>Unique Users</li>
</ol>
</td>
</tr>
<tr>
<td>Other Fields</td>
<td>We can leave the other fields as empty as they are not mandatory to create the dashboard</td>
<td/>
</tr>
</tbody>
</table>
<p>Here is the graphic for this page:</p>
<div><img src="img/91059ba0-71e3-47ab-adf9-6752f0d7b4a4.png" style="width:41.17em;height:23.92em;"/></div>
<p>Click on Save button at the bottom of the screen once the changes look good.</p>
<p>This will take us to the next step where we can see that the dashboard is successfully created:</p>
<div><img src="img/33f7bb5a-fc17-4a85-977e-bc84f3c69250.png" style="width:45.25em;height:21.67em;"/></div>
<p>We can see the My Dashboard 1 in the list of dashboards. In order to access this dashboard click on it, Where we are taken to the dashboard screen:</p>
<div><img src="img/4f258abd-6893-4f4d-9e83-bfa0e4cab0eb.png"/></div>
<p>As we can see we have a very powerful way of representing all the raw data. This will definitely have an impact on the end users in making sure that the message is conveyed.</p>
<p>So far we have learned how to create slices and Dashboards from the data that is stored in the Apache Druid Columnar Database. In the next section we will see how to connect to RDBMS and generate slices and dashboards from that data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache Superset with RDBMS</h1>
                
            
            
                
<p>Apache Superset is built using Python programming language and supports many relational databases as it uses SQLAlchemy as the database driver. The installation of these drivers are out of scope in this section. But, it should be very easy to install those. Most of the time the Operating system vendors package them for us. So, we need not worry about the manual installation of these.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Supported databases</h1>
                
            
            
                
<p>Here are some of the database that are supported by Apache Superset:</p>
<table>
<tbody>
<tr>
<td><strong>Database Name</strong></td>
<td><strong>Python Package Name</strong></td>
<td><strong>Driver URI Prefix</strong></td>
<td><strong>Details</strong></td>
</tr>
<tr>
<td>MySQL</td>
<td><kbd>mysqlclient</kbd></td>
<td><kbd>mysql://</kbd></td>
<td>Oracle MySQL Database</td>
</tr>
<tr>
<td>PostgreSQL</td>
<td><kbd>psycopg2</kbd></td>
<td><kbd>postgresql+psycopg2://</kbd></td>
<td>The worlds most advanced opensource database</td>
</tr>
<tr>
<td>Presto</td>
<td><kbd>pyhive</kbd></td>
<td><kbd>presto://</kbd></td>
<td>Opensource distributed query Engine</td>
</tr>
<tr>
<td>Oracle</td>
<td><kbd>cx_Oracle</kbd></td>
<td><kbd>oracle://</kbd></td>
<td>Multi-model Database management system created by Oracle Corporation.</td>
</tr>
<tr>
<td>Sqlite</td>
<td/>
<td><kbd>sqlite://</kbd></td>
<td>Fast, Scalable Embedded Database Library</td>
</tr>
<tr>
<td>Redshift</td>
<td><kbd>sqlalchemy-redshift</kbd></td>
<td><kbd>postgresql+psycopg2://</kbd></td>
<td>Amazon Redshift is Columnar database built on PostgreSQL</td>
</tr>
<tr>
<td>MSSQL</td>
<td><kbd>pymssql</kbd></td>
<td><kbd>mssql://</kbd></td>
<td>Microsoft SQL Server</td>
</tr>
<tr>
<td>Impala</td>
<td><kbd>impyla</kbd></td>
<td><kbd>impala://</kbd></td>
<td>Apache Impala is Massively Parallel Processing SQL Engine that runs on Hadoop</td>
</tr>
<tr>
<td>SparkSQL</td>
<td><kbd>pyhive</kbd></td>
<td><kbd>jdbc+hive://</kbd></td>
<td>Apache Spark Module for writing SQL in Spark Programs.</td>
</tr>
<tr>
<td>Greenplum</td>
<td><kbd>psycopg2</kbd></td>
<td><kbd>postgresql+psycopg2://</kbd></td>
<td>Greenplum is advanced , fully featured opensource data platform</td>
</tr>
<tr>
<td>Athena</td>
<td><kbd>PyAthenaJDBC</kbd></td>
<td><kbd>awsathena+jdbc://</kbd></td>
<td>Amazon Athena is Serverless Interactive Query Service</td>
</tr>
<tr>
<td>Vertica</td>
<td><kbd>sqlalchemy-vertica-python</kbd></td>
<td><kbd>vertica+vertica_python://</kbd></td>
<td>Vertica is Bigdata analytics software</td>
</tr>
<tr>
<td>ClickHouse</td>
<td><kbd>sqlalchemy-clickhouse</kbd></td>
<td><kbd>clickhouse://</kbd></td>
<td>
<p class="mce-root">Opensource distributed, columnar datastore</p>
</td>
</tr>
</tbody>
</table>
<p>Portions of the above table is extracted from the official documentation of Apache Superset (<a href="https://superset.incubator.apache.org/installation.html#database-dependencies" target="_blank">https://superset.incubator.apache.org/installation.html#database-dependencies</a>)</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding employee database</h1>
                
            
            
                
<p>If you remember, in the previous sections we have imported a sample database called Employees and loaded it into the MySQL Database. We will dig further into this sample datastore so that we will learn what types of analytics we can generate from this.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Employees table</h1>
                
            
            
                
<p>The <kbd>employees</kbd> table contains details of Employees (randomly generated data) with the following properties</p>
<table>
<tbody>
<tr>
<td><strong>Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INTEGER</kbd></td>
<td>Employee Number</td>
</tr>
<tr>
<td><kbd>birth_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Employee Date Of Birth</td>
</tr>
<tr>
<td><kbd>first_name</kbd></td>
<td><kbd>STRING</kbd></td>
<td>First Name of Employee</td>
</tr>
<tr>
<td><kbd>last_name</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Last Name of Employee</td>
</tr>
<tr>
<td><kbd>gender</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Gender of Employee, M if Male, F if Female</td>
</tr>
<tr>
<td><kbd>hire_date</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Latest Joining date of Employee</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Departments table</h1>
                
            
            
                
<p>The <kbd>departments</kbd> table consists of basic details of every department in the organisation. This is further understood with this table:</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>dept_no</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Department Number</td>
</tr>
<tr>
<td><kbd>dept_name</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Department Name</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Department manager table</h1>
                
            
            
                
<p>The <kbd>dept_manager</kbd> table has records about Employee acting as manager for a given department. More details are in this table:</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Dataype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INT</kbd></td>
<td>Employee ID who is acting as manager for this department</td>
</tr>
<tr>
<td><kbd>dept_no</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Department ID</td>
</tr>
<tr>
<td><kbd>from_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Starting date from which Employee is acting as Manager for this department.</td>
</tr>
<tr>
<td><kbd>to_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Ending date till where the Employee has acted as Manager for this department.</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Department Employees Table</h1>
                
            
            
                
<p>The <kbd>dept_emp</kbd> table consists of all the records which show how long each employee belonged to a department.</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INT</kbd></td>
<td>Employee ID</td>
</tr>
<tr>
<td><kbd>dept_no</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Department ID</td>
</tr>
<tr>
<td><kbd>from_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Starting date from which employee belongs to this department</td>
</tr>
<tr>
<td><kbd>to_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Last date of employee in this department</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Titles table</h1>
                
            
            
                
<p>The <strong>titles</strong> table consists of all the roles of employees from a given date to end date. More details are shown as follows:</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INT</kbd></td>
<td>Employee Id</td>
</tr>
<tr>
<td><kbd>title</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Designation of the employee</td>
</tr>
<tr>
<td><kbd>from_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Starting date from which employee has assumed this role</td>
</tr>
<tr>
<td><kbd>to_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Last date where the employee has performed this role</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Salaries table</h1>
                
            
            
                
<p>The <kbd>salaries</kbd> table consists of salary history of a given employee. More details are explained in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INT</kbd></td>
<td>Employee Id</td>
</tr>
<tr>
<td><kbd>salary</kbd></td>
<td><kbd>INT</kbd></td>
<td>Salary of Employee</td>
</tr>
<tr>
<td><kbd>from_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Starting day for which salary is calculated</td>
</tr>
<tr>
<td><kbd>to_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Last day for which salary is calculated.</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Normalized employees table</h1>
                
            
            
                
<p>The <kbd>employee_norm</kbd> table consists of data from employees, salaries, departments, <kbd>dept_emp</kbd> and titles table. Lets look at this table in detail:</p>
<table>
<tbody>
<tr>
<td><strong>Table Column</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>emp_no</kbd></td>
<td><kbd>INT</kbd></td>
<td>Employee ID</td>
</tr>
<tr>
<td><kbd>birth_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Date of Birth of Employee</td>
</tr>
<tr>
<td><kbd>full_name</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Employee Full Name</td>
</tr>
<tr>
<td><kbd>gender</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Gender of Employee</td>
</tr>
<tr>
<td><kbd>hire_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Joining date of Employee</td>
</tr>
<tr>
<td><kbd>salary</kbd></td>
<td><kbd>INT</kbd></td>
<td>Salary of Employee for the period</td>
</tr>
<tr>
<td><kbd>from_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Salary period start</td>
</tr>
<tr>
<td><kbd>to_date</kbd></td>
<td><kbd>DATE</kbd></td>
<td>Salary period end</td>
</tr>
<tr>
<td><kbd>dept_name</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Department where the employee is working during this salary period</td>
</tr>
<tr>
<td><kbd>title</kbd></td>
<td><kbd>STRING</kbd></td>
<td>Designation of the employee during this time period</td>
</tr>
</tbody>
</table>
<p> </p>
<p>With this knowledge of various tables in the Employee database we now have some understanding of the data we have so far. Now, the next task is to find out what types of analytics we can generate from this data. We will learn this in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Superset Slices for employees database</h1>
                
            
            
                
<p>Once we have some basic understanding of the type of data that is stored in the MySQL database. We will now see what types of we can answer from this data.</p>
<p><strong>Uni-dimensional insights:</strong></p>
<ul>
<li>How many employees are there in the organisation?</li>
<li>What is the total salary paid for all employees in the organisation?</li>
<li>How many departments are there?</li>
</ul>
<p><strong>Multi dimensional insights</strong></p>
<ul>
<li>What is the total salary paid for every year?</li>
<li>What is the total salary per department?</li>
<li>Who is the top paid employee for every year?</li>
</ul>
<p>If we think along these lines we should be able to answer very important questions regarding the data and should be able generate nice graphics.</p>
<p>Lets take few examples of what types of visualisations we can generate in the coming sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Register MySQL database/table</h1>
                
            
            
                
<p>Before we start generating Slices for the employee tables, We should first register it. The registration process includes the following steps.</p>
<p>Open the Databases by clicking on the Databases dropdown from the Sources menu in the top navigation bar as shown here:</p>
<div><img src="img/3ddfbcc9-a012-48e3-b287-5ba19ec18e8c.png" style="width:15.58em;height:13.50em;"/></div>
<p>After this we need to click on the plus (+) icon from the page:</p>
<div><img src="img/cac387f0-9a8d-4256-ac05-21b6ec2616b1.png" style="width:23.50em;height:11.75em;"/></div>
<p>This will take us to a page where we can register the new database. The screen looks like this:</p>
<div><img src="img/32fa6696-267f-43c4-af46-86e6b86b2745.png"/></div>
<p>We will fill the following details as shown here.</p>
<table>
<tbody>
<tr>
<td><strong>Field Name</strong></td>
<td><strong>Value</strong></td>
<td><strong>Description</strong></td>
<td/>
</tr>
<tr>
<td>Database</td>
<td><kbd>employees</kbd></td>
<td>Name of the database that we want to register. (Enter the same name as its in the MySQL Database)</td>
<td/>
</tr>
<tr>
<td>SQLAlchemy URI</td>
<td><kbd>mysql+pymysql://superset:superset@master:3306/employees</kbd></td>
<td>URI to access this database programatically. This will include the protocol/driver, username, password, hostname &amp; dbname</td>
<td/>
</tr>
<tr>
<td>Other Fields</td>
<td/>
<td>Keep them as default</td>
<td/>
</tr>
</tbody>
</table>
<p> </p>
<p>After this click on Save Button, which will save the database details with Apache Superset. We are taken to the list of tables page which looks like this:</p>
<div><img src="img/96399df1-4b77-4313-84ed-669d011ddb85.png"/></div>
<p>As we can see, we have the employees database registered with MySQL backend.</p>
<p>In the next step we need to chose the tables from the top menu:</p>
<div><img src="img/d624d6e5-a390-494e-9a63-9213dff650b5.png" style="width:16.42em;height:14.17em;"/></div>
<p>Since we do not have any tables registered, we will see a empty page like this:</p>
<div><img src="img/55176447-b183-47af-b93c-c208d6ed065f.png"/></div>
<p>In order to register a new table we have to click on the plus (icon) in the UI, Which takes us to the following page:</p>
<div><img src="img/a54b67c3-3d25-4ca9-8727-71335bf1a2d2.png"/></div>
<p>Enter the values for the fields as shown below and click Save once done:</p>
<table>
<tbody>
<tr>
<td><strong>Field Name</strong></td>
<td><strong>Value</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Table name</td>
<td><kbd>employee_norm</kbd></td>
<td>Name of the table that we want to register.</td>
</tr>
<tr>
<td>Database</td>
<td><kbd>employees</kbd></td>
<td>Select the database that is already registered with Superset.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now we can see that the table is successfully registered as shown in the screen in the following screenshot:</p>
<div><img src="img/81790eb0-b29e-4aa1-9927-35e994db7b02.png"/></div>
<p>One of the important features of Superset is that it will automatically select the different types of operations that we can perform on the columns of the table according to the datatype. This drives what types of dimensions, metrics we are shown in the rest of the UI.</p>
<p>In order to select these options, we need to edit the table by clicking on the edit icon and we are shown this page:</p>
<div><img src="img/793aa239-22f3-4b32-8810-9b5bd6c9e791.png"/></div>
<p>As we can see, Apache Superset has automatically recognized the datatype of each and every field and it also provided us with an option to chose these dimensions for various activities. These activities are listed in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Activity</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Groupable</td>
<td>If the checkbox is selected, then the field can be used as part of Grouping operations (<kbd>GROUP BY</kbd> in SQL).</td>
</tr>
<tr>
<td>Filterable</td>
<td>If the checkbox is selected, then the field can be used as part of Conditional operations (<kbd>WHERE</kbd> clause).</td>
</tr>
<tr>
<td>Count Distinct</td>
<td>If the checkbox is selected, then the field can be used as part of count (<kbd>DISTINCT</kbd>) operation on the field.</td>
</tr>
<tr>
<td>Sum</td>
<td>If the checkbox is selected, then the field can be used as part of <kbd>SUM()</kbd> function.</td>
</tr>
<tr>
<td>Min/Max</td>
<td>Indicates that the field can be used as part of finding minimum and maximum value.</td>
</tr>
<tr>
<td>Is Temporal</td>
<td>Indicates the field is a time dimension.</td>
</tr>
</tbody>
</table>
<p>Make changes as shown above and click on Save button.</p>
<p>Now we are ready to start creating slices and dashboard in the next steps.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Slices and Dashboard creation</h1>
                
            
            
                
<p>As we have seen in the previous sections, In order to create Dashboards we first need to create slices. In this section we will learn to create few slices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Department salary breakup</h1>
                
            
            
                
<p>In this slice we will learn how to create a visualization that will show the percentage of salary breakdown per department:</p>
<table>
<tbody>
<tr>
<td><strong>UI Location</strong></td>
<td><strong>Graphic </strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Sidebar</td>
<td><img src="img/767c4c8e-67d2-4cb9-a705-2d344b10d236.png" style="width:10.92em;height:17.92em;"/></td>
<td>
<p class="mce-root"><strong>Datasource &amp; Chart Type</strong>: select [employees].[employee_norm] as the <strong>datasource</strong> and Distribution - NVD3 - Pie Chart as chart type</p>
<p>In the <strong>Time</strong> section, select birth_date as Time Column and 100 years ago as <strong>Since</strong> column.</p>
<p>In the <strong>Metrics</strong> section, select sum_salary as the value from dropdown and dept_name as <strong>Group By.</strong></p>
<p> </p>
</td>
</tr>
<tr>
<td>Graphic Output</td>
<td><img src="img/665462dd-38a7-469b-a8bd-ecfb036cec2e.png" style="width:10.92em;height:6.08em;"/></td>
<td>
<p class="mce-root">Clicking on Query button will render this good liking chart. Save it with the name Department Salary Breakup.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Just like in the previous section, See how easy it is to create good looking graphic without any programming knowledge.</p>
<p>In the next section we will learn about another type of graphic from the same employees database.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Salary Diversity</h1>
                
            
            
                
<p>This is a important graphic, where we identify how the salary diversity is between genders across the history of organisation. Here we use average salary as a basis for the analysis.</p>
<table>
<tbody>
<tr>
<td><strong>UI Location</strong></td>
<td><strong>Graphic</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Sidebar</td>
<td><img src="img/87174669-baa2-45d8-8df5-c407e6a5d63e.png" style="width:18.42em;height:26.67em;"/></td>
<td><strong>Datasource &amp; Chart Type</strong>: select [employees].[employee_norm] as the datasource and Time Series - line chart as chart type
<p class="mce-root">In the Time section, select birth_date as Time Column &amp; 100 years ago as since column.</p>
<p class="mce-root">In the Metrics section, select avg_salary as the Metric and gender as <kbd>Group By</kbd>.</p>
</td>
</tr>
<tr>
<td>Output</td>
<td><img src="img/c15c7256-324e-4b28-aea1-0c5935ffec73.png" style="width:18.33em;height:10.25em;"/></td>
<td>Graphic showing the average salary per Gender for every Year. Save this with the title <strong>Salary Diversity</strong></td>
</tr>
</tbody>
</table>
<p> </p>
<p>As we can see from the graphic, the salary breakup is even between genders and are very close. There is also a similar increase in the average salary over the period.</p>
<p>In the next section we will learn to generate another type of graphic that will give us different insight into the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Salary Change Per Role Per Year</h1>
                
            
            
                
<p>This is a important statistic where we want to find out how much salary change is there for different Titles in the organisation across years.</p>
<table>
<tbody>
<tr>
<td><strong>UI Location</strong></td>
<td><strong>Graphic</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Sidebar</td>
<td><img src="img/77e45d70-7738-40c3-9598-52ab46387de3.png" style="width:15.33em;height:22.00em;"/></td>
<td>Datasource &amp; Chart Type: select [employees].[employee_norm] as the datasource and Time Series - Percentage Change as chart type<br/>
In the Time section, Select from_date as <strong>Time</strong> column , Year as <strong>Time Granularity</strong> &amp; 100 years ago as <strong>Since</strong> column.
<p class="mce-root">In the <strong>Metrics</strong> Section, select sum_salary as the <strong>Metric</strong> and title as <strong>Group</strong> By.</p>
</td>
</tr>
<tr>
<td>Output</td>
<td><img src="img/e78d9c2b-5723-4430-9238-abfc62336bc1.png" style="width:17.50em;height:9.92em;"/></td>
<td>Clicking on Query, yields us the following graphic. Save this with the name <strong>Salary Change Per Role Per Year</strong>.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>From this graphic we can find out that few roles have very large difference in the total salary within the organisation.</p>
<p>So far we have created three slices, we will create a new dashboard with the slices created so far.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dashboard creation</h1>
                
            
            
                
<p>In this step we will create a new dashboard by going to the dashboards page and clicking on the Add Dashboard icon (as shown in previous sections).</p>
<p>We are presented with the following screen where we select the three slices we have created so far and click Save:</p>
<div><img src="img/66ae3349-1abb-46f8-a9c4-81e993e1b2ef.png"/></div>
<p>Once the dashboard is saved successfully we can see it like this:</p>
<div><img src="img/b410d0cb-ad1e-4d4b-a76e-f519e88db039.png"/></div>
<p>As we can see, Dashboards are very powerful way to express large amounts of data in a simple fashion.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned about data visualization and how it helps the users to receive the required message without any knowledge of the underlying data. We then saw the different ways to visualize our data graphically.</p>
<p>We walked through Hadoop applications such as Apache Druid and Apache Superset that are used to visualize data and learned how to use them with RDBMses such as MySQL. We also saw a sample database to help us understand the application better.</p>
<p>In the next chapter, we will learn how to build our Hadoop cluster on the cloud.</p>


            

            
        
    </body></html>