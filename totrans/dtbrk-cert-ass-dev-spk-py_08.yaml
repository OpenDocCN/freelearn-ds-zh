- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning with Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has gained popularity in recent times. In this chapter, we
    will do a comprehensive exploration of Spark **Machine Learning** (**ML**), a
    powerful framework for scalable ML on Apache Spark. We will delve into the foundational
    concepts of ML and how Spark ML leverages these principles to enable efficient
    and scalable data-driven insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering the ML life cycle with the help of a real-world example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different case studies for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future trends in Spark ML and distributed ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML encompasses diverse methodologies tailored to different data scenarios. We
    will start by learning about different key concepts in ML.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is a field of study that focuses on the development of algorithms and models
    that enable computer systems to learn and make predictions or decisions without
    being explicitly programmed. It is a subset of **artificial intelligence** (**AI**)
    that aims to provide systems with the ability to automatically learn and improve
    from data and experience.
  prefs: []
  type: TYPE_NORMAL
- en: In today’s world, where vast amounts of data are being generated at an unprecedented
    rate, ML plays a critical role in extracting meaningful insights, making accurate
    predictions, and automating decision-making processes. As data grows, machines
    can learn the patterns better, thus making it even easier to gain insights from
    this data. It finds applications in various domains, including finance, healthcare,
    marketing, image and speech recognition, recommendation systems, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: The key concepts of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand ML, it is important to grasp the fundamental concepts that underpin
    its methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data is the foundation of any ML process. It can be structured, semi-structured,
    or unstructured and encompasses various types, such as numerical, categorical,
    text, images, and more. ML algorithms require high-quality, relevant, and representative
    data to learn patterns and make accurate predictions. When dealing with ML problems,
    it is imperative to have data that can answer the question that we’re trying to
    solve. The quality of data used in any analysis or model-building process significantly
    impacts the outcomes and decisions derived from it. Bad or poor-quality data can
    lead to inaccurate, unreliable, or misleading results, ultimately affecting the
    overall performance and credibility of any analysis or model.
  prefs: []
  type: TYPE_NORMAL
- en: An ML model trained on bad data is likely to make inaccurate predictions or
    classifications. For instance, a model trained on incomplete or biased data might
    incorrectly identify loyal customers as potential churners or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Decision-makers relying on flawed or biased analysis derived from bad data might
    implement strategies based on inaccurate insights. For instance, marketing campaigns
    targeting the wrong customer segments due to flawed churn predictions can lead
    to wasted resources and missed opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to make sure that the data we’re using for ML problems is
    representative of the population that we want to build the models for. The other
    thing to note is that data might have some inherent biases in it. It is our responsibility
    to look for them and be aware of them when using this data to build ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Features are the measurable properties or characteristics of the data that the
    ML algorithm uses to make predictions or decisions. They are the variables or
    attributes that capture the relevant information from the data. Out of the vast
    amounts of data that are present, we want to understand which features of this
    data would be useful for solving a particular problem. Relevant features would
    generate better models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering, the process of selecting, extracting, and transforming
    features, plays a crucial role in improving the performance of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Labels and targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Labels or targets are the desired outputs or outcomes that the ML model aims
    to predict or classify. In supervised learning, where the model learns from labeled
    data, the labels represent the correct answers or class labels associated with
    the input data. In unsupervised learning, the model identifies patterns or clusters
    in the data without any explicit labels.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In ML, models are trained using a subset of the available data, which is called
    the **training set**. The training process involves feeding the input data and
    corresponding labels to the model, which learns from this data to make predictions.
    Once the model is trained, its performance is evaluated using a separate subset
    of data called the testing set. This evaluation helps assess the model’s ability
    to generalize and make accurate predictions on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms and models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ML algorithms are mathematical or statistical procedures that learn patterns
    and relationships in the data and make predictions or decisions. They can be categorized
    into various types, including regression, classification, clustering, dimensionality
    reduction, and reinforcement learning. These algorithms, when trained on data,
    generate models that capture the learned patterns and can be used to make predictions
    on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing different ML algorithms in depth is beyond the scope of this book.
    We will talk about different types of ML problems in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Types of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML problems can be broadly categorized into two distinct categories. In this
    section, we will explore both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning is a type of ML where the algorithm learns from labeled
    training data to make predictions or decisions. In supervised learning, the training
    data consists of input features and corresponding output labels or target values.
    The goal is to learn a mapping function that can accurately predict the output
    for new, unseen inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of supervised learning involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: The first step is to collect and preprocess the training
    data. This includes cleaning the data, handling missing values, and transforming
    the data into a suitable format for the learning algorithm. The data should be
    split into features (input variables) and labels (output variables).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**: Once the data has been prepared, the supervised learning
    algorithm is trained on the labeled training data. The algorithm learns the patterns
    and relationships between the input features and the corresponding output labels.
    The goal is to find a model that can generalize well to unseen data and make accurate
    predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model evaluation**: After training the model, it needs to be evaluated to
    assess its performance. This is done using a separate set of data called the testing
    or validation set. The model’s predictions are compared with the actual labels
    in the testing set, and various evaluation metrics such as accuracy, precision,
    recall, or mean squared error are calculated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model deployment and prediction**: Once the model is trained and evaluated,
    it can be deployed to make predictions on new, unseen data. The trained model
    takes the input features of the new data and produces predictions or decisions
    based on what it has learned during the training phase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Examples of supervised learning algorithms include linear regression, logistic
    regression, **support vector machines** (**SVM**), decision trees, random forests,
    gradient boosting, and neural networks. Again, going in-depth on these algorithms
    is beyond the scope of this book. You can read more about them here: [https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning is a type of ML where the algorithm learns patterns and
    relationships in the data without any labeled output. In unsupervised learning,
    the training data consists only of input features, and the goal is to discover
    hidden patterns, structures, or clusters within the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of unsupervised learning involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: Similar to supervised learning, the first step is to
    collect and preprocess the data. However, in unsupervised learning, there are
    no labeled output values or target variables. The data is transformed and prepared
    in a way that it’s suitable for the specific unsupervised learning algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**: In unsupervised learning, the algorithm is trained on the
    input features without any specific target variable. The algorithm explores the
    data and identifies patterns or clusters based on statistical properties or similarity
    measures. The goal is to extract meaningful information from the data without
    any predefined labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model evaluation** (optional): Unlike supervised learning, unsupervised learning
    does not have a direct evaluation metric based on known labels. Evaluation in
    unsupervised learning is often subjective and depends on the specific task or
    problem domain. It is also a more manual process than it is in supervised learning.
    Evaluation can involve visualizing the discovered clusters, assessing the quality
    of dimensionality reduction, or using domain knowledge to validate the results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pattern discovery and insights**: The primary objective of unsupervised learning
    is to discover hidden patterns, structures, or clusters in the data. Unsupervised
    learning algorithms can reveal insights about the data, identify anomalies or
    outliers, perform dimensionality reduction, or generate recommendations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples of unsupervised learning algorithms include K-means clustering, hierarchical
    clustering, **principal component analysis** (**PCA**), association rule mining,
    and **self-organizing** **maps** (**SOM**).
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, supervised learning and unsupervised learning are two key types
    of ML. Supervised learning relies on labeled data to learn patterns and make predictions,
    while unsupervised learning discovers patterns and structures in unlabeled data.
    Both types have their own set of algorithms and techniques, as well as different
    choices. Discussing unsupervised learning in depth is beyond the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore supervised ML, a cornerstone in the realm
    of AI and data science that represents a powerful approach to building predictive
    models and making data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Types of supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we know, supervised learning is a branch of ML where algorithms learn patterns
    and relationships from labeled training data. It involves teaching or supervising
    the model by presenting input data along with corresponding output labels, allowing
    the algorithm to learn the mapping between the input and output variables. We’ll
    explore three key types of supervised learning – classification, regression, and
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification is a type of ML task where the goal is to categorize or classify
    data into predefined classes or categories based on its features. The algorithm
    learns from labeled training data to build a model that can predict the class
    label of new, unseen data instances.
  prefs: []
  type: TYPE_NORMAL
- en: In classification, the output is discrete and represents class labels. Some
    common algorithms that are used for classification tasks include logistic regression,
    decision trees, random forests, SVM, and naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a spam email classification task, where the goal is to
    predict whether an incoming email is spam or not. The algorithm is trained on
    a dataset of labeled emails, where each email is associated with a class label
    indicating whether it is spam or not. The trained model can then classify new
    emails as spam or non-spam based on their features, such as the content, subject,
    or sender.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression is another type of ML task that focuses on predicting continuous
    or numerical values based on input features. In regression, the algorithm learns
    from labeled training data to build a model that can estimate or forecast the
    numerical value of a target variable given a set of input features.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models are used when the output is a continuous value, such as predicting
    house prices, stock market trends, or predicting the sales of a product based
    on historical data. Some commonly used regression algorithms include linear regression,
    decision trees, random forests, gradient boosting, and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a case where you want to predict the price of a house
    based on its various features, such as the area, number of bedrooms, location,
    and so on. In this case, the algorithm is trained on a dataset of labeled house
    data, where each house is associated with its corresponding price. The trained
    regression model can then predict the price of a new house based on its features.
  prefs: []
  type: TYPE_NORMAL
- en: Time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series analysis is a specialized area of ML that deals with data collected
    over time, where the order of observations is important. In time series analysis,
    the goal is to understand and forecast the patterns, trends, and dependencies
    within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series models are used to predict future values based on historical data
    points. They are widely used in fields such as finance, stock market prediction,
    weather forecasting, and demand forecasting. Some popular time series algorithms
    include **Autoregressive Integrated Moving Average** (**ARIMA**), exponential
    smoothing methods, and **long short-term memory** (**LSTM**) networks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you have historical stock market data for a particular
    company, including the date and the corresponding stock prices. The time series
    algorithm can analyze the patterns and trends in the data and make predictions
    about future stock prices based on historical price fluctuations.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, supervised learning encompasses various types, including classification,
    regression, and time series analysis. Each type addresses specific learning tasks
    and requires different algorithms and techniques. Understanding these types helps
    in choosing the appropriate algorithms and approaches for specific data analysis
    and prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore how to leverage Spark for ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: ML with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides a powerful and scalable platform for performing large-scale ML
    tasks. Spark’s **ML library**, also known as **MLlib**, offers a wide range of
    algorithms and tools for building and deploying ML models.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of using Spark for ML include its distributed computing capabilities,
    efficient data processing, scalability, and integration with other Spark components,
    such as Spark SQL and Spark Streaming. Spark’s MLlib supports both batch and streaming
    data processing, enabling the development of real-time ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a transformative field that enables computers to learn from data and make
    predictions or decisions. By understanding the key concepts and leveraging tools
    such as Spark’s MLlib, we can harness the power of ML to gain insights, automate
    processes, and drive innovation across various domains.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the benefits of using Spark for ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Apache Spark for large-scale ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By leveraging Spark’s distributed computing capabilities and rich ecosystem,
    data scientists and engineers can effectively tackle complex ML challenges on
    massive datasets. It offers various advantages due to its distributed computing
    capabilities, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed and performance**: One of the key advantages of Apache Spark is its
    ability to handle large-scale data processing with exceptional speed. Spark leverages
    in-memory computing and optimized data processing techniques, such as **data parallelism**
    and **task pipelining**, to accelerate computations. This makes it highly efficient
    for iterative algorithms often used in ML, reducing the overall processing time
    significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed computing**: Spark’s distributed computing model allows it to
    distribute data and computations across multiple nodes in a cluster, enabling
    parallel processing. This distributed nature enables Spark to scale horizontally,
    leveraging the computing power of multiple machines and processing data in parallel.
    This makes it well-suited for large-scale ML tasks that require processing massive
    volumes of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: Another advantage of Apache Spark is its built-in fault
    tolerance mechanism. Spark automatically tracks the lineage of **Resilient Distributed
    Datasets** (**RDDs**), which are the fundamental data abstraction in Spark, allowing
    it to recover from failures and rerun failed tasks. This ensures the reliability
    and resilience of Spark applications, making it a robust platform for handling
    large-scale ML workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versatility and flexibility**: Spark provides a wide range of APIs and libraries
    that facilitate various data processing and analytics tasks, including ML. Spark’s
    MLlib library offers a rich set of distributed ML algorithms and utilities, making
    it easy to develop and deploy scalable ML models. Additionally, Spark integrates
    well with other popular data processing frameworks and tools, enabling seamless
    integration into existing data pipelines and ecosystems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time and streaming capabilities**: As we discussed in the previous chapter,
    Spark extends its capabilities beyond batch processing with its streaming component
    called Spark Streaming. This is particularly valuable in scenarios where immediate
    insights or decisions are required based on continuously arriving data, such as
    real-time fraud detection, sensor data analysis, or sentiment analysis on social
    media streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ecosystem and community support**: Apache Spark has a vibrant and active
    community of developers and contributors, ensuring continuous development, improvement,
    and support. Spark benefits from a rich ecosystem of tools and extensions, providing
    additional functionality and integration options. The community-driven nature
    of Spark ensures a wealth of resources, documentation, tutorials, and online forums
    for learning and troubleshooting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, Apache Spark offers significant advantages for large-scale ML tasks.
    Its speed, scalability, fault tolerance, versatility, and real-time capabilities
    make it a powerful framework for processing big data and developing scalable ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s take a look at different libraries that Spark provides to make use
    of ML capabilities in the distributed framework.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib versus Spark ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark provides two libraries for ML: Spark MLlib and Spark ML. Although
    they share a similar name, there are some key differences between the two libraries
    in terms of their design, APIs, and functionality. Let’s compare Spark MLlib and
    Spark ML to understand their characteristics and use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark MLlib is the original ML library in Apache Spark. It was introduced in
    earlier versions of Spark and provides a rich set of distributed ML algorithms
    and utilities. MLlib is built on top of the RDD API, which is the core data abstraction
    in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark MLlib has a few key features that set it apart from other non-distributed
    ML libraries such as `scikit-learn`. Let’s look at a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RDD-based API**: MLlib leverages the RDD abstraction for distributed data
    processing, making it suitable for batch processing and iterative algorithms.
    The RDD API allows for efficient distributed computing but can be low-level and
    complex for some use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diverse algorithms**: MLlib offers a wide range of distributed ML algorithms,
    including classification, regression, clustering, collaborative filtering, dimensionality
    reduction, and more. These algorithms are implemented to work with large-scale
    data and can handle various tasks in the ML pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: MLlib provides utilities for feature extraction, transformation,
    and selection. It includes methods for handling categorical and numerical features,
    text processing, and feature scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model persistence**: MLlib supports model persistence, allowing trained models
    to be saved to disk and loaded later for deployment or further analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will explore the Spark ML library. This is the newer
    library that also provides ML capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML, introduced in Spark 2.0, is the newer ML library in Apache Spark.
    It is designed to be more user-friendly, with a higher-level API and a focus on
    DataFrames, which are a structured and optimized distributed data collection introduced
    in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of Spark ML are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame-based API**: Spark ML leverages the DataFrame API, which provides
    a more intuitive and higher-level interface compared to the RDD API. DataFrames
    offer a structured and tabular data representation, making it easier to work with
    structured data and integrate with Spark SQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipelines**: Spark ML introduces the concept of pipelines, which provides
    a higher-level abstraction for constructing ML workflows. Pipelines enable the
    chaining of multiple data transformations and model training stages into a single
    pipeline, simplifying the development and deployment of complex ML pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated feature transformers**: Spark ML includes a rich set of feature
    transformers, such as StringIndexer, OneHotEncoder, VectorAssembler, and more.
    These transformers seamlessly integrate with DataFrames and simplify the feature
    engineering process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified API**: Spark ML unifies the APIs for different ML tasks, such as
    classification, regression, clustering, and recommendation. This provides a consistent
    and cohesive programming interface across different algorithms and simplifies
    the learning curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know the key features of both Spark MLlib and Spark ML, let’s explore
    when to use each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You would benefit from using Spark MLlib in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: You are working with older versions of Spark that do not support Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You require low-level control and need to work directly with RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need access to a specific algorithm or functionality that is not available
    in Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should prefer to use Spark ML in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: You are using Spark 2.0 or later versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You prefer a higher-level API and want to leverage DataFrames and Spark SQL
    capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to build end-to-end ML pipelines with integrated feature transformers
    and pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both Spark MLlib and Spark ML provide powerful ML capabilities in Apache Spark.
    As we’ve seen, Spark MLlib is the original library with a rich set of distributed
    algorithms, while Spark ML is a newer library with a more user-friendly API and
    integration with DataFrames. The choice between the two depends on your Spark
    version, preference for API style, and specific requirements of your ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: ML life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ML life cycle encompasses the end-to-end process of developing and deploying
    ML models. It involves several stages, each with its own set of tasks and considerations.
    Understanding the ML life cycle is crucial for building robust and successful
    ML solutions. In this section, we will explore the key stages of the ML life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem definition**: The first stage of the ML life cycle is problem definition.
    It involves clearly defining the problem you want to solve and understanding the
    goals and objectives of your ML project. This stage requires collaboration between
    domain experts and data scientists to identify the problem, define success metrics,
    and establish the scope of the project.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data acquisition and understanding**: Once the problem has been defined,
    the next step is to acquire the necessary data for training and evaluation. Data
    acquisition may involve collecting data from various sources, such as databases,
    APIs, or external datasets. It is important to ensure data quality, completeness,
    and relevance to the problem at hand. Additionally, data understanding involves
    exploring and analyzing the acquired data to gain insights into its structure,
    distributions, and potential issues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data preparation and feature engineering**: Data preparation and feature
    engineering are crucial steps in the ML life cycle. It involves transforming and
    preprocessing the data to make it suitable for training ML models. This includes
    tasks such as cleaning the data, handling missing values, encoding categorical
    variables, scaling features, and creating new features through feature engineering
    techniques. Proper data preparation and feature engineering significantly impact
    the performance and accuracy of ML models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training and evaluation**: In this stage, ML models are trained on
    the prepared data. Model training involves selecting an appropriate algorithm,
    defining the model architecture, and optimizing its parameters using training
    data. The trained model is then evaluated using evaluation metrics and validation
    techniques to assess its performance. This stage often requires iterating and
    fine-tuning the model to achieve the desired accuracy and generalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once the model has been trained and evaluated, it is
    ready for deployment. Model deployment involves integrating the model into the
    production environment, making predictions on new data, and monitoring its performance.
    This may involve setting up APIs, creating batch or real-time inference systems,
    and ensuring the model’s scalability and reliability. Deployment also includes
    considerations for model versioning, monitoring, and retraining to maintain the
    model’s effectiveness over time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model monitoring and maintenance**: Once the model has been deployed, it
    is important to continuously monitor its performance and maintain its effectiveness.
    Monitoring involves tracking model predictions, detecting anomalies, and collecting
    feedback from users or domain experts. It also includes periodic retraining of
    the model using new data to adapt to changing patterns or concepts. Model maintenance
    involves addressing model drift, updating dependencies, and managing the model’s
    life cycle in the production environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model iteration and improvement**: The ML life cycle is an iterative process,
    and models often require improvement over time. Based on user feedback, performance
    metrics, and changing business requirements, models may need to be updated, retrained,
    or replaced. Iteration and improvement are essential for keeping the models up-to-date
    and ensuring they continue to deliver accurate predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ML life cycle involves problem definition, data acquisition, data preparation,
    model training, model deployment, model monitoring, and model iteration. Each
    stage plays a critical role in developing successful ML solutions. By following
    a well-defined life cycle, organizations can effectively build, deploy, and maintain
    ML models to solve complex problems and derive valuable insights from their data.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s dive into a case study where we’ll explore the art of predicting house
    prices using historical data. Picture this: we have a treasure trove of valuable
    information about houses, including details such as zoning, lot area, building
    type, overall condition, year built, and sale price. Our goal is to harness the
    power of ML to accurately forecast the price of a new house that comes our way.'
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this feat, we’ll embark on a journey to construct an ML model
    exclusively designed for predicting house prices. This model will leverage the
    existing historical data and incorporate additional features. By carefully analyzing
    and understanding the relationships between these features and the corresponding
    sale prices, our model will become a reliable tool for estimating the value of
    any new house that enters the market.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we will go through some of the steps defined in the previous
    section, where we talked about the ML life cycle. Since housing prices are continuous,
    we will use a linear regression model to predict these prices.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by preparing the data to make it usable for an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation and feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we know, data preparation and feature engineering are crucial steps in the
    ML process. Proper data preparation and feature engineering techniques can significantly
    improve the performance and accuracy of models. In this section, we will explore
    common data preparation and feature engineering tasks with code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in building a model is to find the relevant data. We are going
    to use house price data (located at [https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQpMiIMJrmiTGarbs/edit#gid=1150341366](https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQpMiIMJrmiTGarbs/edit#gid=1150341366))
    for this purpose. This data has 2,920 rows and 13 columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset has the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Id`: Unique identifier for each row of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSSubClass`: Subclass of the property'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSZoning`: Zoning of the property'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LotArea`: Total area of the lot where the property is situated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LotConfig`: Configuration of the lot – for example, if it’s a corner lot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BldgType`: Type of home – for example, single, family, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OverallCond`: General condition of the house'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YearBuilt`: The year the house was built in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YearRemodAdd`: The year any remodeling was done'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Exterior1st`: Type of exterior – for example, vinyl, siding, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFinSF2`: Total size of finished basement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TotalBsmtSF`: Total size of basement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SalePrice`: The sale price of the house'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will download this data from the link provided at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know some of the data points that exist in the data, let’s learn
    how to load it.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At this point, we already have the data downloaded on our computer and to our
    Databricks environment as a CSV file. As you may recall from the previous chapters,
    we learned how to load a dataset into a DataFrame through various techniques.
    We will use a CSV file here to load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the result in Figure 8.1\. Please note that we can see only part
    of the result in the image since the dataset is too large to be displayed in full.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19176_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s print the schema of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed, some of the column types are strings. We will clean
    up this data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cleaning the data involves handling missing values, outliers, and inconsistent
    data. Before we clean up the data, we will see how many rows are in the data.
    We can do this by using the `count()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this statement is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the data contains 2,919 rows before we apply any cleaning. Now,
    we will drop missing values from this dataset, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This shows that we have dropped some rows of data and that the data size is
    smaller now.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss categorical variables and how to handle
    them, specifically those represented as strings in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Handling categorical variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the realm of statistics and data analysis, a categorical variable is a type
    of variable that represents categories or groups and can take on a limited, fixed
    number of distinct values or levels. These variables signify qualitative characteristics
    and do not possess inherent numerical significance or magnitude. Instead, they
    represent different attributes or labels that classify data into specific groups
    or classes. Categorical variables need to be encoded to numerical values before
    training machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we have a few columns that are string types. Those need to be
    encoded into numerical values so that the model can correctly use them. For this
    purpose, we’ll use Spark’s `StringIndexer` library to index the string columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use `StringIndexer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are taking the `MSZoning` column and converting it
    into an indexed column. To achieve this, we created a `StringIndexer` value by
    the name of `mszoning` indexer. We gave it `MSZoning` as the input column to work
    on. The output column’s name is `MSZoningIndex`. We will use this output column
    in the next step. After that, we’ll fit `mszoning_indexer` to `cleaned_data`.
  prefs: []
  type: TYPE_NORMAL
- en: In the resulting DataFrame, you will notice that one additional column was added
    by the name of `MSZoningIndex`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will use a pipeline to transform all the features in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: A **pipeline** brings together a series of essential steps, each contributing
    to transforming raw data into valuable predictions and analyses. The pipeline
    serves as a structured pathway, composed of distinct stages or components, arranged
    in a specific order. Each stage represents a unique operation or transformation
    that refines the data, molding it into a more suitable format for ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of a pipeline lies its ability to seamlessly connect these stages,
    forming a well-coordinated flow of transformations. This orchestration ensures
    that the data flows effortlessly through each stage, with the output of one stage
    becoming the input for the next. It eradicates the need for manual intervention,
    automating the entire process and saving us valuable time and effort. We integrate
    a variety of operations into the pipeline, such as data cleaning, feature engineering,
    encoding categorical variables, scaling numerical features, and much more. Each
    operation plays its part in transforming the data, to make it usable for the ML
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The ML pipeline empowers us to streamline our workflows, experiment with different
    combinations of transformations, and maintain consistency in our data processing
    tasks. It provides a structured framework that allows us to effortlessly reproduce
    and share our work, fostering collaboration and fostering a deeper understanding
    of the data transformation process.
  prefs: []
  type: TYPE_NORMAL
- en: In ML and data preprocessing, a **one-hot encoder** is a technique that’s used
    to convert categorical variables into a numerical format, allowing algorithms
    to better understand and process categorical data. It’s particularly useful when
    working with categorical features that lack ordinal relationships or numerical
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `StringIndexer` and `OneHotEncoder` in this pipeline. Let’s see
    how we can achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To begin our code, we import the required modules from the PySpark library.
    The `StringIndexer` and `OneHotEncoder` modules will be used to handle the string
    columns of the housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As we embark on the process of transforming categorical columns into numerical
    representations that can be understood by ML algorithms, let’s take a closer look
    at the magic happening in our code.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create `StringIndexer` instances for each categorical column
    we wish to transform. Each instance takes an input column, such as `MSZoning`
    or `LotConfig`, and produces a corresponding output column with a numerical index.
    For example, the `MSZoningIndex` column captures the transformed index values
    of the `MSZoning` column.
  prefs: []
  type: TYPE_NORMAL
- en: With the categorical columns successfully indexed, we progress to the next stage.
    Now, we want to convert these indices into binary vectors. For that, we can use
    `OneHotEncoder`. The resulting vectors represent each categorical value as a binary
    array, with a value of 1 indicating the presence of that category and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: We create `OneHotEncoder` instances for each indexed column, such as `MSZoningIndex`
    or `LotConfigIndex`, and generate new output columns holding the binary vector
    representations. These output columns, such as `MSZoningVector` or `LotConfigVector`,
    are used to capture the encoded information.
  prefs: []
  type: TYPE_NORMAL
- en: As our code progresses, we assemble a pipeline – a sequence of transformations
    – where each transformation represents a stage. In our case, each stage encompasses
    the steps of indexing and one-hot encoding for a specific categorical column.
    We arrange the stages in the pipeline, ensuring the correct order of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: By structuring our pipeline, we orchestrate a seamless flow of operations. The
    pipeline connects the dots between different stages, making it effortless to apply
    these transformations to our dataset as a whole. Our pipeline acts as a conductor,
    leading our data through the transformations, ultimately making it into a format
    ready for ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will fit this pipeline to our cleaned dataset so that all the columns
    can be transformed together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The resulting DataFrame will have the additional columns that we created in
    the pipeline with transformation. We have created index and vector columns for
    each of the string columns.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to remove the unnecessary and redundant columns from our dataset.
    We will do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleanup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we will make sure that we are only using the features needed by
    ML. To achieve this, we will remove different additional columns, such as the
    identity column, which don’t serve the model. Moreover, we will also remove the
    features that we have already applied transformations to, such as string columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to delete the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the resulting column list, the `Id`, `MSZoning`, `LotConfig`,
    `BldgType`, and `Exterior1st` columns have been deleted from the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in the process is assembling the data.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling the vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we will assemble a vector based on the features that we want.
    This step is necessary for Spark ML to work with data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code captures how we can achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created a features column that contains
    the assembled vector. We will use this column for our model training after scaling
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Once the vector has been assembled, the next step in the process is to scale
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature scaling ensures that all features are on a similar scale, preventing
    certain features from dominating the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code selects only the scaled features and the target column –
    that is, `SalePrice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `df_model_final` now only has two columns. `SalePrice` is the
    column that we’re going to predict so that is our target column. `scaledFeatures`
    contains all the features that we are going to use to train our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: These examples demonstrate common data preparation and feature engineering tasks
    using PySpark. However, the specific techniques and methods applied may vary,
    depending on the dataset and the requirements of the ML task. It is essential
    to understand the characteristics of the data and choose appropriate techniques
    to preprocess and engineer features effectively. Proper data preparation and feature
    engineering lay the foundation for building accurate and robust ML models.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in this process is training and evaluating the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model training and evaluation are crucial steps in the ML process. In this section,
    we will explore how to train ML models and evaluate their performance using various
    metrics and techniques. We will use PySpark as the framework for model training
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before training a model, it is important to split the dataset into training
    and testing sets. The training set is used to train the model, while the testing
    set is used to evaluate its performance. Here’s an example of how to split the
    data using PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are doing a random split of the data, putting 75%
    of the data into the training set and 25% of the data into the test set. There
    are other split techniques as well. You should look at your data carefully and
    then define the split that works best for your data and model training.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we split the data is that once we train the model, we want to see
    how the trained model predicts on a dataset that it has never seen. In this case,
    that is our test dataset. This would help us evaluate the model and determine
    the quality of the model. Based on this, we can deploy different techniques to
    improve our model.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is model training.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the data has been split, we can train an ML model on the training data.
    PySpark provides a wide range of algorithms for various types of ML tasks. For
    this example, we are going to use linear regression as our model of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of training a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are using the training data and fitting it into a
    linear regressor model. We also added a parameter for `labelCol` that tells the
    model that this is the column that is our target column to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model has been trained, the next step is to determine how good the
    model is. We’ll do this in the next section by evaluating the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After training the model, we need to evaluate its performance on the test data.
    Evaluation metrics provide insights into how well the model is performing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) is a fundamental statistical metric that’s
    used to evaluate the performance of regression models by quantifying the average
    of the squared differences between predicted and actual values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**R-squared**, often denoted as **R2**, is a statistical measure that represents
    the proportion of the variance in the dependent variable that is predictable or
    explained by the independent variables in a regression model. It serves as an
    indicator of how well the independent variables explain the variability of the
    dependent variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of evaluating a regression model using the MSE and R2 metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the test data’s performance as depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Based on the results of the model, we can tune it further. We’ll see some of
    the techniques to achieve this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-validation is one of the different methods to improve an ML model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is used to assess the model’s performance more robustly by
    dividing the data into multiple subsets for training and evaluation. So, instead
    of just using train and test data, we use a validation set as well, where the
    model never sees that data and is only used for measuring performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-validation follows a simple principle: rather than relying on a single
    train-test split, we divide our dataset into multiple subsets, or **folds**. Each
    fold acts as a mini train-test split, with a portion of the data used for training
    and the remainder reserved for testing. By rotating the folds, we ensure that
    every data point gets an opportunity to be part of the test set, thereby mitigating
    biases and providing a more representative evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: The most common form of cross-validation is **k-fold cross-validation**. In
    this method, the dataset is divided into k equal-sized folds. The model is trained
    and evaluated k times, with each fold serving as the test set once while the remaining
    folds collectively form the training set. By averaging the performance metrics
    obtained from each iteration, we obtain a more robust estimation of our model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Through cross-validation, we gain valuable insights into the generalization
    capabilities of our model. It allows us to gauge its performance across different
    subsets of the data, capturing the inherent variations and nuances that exist
    within our dataset. This technique helps us detect potential issues such as **overfitting**,
    where the model performs exceptionally well on the training set but fails to generalize
    to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to k-fold cross-validation, there are variations and extensions
    tailored to specific scenarios. **Stratified cross-validation** ensures that each
    fold maintains the same class distribution as the original dataset, preserving
    the representativeness of the splits. **Leave-one-out cross-validation**, on the
    other hand, treats each data point as a separate fold, providing a stringent evaluation
    but at the cost of increased computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning** is the process of optimizing the hyperparameters
    of an ML algorithm to improve its performance. Hyperparameters are settings or
    configurations that are external to the model and cannot be learned from the training
    data directly. Unlike model parameters, which are learned during the training
    process, hyperparameters need to be specified beforehand and are crucial in determining
    the behavior and performance of an ML model. We will use hyperparameter tuning
    to improve model performance. **Hyperparameters** are parameters that are not
    learned from the data but are set before training. Tuning hyperparameters can
    significantly impact the model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Picture this: our model is a complex piece of machinery, composed of various
    knobs and levers known as hyperparameters. These hyperparameters govern the behavior
    and characteristics of our model, influencing its ability to learn, generalize,
    and make accurate predictions. However, finding the optimal configuration for
    these hyperparameters is no easy feat.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning is the art of systematically searching and selecting the
    best combination of hyperparameters for our model. It allows us to venture beyond
    default settings and discover the configurations that align harmoniously with
    our data, extracting the most meaningful insights and delivering superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of hyperparameter tuning is to get optimal values. We explore different
    hyperparameter settings, traversing through a multidimensional landscape of possibilities.
    This exploration can take various forms, such as grid search, random search, or
    more advanced techniques such as Bayesian optimization or genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**, a popular method, involves defining a grid of potential values
    for each hyperparameter. The model is then trained and evaluated for every possible
    combination within the grid. By exhaustively searching through the grid, we unearth
    the configuration that yields the highest performance, providing us with a solid
    foundation for further refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: Random search takes a different approach. It samples hyperparameter values randomly
    from predefined distributions and evaluates the model’s performance for each sampled
    configuration. This randomized exploration enables us to cover a wider range of
    possibilities, potentially discovering unconventional yet highly effective configurations.
  prefs: []
  type: TYPE_NORMAL
- en: These examples demonstrate the process of model training and evaluation using
    PySpark. However, the specific algorithms, evaluation metrics, and techniques
    applied may vary, depending on the ML task at hand. It is important to understand
    the problem domain, select appropriate algorithms, and choose relevant evaluation
    metrics to train and evaluate models effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model deployment is the process of making trained ML models available for use
    in production environments. In this section, we will explore various approaches
    and techniques for deploying ML models effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serialization and persistence**: Once a model has been trained, it needs
    to be serialized and persisted to disk for later use. Serialization is the process
    of converting the model object into a format that can be stored, while persistence
    involves saving the serialized model to a storage system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model serving**: Model serving involves making the trained model available
    as an API endpoint or service that can receive input data and return predictions.
    This allows other applications or systems to integrate and use the model for real-time
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model monitoring and management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once a model has been deployed, it is important to monitor its performance
    and behavior in the production environment and maintain its effectiveness over
    time. Monitoring can help identify issues such as data drift, model degradation,
    or anomalies. Additionally, model management involves versioning, tracking, and
    maintaining multiple versions of the deployed models. These practices ensure that
    models remain up to date and perform optimally over time. In this section, we
    will explore the key aspects of model monitoring and maintenance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability and performance**: When deploying ML models, scalability and
    performance are essential considerations. Models should be designed and deployed
    in a way that allows for efficient processing of large volumes of data and can
    handle high throughput requirements. Technologies such as Apache Spark provide
    distributed computing capabilities that enable scalable and high-performance model
    deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model updates and retraining**: ML models may need to be updated or retrained
    periodically to adapt to changing data patterns or improve performance. Deployed
    models should have mechanisms in place to facilitate updates and retraining without
    interrupting the serving process. This can involve automated processes, such as
    monitoring for data drift or retraining triggers based on specific conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance metrics**: To monitor a deployed model, it is important to define
    and track relevant performance metrics. These metrics can vary, depending on the
    type of ML problem and the specific requirements of the application. Some commonly
    used performance metrics include accuracy, precision, recall, F1 score, and **area
    under the ROC curve** (**AUC**). By regularly evaluating these metrics, deviations
    from the expected performance can be identified, indicating the need for further
    investigation or maintenance actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift detection**: Data drift refers to the phenomenon where the statistical
    properties of the input data change over time, leading to a degradation in model
    performance. Monitoring for data drift is crucial to ensure that the deployed
    model continues to provide accurate predictions. Techniques such as statistical
    tests, feature distribution comparison, and outlier detection can be employed
    to detect data drift. When data drift is detected, it may be necessary to update
    the model or retrain it using more recent data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance monitoring**: Monitoring the performance of a deployed
    model involves tracking its predictions and comparing them with ground truth values.
    This can be done by periodically sampling a subset of the predictions and evaluating
    them against the actual outcomes. Monitoring can also include analyzing prediction
    errors, identifying patterns or anomalies, and investigating the root causes of
    any performance degradation. By regularly monitoring the model’s performance,
    issues can be identified early on and corrective actions can be taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model retraining and updates**: Models that are deployed in production may
    require periodic updates or retraining to maintain their effectiveness. When new
    data becomes available or significant changes occur in the application domain,
    retraining the model with fresh data can help improve its performance. Additionally,
    bug fixes, feature enhancements, or algorithmic improvements may necessitate updating
    the deployed model. It is important to have a well-defined process and infrastructure
    in place to handle model retraining and updates efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versioning and model governance**: Maintaining proper versioning and governance
    of deployed models is crucial for tracking changes, maintaining reproducibility,
    and ensuring regulatory compliance. Version control systems can be used to manage
    model versions, track changes, and provide a historical record of model updates.
    Additionally, maintaining documentation related to model changes, dependencies,
    and associated processes contributes to effective model governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration and feedback**: Model monitoring and maintenance often involve
    collaboration among different stakeholders, including data scientists, engineers,
    domain experts, and business users. Establishing channels for feedback and communication
    can facilitate the exchange of insights, identification of issues, and implementation
    of necessary changes. Regular meetings or feedback loops can help align the model’s
    performance with the evolving requirements of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, model deployment is a critical step in the ML life cycle. It involves
    serializing and persisting trained models, serving them as APIs or services, monitoring
    their performance, ensuring scalability and performance, and managing updates
    and retraining.
  prefs: []
  type: TYPE_NORMAL
- en: By actively monitoring and maintaining deployed models, organizations can ensure
    that their ML systems continue to provide accurate and reliable predictions. Effective
    model monitoring techniques, coupled with proactive maintenance strategies, enable
    timely identification of performance issues and support the necessary actions
    to keep the models up to date and aligned with business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Model iteration and improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model iteration and improvement is a crucial phase in the ML life cycle that
    focuses on enhancing the performance and effectiveness of deployed models. By
    continuously refining and optimizing models, organizations can achieve better
    predictions and drive greater value from their ML initiatives. In this section,
    we will explore the key aspects of model iteration and improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collecting feedback and gathering insights**: The first step in model iteration
    and improvement is to gather feedback from various stakeholders, including end
    users, domain experts, and business teams. Feedback can provide valuable insights
    into the model’s performance, areas for improvement, and potential issues encountered
    in real-world scenarios. This feedback can be collected through surveys, user
    interviews, or monitoring the model’s behavior in the production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analyzing model performance**: To identify areas for improvement, it is important
    to thoroughly analyze the model’s performance. This includes examining performance
    metrics, evaluating prediction errors, and conducting in-depth analyses of misclassified
    or poorly predicted instances. By understanding the strengths and weaknesses of
    the model, data scientists can focus their efforts on specific areas that require
    attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploring new features and data**: One way to improve model performance is
    by incorporating new features or utilizing additional data sources. Exploratory
    data analysis can help identify potential features that may have a strong impact
    on predictions. Feature engineering techniques, such as creating interaction terms,
    scaling, or transforming variables, can also be employed to enhance the representation
    of the data. Additionally, incorporating new data from different sources can provide
    fresh insights and improve the model’s generalization capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm selection and hyperparameter tuning**: Experimenting with different
    algorithms and hyperparameters can lead to significant improvements in model performance.
    Data scientists can explore alternative algorithms or variations of the existing
    algorithm to identify the best approach for the given problem. Hyperparameter
    tuning techniques, such as grid search or Bayesian optimization, can be used to
    find optimal values for model parameters. This iterative process helps identify
    the best algorithm and parameter settings that yield superior results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods involve combining multiple models to
    create a more robust and accurate prediction. Techniques such as bagging, boosting,
    or stacking can be applied to build an ensemble model from multiple base models.
    Ensemble methods can often improve model performance by reducing bias, variance,
    and overfitting. Experimenting with different ensemble strategies and model combinations
    can lead to further enhancements in prediction accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B testing and controlled experiments**: A/B testing or controlled experiments
    can be conducted to evaluate the impact of model improvements in a controlled
    setting. By randomly assigning users or data samples to different versions of
    the model, organizations can measure the performance of the new model against
    the existing one. This approach provides statistically significant results to
    determine if the proposed changes lead to desired improvements or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous monitoring and evaluation**: Once the improved model has been
    deployed, continuous monitoring and evaluation are essential to ensure its ongoing
    performance. Monitoring for data drift, analyzing performance metrics, and conducting
    periodic evaluations help identify potential degradation or the need for further
    improvements. This feedback loop allows for continuous iteration and refinement
    of the deployed model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By embracing a culture of iteration and improvement, organizations can continuously
    enhance the performance and accuracy of their ML models. Through collecting feedback,
    analyzing model performance, exploring new features and algorithms, conducting
    experiments, and continuous monitoring, models can be iteratively refined to achieve
    better predictions and drive tangible business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Case studies and real-world examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore two prominent use cases of ML: customer churn
    prediction and fraud detection. These examples demonstrate the practical applications
    of ML techniques in addressing real-world challenges and achieving significant
    business value.'
  prefs: []
  type: TYPE_NORMAL
- en: Customer churn prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customer churn refers to the phenomenon where customers discontinue their relationship
    with a company, typically by canceling a subscription or switching to a competitor.
    Predicting customer churn is crucial for businesses as it allows them to proactively
    identify customers who are at risk of leaving and take appropriate actions to
    retain them. ML models can analyze various customer attributes and behavior patterns
    to predict churn likelihood. Let’s dive into a customer churn prediction case
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – telecommunications company
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A telecommunications company wants to reduce customer churn by predicting which
    customers are most likely to cancel their subscriptions. The company collects
    extensive customer data, including demographics, call records, service usage,
    and customer complaints. By leveraging ML, they aim to identify key indicators
    of churn and build a predictive model to forecast future churners:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: The company gathers and preprocesses the customer data,
    ensuring it is cleaned, formatted, and ready for analysis. They combine customer
    profiles with historical churn information to create a labeled dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: To capture meaningful patterns, the company engineers
    relevant features from the available data. These features may include variables
    such as average call duration, number of complaints, monthly service usage, and
    tenure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection and training**: The company selects an appropriate ML algorithm,
    such as logistic regression, decision trees, or random forests, to build the churn
    prediction model. They split the dataset into training and testing sets, train
    the model on the training data, and evaluate its performance on the testing data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation**: The model’s performance is assessed using evaluation
    metrics such as accuracy, precision, recall, and F1 score. The company analyzes
    the model’s ability to correctly identify churners and non-churners, striking
    a balance between false positives and false negatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment and monitoring**: Once the model meets the desired performance
    criteria, it is deployed into the production environment. The model continuously
    monitors incoming customer data, generates churn predictions, and triggers appropriate
    retention strategies for at-risk customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fraud detection is another critical application of ML that aims to identify
    fraudulent activities and prevent financial losses. ML models can learn patterns
    of fraudulent behavior from historical data and flag suspicious transactions or
    activities in real time. Let’s explore a fraud detection case study.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – financial institution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A financial institution wants to detect fraudulent transactions in real time
    to protect its customers and prevent monetary losses. The institution collects
    transaction data, including transaction amounts, timestamps, merchant information,
    and customer details. By leveraging ML algorithms, they aim to build a robust
    fraud detection system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preprocessing**: The financial institution processes and cleans the
    transaction data, ensuring data integrity and consistency. They may also enrich
    the data by incorporating additional information, such as IP addresses or device
    identifiers, to enhance fraud detection capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Relevant features are extracted from the transaction
    data to capture potential indicators of fraudulent activity. These features may
    include transaction amounts, frequency, geographical location, deviation from
    typical spending patterns, and customer transaction history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**: The financial institution selects suitable ML algorithms,
    such as anomaly detection techniques or supervised learning methods (for example,
    logistic regression and gradient boosting), to train the fraud detection model.
    The model is trained on historical data labeled as fraudulent or non-fraudulent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time monitoring**: Once the model has been trained, it is deployed to
    analyze incoming transactions in real time. The model assigns a fraud probability
    score to each transaction, and transactions exceeding a certain threshold are
    flagged for further investigation or intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous improvement**: The financial institution continuously refines
    the fraud detection model by monitoring its performance and incorporating new
    data. They periodically evaluate the model’s effectiveness, adjust thresholds,
    and update the model to adapt to evolving fraud patterns and techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying ML techniques to customer churn prediction and fraud detection,
    organizations can enhance their decision-making processes, improve customer retention,
    and mitigate financial risks. These case studies highlight the practical application
    of ML in real-world scenarios, demonstrating its value in various industries.
  prefs: []
  type: TYPE_NORMAL
- en: Future trends in Spark ML and distributed ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the field of ML continues to evolve, there are several future trends and
    advancements that we can expect in Spark ML and distributed ML. Here are a few
    key areas to watch:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning integration**: Spark ML is likely to see deeper integration
    with deep learning frameworks such as TensorFlow and PyTorch. This will enable
    users to seamlessly incorporate deep learning models into their Spark ML pipelines,
    unlocking the power of neural networks for complex tasks such as image recognition
    and natural language processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated ML**: Automation will play a significant role in simplifying and
    accelerating the machine learning process. We can anticipate advancements in automated
    feature engineering, hyperparameter tuning, and model selection techniques within
    Spark ML. These advancements will make it easier for users to build high-performing
    models with minimal manual effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainable AI**: As the demand for transparency and interpretability in
    machine learning models grows, Spark ML is likely to incorporate techniques for
    model interpretability. This will enable users to understand and explain the predictions
    made by their models, making them more trustworthy and compliant with regulatory
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative AI (GenAI):** GenAI is the latest rage. As use cases for GenAI
    become more in demand, the current platforms may incorporate some of the LLMs
    that are used in GenAI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge computing and IoT**: With the rise of edge computing and the **Internet
    of Things** (**IoT**), Spark ML is expected to extend its capabilities to support
    ML inference and training on edge devices. This will enable real-time, low-latency
    predictions and distributed learning across edge devices, opening up new possibilities
    for applications in areas like smart cities, autonomous vehicles, and industrial
    IoT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And with that we’ve concluded the learning portion of the book. Let’s briefly
    recap what we’ve covered.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, Spark ML provides a powerful and scalable framework for distributed
    ML tasks. Its integration with Apache Spark offers significant advantages in terms
    of processing large-scale datasets, parallel computing, and fault tolerance. Throughout
    this chapter, we explored the key concepts, techniques, and real-world examples
    of Spark ML.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the ML life cycle, emphasizing the importance of data preparation,
    model training, evaluation, deployment, monitoring, and continuous improvement.
    We also compared Spark MLlib and Spark ML, highlighting their respective features
    and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we discussed various key concepts and techniques related
    to Spark ML. We explored different types of ML, such as classification, regression,
    time series analysis, supervised learning, and unsupervised learning. We highlighted
    the importance of data preparation and feature engineering in building effective
    ML pipelines. We also touched upon fault-tolerance and reliability aspects in
    Spark ML, ensuring robustness and data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we examined real-world use cases, including customer churn prediction
    and fraud detection, to demonstrate the practical applications of Spark ML in
    solving complex business challenges. These case studies showcased how organizations
    can leverage Spark ML to enhance decision-making, improve customer retention,
    and mitigate financial risks.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue your journey in ML with Spark ML, it is important to keep the
    iterative and dynamic nature of the field in mind. Stay updated with the latest
    advancements, explore new techniques, and embrace a mindset of continuous learning
    and improvement.
  prefs: []
  type: TYPE_NORMAL
- en: By harnessing the power of Spark ML, you can unlock valuable insights from your
    data, build sophisticated ML models, and make informed decisions that drive business
    success. So, leverage the capabilities of Spark ML, embrace the future trends,
    and embark on your journey toward mastering distributed ML.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this chapter. Hopefully, it will help you on your exciting journey
    in the world of ML models. The next two chapters are mock tests to prepare you
    for the certification exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 5: Mock Papers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will provide two mock papers to help readers prepare for the certification
    exam by practicing questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19176_09.xhtml#_idTextAnchor242)*,* *Mock Paper 1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19176_10.xhtml#_idTextAnchor246)*,* *Mock Paper 2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
