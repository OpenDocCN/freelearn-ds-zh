- en: Exploring Text Data and Unstructured Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 探索文本数据和非结构化数据
- en: The need to become literate with both structured and unstructured data continues
    to evolve. Working with structured data has well-established techniques such as
    merging and uniform data types, which we have reviewed in prior chapters. However,
    working with unstructured data is a relatively new concept and is rapidly turning
    into a must-have skill in data analysis. **Natural Language Processing** (**NLP**)
    has evolved into an essential skill, so this chapter introduces the concepts and
    tools available to analyze narrative free text. As technology has advanced, using
    these techniques can help you to provide transparency to unstructured data, which
    would have been difficult to uncover only a few years ago.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 需要熟练掌握结构化和非结构化数据的需求持续演变。处理结构化数据有成熟的技巧，如合并和统一数据类型，这些我们在前面的章节中已经回顾过。然而，处理非结构化数据是一个相对较新的概念，正在迅速成为数据分析中的一项必备技能。**自然语言处理**（**NLP**）已经发展成为一项基本技能，因此本章介绍了可用于分析叙述自由文本的概念和工具。随着技术的进步，使用这些技术可以帮助你为非结构化数据提供透明度，这在几年前是难以发现的。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Preparing to work with unstructured data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备处理非结构化数据
- en: Tokenization explained
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇化解释
- en: Counting words and exploring results
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数单词并探索结果
- en: Normalizing text techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本归一化技术
- en: Excluding words from analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除分析中的词汇
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The GitHub repository of this book is at [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书在GitHub上的仓库地址为[https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10)。
- en: You can download and install the required software from [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)下载并安装所需的软件。
- en: Preparing to work with unstructured data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备处理非结构化数据
- en: Today, we are living in a digital age where data is entangled into our lives
    in ways not technically possible or even imaginable before. From social media
    to mobile to the **Internet of Things** (**IoT**), humanity is living in what
    is commonly known as the information age. This age is where an exponentially growing
    of data about you is available to you instantaneously anywhere in the world. What
    has made this possible has been a combination of people and technology, including
    contributions from the **Evolution of Data Analysis**, which was introduced in
    [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals of Data
    Analysis*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们生活在一个数字时代，数据以一种在以前技术上不可能或甚至无法想象的方式融入了我们的生活。从社交媒体到移动设备，再到**物联网**（**IoT**），人类正生活在通常被称为信息时代的环境中。在这个时代，关于你的数据呈指数级增长，你可以随时随地即时获取。使这一切成为可能的是人和技术相结合的结果，包括来自**数据分析的演变**的贡献，这在[第1章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，《数据分析基础》中有所介绍。
- en: It is commonly predicted by multiple sources that 80 percent of all of the data
    created around the world will be unstructured over the next few years. If you
    recall from [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*., unstructured data is commonly defined as information that
    does not offer uniformity and pre-defined organization. Examples of unstructured
    data include free text, chatbots, **Artificial Intelligence** (**AI**), photos,
    videos, and audio files. Social media, of course, produces the highest volume,
    velocity, and variety of unstructured data. So, we're going to focus our examples
    on those data sources, but you can apply the concepts learned in this chapter
    to any narrative-based text data sources such as help desk ticket logs or email
    communications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多个来源普遍预测，在未来几年内，全球产生的所有数据中将有80%是非结构化的。如果你还记得[第1章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，《数据分析基础》中的内容，非结构化数据通常被定义为不提供统一性和预定义组织的信息。非结构化数据的例子包括自由文本、聊天机器人、**人工智能**（**AI**）、照片、视频和音频文件。社交媒体当然产生了最高量的非结构化数据，速度和多样性。因此，我们将重点关注这些数据源，但你也可以将本章学到的概念应用于任何基于叙述的文本数据源，如帮助台票据日志或电子邮件通信。
- en: What's important to understand about working with unstructured data is that
    it's inherently messy. Without structure, defined data types, and conformity applied
    to data when it's captured, uncertainty is created. This should not prohibit or
    discourage you from working with the data, but just being conscious that precision
    is going to be a luxury in some cases. For example, words in a sentence can and
    will be misspelled. The meaning of a word or phrase can be misrepresented and
    the context is sometimes lost. With all of these shortcomings, the technology
    continues to be improved to a point where you will find it being used in everyday
    life. For example, chatbots are now replacing many customer service solutions
    so you may think you're talking with an agent but it is actually software with
    NLP algorithms installed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于处理非结构化数据的重要理解是，它本质上是混乱的。没有结构、定义的数据类型以及在数据捕获时应用的一致性，就会产生不确定性。这不应该阻止或阻止你处理数据，但你应该意识到在某些情况下，精确性将是一种奢侈。例如，句子中的单词可能会拼写错误。一个词或短语的含义可能会被错误地表达，有时上下文也会丢失。尽管存在所有这些缺点，这项技术仍在不断改进，以至于你会在日常生活中发现它的应用。例如，聊天机器人现在正在取代许多客户服务解决方案，所以你可能认为你正在与一个代理交谈，但实际上是安装了NLP算法的软件。
- en: 'The concepts of NLP have been around for decades. Using NLP concepts have some
    fundamentals that you may have already used in the past such as rule-based conditions
    or tags. A rule-based example would be when a specific keyword or collection of
    words is found in a dataset with a flag field used to identify it, similar to
    the following table where we have three columns with five rows including a header
    row to identify the purpose of each record:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的概念已经存在了几十年。使用NLP概念有一些基本原理，你可能过去已经使用过，比如基于规则的条件或标签。一个基于规则的例子是，当在数据集中找到一个特定的关键词或一组单词，并使用标志字段来识别它时，类似于以下表格，其中我们有三列和五行，包括一个标题行来识别每条记录的目的：
- en: '![](img/c459f02a-2498-4c02-ba87-b87887b0e0b2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c459f02a-2498-4c02-ba87-b87887b0e0b2.png)'
- en: In the table, I have changed the font color for any variation of the keyword
    `balanced` to make it easier to identify. As you can observe from the table, the
    word does have minor variations that would seem obvious to a human but must be
    programmed as a rule to account for the differences. So, to identify any variation
    of the keyword such as whether it is capitalized or conjugated, it must be accounted for
    using conditional logic. In this example, a conditional field named `key_word_found_flag`
    is used to determine whether the phrase field contains the keyword, which is using
    a value of `1` to identify a true case.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格中，我将关键词“balanced”的任何变体都改变了字体颜色，以便更容易识别。正如你可以从表格中观察到的，这个词确实有一些细微的变体，这对人类来说似乎是显而易见的，但必须编程为规则来考虑这些差异。因此，为了识别任何关键词的变体，例如它是否是大写或变形的，必须使用条件逻辑来考虑。在这个例子中，使用了一个名为“key_word_found_flag”的条件字段来确定短语字段是否包含关键词，它使用值“1”来识别一个真实案例。
- en: Rule-based NLP has evolved from earlier concepts that used basic wildcard searches
    and ASCII character identification to machine-learned statistical models. I don't
    plan on going into all of the details behind the mathematical models used by NLP,
    but I encourage you to explore the subject. I will focus, in this chapter, on
    using predefined NLP libraries as is without adjusting or training the models.
    The intention is to help you to gain some confidence in when, where, and how it
    can be used. Over the last few years, I have been impressed by the level of accuracy
    behind solutions that leverage NLP, but I recognize it is an evolving technology.
    I was reminded of this when I interacted with Alexa the other day when it told
    me it could not understand my question regardless of how I phrased it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的NLP已经从早期使用基本通配符搜索和ASCII字符识别的概念发展到了机器学习的统计模型。我并不打算深入探讨NLP所使用的数学模型的细节，但我鼓励你探索这个主题。在本章中，我将专注于使用预定义的NLP库，而不对其进行调整或训练模型。目的是帮助你建立一些信心，了解何时、何地以及如何使用它。在过去的几年里，我对利用NLP的解决方案背后的准确性水平印象深刻，但我认识到它是一个不断发展的技术。前几天当我与Alexa互动时，它告诉我无论我如何措辞，它都无法理解我的问题，这让我想起了这一点。
- en: Anytime you use or create solutions with NLP, the potential for a **False Positive
    (FP)** or **False Negative (FN)** exists. An FP or FN is when the NLP algorithm
    has incorrectly predicted the output. When the output returns true, but the actual
    result is false, it is considered an FP. When the model returns false or was missing
    but the correct result was to return true, it is called an FN. Conversely, when
    the algorithm or model correctly predicts the output, this is called either a
    **True Positive (TP)** or **True Negative (TN)**. It is common to use a confusion
    matrix to identify and train the model to reduce FPs and FNs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时使用或创建与自然语言处理相关的解决方案，都存在**假阳性（FP）**或**假阴性（FN）**的可能性。FP或FN是指自然语言处理算法错误地预测了输出。当输出返回为真，但实际结果是假时，这被认为是FP。当模型返回为假或缺失，但正确的结果应该是返回真时，这被称为FN。相反，当算法或模型正确预测输出时，这被称为**真阳性（TP）**或**真阴性（TN）**。通常使用混淆矩阵来识别和训练模型，以减少FP和FN。
- en: 'I created a version of a confusion matrix found in the following screenshot to
    help to identify NLP predictions where the results for positive are in green and
    negative in red. What determines the color and designation is comparing the actual
    verse in the predicted output. For example, if a computer model is used to predict
    the outcome of testing a person for a disease, a **TP** or **TN** would indicate
    the model either accurately predicted that the person has the disease or verified
    they do not have the disease. For those cases, the green color boxes indicate
    a match to the actual outcomes. For any other outcome, the boxes are colored in
    red because the prediction inaccurately returned a false result:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个类似于以下截图中的混淆矩阵版本，以帮助识别自然语言处理预测结果，其中阳性结果用绿色表示，阴性结果用红色表示。颜色和标识的确定是通过比较预测输出中的实际诗句来实现的。例如，如果使用计算机模型来预测测试某个人是否患有疾病的结果，一个**TP**或**TN**将表明模型准确预测了该人患有疾病或确认他们没有疾病。对于这些情况，绿色方框表示与实际结果相匹配。对于任何其他结果，方框都涂成红色，因为预测不准确地返回了错误的结果：
- en: '![](img/813ec9d4-abe0-40c3-b27b-e20eda1d3101.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/813ec9d4-abe0-40c3-b27b-e20eda1d3101.png)'
- en: Validating the results of NLP models can and should be done by a human when
    the data volumes are small. This will ensure trust exists with the code behind
    the algorithm. However, when data volumes are large, this becomes a time-consuming
    task, and in many cases, unrealistic to verify without alternative solutions such
    as crowd-sourcing. Crowd-sourcing solutions are when humans are used for data
    entry in the process, but at a large scale by breaking up tasks and distributing
    them to a population of people to validate data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量较小时，验证自然语言处理模型的结果可以通过人工进行，这可以确保算法背后的代码具有可信度。然而，当数据量较大时，这变成了一项耗时的工作，在许多情况下，没有替代解决方案（如众包）就无法实现验证。众包解决方案是指在数据录入过程中使用人类，但通过分解任务并将它们分配给大量人群来大规模验证数据。
- en: Typical NLP or data science models will use supervised learning techniques where
    training sets of data are used to teach the model to be accurate in its prediction
    output. The labeled data used for training will be a small population of data
    classified by a person or people with an accurate outcome. So, the statistical
    models used to accurately predict the results are based on teaching the NLP model
    based on trained data. Using mislabeled training data in supervised NLP models
    will result in more false positives and inaccurate results. If you work with a
    data scientist or data engineering team using supervised learning, you can showcase
    your data literacy knowledge by asking what process is used to retrain and re-evaluate
    the accuracy of the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的自然语言处理或数据科学模型将使用监督学习技术，其中使用训练数据集来教会模型在预测输出中保持准确。用于训练的标记数据将是一个由具有准确结果的人或人们分类的小数据集。因此，用于准确预测结果的统计模型是基于根据训练数据教导自然语言处理模型。在监督式自然语言处理模型中使用错误标记的训练数据会导致更多的假阳性和不准确的结果。如果你与使用监督学习的数据科学家或数据工程团队合作，你可以通过询问用于重新训练和评估模型准确性的过程来展示你的数据素养知识。
- en: Now that we have some fundamental understanding of working with unstructured
    data, let's walk through how to gather it from its source. For social media data
    from platforms such as Twitter, using their API connection would be an option
    and allows you to consume data in near real time by streaming the data into your
    Jupyter notebook. Using any social media's APIs will require you to set up a user
    account and create a secret key so you can use a REST client connection via HTTP.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对处理非结构化数据有一些基本理解，让我们来看看如何从其来源收集它。对于来自Twitter等平台的社交媒体数据，使用它们的API连接将是一个选项，并允许您通过将数据流式传输到Jupyter笔记本中几乎实时地消费数据。使用任何社交媒体的API将需要您设置用户账户并创建一个密钥，这样您就可以通过HTTP使用REST客户端连接。
- en: Depending on the network connection used by your workstation, you may need to
    adjust your firewall or proxy settings to allow you access to consume data via
    APIs and REST.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您工作站使用的网络连接，您可能需要调整防火墙或代理设置，以便允许您通过API和REST访问数据：
- en: Because there are restrictions and different API limits on exporting social
    media data by platform, we will be using the sample data pre-installed with the
    packages from the Python libraries. The **Natural Language Toolkit** (**NLTK**)
    package is available to help us to work with unstructured data in multiple use
    cases. The software is open source and contains dozens of modules to support NLP using
    a concept called a corpus. A corpus creates a taxonomy that can be used by linguists,
    computer scientists, and machine learning enthusiasts. A taxonomy is a breakdown
    of the language of a body of a text into its fundamental elements. Examples include
    the American Heritage Dictionary and Wikipedia. It also serves as the foundation
    for counting and searching words when using software.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同平台在导出社交媒体数据时存在限制和不同的API限制，我们将使用Python库中预安装的样本数据。**自然语言工具包**（**NLTK**）包可以帮助我们处理多种用例中的非结构化数据。该软件是开源的，包含数十个模块，使用称为语料库的概念来支持NLP。语料库创建了一个分类法，可以被语言学家、计算机科学家和机器学习爱好者使用。分类法是将文本体的语言分解为其基本元素的分解。例如，包括美国遗产词典和维基百科。它还作为使用软件时计数和搜索单词的基础。
- en: Corpus in action
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语料库的实际应用
- en: 'For our first example, we are going to use the Brown Corpus, which contains
    hundreds of sample language text categorized by an eclectic mix of subjects such
    as mystery short stories, political press releases, and religion. The original
    collection had over a million words defined and tagged with a part of speech such
    as noun, verb, and preposition. To install the Brown Corpus and use it in your
    Jupyter notebook, use the following steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个例子中，我们将使用布朗语料库，它包含数百个按神秘短篇小说、政治新闻稿和宗教等主题的杂乱组合分类的样本语言文本。原始集合有超过一百万个单词被定义并标记为名词、动词和介词等词性。要在Jupyter笔记本中使用布朗语料库，请按照以下步骤操作：
- en: Launch a new Jupyter notebook and name it `ch_10_exercises`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个新的Jupyter笔记本，并将其命名为`ch_10_exercises`。
- en: 'Import the following libraries by adding the following command in your Jupyter
    notebook and running the cell. Feel free to follow along by creating your own
    notebook (I have placed a copy in GitHub for reference):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在Jupyter笔记本中添加以下命令并运行单元格来导入以下库。您可以自由地通过创建自己的笔记本来跟随（我在GitHub上放置了一个副本以供参考）：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml),
    *Overview of Python and Installing Jupyter Notebook*, for help with setting up
    your environment. If you are behind a firewall, there is an `nltk.set_proxy` option
    available. Check the documentation at [http://www.nltk.org/](http://www.nltk.org/) for
    more details.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该库应该已经通过Anaconda可用。有关设置环境的帮助，请参阅[第2章](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml)，*Python和Jupyter
    Notebook安装概述*。如果您在防火墙后面，有一个`nltk.set_proxy`选项可用。有关更多详细信息，请查看[http://www.nltk.org/](http://www.nltk.org/)上的文档。
- en: 'Next, we download the specific corpus we want to use. Alternatively, you can
    download all of the packages using the `all` parameter:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们下载我们想要使用的特定语料库。或者，您可以使用`all`参数下载所有包：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output would look like the following screenshot where the package download
    is confirmed and the output is verified with `True`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中确认了包的下载，并且输出通过`True`进行了验证：
- en: '![](img/0373d61e-eb3d-4b3e-85b4-fd106c67527e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0373d61e-eb3d-4b3e-85b4-fd106c67527e.png)'
- en: 'To confirm the package is available in your Jupyter notebook, we can use the
    following command to reference the corpus using a common alias of `brown` for
    reference:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要确认包在你的Jupyter笔记本中可用，我们可以使用以下命令通过常用的别名`brown`引用语料库：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To display a list of the few words available in the Brown Corpus, use the following
    command:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示布朗语料库中可用的一些单词的列表，请使用以下命令：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output would look like the following screenshot where six words in single
    quotes are displayed along with an ellipsis inside square brackets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中显示六个单引号内的单词，以及方括号内的省略号：
- en: '![](img/50455bd2-1623-4811-9cee-30396d9c18da.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/50455bd2-1623-4811-9cee-30396d9c18da.png)'
- en: 'To count all of the words available, we can use the `len()` function, which
    counts the length of a string or the number of items in an object such as an array
    of values. Since our values are separated by commas, it will count all of the
    words available. To make it easier to format, let''s assign the output to a variable
    called `count_of_words`, which we can use in the next step:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算所有可用的单词，我们可以使用`len()`函数，该函数计算字符串的长度或对象（如值数组）中的项目数量。由于我们的值由逗号分隔，它将计算所有可用的单词。为了使其格式化更容易，让我们将输出分配给一个名为`count_of_words`的变量，我们可以在下一步中使用它：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To make the output easier to understand for the consumer of this data, we use
    the `print()` and `format()` functions to display the results using the following
    command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使数据消费者更容易理解输出，我们使用`print()`和`format()`函数通过以下命令显示结果：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output would look like the following screenshot where a sentence will appear
    that includes a dynamic count of all of the words assigned to the `count_of_words`
    variable. We also formatted the value to display with a comma:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中将显示一个句子，该句子包括分配给`count_of_words`变量的所有单词的动态计数。我们还格式化了值以显示逗号：
- en: '![](img/af69b04d-4b70-42dd-b5bb-44999c5bd05d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af69b04d-4b70-42dd-b5bb-44999c5bd05d.png)'
- en: Excellent, you have now successfully loaded your first NLP library and were
    able to run a few commands against a popular corpus package. Let's continue dissecting
    the different elements of NLP by explaining why tokenization is important.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，你现在已经成功加载了你的第一个NLP库，并且能够对流行的语料库包运行一些命令。让我们继续通过解释为什么分词很重要来剖析NLP的不同元素。
- en: Tokenization explained
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词解释
- en: Tokenization is the process of breaking unstructured text such as paragraphs,
    sentences, or phrases down into a list of text values called tokens. A token is
    the lowest unit used by NLP functions to help to identify and work with the data.
    The process creates a natural hierarchy to help to identify the relationship from
    the highest to the lowest unit. Depending on the source data, the token could
    represent a word, sentence, or individual character.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将非结构化文本（如段落、句子或短语）分解成一系列称为标记的文本值的过程。标记是NLP函数使用的最低单位，用于帮助识别和处理数据。这个过程创建了一个自然层次结构，有助于从最高单位到最低单位识别关系。根据源数据，标记可以代表单词、句子或单个字符。
- en: The process to tokenize a body of text, sentence, or phrase, typically starts
    with breaking apart words using the white space in between them. However, to correctly
    identify each token accurately requires the library package to account for exceptions
    such as hyphens, apostrophes, and a language dictionary, to ensure the value is
    properly identified. Hence, tokenization requires the language of origin of the
    text to be known to process it. Google Translate, for example, is an NLP solution
    that can identify the language, but still has the option for users to define it
    to ensure the translation is accurate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分词文本、句子或短语的过程通常从使用它们之间的空白字符将单词分开开始。然而，为了准确识别每个标记，需要库包考虑到诸如连字符、撇号和语言字典等例外情况，以确保值被正确识别。因此，分词需要知道文本的原始语言才能进行处理。例如，Google
    Translate是一个可以识别语言的NLP解决方案，但用户仍然可以选择定义它，以确保翻译的准确性。
- en: This is one of the reasons why tokenization is an evolving process. For example,
    as new words are added to the English language, the NLP reference libraries require
    an update. Handling sarcasm, dual meanings, and catchphrases may get lost in translation
    when using NLP solutions such as Alexa or Siri. For example, the phrase **social
    ambassador** has an obvious meaning to a human but would require the library to
    be trained to identify it as a token phrase.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么分词是一个不断发展的过程的原因之一。例如，随着新单词被添加到英语中，NLP参考库需要更新。处理讽刺、双重含义和流行语可能会在使用NLP解决方案（如Alexa或Siri）时在翻译中丢失。例如，短语**社会大使**对人类来说有明显的含义，但需要库被训练以识别它作为一个标记短语。
- en: There are a few techniques used by NLP to address this issue. The first is called
    n-grams, which is the process of combining words within the same sentence as a
    group, typically of two or three words, to create a pattern that is recognizable
    by the NLP library. Acronyms can also be used in n-grams but require identification
    or training to be effective. An n-gram could then be used to identify **social
    ambassador** to understand these two values can be used together.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: NLP用于解决这个问题的技术有几个。第一个被称为n-gram，这是将同一句子中的单词作为一组组合的过程，通常是两到三个单词，以创建一个可以被NLP库识别的模式。缩写也可以用于n-gram中，但需要识别或训练才能有效。n-gram可以用来识别**社会大使**，以理解这两个值可以一起使用。
- en: Another common reference for an n-gram is a bi-gram, which only uses two words.
    The **n** denotes the number of grams so a uni-gram stands for one gram, a bi-gram
    is for two, a tri-gram is for three, and so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram的另一个常见参考是二元组，它只使用两个单词。**n**表示gram的数量，所以uni-gram代表一个gram，bi-gram代表两个，tri-gram代表三个，依此类推。
- en: Another concept is called a **bag of words**, which is when a high occurrence
    of specific words exist in the source data. The use of a **bag of words** is another
    helpful way to identify patterns and key term searches against large text sources
    of data. For example, a prediction model to improve response time from system
    outages can use historical text logs found in help desk tickets. The **bag of
    words** technique can be used to create multiple flag fields (yes or no) as inputs
    into the algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个概念被称为**词袋**，这指的是在源数据中存在特定单词的高频出现。使用**词袋**是另一种有助于识别大型文本数据源中模式和关键术语搜索的有用方法。例如，一个用于提高系统故障响应时间的预测模型可以使用在帮助台票据中找到的历史文本日志。**词袋**技术可以用来创建多个标志字段（是或否）作为算法的输入。
- en: So, the study of tokenization and NLP is a deep subject that will remain an
    evolving science. I recommend continuing to research the subject in more detail.
    The Stanford University NLP site is a fantastic source of information that I have
    added to the *Further reading* section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分词和NLP的研究是一个深奥的主题，它将保持为一个不断发展的科学。我建议继续深入研究这个主题。斯坦福大学NLP网站是*进一步阅读*部分中一个极好的信息来源。
- en: Let's explore the additional features available in the NLTK library in our Jupyter
    notebook. Another downloadable library available is called **Punkt**, which is
    used to tokenize sentences into words. I have included a link to the downloads
    available from NLTK in the *Further reading* section. The code behind the algorithm
    requires a high volume of text so the model can be trained but the NLTK data package
    includes a pre-trained option in English that we can use.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Jupyter笔记本中探索NLTK库中可用的附加功能。另一个可下载的库被称为**Punkt**，它用于将句子分解成单词。我在*进一步阅读*部分中包含了一个链接，指向NLTK提供的下载。算法背后的代码需要大量的文本以便模型可以进行训练，但NLTK数据包包括一个预训练的英文选项，我们可以使用。
- en: Tokenize in action
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词实战
- en: 'You will continue by going back into the `ch_10_exercises` notebook in Jupyter:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你将继续通过回到Jupyter中的`ch_10_exercises`笔记本：
- en: 'Import the following libraries by adding the following command in your Jupyter
    notebook and run the cell. Feel free to follow along by creating your own notebook
    (I have placed a copy on GitHub for reference):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本中添加以下命令并运行单元格以导入以下库。你可以自由地跟随创建自己的笔记本（我已经在GitHub上放置了一个副本以供参考）：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output would look like the following screenshot where the package download
    is confirmed and the output is verified with `True`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中确认了包的下载，并且输出通过`True`进行了验证：
- en: '![](img/d462896d-7985-4963-a825-2a37bc0bacb3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d462896d-7985-4963-a825-2a37bc0bacb3.png)'
- en: 'Next, we will create a new variable called `input_sentence` and assign it to
    a free-form text sentence that must be encapsulated in double quotes and on a
    single line input. There will be no input after you run the cell:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个名为`input_sentence`的新变量，并将其分配给一个必须用双引号括起来并单行输入的自由文本句子。在您运行单元格后，将不会有输入：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will use the `word_tokenize()` function that is available in the NLTK
    library to break up the individual words and any punctuation:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用NLTK库中可用的`word_tokenize()`函数来分解单个单词和任何标点符号：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output would look like the following screenshot where the individual words
    are broken out from the sentence as an array of values with single quotes around
    each text value surrounded by square brackets:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中单个单词从句子中分离出来，形成一个包含单引号和方括号的值数组：
- en: '![](img/30e87028-a530-4237-b0ef-f1d6579a93fb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/30e87028-a530-4237-b0ef-f1d6579a93fb.png)'
- en: 'Next, let''s tokenize by sentence, which requires you to import the `sent_tokenize` option
    from the NLTK `tokenize` library using the following command:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们通过句子进行分词，这需要您使用以下命令从NLTK的`tokenize`库中导入`sent_tokenize`选项：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let''s assign a new variable called `input_data` to a collection of sentences
    that we can use later in our code. There will be no input after you run the cell:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将一个名为`input_data`的新变量分配给一组句子，我们可以在代码的后续部分中使用。在您运行单元格后，将不会有输入：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we will pass the `input_data` variable as a parameter to the `sent_tokenize()`
    function, which will look at the string of text and break it down into individual
    token values. We wrap the output with the `print()` function to display the results
    cleanly in the notebook:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将`input_data`变量作为参数传递给`sent_tokenize()`函数，该函数将查看文本字符串并将其分解成单个标记值。我们用`print()`函数包装输出，以便在笔记本中清晰地显示结果：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output would look like the following screenshot where the individual sentences
    are broken down as an array of string values with single quotes around them surrounded
    by square brackets:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中单个句子被分解为一个包含单引号和方括号的字符串值数组：
- en: '![](img/9e1dda99-e155-412c-8924-f92617a105fc.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/9e1dda99-e155-412c-8924-f92617a105fc.png)'
- en: So, now you can see that having these NLTK library features can help you to
    work with unstructured data by breaking down language into foundational pieces.
    As a data analyst, you will be faced with free text in many different forms, so
    now you have some additional resources to leverage. Once you have the data tokenized,
    additional options are available that we will explore in the next section, such
    as counting the frequency of words to identify patterns within the underlying
    source.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在您可以看到，拥有这些NLTK库功能可以帮助您通过将语言分解成基础部分来处理非结构化数据。作为一名数据分析师，您将面临许多不同形式的自由文本，因此现在您有一些额外的资源可以利用。一旦数据被分词，将提供额外的选项，我们将在下一节中探讨，例如通过计数单词频率来识别底层源中的模式。
- en: Counting words and exploring results
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数单词并探索结果
- en: Counting word frequency will provide initial metadata about the unstructured
    source text. Exposing the count of the occurrence of a word or when specific words
    are missing within a body of text is known as **text mining**. Text mining will
    provide analytics about the data, so a data analyst can determine the value of
    a data asset along with how it can be used to answer business questions. Likewise,
    you can identify keyword patterns that occur during unexpected outages that impact
    users by looking at application system logs. Once those words or phrases are identified,
    you can work with the developers to identify the root cause and reduce the impact
    on your application users.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单词频率将提供关于非结构化源文本的初始元数据。在文本体中暴露单词或特定单词缺失的计数被称为**文本挖掘**。文本挖掘将提供关于数据的分析，以便数据分析师可以确定数据资产的价值以及如何使用它来回答业务问题。同样，您可以通过查看应用程序系统日志来识别在意外中断期间影响用户的关键词模式。一旦确定了这些单词或短语，您就可以与开发者合作来识别根本原因，并减少对应用程序用户的影响。
- en: A popular option available for text analysis is the use of regular expressions
    or **regex** for short. The regex concept is when you use a combination of rules
    and search patterns to extract features from very large, unstructured text. Regex
    becomes useful when reading the text line by line would be unreasonable, based
    on the amount of time and number of people required. Regex has a wide range of
    applications including how to separate emails, phone numbers, and hashtags from
    the source text.  If you are working with help desk ticket logs, for example,
    you would tag successful matches to the regex rules with a unique ticket ID so
    you can join the unstructured data back to the data model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析中一个流行的选项是使用正则表达式或简称**regex**。正则表达式的概念是当你使用一系列规则和搜索模式从非常大的非结构化文本中提取特征时。当逐行读取文本不合理，基于所需的时间和人数时，正则表达式变得有用。正则表达式有广泛的应用，包括如何从源文本中分离电子邮件、电话号码和标签。例如，如果你在处理帮助台票据日志，你会将正则表达式规则的成功匹配标记为具有唯一票据ID，这样你就可以将非结构化数据重新连接到数据模型。
- en: Regex covers a variety of techniques including wildcard character searches,
    pattern matching based on qualifiers, and anchors used to identify the beginning
    or end of textual data. Regex rules are usually combined with standard software
    engineering, so the code can be modular and automated when looking at high-frequency
    data sources such as a chatbot or system logs. For example, if you created a regex
    rule against any combination of the keyword `frustrated` in a customer service
    system, you can create a flag field named `is_customer_frustrated_yes_no` with
    a value of 1 for true and 0 for false. Regex rules can and should evolve over
    time, based on the data and validation, the rules that are accurate is important.
    This can be done with a random sampling of data by manually validating the conditions
    exist and returning the correct result.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式涵盖了各种技术，包括通配符字符搜索、基于限定符的模式匹配以及用于识别文本数据开始或结束的锚点。正则表达式规则通常与标准软件工程相结合，因此代码在查看高频数据源（如聊天机器人或系统日志）时可以模块化和自动化。例如，如果你在客户服务系统中创建了一个针对关键词`frustrated`任何组合的正则表达式规则，你可以创建一个名为`is_customer_frustrated_yes_no`的标志字段，其值为1表示真，0表示假。正则表达式规则可以并且应该随着时间的推移而演变，基于数据和验证，准确的规则非常重要。这可以通过对数据进行随机抽样、手动验证条件是否存在并返回正确结果来完成。
- en: Counting words
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词计数
- en: Before we explore those options, let's continue with the Jupyter notebook exercise
    and walk through how to count the frequency of words from a population of free
    text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索这些选项之前，让我们继续进行Jupyter笔记本练习，并了解如何从自由文本的总体中计算单词频率。
- en: 'You will continue by going back into the `ch_10_exercises` notebook in Jupyter:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你将继续通过回到Jupyter中的`ch_10_exercises`笔记本：
- en: 'Import the probability module available in the NTLK library to count the frequency
    of the words available in a body of text. There will be no result returned after
    you run the cell:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入NTLK库中可用的概率模块，以统计文本体中单词的频率。运行单元格后不会返回任何结果：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, explore a large body of text using the Brown Corpus. To do this, we assign
    the population of all of the token words available by using the `FreqDist()` function
    and assigning it to a variable named `input_data`. To see the results of the processing
    of this data, we can print the variable:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用布朗语料库探索大量文本。为此，我们使用`FreqDist()`函数将所有可用的标记词的总体分配给一个名为`input_data`的变量。为了查看此数据的处理结果，我们可以打印变量：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output would look like the following screenshot where the frequency distribution
    results are calculated and printed using the `print()` function against the assigned
    variable, `input_data`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中使用`print()`函数针对分配的变量`input_data`计算并打印频率分布结果：
- en: '![](img/9aa1085a-c5ca-4595-bf58-1bd75fda230d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9aa1085a-c5ca-4595-bf58-1bd75fda230d.png)'
- en: 'To see a list of the most common words that exist in `input_data`, we can use
    the `most_common()` function along with a parameter to control how many are displayed.
    In this case, we want to see the top 10:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看`input_data`中存在的最常见单词列表，我们可以使用`most_common()`函数并配合一个参数来控制显示的数量。在这种情况下，我们想查看前10个：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output would look like the following screenshot where a list of name-value
    pairs are displayed like a two-column table with ten rows to show each token,
    which can be a word, punctuation mark, or character enclosed with single quotes
    along with an integer value that provides the cumulative count of the times the
    word appears in the source data:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中显示了一个包含十个行的两列表格，每个行显示一个标记，这可能是一个单词、标点符号或用单引号括起来的字符，以及一个整数值，表示该单词在源数据中出现的累积次数：
- en: '![](img/b131e99b-c597-46a7-ba8d-c58a9da7b8b7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b131e99b-c597-46a7-ba8d-c58a9da7b8b7.png)'
- en: Through this exercise, we can identify and extract words from a body of unstructured text,
    which is the foundation for creating regex rules. The next section will focus
    on normalizing words for consistency to improve the accuracy of NLP model predictions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，我们可以从非结构化文本中识别和提取单词，这是创建正则表达式规则的基础。下一节将专注于规范单词以提高NLP模型预测的准确性。
- en: Normalizing text techniques
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本规范化技术
- en: In most cases, making the regex rules **smarter** by adding new code logic or
    libraries will be required. One such way to do this is by using the concepts behind
    normalizing your text called stemming and lemmatization. Both terms are rooted
    in the study of linguistics, and how they are adopted to be used in technology
    has exploded due to integrating NLP solutions into everything, from customer service
    to speech-to-text features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，通过添加新的代码逻辑或库来使正则表达式规则**更智能**是必要的。这样做的一种方法是通过使用规范文本的概念，即词干提取和词形还原。这两个术语都根植于语言学的研究，并且由于将NLP解决方案集成到从客户服务到语音到文本功能等各个方面，它们在技术中的应用已经爆炸式增长。
- en: When applied to NLP, stemming is when any word is programmatically identified
    to its common root form. In this process, any suffix, plural form, or synonym
    that exists for the word is identified. Stemmers require a reference dictionary
    or lookup to be accurate, so the source language is required. Lemmatization takes
    into account all of the variations of a word so it can be rooted back to a dictionary
    source. From my research, both stemming and lemmatization are used together in
    NLP and you can start by using the open source libraries available, which I included
    in the <q>Further reading</q> section. These libraries should be sufficient to
    cover common words but analogies or custom-defined lingo in your organization
    will require a new corpus. Simple examples of using stemming or a lemma include
    identifying when the word **fishes** appears, and returning **fish,**or **geese** returning **goose**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于NLP时，词干提取是指任何单词被程序性地识别为其常见的词根形式。在这个过程中，任何后缀、复数形式或同义词都会被识别。词干提取器需要一个参考字典或查找表才能准确，因此需要源语言。词形还原考虑了单词的所有变体，以便将其根植于词典源。根据我的研究，词干提取和词形还原在NLP中通常一起使用，你可以从使用开源库开始，这些库我在《进一步阅读》部分中提到了。这些库应该足以涵盖常见单词，但你的组织中的类比或自定义术语将需要一个新语料库。使用词干提取或词形还原的简单例子包括识别单词**fishes**出现时，返回**fish**，或者**geese**返回**goose**。
- en: The subject for both concepts is pretty vast, so I encourage you to continue
    learning about it, but the bottom line is the benefits of using these concepts
    will help to clean and normalize data for analysis. Having the data normalized
    where multiple similar values are grouped together as a single value is necessary.
    It reduces the volume of data you are analyzing and prepares the results for deeper
    analytics such as creating a data science or machine learning model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个概念的主题相当广泛，所以我鼓励你继续学习它们，但底线是使用这些概念的好处将有助于清理和规范数据以进行分析。将多个相似值组合为单个值的数据规范化是必要的。这减少了你需要分析的数据量，并为创建数据科学或机器学习模型等更深入的统计分析做好了准备。
- en: Stemming and lemmatization in action
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取和词形还原的实际应用
- en: For our exercises, we will use the Porter Stemmer, which is commonly used to
    help to prepare text data and normalize data in NLP.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的练习，我们将使用Porter词干提取器，它通常用于帮助准备文本数据和规范NLP中的数据。
- en: 'Let''s continue by going back to the `ch_10_exercises` notebook in Jupyter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续，回到Jupyter中的`ch_10_exercises`笔记本：
- en: 'Import the `PorterStemmer` module available in the NTLK library to normalize
    a word. There will be no result returned after you run the cell:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入NTLK库中可用的`PorterStemmer`模块以规范一个单词。运行单元格后不会返回任何结果：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To import an instance of this feature so it can be referenced later in the
    code, we use the following code. There will be no result returned after you run
    the cell:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要导入此功能的实例以便在代码中稍后引用，我们使用以下代码。运行单元格后不会返回任何结果：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, you can pass individual words into the instance to see how the word would
    be normalized:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以将单个单词传递到实例中，以查看该单词将被如何规范化：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will look like the following screenshot where the stem of the word
    `fishing` will be displayed as `fish`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中单词 `fishing` 的词干将显示为 `fish`：
- en: '![](img/0b061e67-3cb5-40a2-8100-433b2107ca16.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b061e67-3cb5-40a2-8100-433b2107ca16.png)'
- en: 'To use lemma features, we need to download the `WordNet` corpus using the following
    command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用词元特征，我们需要使用以下命令下载 `WordNet` 语料库：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To import an instance of this feature so it can be referenced later in the
    code, we use the following code. There will be no result returned after you run
    the cell:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要导入此功能的实例以便在代码中稍后引用，我们使用以下代码。运行单元格后不会返回任何结果：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To see how the lemma would output for the same word we used a stem for earlier,
    we pass the same word into the `lemmatize()` function:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查看我们之前用于词干化的相同单词的词元输出，我们将相同的单词传递到 `lemmatize()` 函数中：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output would look like the following screenshot where the lemma of the
    word `fishing` will be displayed as `fishing`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中单词 `fishing` 的词元将显示为 `fishing`：
- en: '![](img/72a41c72-fe8c-4f97-9885-d0d8e49bc93e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/72a41c72-fe8c-4f97-9885-d0d8e49bc93e.png)'
- en: So, why are the results different? The algorithms used by each of the NLTK corpora
    apply different approaches to normalize the words. Lemmatization will adjust to
    the form or structure of the word as it is defined in the dictionary that is included
    in the WordNet corpus whereas stemming is intended to break down the word to its
    root. Each is a tool that is available and, depending on the use case, you may
    need to adjust which approach to use to normalize the data for analysis.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么结果不同？NLTK 语料库中每个算法都采用不同的方法来规范化单词。词元化将调整到字典中定义的单词的形式或结构，而词干化旨在将单词分解为其词根。每个都是可用的工具，并且根据用例，您可能需要调整使用哪种方法来规范化分析数据。
- en: To take it one step forward, let's pass a list of words into each instance using
    a loop and print out the results to see how they compare to the original word.
    We will use a sample from the Brown words by limiting the results.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步，让我们使用循环将单词列表传递到每个实例中，并打印出结果以查看它们与原始单词的比较。我们将通过限制结果来使用布朗单词的样本：
- en: If you pass all over a million words into the loop in your Jupyter Notebook
    session, it will take much longer to run and take up resources (RAM and CPU) to
    process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将超过一百万个单词传递到你的 Jupyter Notebook 会话的循环中，它将需要更长的时间来运行，并且需要占用资源（RAM 和 CPU）来处理。
- en: 'To create a list but limit the words to only a sample by assigning it to a
    variable, we use the following command. There will be no result returned after
    you run the cell:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个列表但限制单词仅限于样本，我们可以将其分配给一个变量，我们使用以下命令。运行单元格后不会返回任何结果：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we create a loop against each value in the list and print the results.
    We include some formatting to make it easier to understand the results for each
    row. Be sure to include a carriage return to create a new line to use the `print()`
    function without errors:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个循环，针对列表中的每个值打印结果。我们包括一些格式化，以便更容易理解每行的结果。确保包括换行符以创建新行，以便在没有错误的情况下使用
    `print()` 函数：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will look like the following screenshot where ten lines are printed
    with a colon delimiter used to separate the results for the original word, the
    stem of the word, and the lemma of the word:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中打印了带有冒号分隔符的十行结果，用于分隔原始单词、单词的词干和单词的词元：
- en: '![](img/460f1849-9d73-409d-959e-66fbd21d5356.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/460f1849-9d73-409d-959e-66fbd21d5356.png)'
- en: Now that we learned how to normalize the words within your unstructured data,
    let's find out how to exclude words or phrases from your data to reduce the noise
    so we can focus on valuable keywords that can provide insight.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何在非结构化数据中规范化单词，让我们了解如何从数据中排除单词或短语以减少噪声，这样我们就可以专注于可以提供洞察力的有价值的关键词。
- en: Excluding words from analysis
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排除分析中的单词
- en: Visually sifting through millions of words is impractical in data analysis because
    language includes many linking verbs that are repeated throughout the body of
    a text. Common words such as **am**, **is**, **are**, **was**, **were**, **being**,
    and **been** would be at the top of the `most_common()` list when you apply NLP
    against the source data even after it has been normalized. In the evolution of
    improving NLP libraries, a dictionary of **stopwords** was created to include
    a more comprehensive list of words that provide less value in text analytics.
    Example **stopwords **include linking verbs along with words such as **the**,
    **an**, **a**, and **until**. The goal is to create a subset of data that you
    can focus your analysis on after filtering out these stopwords from your token
    values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，通过视觉筛选数百万个单词是不切实际的，因为语言中包含许多在文本主体中重复的连接动词。当你在源数据上应用NLP并对其进行标准化后，像**am**、**is**、**are**、**was**、**were**、**being**和**been**这样的常见单词将出现在`most_common()`列表的顶部。在改进NLP库的过程中，创建了一个**停用词**字典，包括一个更全面的单词列表，这些单词在文本分析中提供的价值较小。例如，**停用词**包括连接动词以及像**the**、**an**、**a**和**until**这样的单词。目标是创建一个数据子集，在从标记值中过滤掉这些停用词后，你可以专注于分析。
- en: NLP can require high CPU and RAM resources especially working with a large collection
    of words, so you may need to break up your data into logical chucks, such as alphabetically,
    to complete your analysis.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）可能需要大量的CPU和RAM资源，尤其是在处理大量词汇集合时，因此你可能需要将你的数据分成逻辑块，例如按字母顺序，以完成你的分析。
- en: 'You will continue by going back to the `ch_10_exercises` notebook in Jupyter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你将继续回到Jupyter中的`ch_10_exercises`笔记本：
- en: 'Download the `stopwords` corpus from the NLTK library using the following command:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从NLTK库下载`stopwords`语料库：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, import the stopwords and `word_tokenize` features so they can be used
    later in the exercise:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入停用词和`word_tokenize`功能，以便在练习中稍后使用：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let''s assign a new variable called `input_data` to a collection of sentences
    that we can use later in our code. There will be no input after you run the cell:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将一个名为`input_data`的新变量分配给一组句子，我们可以在稍后的代码中使用它。运行单元格后不会有输入：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will assign object variables called `stop_words` and `word_tokens` so they
    can be referenced later in the code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将分配名为`stop_words`和`word_tokens`的对象变量，以便在代码中稍后引用：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we have a few lines of code that will loop through the word tokens
    from `input_data` and compare them to `stop_words`. If they match, they will be
    excluded. The final result prints the original `input_data`, which has been tokenized
    along with the results after the stopwords have been removed. Be sure to use the
    correct indentation when entering the code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有几行代码将循环遍历`input_data`中的单词标记，并将它们与`stop_words`进行比较。如果它们匹配，将被排除。最终结果将打印出经过标记化并移除停用词后的原始`input_data`。在输入代码时，请确保使用正确的缩进：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output would look like the following screenshot where the original sentence
    is displayed as tokens that include all words. The second line of the output will
    have fewer token words because stopwords such as `the` have been removed:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中原始句子以包含所有单词的标记形式显示。输出的第二行将包含较少的标记单词，因为像`the`这样的停用词已经被移除：
- en: '![](img/da7050f1-9b2c-4400-83d6-537c97aaf38c.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da7050f1-9b2c-4400-83d6-537c97aaf38c.png)'
- en: Excellent, we have learned now to exclude common words, which removes the noise
    from large volumes of text. The focus of your analysis will be on keywords and
    phrases to provide context within the text without reading through the entire
    body of unstructured data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们现在已经学会了如何排除常见单词，这从大量文本中移除了噪声。你的分析重点将放在关键词和短语上，以在文本中提供上下文，而无需阅读整个非结构化数据体。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations, you have successfully walked through the foundations of **Natural
    Language Processing** (**NLP**), along with key features that are available when
    working with unstructured data. We explored the **Natural Language Toolkit** (**NLTK**)
    Python library, which offers many options to work with free text by downloading
    different corpora to analyze large bodies of text. We learned how to split raw
    text into meaningful units called tokens so it can be interpreted and refined.
    We learned about regex and pattern matching using words as it applies to NLP.
    We also explored how to count the frequency of words in a collection of text using
    probability and statistical modules. Next, we learned how to normalize words using
    stemming and lemmatization functions, which shows how variations in words can
    impact your data analysis. We explained the concepts of n-grams and how to use
    `stopwords` to remove the noise that is common when working with large bodies
    of free text data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你已经成功走过了**自然语言处理**（**NLP**）的基础，以及处理非结构化数据时可用的重要功能。我们探讨了**自然语言工具包**（**NLTK**）Python
    库，它通过下载不同的语料库来分析大量文本，提供了许多处理自由文本的选项。我们学习了如何将原始文本分割成有意义的单元，称为标记（tokens），以便进行解释和精炼。我们了解了正则表达式和模式匹配在
    NLP 中的应用。我们还探讨了如何使用概率和统计模块来计算文本集中单词的频率。接下来，我们学习了如何使用词干提取和词形还原函数来规范化单词，这展示了单词的变体如何影响你的数据分析。我们解释了
    n-gram 的概念以及如何使用 `stopwords` 来移除在处理大量自由文本数据时常见的噪声。
- en: In the next chapter, [Chapter 11](1a658723-79aa-4447-8dbd-74206cad9aa1.xhtml),
    *Practical Sentiment Analysis,* we will show how prediction models can be applied
    to unstructured data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第11章](1a658723-79aa-4447-8dbd-74206cad9aa1.xhtml)，《实用情感分析》，我们将展示如何将预测模型应用于非结构化数据。
- en: Further reading
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on the relative topics of this chapter, you can refer
    to the following links:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于本章相关主题的信息，您可以参考以下链接：
- en: Creating random sample data: [https://www.mockaroo.com/](https://www.mockaroo.com/)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建随机样本数据：[https://www.mockaroo.com/](https://www.mockaroo.com/)
- en: The NLTK source code and documentation: [https://www.nltk.org/](https://www.nltk.org/)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK 源代码和文档：[https://www.nltk.org/](https://www.nltk.org/)
- en: NLP Stanford University reference: [https://nlp.stanford.edu/](https://nlp.stanford.edu/)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学 NLP 参考资料：[https://nlp.stanford.edu/](https://nlp.stanford.edu/)
