- en: Exploring Text Data and Unstructured Data
  prefs: []
  type: TYPE_NORMAL
- en: The need to become literate with both structured and unstructured data continues
    to evolve. Working with structured data has well-established techniques such as
    merging and uniform data types, which we have reviewed in prior chapters. However,
    working with unstructured data is a relatively new concept and is rapidly turning
    into a must-have skill in data analysis. **Natural Language Processing** (**NLP**)
    has evolved into an essential skill, so this chapter introduces the concepts and
    tools available to analyze narrative free text. As technology has advanced, using
    these techniques can help you to provide transparency to unstructured data, which
    would have been difficult to uncover only a few years ago.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to work with unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting words and exploring results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing text techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excluding words from analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GitHub repository of this book is at [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software from [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to work with unstructured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we are living in a digital age where data is entangled into our lives
    in ways not technically possible or even imaginable before. From social media
    to mobile to the **Internet of Things** (**IoT**), humanity is living in what
    is commonly known as the information age. This age is where an exponentially growing
    of data about you is available to you instantaneously anywhere in the world. What
    has made this possible has been a combination of people and technology, including
    contributions from the **Evolution of Data Analysis**, which was introduced in
    [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals of Data
    Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: It is commonly predicted by multiple sources that 80 percent of all of the data
    created around the world will be unstructured over the next few years. If you
    recall from [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*., unstructured data is commonly defined as information that
    does not offer uniformity and pre-defined organization. Examples of unstructured
    data include free text, chatbots, **Artificial Intelligence** (**AI**), photos,
    videos, and audio files. Social media, of course, produces the highest volume,
    velocity, and variety of unstructured data. So, we're going to focus our examples
    on those data sources, but you can apply the concepts learned in this chapter
    to any narrative-based text data sources such as help desk ticket logs or email
    communications.
  prefs: []
  type: TYPE_NORMAL
- en: What's important to understand about working with unstructured data is that
    it's inherently messy. Without structure, defined data types, and conformity applied
    to data when it's captured, uncertainty is created. This should not prohibit or
    discourage you from working with the data, but just being conscious that precision
    is going to be a luxury in some cases. For example, words in a sentence can and
    will be misspelled. The meaning of a word or phrase can be misrepresented and
    the context is sometimes lost. With all of these shortcomings, the technology
    continues to be improved to a point where you will find it being used in everyday
    life. For example, chatbots are now replacing many customer service solutions
    so you may think you're talking with an agent but it is actually software with
    NLP algorithms installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concepts of NLP have been around for decades. Using NLP concepts have some
    fundamentals that you may have already used in the past such as rule-based conditions
    or tags. A rule-based example would be when a specific keyword or collection of
    words is found in a dataset with a flag field used to identify it, similar to
    the following table where we have three columns with five rows including a header
    row to identify the purpose of each record:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c459f02a-2498-4c02-ba87-b87887b0e0b2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the table, I have changed the font color for any variation of the keyword
    `balanced` to make it easier to identify. As you can observe from the table, the
    word does have minor variations that would seem obvious to a human but must be
    programmed as a rule to account for the differences. So, to identify any variation
    of the keyword such as whether it is capitalized or conjugated, it must be accounted for
    using conditional logic. In this example, a conditional field named `key_word_found_flag`
    is used to determine whether the phrase field contains the keyword, which is using
    a value of `1` to identify a true case.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based NLP has evolved from earlier concepts that used basic wildcard searches
    and ASCII character identification to machine-learned statistical models. I don't
    plan on going into all of the details behind the mathematical models used by NLP,
    but I encourage you to explore the subject. I will focus, in this chapter, on
    using predefined NLP libraries as is without adjusting or training the models.
    The intention is to help you to gain some confidence in when, where, and how it
    can be used. Over the last few years, I have been impressed by the level of accuracy
    behind solutions that leverage NLP, but I recognize it is an evolving technology.
    I was reminded of this when I interacted with Alexa the other day when it told
    me it could not understand my question regardless of how I phrased it.
  prefs: []
  type: TYPE_NORMAL
- en: Anytime you use or create solutions with NLP, the potential for a **False Positive
    (FP)** or **False Negative (FN)** exists. An FP or FN is when the NLP algorithm
    has incorrectly predicted the output. When the output returns true, but the actual
    result is false, it is considered an FP. When the model returns false or was missing
    but the correct result was to return true, it is called an FN. Conversely, when
    the algorithm or model correctly predicts the output, this is called either a
    **True Positive (TP)** or **True Negative (TN)**. It is common to use a confusion
    matrix to identify and train the model to reduce FPs and FNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a version of a confusion matrix found in the following screenshot to
    help to identify NLP predictions where the results for positive are in green and
    negative in red. What determines the color and designation is comparing the actual
    verse in the predicted output. For example, if a computer model is used to predict
    the outcome of testing a person for a disease, a **TP** or **TN** would indicate
    the model either accurately predicted that the person has the disease or verified
    they do not have the disease. For those cases, the green color boxes indicate
    a match to the actual outcomes. For any other outcome, the boxes are colored in
    red because the prediction inaccurately returned a false result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/813ec9d4-abe0-40c3-b27b-e20eda1d3101.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating the results of NLP models can and should be done by a human when
    the data volumes are small. This will ensure trust exists with the code behind
    the algorithm. However, when data volumes are large, this becomes a time-consuming
    task, and in many cases, unrealistic to verify without alternative solutions such
    as crowd-sourcing. Crowd-sourcing solutions are when humans are used for data
    entry in the process, but at a large scale by breaking up tasks and distributing
    them to a population of people to validate data.
  prefs: []
  type: TYPE_NORMAL
- en: Typical NLP or data science models will use supervised learning techniques where
    training sets of data are used to teach the model to be accurate in its prediction
    output. The labeled data used for training will be a small population of data
    classified by a person or people with an accurate outcome. So, the statistical
    models used to accurately predict the results are based on teaching the NLP model
    based on trained data. Using mislabeled training data in supervised NLP models
    will result in more false positives and inaccurate results. If you work with a
    data scientist or data engineering team using supervised learning, you can showcase
    your data literacy knowledge by asking what process is used to retrain and re-evaluate
    the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some fundamental understanding of working with unstructured
    data, let's walk through how to gather it from its source. For social media data
    from platforms such as Twitter, using their API connection would be an option
    and allows you to consume data in near real time by streaming the data into your
    Jupyter notebook. Using any social media's APIs will require you to set up a user
    account and create a secret key so you can use a REST client connection via HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the network connection used by your workstation, you may need to
    adjust your firewall or proxy settings to allow you access to consume data via
    APIs and REST.
  prefs: []
  type: TYPE_NORMAL
- en: Because there are restrictions and different API limits on exporting social
    media data by platform, we will be using the sample data pre-installed with the
    packages from the Python libraries. The **Natural Language Toolkit** (**NLTK**)
    package is available to help us to work with unstructured data in multiple use
    cases. The software is open source and contains dozens of modules to support NLP using
    a concept called a corpus. A corpus creates a taxonomy that can be used by linguists,
    computer scientists, and machine learning enthusiasts. A taxonomy is a breakdown
    of the language of a body of a text into its fundamental elements. Examples include
    the American Heritage Dictionary and Wikipedia. It also serves as the foundation
    for counting and searching words when using software.
  prefs: []
  type: TYPE_NORMAL
- en: Corpus in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our first example, we are going to use the Brown Corpus, which contains
    hundreds of sample language text categorized by an eclectic mix of subjects such
    as mystery short stories, political press releases, and religion. The original
    collection had over a million words defined and tagged with a part of speech such
    as noun, verb, and preposition. To install the Brown Corpus and use it in your
    Jupyter notebook, use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch a new Jupyter notebook and name it `ch_10_exercises`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the following libraries by adding the following command in your Jupyter
    notebook and running the cell. Feel free to follow along by creating your own
    notebook (I have placed a copy in GitHub for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml),
    *Overview of Python and Installing Jupyter Notebook*, for help with setting up
    your environment. If you are behind a firewall, there is an `nltk.set_proxy` option
    available. Check the documentation at [http://www.nltk.org/](http://www.nltk.org/) for
    more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we download the specific corpus we want to use. Alternatively, you can
    download all of the packages using the `all` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the package download
    is confirmed and the output is verified with `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0373d61e-eb3d-4b3e-85b4-fd106c67527e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To confirm the package is available in your Jupyter notebook, we can use the
    following command to reference the corpus using a common alias of `brown` for
    reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To display a list of the few words available in the Brown Corpus, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where six words in single
    quotes are displayed along with an ellipsis inside square brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50455bd2-1623-4811-9cee-30396d9c18da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To count all of the words available, we can use the `len()` function, which
    counts the length of a string or the number of items in an object such as an array
    of values. Since our values are separated by commas, it will count all of the
    words available. To make it easier to format, let''s assign the output to a variable
    called `count_of_words`, which we can use in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the output easier to understand for the consumer of this data, we use
    the `print()` and `format()` functions to display the results using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where a sentence will appear
    that includes a dynamic count of all of the words assigned to the `count_of_words`
    variable. We also formatted the value to display with a comma:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af69b04d-4b70-42dd-b5bb-44999c5bd05d.png)'
  prefs: []
  type: TYPE_IMG
- en: Excellent, you have now successfully loaded your first NLP library and were
    able to run a few commands against a popular corpus package. Let's continue dissecting
    the different elements of NLP by explaining why tokenization is important.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is the process of breaking unstructured text such as paragraphs,
    sentences, or phrases down into a list of text values called tokens. A token is
    the lowest unit used by NLP functions to help to identify and work with the data.
    The process creates a natural hierarchy to help to identify the relationship from
    the highest to the lowest unit. Depending on the source data, the token could
    represent a word, sentence, or individual character.
  prefs: []
  type: TYPE_NORMAL
- en: The process to tokenize a body of text, sentence, or phrase, typically starts
    with breaking apart words using the white space in between them. However, to correctly
    identify each token accurately requires the library package to account for exceptions
    such as hyphens, apostrophes, and a language dictionary, to ensure the value is
    properly identified. Hence, tokenization requires the language of origin of the
    text to be known to process it. Google Translate, for example, is an NLP solution
    that can identify the language, but still has the option for users to define it
    to ensure the translation is accurate.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the reasons why tokenization is an evolving process. For example,
    as new words are added to the English language, the NLP reference libraries require
    an update. Handling sarcasm, dual meanings, and catchphrases may get lost in translation
    when using NLP solutions such as Alexa or Siri. For example, the phrase **social
    ambassador** has an obvious meaning to a human but would require the library to
    be trained to identify it as a token phrase.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few techniques used by NLP to address this issue. The first is called
    n-grams, which is the process of combining words within the same sentence as a
    group, typically of two or three words, to create a pattern that is recognizable
    by the NLP library. Acronyms can also be used in n-grams but require identification
    or training to be effective. An n-gram could then be used to identify **social
    ambassador** to understand these two values can be used together.
  prefs: []
  type: TYPE_NORMAL
- en: Another common reference for an n-gram is a bi-gram, which only uses two words.
    The **n** denotes the number of grams so a uni-gram stands for one gram, a bi-gram
    is for two, a tri-gram is for three, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Another concept is called a **bag of words**, which is when a high occurrence
    of specific words exist in the source data. The use of a **bag of words** is another
    helpful way to identify patterns and key term searches against large text sources
    of data. For example, a prediction model to improve response time from system
    outages can use historical text logs found in help desk tickets. The **bag of
    words** technique can be used to create multiple flag fields (yes or no) as inputs
    into the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: So, the study of tokenization and NLP is a deep subject that will remain an
    evolving science. I recommend continuing to research the subject in more detail.
    The Stanford University NLP site is a fantastic source of information that I have
    added to the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore the additional features available in the NLTK library in our Jupyter
    notebook. Another downloadable library available is called **Punkt**, which is
    used to tokenize sentences into words. I have included a link to the downloads
    available from NLTK in the *Further reading* section. The code behind the algorithm
    requires a high volume of text so the model can be trained but the NLTK data package
    includes a pre-trained option in English that we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will continue by going back into the `ch_10_exercises` notebook in Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries by adding the following command in your Jupyter
    notebook and run the cell. Feel free to follow along by creating your own notebook
    (I have placed a copy on GitHub for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the package download
    is confirmed and the output is verified with `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d462896d-7985-4963-a825-2a37bc0bacb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will create a new variable called `input_sentence` and assign it to
    a free-form text sentence that must be encapsulated in double quotes and on a
    single line input. There will be no input after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the `word_tokenize()` function that is available in the NLTK
    library to break up the individual words and any punctuation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the individual words
    are broken out from the sentence as an array of values with single quotes around
    each text value surrounded by square brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30e87028-a530-4237-b0ef-f1d6579a93fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s tokenize by sentence, which requires you to import the `sent_tokenize` option
    from the NLTK `tokenize` library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assign a new variable called `input_data` to a collection of sentences
    that we can use later in our code. There will be no input after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will pass the `input_data` variable as a parameter to the `sent_tokenize()`
    function, which will look at the string of text and break it down into individual
    token values. We wrap the output with the `print()` function to display the results
    cleanly in the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the individual sentences
    are broken down as an array of string values with single quotes around them surrounded
    by square brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e1dda99-e155-412c-8924-f92617a105fc.png)'
  prefs: []
  type: TYPE_IMG
- en: So, now you can see that having these NLTK library features can help you to
    work with unstructured data by breaking down language into foundational pieces.
    As a data analyst, you will be faced with free text in many different forms, so
    now you have some additional resources to leverage. Once you have the data tokenized,
    additional options are available that we will explore in the next section, such
    as counting the frequency of words to identify patterns within the underlying
    source.
  prefs: []
  type: TYPE_NORMAL
- en: Counting words and exploring results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Counting word frequency will provide initial metadata about the unstructured
    source text. Exposing the count of the occurrence of a word or when specific words
    are missing within a body of text is known as **text mining**. Text mining will
    provide analytics about the data, so a data analyst can determine the value of
    a data asset along with how it can be used to answer business questions. Likewise,
    you can identify keyword patterns that occur during unexpected outages that impact
    users by looking at application system logs. Once those words or phrases are identified,
    you can work with the developers to identify the root cause and reduce the impact
    on your application users.
  prefs: []
  type: TYPE_NORMAL
- en: A popular option available for text analysis is the use of regular expressions
    or **regex** for short. The regex concept is when you use a combination of rules
    and search patterns to extract features from very large, unstructured text. Regex
    becomes useful when reading the text line by line would be unreasonable, based
    on the amount of time and number of people required. Regex has a wide range of
    applications including how to separate emails, phone numbers, and hashtags from
    the source text.  If you are working with help desk ticket logs, for example,
    you would tag successful matches to the regex rules with a unique ticket ID so
    you can join the unstructured data back to the data model.
  prefs: []
  type: TYPE_NORMAL
- en: Regex covers a variety of techniques including wildcard character searches,
    pattern matching based on qualifiers, and anchors used to identify the beginning
    or end of textual data. Regex rules are usually combined with standard software
    engineering, so the code can be modular and automated when looking at high-frequency
    data sources such as a chatbot or system logs. For example, if you created a regex
    rule against any combination of the keyword `frustrated` in a customer service
    system, you can create a flag field named `is_customer_frustrated_yes_no` with
    a value of 1 for true and 0 for false. Regex rules can and should evolve over
    time, based on the data and validation, the rules that are accurate is important.
    This can be done with a random sampling of data by manually validating the conditions
    exist and returning the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: Counting words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we explore those options, let's continue with the Jupyter notebook exercise
    and walk through how to count the frequency of words from a population of free
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will continue by going back into the `ch_10_exercises` notebook in Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the probability module available in the NTLK library to count the frequency
    of the words available in a body of text. There will be no result returned after
    you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, explore a large body of text using the Brown Corpus. To do this, we assign
    the population of all of the token words available by using the `FreqDist()` function
    and assigning it to a variable named `input_data`. To see the results of the processing
    of this data, we can print the variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the frequency distribution
    results are calculated and printed using the `print()` function against the assigned
    variable, `input_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aa1085a-c5ca-4595-bf58-1bd75fda230d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To see a list of the most common words that exist in `input_data`, we can use
    the `most_common()` function along with a parameter to control how many are displayed.
    In this case, we want to see the top 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where a list of name-value
    pairs are displayed like a two-column table with ten rows to show each token,
    which can be a word, punctuation mark, or character enclosed with single quotes
    along with an integer value that provides the cumulative count of the times the
    word appears in the source data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b131e99b-c597-46a7-ba8d-c58a9da7b8b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Through this exercise, we can identify and extract words from a body of unstructured text,
    which is the foundation for creating regex rules. The next section will focus
    on normalizing words for consistency to improve the accuracy of NLP model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing text techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, making the regex rules **smarter** by adding new code logic or
    libraries will be required. One such way to do this is by using the concepts behind
    normalizing your text called stemming and lemmatization. Both terms are rooted
    in the study of linguistics, and how they are adopted to be used in technology
    has exploded due to integrating NLP solutions into everything, from customer service
    to speech-to-text features.
  prefs: []
  type: TYPE_NORMAL
- en: When applied to NLP, stemming is when any word is programmatically identified
    to its common root form. In this process, any suffix, plural form, or synonym
    that exists for the word is identified. Stemmers require a reference dictionary
    or lookup to be accurate, so the source language is required. Lemmatization takes
    into account all of the variations of a word so it can be rooted back to a dictionary
    source. From my research, both stemming and lemmatization are used together in
    NLP and you can start by using the open source libraries available, which I included
    in the <q>Further reading</q> section. These libraries should be sufficient to
    cover common words but analogies or custom-defined lingo in your organization
    will require a new corpus. Simple examples of using stemming or a lemma include
    identifying when the word **fishes** appears, and returning **fish,**or **geese** returning **goose**.
  prefs: []
  type: TYPE_NORMAL
- en: The subject for both concepts is pretty vast, so I encourage you to continue
    learning about it, but the bottom line is the benefits of using these concepts
    will help to clean and normalize data for analysis. Having the data normalized
    where multiple similar values are grouped together as a single value is necessary.
    It reduces the volume of data you are analyzing and prepares the results for deeper
    analytics such as creating a data science or machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our exercises, we will use the Porter Stemmer, which is commonly used to
    help to prepare text data and normalize data in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue by going back to the `ch_10_exercises` notebook in Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `PorterStemmer` module available in the NTLK library to normalize
    a word. There will be no result returned after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To import an instance of this feature so it can be referenced later in the
    code, we use the following code. There will be no result returned after you run
    the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can pass individual words into the instance to see how the word would
    be normalized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot where the stem of the word
    `fishing` will be displayed as `fish`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b061e67-3cb5-40a2-8100-433b2107ca16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To use lemma features, we need to download the `WordNet` corpus using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To import an instance of this feature so it can be referenced later in the
    code, we use the following code. There will be no result returned after you run
    the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how the lemma would output for the same word we used a stem for earlier,
    we pass the same word into the `lemmatize()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the lemma of the
    word `fishing` will be displayed as `fishing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72a41c72-fe8c-4f97-9885-d0d8e49bc93e.png)'
  prefs: []
  type: TYPE_IMG
- en: So, why are the results different? The algorithms used by each of the NLTK corpora
    apply different approaches to normalize the words. Lemmatization will adjust to
    the form or structure of the word as it is defined in the dictionary that is included
    in the WordNet corpus whereas stemming is intended to break down the word to its
    root. Each is a tool that is available and, depending on the use case, you may
    need to adjust which approach to use to normalize the data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: To take it one step forward, let's pass a list of words into each instance using
    a loop and print out the results to see how they compare to the original word.
    We will use a sample from the Brown words by limiting the results.
  prefs: []
  type: TYPE_NORMAL
- en: If you pass all over a million words into the loop in your Jupyter Notebook
    session, it will take much longer to run and take up resources (RAM and CPU) to
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a list but limit the words to only a sample by assigning it to a
    variable, we use the following command. There will be no result returned after
    you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a loop against each value in the list and print the results.
    We include some formatting to make it easier to understand the results for each
    row. Be sure to include a carriage return to create a new line to use the `print()`
    function without errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot where ten lines are printed
    with a colon delimiter used to separate the results for the original word, the
    stem of the word, and the lemma of the word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/460f1849-9d73-409d-959e-66fbd21d5356.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we learned how to normalize the words within your unstructured data,
    let's find out how to exclude words or phrases from your data to reduce the noise
    so we can focus on valuable keywords that can provide insight.
  prefs: []
  type: TYPE_NORMAL
- en: Excluding words from analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visually sifting through millions of words is impractical in data analysis because
    language includes many linking verbs that are repeated throughout the body of
    a text. Common words such as **am**, **is**, **are**, **was**, **were**, **being**,
    and **been** would be at the top of the `most_common()` list when you apply NLP
    against the source data even after it has been normalized. In the evolution of
    improving NLP libraries, a dictionary of **stopwords** was created to include
    a more comprehensive list of words that provide less value in text analytics.
    Example **stopwords **include linking verbs along with words such as **the**,
    **an**, **a**, and **until**. The goal is to create a subset of data that you
    can focus your analysis on after filtering out these stopwords from your token
    values.
  prefs: []
  type: TYPE_NORMAL
- en: NLP can require high CPU and RAM resources especially working with a large collection
    of words, so you may need to break up your data into logical chucks, such as alphabetically,
    to complete your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will continue by going back to the `ch_10_exercises` notebook in Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `stopwords` corpus from the NLTK library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, import the stopwords and `word_tokenize` features so they can be used
    later in the exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assign a new variable called `input_data` to a collection of sentences
    that we can use later in our code. There will be no input after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will assign object variables called `stop_words` and `word_tokens` so they
    can be referenced later in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have a few lines of code that will loop through the word tokens
    from `input_data` and compare them to `stop_words`. If they match, they will be
    excluded. The final result prints the original `input_data`, which has been tokenized
    along with the results after the stopwords have been removed. Be sure to use the
    correct indentation when entering the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the original sentence
    is displayed as tokens that include all words. The second line of the output will
    have fewer token words because stopwords such as `the` have been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da7050f1-9b2c-4400-83d6-537c97aaf38c.png)'
  prefs: []
  type: TYPE_IMG
- en: Excellent, we have learned now to exclude common words, which removes the noise
    from large volumes of text. The focus of your analysis will be on keywords and
    phrases to provide context within the text without reading through the entire
    body of unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, you have successfully walked through the foundations of **Natural
    Language Processing** (**NLP**), along with key features that are available when
    working with unstructured data. We explored the **Natural Language Toolkit** (**NLTK**)
    Python library, which offers many options to work with free text by downloading
    different corpora to analyze large bodies of text. We learned how to split raw
    text into meaningful units called tokens so it can be interpreted and refined.
    We learned about regex and pattern matching using words as it applies to NLP.
    We also explored how to count the frequency of words in a collection of text using
    probability and statistical modules. Next, we learned how to normalize words using
    stemming and lemmatization functions, which shows how variations in words can
    impact your data analysis. We explained the concepts of n-grams and how to use
    `stopwords` to remove the noise that is common when working with large bodies
    of free text data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 11](1a658723-79aa-4447-8dbd-74206cad9aa1.xhtml),
    *Practical Sentiment Analysis,* we will show how prediction models can be applied
    to unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the relative topics of this chapter, you can refer
    to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating random sample data: [https://www.mockaroo.com/](https://www.mockaroo.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLTK source code and documentation: [https://www.nltk.org/](https://www.nltk.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP Stanford University reference: [https://nlp.stanford.edu/](https://nlp.stanford.edu/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
