<html><head></head><body>
		<div><h1 id="_idParaDest-38"><em class="italic"><a id="_idTextAnchor039"/>Chapter 3</em>: Reading and Writing Files</h1>
			<p>In the previous chapter, we looked at how to install various tools, such as NiFi, Airflow, PostgreSQL, and Elasticsearch. In this chapter, you will be learning how to use these tools. One of the most basic tasks in data engineering is moving data from a text file to a database. In this chapter, you will read data from and write data to several different text-based formats, such as CSV and JSON.  </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Reading and writing files in Python</li>
				<li>Processing files in Airflow</li>
				<li>NiFi processors for handling files</li>
				<li>Reading and writing data to databases in Python</li>
				<li>Databases in Airflow</li>
				<li>Database processors in NiFi</li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor040"/>Writing and reading files in Python</h1>
			<p>The title of <a id="_idIndexMarker123"/>this section may sound strange as you are probably used to seeing it written as reading and writing, but in this section, you will write data to files first, then <a id="_idIndexMarker124"/>read it. By writing it, you will understand the structure of the data and you will know what it is you are trying to read.</p>
			<p>To write <a id="_idIndexMarker125"/>data, you will use a library named <code>faker</code>. <code>faker</code> allows you to <a id="_idIndexMarker126"/>easily create fake data for common fields. You can generate an address by simply calling <code>address()</code>, or a female name using <code>name_female()</code>. This will simplify the creation of fake data while at the same time making it more realistic. </p>
			<p>To install <code>faker</code>, you can use <code>pip</code>:</p>
			<pre>pip3 install faker</pre>
			<p>With <code>faker</code> now installed, you are ready to start writing files. The next section will start with CSV files.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Writing and reading CSVs</h2>
			<p>The most <a id="_idIndexMarker127"/>common file type you will encounter is <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>). A CSV is a file made up of fields separated <a id="_idIndexMarker128"/>by commas. Because commas are fairly <a id="_idIndexMarker129"/>common in text, you need to be able to handle them in CSV files. This can be accomplished by using escape characters, usually a pair of quotes around text strings that could contain a comma that is not used to signify a new field. These quotes are called escape characters. The Python standard library for handling CSVs simplifies the process of handling CSV data. </p>
			<h3>Writing CSVs using the Python CSV Library</h3>
			<p>To <a id="_idIndexMarker130"/>write a CSV with <a id="_idIndexMarker131"/>the CSV library, you need to use the following steps:</p>
			<ol>
				<li>Open a file in writing mode. To open a file, you need to specify a filename and a mode. The mode for writing is <code>w</code>, but you can also open a file for reading with <code>r</code>, appending with <code>a</code>, or reading and writing with <code>r+</code>. Lastly, if you are handling files that are not text, you can add <code>b</code>, for binary mode, to any of the preceding modes to write in bytes; for example, <code>wb</code> will allow you to write in bytes:<pre>output = open('myCSV.CSV',mode='w')</pre></li>
				<li>Create <code>CSV_writer</code>. At a minimum, you must specify a file to write to, but you can also pass additional parameters, such as a dialect. A dialect can be a defined CSV type, such as Excel, or it can be options such as the delimiter to use or the level of quoting. The defaults are usually what you will need; for example, the delimiter defaults to a comma (it is a CSV writer after all) and quoting defaults to <code>QUOTE_MINIMAL</code>, which will only add quotes when there are special characters or the delimiter within a field. So, you can create the writer as shown:<pre>mywriter=csv.writer(output) </pre></li>
				<li>Include a header. You might be able to remember what the fields are in your CSV, but it is best to include a header. Writing a header is the same as writing any other row: define the values, then you will use <code>writerow()</code>, as shown:<pre>header=['name','age']
mywriter.writerow(header)</pre></li>
				<li>Write the data to a file. You can now write a data row by using <code>writerow(0)</code> and passing some data, as shown:<pre>data=['Bob Smith',40]
mywriter.writerow(data)
output.close()</pre></li>
			</ol>
			<p>Now, if <a id="_idIndexMarker132"/>you look in the <a id="_idIndexMarker133"/>directory, you will have a CSV file named <code>myCSV.CSV</code> and the contents should look as in the following screenshot:</p>
			<div><div><img src="img/B15739_03_01.jpg" alt="Figure 3.1 – The contents of mycsv.csv&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – The contents of mycsv.csv</p>
			<p>Notice that when you used <code>cat</code> to view the file, the newlines were added. By default, <code>CSV_writer</code> uses a return and a newline (<code>'\r\n'</code>).</p>
			<p>The <a id="_idIndexMarker134"/>preceding example was very basic. However, if you are trying to write a lot of data, you would most likely <a id="_idIndexMarker135"/>want to loop through some condition or iterate through existing data. In the following example, you will use <code>Faker</code> to generate 1,000 records: </p>
			<pre>from faker import Faker
import csv
output=open('data.CSV','w')
fake=Faker()
header=['name','age','street','city','state','zip','lng','lat']
mywriter=csv.writer(output)
mywriter.writerow(header)
for r in range(1000):
    mywriter.writerow([fake.name(),fake.random_int(min=18, 
    max=80, step=1), fake.street_address(), fake.city(),fake.
    state(),fake.zipcode(),fake.longitude(),fake.latitude()])
    output.close()</pre>
			<p>You <a id="_idIndexMarker136"/>should now have a <code>data.CSV</code> file with 1,000 rows of names and ages.</p>
			<p>Now <a id="_idIndexMarker137"/>that you have written a CSV, the next section will walk you through reading it using Python.</p>
			<h3>Reading CSVs</h3>
			<p>Reading <a id="_idIndexMarker138"/>a CSV is somewhat similar to writing one. The same steps are followed with slight modifications: </p>
			<ol>
				<li value="1">Open a file using <code>with</code>. Using <code>with</code> has some additional benefits, but for now, the one you will reap is not having to use <code>close()</code> on the file. If you do not specify a mode, <code>open</code> defaults to read (<code>r</code>). After <code>open</code>, you will need to specify what to refer to the file as; in this case, you will open the <code>data.CSV</code> file and refer to it as <code>f</code>:<pre>with open('data.csv') as f:</pre></li>
				<li>Create the reader. Instead of just using <code>reader()</code>, you will use <code>DictReader()</code>. By using the dictionary reader, you will be able to call fields in the data by name instead of position. For example, instead of calling the first item in a row as <code>row[0]</code>, you can now call it as <code>row['name']</code>. Just like the writer, the defaults are usually sufficient, and you will only need to specify a file to read. The following code opens <code>data.CSV</code> using the <code>f</code> variable name:<pre>myreader=CSV.DictReader(f)</pre></li>
				<li>Grab the headers by reading a single line with <code>next()</code>:<pre>headers=next(myreader)</pre></li>
				<li>Now, you can iterate through the rest of the rows using the following:<pre>for row in myreader:</pre></li>
				<li>Lastly, you can print the names using the following:<pre>    print(row['name'])</pre></li>
			</ol>
			<p>You should <a id="_idIndexMarker139"/>only see the 1,000 names scroll by. Now you have a Python dictionary that you can manipulate any way you need. There is another way to handle CSV data in Python and that requires <code>pandas</code>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Reading and writing CSVs using pandas DataFrames</h2>
			<p><code>pandas</code> DataFrames are a powerful tool not only for reading and writing data but also for the <a id="_idIndexMarker140"/>querying and <a id="_idIndexMarker141"/>manipulation of data. It does require a larger overhead than the built-in CSV library, but there <a id="_idIndexMarker142"/>are times when it may be worth the <a id="_idIndexMarker143"/>trade-off. You may already have <code>pandas</code> installed, depending on your Python environment, but if you do not, you can install it with the following:</p>
			<pre><strong class="bold">pip3 install pandas</strong></pre>
			<p>You can think of a <code>pandas</code> DataFrame as an Excel sheet or a table. You will have rows, columns, and an index. To load CSV data into a DataFrame, the following steps must be followed: </p>
			<ol>
				<li value="1">Import <code>pandas</code> (usually as <code>pd</code>):<pre>import pandas as pd</pre></li>
				<li>Then, read the file using <code>read_csv()</code>. The <code>read_csv()</code> method takes several optional parameters, and one required parameter – the file or file-like buffer. The two optional parameters that may be of interest are <code>header</code>, which by defaultattempts to infer the headers. If you set <code>header=0</code>, then you can use the <code>names</code> parameter with an array of column names. If you have a large file and you just want to look at a piece of it, you can use <code>nrows</code> to specify the number of rows to read, so <code>nrows=100</code> means it will only read 100 rows for the data. In the following snippet, you will load the entire file using the defaults:<pre>df=pd.read_csv()('data.CSV')</pre></li>
				<li>Let's <a id="_idIndexMarker144"/>now look at the first 10 records by using the following:<pre>df.head(10)</pre></li>
			</ol>
			<p>Because <a id="_idIndexMarker145"/>you used <code>Faker</code> to <a id="_idIndexMarker146"/>generate data, you will have <a id="_idIndexMarker147"/>the same schema as in the following screenshot, but will have different values:</p>
			<div><div><img src="img/B15739_03_02.jpg" alt="Figure 3.2 – Reading a CSV into a DataFrame and printing head()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Reading a CSV into a DataFrame and printing head()</p>
			<p>You <a id="_idIndexMarker148"/>can create a DataFrame <a id="_idIndexMarker149"/>in Python with the following steps:</p>
			<ol>
				<li value="1">Create <a id="_idIndexMarker150"/>a dictionary of data. A dictionary <a id="_idIndexMarker151"/>is a data structure that stores data as a key:value pair. The value can be of any Python data type – for example, an array. Dictionaries have methods for finding <code>keys()</code>, <code>values()</code>, and <code>items()</code>. They also allow you to find the value of a key by using the key name in brackets – for example, <code>dictionary['key']</code> will return the value for that key:<pre>data={'Name':['Paul','Bob','Susan','Yolanda'],
'Age':[23,45,18,21]}</pre></li>
				<li>Pass <a id="_idIndexMarker152"/>the data to the DataFrame:<pre>df=pd.DataFrame(data)</pre></li>
				<li>The <a id="_idIndexMarker153"/>columns are specified as the keys in the dictionary. Now that you have a DataFrame, you can write the contents to a CSV using <code>to_csv()</code> and passing a filename. In <a id="_idIndexMarker154"/>the example, we did not set an index, which means the row names will be a <a id="_idIndexMarker155"/>number from 0 to <em class="italic">n</em>, where <em class="italic">n</em> is the length of the DataFrame. When you export to CSV, these values will be written to the file, but the column name will be blank. So, in a case where you do not need the row names or index to be written to the file, pass the <code>index</code> parameter to <code>to_csv()</code>, as shown:<pre>df.to_csv('fromdf.CSV',index=False)</pre></li>
			</ol>
			<p>You will now have a CSV file with the contents of the DataFrame. How we can use the contents of this DataFrame for executing SQL queries will be covered in the next chapter. They will become an important tool in your toolbox and the rest of the book will lean on them heavily. </p>
			<p>For now, let's move on to the next section, where you will learn about another common text format – <strong class="bold">JSON</strong>.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>Writing JSON with Python</h2>
			<p>Another <a id="_idIndexMarker156"/>common data format you will probably <a id="_idIndexMarker157"/>deal with is <code>JSON–JSON</code>. </p>
			<p>To write JSON using Python and the standard library, the following steps need to be observed:</p>
			<ol>
				<li value="1">Import the library and open the file you will write to. You also create the <code>Faker</code> object:<pre>from faker import Faker
import json
output=open('data.JSON','w')
fake=Faker()</pre></li>
				<li>We will create 1,000 records, just as we did in the CSV example, so you will need to create a dictionary to hold the data. As mentioned earlier, the value of a key can be any <a id="_idIndexMarker160"/>Python data type – including an array of values. After creating the dictionary to hold the records, add a <code>'records'</code> key and initialize it with a blank array, as shown:<pre>alldata={}
alldata['records']=[]</pre></li>
				<li>To <a id="_idIndexMarker161"/>write the records, you use <code>Faker</code> to create a dictionary, then append it to the array:<pre>for x in range(1000):
	data={"name":fake.name(),"age":fake.random_int
           (min=18, max=80, step=1),
           "street":fake.street_address(),
           "city":fake.city(),"state":fake.state(),
           "zip":fake.zipcode(),
           "lng":float(fake.longitude()),
           "lat":float(fake.latitude())}
	alldata['records'].append(data)	</pre></li>
				<li>Lastly, to write the JSON to a file, use the <code>JSON.dump()</code> method. Pass the data that you want to write and a file to write to:<pre>json.dump(alldata,output)</pre></li>
			</ol>
			<p>You now have a <code>data.JSON</code> file that has an array with 1,000 records. You can read this file by taking the following steps:</p>
			<ol>
				<li value="1">Open the file using the following:<pre>with open("data.JSON","r") as f:</pre></li>
				<li>Use <code>JSON.load()</code> and pass the file reference to the method:<pre>data=json.load(f)</pre></li>
				<li>Inspect the json by looking at the first record using the following:<pre>data['records'][0]</pre><p>Or just use the name:</p><pre>data['records'][0]['name']</pre></li>
			</ol>
			<p>When <a id="_idIndexMarker162"/>you <code>loads</code> and <code>dumps</code> are different than <code>load</code> and <code>dump</code>. Both are valid methods of the JSON library. The difference is that <code>loads</code> and <code>dumps</code> are for strings – they do not serialize the JSON.</p>
			<h3>pandas DataFrames</h3>
			<p>Reading <a id="_idIndexMarker164"/>and writing <a id="_idIndexMarker165"/>JSON with DataFrames is similar to what we did with CSV. The only difference is that you change <code>to_csv</code> to <code>to_json()</code> and <code>read_csv()</code> to <code>read_json()</code>. </p>
			<p>If you <a id="_idIndexMarker166"/>have a clean, well-formatted JSON file, you can read it using the following code:</p>
			<pre>df=pd.read_json('data.JSON')</pre>
			<p>In the <a id="_idIndexMarker167"/>case of the <code>data.JSON</code> file, the records are nested in a <code>records</code> dictionary. So, loading the JSON is not as straightforward as the preceding code. You will need a few extra steps, which are as follows. To load JSON data from the file, do the following: </p>
			<ol>
				<li value="1">Use the <code>pandas</code> <code>JSON</code> library:<pre>import pandas.io.json as pd_JSON</pre></li>
				<li>Open the file and load it with the <code>pandas</code> version of <code>JSON.loads()</code>:<pre>f=open('data.JSON','r')
data=pd_JSON.loads(f.read())</pre></li>
				<li>To create the DataFrame, you need to normalize the JSON. Normalizing is how you can flatten the JSON to fit in a table. In this case, you want to grab the individual JSON records held in the <code>records</code> dictionary. Pass that path – <code>records</code> – to the <code>record_path</code> parameter of <code>json_normalize()</code>:<pre>df=pd_JSON.json_normalize(data,record_path='records')</pre></li>
			</ol>
			<p>You will now have a DataFrame that contains all the records in the <code>data.JSON</code> file. You can now write them back to JSON, or CSV, using DataFrames.</p>
			<p>When <a id="_idIndexMarker168"/>writing to JSON, you can pass the <code>orient</code> parameter, which determines the format of the JSON that is returned. The default <a id="_idIndexMarker169"/>is columns, which for the <code>data.JSON</code> file <a id="_idIndexMarker170"/>you created in the previous section would look like the following data:</p>
			<pre>&gt;&gt;&gt; df.head(2).to_json()
'{"name":{"0":"Henry Lee","1":"Corey Combs DDS"},"age":{"0":42,"1":43},"street":{"0":"57850 Zachary Camp","1":"60066 Ruiz Plaza Apt. 752"},"city":{"0":"Lake Jonathon","1":"East Kaitlin"},"state":{"0":"Rhode Island","1":"Alabama"},"zip":{"0":"93363","1":"16297"},"lng":{"0":-161.561209,"1":123.894456},"lat":
{"0":-72.086145,"1":-50.211986}}'</pre>
			<p>By <a id="_idIndexMarker171"/>changing the <code>orient</code> value to <code>records</code>, you get each row as a record in the JSON, as shown:</p>
			<pre>&gt;&gt;&gt; df.head(2).to_JSON(orient='records')
'[{"name":"Henry Lee","age":42,"street":"57850, Zachary Camp","city":"Lake Jonathon","state":"Rhode Island", "zip":"93363","lng":-161.561209,"lat":72.086145},{"name":"Corey Combs DDS","age":43,"street":"60066 Ruiz Plaza Apt. 752","city":"EastKaitlin","state":"Alabama","zip":"16297","lng":123.894456, "lat":-50.211986}]'</pre>
			<p>I find <a id="_idIndexMarker172"/>that working <a id="_idIndexMarker173"/>with JSON that is oriented around <code>records</code> makes processing it in tools such as Airflow much <a id="_idIndexMarker174"/>easier than JSON in other formats, such <a id="_idIndexMarker175"/>as split, index, columns, values, or table. Now that you know how to handle CSV and JSON files in Python, it is time to learn how to combine tasks into a data pipeline using Airflow and NiFi. In the next section, you will learn how to build pipelines in Apache Airflow.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor044"/>Building data pipelines in Apache Airflow</h1>
			<p>Apache <a id="_idIndexMarker176"/>Airflow uses Python functions, as well as <a id="_idIndexMarker177"/>Bash or other operators, to create tasks that can be combined into a <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) – meaning each task moves <a id="_idIndexMarker178"/>in one direction when completed. Airflow allows you to combine Python functions to create tasks. You can specify the order in which the tasks will run, and which tasks depend on others. This order and dependency are what make it a DAG. Then, you can schedule your DAG in Airflow to specify when, and how frequently, your DAG should run. Using the Airflow GUI, you can monitor and manage your DAG. By using what you learned in the preceding sections, you will now make a data pipeline in Airflow.</p>
			<p>Building a CSV to a JSON data pipeline</p>
			<p>Starting <a id="_idIndexMarker179"/>with a simple DAG will help you understand how Airflow works and will help you to add more functions <a id="_idIndexMarker180"/>to build a better data pipeline. The DAG you build will print out a message using Bash, then read the CSV and print a list of all the names. The following steps will walk you through building the data pipeline:</p>
			<ol>
				<li value="1">Open a new file using the Python IDE or any text editor. Import the required libraries, as shown:<pre>import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import pandas as pd</pre><p>The first two imports bring in <code>datetime</code> and <code>timedelta</code>. These libraries are used for scheduling the DAG. The three Airflow imports bring in the required libraries for building the DAG and using the Bash and Python operators. These are the operators you will use to build tasks. Lastly, you import <code>pandas</code> so that you can easily convert between CSV and JSON.</p></li>
				<li>Next, write a function to read a CSV file and print out the names. By combining the steps for reading CSV data and writing JSON data from the previous sections, you can create a function that reads in the <code>data.CSV</code> file and writes it out to JSON, as shown in the following code:<pre>def CSVToJson():
    df=pd.read_CSV('/home/paulcrickard/data.CSV')
    for i,r in df.iterrows():
        print(r['name'])
    df.to_JSON('fromAirflow.JSON',orient='records')</pre><p>This <a id="_idIndexMarker181"/>function opens the file in a DataFrame. Then, it iterates through the rows, printing only the names, and lastly, it writes the CSV to a JSON file. </p></li>
				<li>Now, you <a id="_idIndexMarker182"/>need to implement the Airflow portion of the pipeline. Specify the arguments that will be passed to <code>DAG()</code>. In this book, you will use a minimal set of parameters. The arguments in this example assign an owner, a start date, the number of retries in the event of a failure, and how long to wait before retrying. They are shown in the following dictionary:<pre>default_args = {
    'owner': 'paulcrickard',
    'start_date': dt.datetime(2020, 3, 18),
    'retries': 1,
    'retry_delay': dt.timedelta(minutes=5),
}</pre></li>
				<li>Next, pass the arguments dictionary to <code>DAG()</code>. Create the DAG ID, which is set to <code>MyCSVDAG</code>, the dictionary of arguments (the <code>default_args</code> variable in the preceding code), and the schedule interval (how often to run the data pipeline). The schedule interval can be set using <code>timedelts</code>, or you can use a crontab format with the following presets or crontab:<p>a) <code>@once</code></p><p>b) <code>@hourly</code> – <code>0 * * * *</code></p><p>c) <code>@daily</code> – <code>0 0 * * *</code></p><p>d) <code>@weekly</code> – <code>0 0 * * 0</code></p><p>e) <code>@monthly</code> – <code>0 0 1 * *</code></p><p>f) <code>@yearly</code> – <code>0 0 1 1 *</code></p><p>crontab uses the format minute, hour, day of month, month, day of week. The value for <code>@yearly</code> is <code>0 0 1 1 *</code>, which means run yearly on January 1 (<code>1 1</code>), at 0:0 (midnight), on any day of the week (<code>*</code>). </p><pre>with DAG('MyCSVDAG',
         default_args=default_args,
         schedule_interval=timedelta(minutes=5),      
         # '0 * * * *',
         ) as dag:</pre></li>
				<li>You <a id="_idIndexMarker184"/>can now create your tasks using operators. Airflow <a id="_idIndexMarker185"/>has several prebuilt operators. You can view them all in the documentation at <a href="https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html">https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html</a>. In this book, you will mostly use the Bash, Python, and Postgres operators. The operators allow you to remove most of the boilerplate code that is required to perform common tasks. In the following snippet, you will create two tasks using the Bash and Python operators:<pre>    print_starting = BashOperator(task_id='starting',
                     bash_command='echo "I am reading the                       
                     CSV now....."')
    
    CSVJson = PythonOperator(task_id='convertCSVtoJson',
                             python_callable=CSVToJson)</pre><p>The preceding snippet creates a task using the <code>BashOperator</code> operator, which prints out a statement to let you know it is running. This task serves no purpose other than to allow you to see how to connect multiple tasks together. The next task, <code>CSVJson</code>, uses the <code>PythonOperator</code> operator to call the function you defined at the beginning of the file (<code>CSVToJson()</code>). The function reads the <code>data.CSV</code> file and prints the <code>name</code> field in every row. </p></li>
				<li>With <a id="_idIndexMarker186"/>the tasks defined, you now need to make the connections between the tasks. You can do this <a id="_idIndexMarker187"/>using the <code>set_upstream()</code> and <code>set_downstream()</code> methods or with the bit shift operator. By using upstream and downstream, you can make the graph go from the Bash task to the Python task using either of two snippets; the following is the first snippet:<pre>print_starting .set_downstream(CSVJson)</pre><p>The following is the second snippet:</p><pre>CSVJson.set_upstream(print_starting)</pre><p>Using the bit shift operator, you can do the same; the following is the first option:</p><pre>print_starting &gt;&gt;  CSVJson</pre><p>The following is the second option:</p><pre>CSVJson &lt;&lt; print_starting</pre><p class="callout-heading">Note</p><p class="callout">Which method you choose is up to you; however, you should be consistent. In this book, you will see the bit shift operator setting the downstream.</p></li>
				<li>To use Airflow and Scheduler in the GUI, you first need to make a directory for your DAGs. During the install and configuration of Apache Airflow, in the previous chapter, we removed the samples and so the DAG directory is missing. If you look at <code>airflow.cfg</code>, you will see the setting for <code>dags_folder</code>. It is in the format of <code>$AIRFLOW_HOME/dags</code>. On my machine, <code>$AIRFLOW_HOME</code> is <code>home/paulcrickard/airflow</code>. This is the directory in which you will make the <code>dags folder.e</code> configuration file showing where the folder should be.</li>
				<li>Copy your DAG code to the folder, then run the following:<pre>airflow webserver
airflow scheduler</pre></li>
				<li>Launch <a id="_idIndexMarker188"/>the GUI by opening your web browser <a id="_idIndexMarker189"/>and going to <code>http://localhost:8080</code>. You will see your DAG, as shown in the following screenshot:<div><img src="img/B15739_03_03.jpg" alt="Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG&#13;&#10;"/></div><p class="figure-caption">Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG</p></li>
				<li>Click on <strong class="bold">DAGs</strong> and select <strong class="bold">Tree View</strong>. Turn the DAG on, and then click <strong class="bold">Go</strong>. As the tasks start running, you will see the status of each run, as shown in the following screenshot:<div><img src="img/B15739_03_04.jpg" alt="Figure 3.4 – Multiple runs of the DAG and the status of each task&#13;&#10;"/></div><p class="figure-caption">Figure 3.4 – Multiple runs of the DAG and the status of each task</p></li>
				<li>You will <a id="_idIndexMarker190"/>see that <a id="_idIndexMarker191"/>there have been successful runs – each task ran and did so successfully. But there is no output or results. To see the results, click on one of the completed squares, as shown in the following screenshot:<div><img src="img/B15739_03_05.jpg" alt="Figure 3.5 – Checking results by hovering over the completed task&#13;&#10;"/></div><p class="figure-caption">Figure 3.5 – Checking results by hovering over the completed task</p></li>
				<li>You <a id="_idIndexMarker192"/>will see a popup <a id="_idIndexMarker193"/>with several options. Click the <strong class="bold">View Log</strong> button, as shown in the following screenshot:<div><img src="img/B15739_03_06.jpg" alt="Figure 3.6 – Selecting View Log to see what happened in your task&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – Selecting View Log to see what happened in your task</p></li>
				<li>You <a id="_idIndexMarker194"/>will be redirected <a id="_idIndexMarker195"/>to the log screen for the task. Looking at a successful run of the CSV task, you should see a log file similar to the one in the following screenshot:</li>
			</ol>
			<div><div><img src="img/B15739_03_07.jpg" alt="Figure 3.7 – Log of the Python task showing the names being printed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Log of the Python task showing the names being printed</p>
			<p>Congratulations! You have built a data pipeline with Python and ran it in Airflow. The result of your pipeline is a JSON file in your <code>dags</code> directory that was created from your <code>data.CSV</code> file. You can <a id="_idIndexMarker196"/>leave it running <a id="_idIndexMarker197"/>and it will continue to run at the specified <code>schedule_interval</code> time. Building more advanced pipelines will only require you to write more functions and connect them with the same process. But before you move on to more advanced techniques, you will need to learn how to use Apache NiFi to build data pipelines.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor045"/>Handling files using NiFi processors</h1>
			<p>In the <a id="_idIndexMarker198"/>previous sections, you learned how to read and write CSV and JSON files using Python. Reading files is such a common task that tools such as <a id="_idIndexMarker199"/>NiFi have prebuilt processors to handle it. In this section, you will learn how to handle files using NiFi processors.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor046"/>Working with CSV in NiFi</h2>
			<p>Working with <a id="_idIndexMarker200"/>files in NiFi requires many more <a id="_idIndexMarker201"/>steps than you had to use when doing the same tasks in Python. There are benefits to using more steps and using Nifi, including that someone who does not know code can look at your data pipeline and understand what it is you are doing. You may even find it easier to remember what it is you were trying to do when you come back to your pipeline in the future. Also, changes to the data pipeline do not require refactoring a lot of code; rather, you can reorder processors via drag and drop.</p>
			<p>In this section, you will create a data pipeline that reads in the <code>data.CSV</code> file you created in Python. It will run a query for people over the age of 40, then write out that record to a file.</p>
			<p>The result of this section is shown in the following screenshot:</p>
			<div><div><img src="img/B15739_03_08.jpg" alt="Figure 3.8 – The data pipeline you will build in this section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – The data pipeline you will build in this section</p>
			<p>The <a id="_idIndexMarker202"/>following sections will walk you through building <a id="_idIndexMarker203"/>a data pipeline.</p>
			<h3>Reading a file with GetFile </h3>
			<p>The first <a id="_idIndexMarker204"/>step in your data pipeline is to read in the <code>data.csv</code> file. To do that, take the following steps:</p>
			<ol>
				<li value="1">Drag the <strong class="bold">Processor</strong> icon from the NiFi toolbar to the canvas. Search for <strong class="bold">GetFile</strong> and then select it.</li>
				<li>To configure the <code>GetFile</code> processor, you must specify the input directory. In the Python examples earlier in this chapter, I wrote the <code>data.CSV</code> file to my home directory, which is <code>home/paulcrickard</code>, so this is what I will use for the input directory. </li>
				<li>Next, you will need to specify a file filter. This field allows the NiFi expression language, so <a id="_idIndexMarker205"/>you could use <code>[^\.].*\.CSV</code> – but for this example, you can just set the value to <code>data.csv</code>. </li>
				<li>Lastly, the <strong class="bold">Keep Source File</strong> property should be set to <strong class="bold">true</strong>. If you leave it as <strong class="bold">false</strong>, NiFi <a id="_idIndexMarker206"/>will delete the file once it has processed it. The complete configuration is shown in the following screenshot:</li>
			</ol>
			<div><div><img src="img/B15739_03_09.jpg" alt="Figure 3.9 – GetFile processor configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – GetFile processor configuration</p>
			<h3>Splitting records into distinct flowfiles</h3>
			<p>Now <a id="_idIndexMarker207"/>you can pass the success relationship <a id="_idIndexMarker208"/>from the <code>GetFile</code> processor to the <code>SplitRecord</code> processor:</p>
			<ol>
				<li value="1">The <code>SplitRecord</code> processor will allow you to separate each row into a separate flowfile. Drag <a id="_idIndexMarker209"/>and drop <a id="_idIndexMarker210"/>it on the canvas. You need to create a record reader and a record writer – NiFi already has several that you can configure. Click on the box next to <strong class="bold">Record Reader</strong> and select <strong class="bold">Create new service</strong>, as shown in the following screenshot: <div><img src="img/B15739_03_10.jpg" alt="Figure 3.10 – A list of available readers&#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – A list of available readers</p></li>
				<li>You will need to choose the type of reader. Select <strong class="bold">CSVReader</strong> from the dropdown. Select the dropdown for <strong class="bold">Record Writer</strong> and choose <strong class="bold">CSVRecordSetWriter</strong>:<div><img src="img/B15739_03_11.jpg" alt="Figure 3.11 – A list of available readers&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – A list of available readers</p></li>
				<li>To <a id="_idIndexMarker211"/>configure <strong class="bold">CSVReader</strong> and <strong class="bold">CSVRecordSetWriter</strong>, click the arrow to the right of either one. This <a id="_idIndexMarker212"/>will open the <strong class="bold">Files Configuration</strong> window on the <strong class="bold">CONTROLLER SERVICES</strong> tab. You will see the screen shown in the following screenshot:</li>
			</ol>
			<div><div><img src="img/B15739_03_12.jpg" alt="Figure 3.12 – Configuring the reader and writer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Configuring the reader and writer</p>
			<p>The three <a id="_idIndexMarker213"/>icons to the right are as follows: </p>
			<ul>
				<li>A gear for settings </li>
				<li>A lightning bolt for enabling and disabling the service (it is currently disabled) </li>
				<li>A trash can to delete it </li>
			</ul>
			<p>Select the gear for <strong class="bold">CSVReader</strong>. The default configuration will work, except for the <strong class="bold">Treat First Line as Header</strong> property, which should be set to <strong class="bold">true</strong>. Click the gear for <strong class="bold">CSVRecordSetWriter</strong> and you can see the available properties. The defaults are sufficient in this example. Now, click the lightning bolt to enable the services.</p>
			<h3>Filtering records with the QueryRecord processor</h3>
			<p>You now <a id="_idIndexMarker214"/>have a pipeline that will read a CSV and split the rows into individual flowfiles. Now you can process each row with the <code>QueryRecord</code> processor. This processor will allow you to execute a SQL command against the flowfile. The contents of the new flowfile will be the results of the SQL query. In this example, you will select all records where the age of the person is over 40: </p>
			<ol>
				<li value="1">Drag and drop the <code>QueryRecord</code> processor to the canvas. To query the flowfile, you need to specify a record reader and writer. You have already created one of each of these and they are available in the dropdown now. The <strong class="bold">Include Zero Record FlowFiles</strong> property should be set to <strong class="bold">false</strong>. This property will route records that do not meet the criteria to the same relationship (which you do not want). </li>
				<li>Lastly, click the plus sign in the right-hand corner and specify a property name in the popup. The name of the property will become a relationship when you create a connection from this processor. Name the property <code>over.40</code>. Then, the value popup will appear. This is where you will enter the SQL query. The results of the query will become the contents of the flowfile. Since you want the records of people over 40 years of age, the query is as follows:<pre>Select * from FlowFile where age &gt; 40</pre><p>The <code>Select</code> <code>*</code> query <a id="_idIndexMarker215"/>is what returns the entire flowfile. If you only wanted the name of the person and for the field to be <code>full_name</code>, you could run the following SQL:</p><pre>Select name as full_name from FlowFile where age &gt; 40</pre></li>
			</ol>
			<p>The point I am attempting to drive home here is that you can execute SQL and modify the flowfile to something other than the contents of the row – for example, running and aggregation and a group by.</p>
			<h3>Extracting data from a flowfile</h3>
			<p>The next <a id="_idIndexMarker216"/>processor will extract a value from the flowfile. That processer is <code>ExtractText</code>. The processor can be used on any flowfile containing text and uses regex to pull any data from the flowfile and assign it to an attribute. </p>
			<p>To configure the processor, click the plus sign and name the property. You will extract the person name from the flowfile, so you can name the property name. The value will be regex and should be as follows:</p>
			<pre>\n([^,]*),</pre>
			<p>Without a full tutorial on regex, the preceding regex statement looks for a newline and a comma – <code>\n</code> and the comma at the end – and grabs the text inside. The parentheses say to take the text and return any characters that are not <code>^</code> or a comma. This regex returns the person's name. The flowfile contains a header of field names in CSV, a new line, followed by values in CSV. The <code>name</code> field is the first field on the second line – after the newline and before <a id="_idIndexMarker217"/>the first comma that specifies the end of the <code>name</code> field. This is why the regex looks for the text between the newline and the comma.</p>
			<h3>Modifying flowfile attributes</h3>
			<p>Now that <a id="_idIndexMarker218"/>you have pulled out the person name as an attribute, you can use the <code>UpdateAttribute</code> processor to change the value of existing attributes. By using this processor, you will modify the default filename attribute that NiFi has provided the flowfile all the way at the beginning in the <code>GetFile</code> processor. Every flowfile will have the filename <code>data.CSV</code>. If you try to write the flowfiles out to CSV, they will all have the same name and will either overwrite or fail. </p>
			<p>Click the plus sign in the configuration for the <code>UpdateAttribute</code> processor and name the new property filename. The value will use the NiFi Expression Language. In the Expression Language, you can grab the value of an attribute using the format <code>${attribute name}</code>. So, to use the name attribute, set the value to <code>${name}</code>.</p>
			<h3>Saving a flowfile to disk</h3>
			<p>Using the <code>PutFile</code> processor, you can write the contents of a flowfile to disk. To configure the <a id="_idIndexMarker219"/>processor, you need to specify a directory in which to write the files. I will again use my home directory.  </p>
			<p>Next, you can specify a conflict resolution strategy. By default, it will be set to fail, but it allows you to overwrite an existing file. If you were running this data pipeline, aggregating data every hour and writing the results to files, maybe you would set the property to overwrite so that the file always holds the most current data. By default, the flowfile will write to a file on disk with the property filename as the filename.</p>
			<h3>Creating relationships between the processors</h3>
			<p>The <a id="_idIndexMarker220"/>last step is to make connections for specified relationships between the processors: </p>
			<ol>
				<li value="1">Grab the <code>GetFile</code> processor, drag the arrow to the <code>SplitRecord</code> processor, and check the relationship success in the popup.</li>
				<li>From the <code>SplitRecord</code> processor, make a connection to the <code>QueryRecord</code> processor and select the relationship splits. This means that any record that was split will be sent to the next processor. </li>
				<li>From <code>QueryRecord</code>, connect to the <code>ExtractText</code> processor. Notice the relationship you created is named <code>over.40</code>. If you added more SQL queries, you would get additional relationships. For this example, use the <code>over.40</code> relationship. </li>
				<li>Connect <code>ExtractText</code> to the <code>UpdateAttribute</code> processor for the relationship matched. </li>
				<li>Lastly, connect <code>UpdateAttribute</code> to the <code>PutFile</code> processor for the relationship success.</li>
			</ol>
			<p>The data pipeline is now complete. You can click on each processor and select <strong class="bold">Run</strong> to start it – or click the run icon in the operate window to start them all at once.</p>
			<p>When the pipeline is completed, you will have a directory with all the rows where the person was over 40. Of the 1,000 records, I have 635 CSVs named for each person. You will have different results based on what <code>Faker</code> used as the age value.</p>
			<p>This section showed you how to read in a CSV file. You also learned how you can split the file into rows and then run queries against them, as well as how to modify attributes of a flowfile and use it in another processor. In the next section, you will build another data <a id="_idIndexMarker221"/>pipeline using JSON. </p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor047"/>Working with JSON in NiFi</h2>
			<p>While <a id="_idIndexMarker222"/>having a different structure, working with JSON in NiFi is very similar to working with CSV. There are, however, a few processors <a id="_idIndexMarker223"/>for dealing exclusively with JSON. In this section, you will build a flow similar to the CSV example – read a file, split it into rows, and write each row to a file – but you will perform some more modifications of the data within the pipeline so that the rows you write to disk are different than what was in the original file. The following diagram shows the completed data pipeline:</p>
			<div><div><img src="img/B15739_03_13.jpg" alt="Figure 3.13 – The completed JSON data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – The completed JSON data pipeline</p>
			<p>To build the data pipeline, take the following steps:</p>
			<ol>
				<li value="1">Place the <code>GetFile</code> processor on to the canvas. To configure the processor, specify the <code>home/paulcrickard</code> – and the <code>data.JSON</code>.</li>
				<li>In the <a id="_idIndexMarker224"/>CSV example, you used the <code>SplitRecord</code> processor. Here, for JSON, you can use the <code>SplitJson</code> processor. You will need to configure the <strong class="bold">JsonPath Expression</strong> property. This <a id="_idIndexMarker225"/>property is looking for an array that contains JSON elements. The JSON file is in the following format:<pre>{"records":[ { } ] }</pre><p>Because each record is in an array, you can pass the following value to the <strong class="bold">JsonPath Expression</strong> property:</p><pre>$.records</pre><p>This will split records inside of the array, which is the result you want.</p></li>
				<li>The records will now become individual flowfiles. You will pass the files to the <code>EvaluateJsonPath</code> processor. This processor allows you to extract values from the flowfile. You can either pass the results to the flowfile content or to an attribute. Set the value of the <code>flowfile-attribute</code>. You can then select attributes to create using the plus sign. You will name the attribute, then specify the value. The value is the JSON path, and you use the format <code>$.key</code>. The configured processor is shown in the following screenshot:<div><img src="img/B15739_03_14.jpg" alt="Figure 3.14 – Configuration for extracting values from the flowfile&#13;&#10;"/></div><p class="figure-caption">Figure 3.14 – Configuration for extracting values from the flowfile</p><p>These <a id="_idIndexMarker226"/>attributes will not be passed down the data pipeline with the flowfile.</p></li>
				<li>Now, you can use the <code>QueryRecord</code> processor, just like you did with the CSV example. The difference with JSON is that you need to create a new record reader <a id="_idIndexMarker227"/>and recordset writer. Select the option to create a new service. Select <code>over.40</code> and set the value to the following:<pre>Select * from FlowFile where age &gt; 40</pre></li>
				<li>The next processor is the <code>AttributesToJSON</code> processor. This processor allows you to replace the flowfile content with the attributes you extracted in the <code>EvaluateJsonPath</code> processor shown in <em class="italic">step 3</em>. Set the <code>flowfile-content</code>. This processor also allows you to specify a comma-separated list of attributes in the <strong class="bold">Attributes List</strong> property. This can come in handy if you only want certain attributes. In this example, you leave it blank and several attributes you do not extract will be added to the flowfile content. All of the metadata attributes that NiFi writes will now be a part of the flowfile. The flowfile will now look as in the following snippet:<pre>### Run it at night ###</pre></li>
				<li>Using the <code>EvalueJsonPath</code> processor again, you will create an attribute named <code>uuid</code>. Now that the metadata from NiFi is in the flowfile, you have the unique ID of the flowfile. Make sure to set <code>flowfile-attribute</code>. You will extract it now so <a id="_idIndexMarker228"/>that you can pass it to the next processor – <code>UpdateAttribute</code>.</li>
				<li>In <a id="_idIndexMarker229"/>the CSV example, you updated the filename using the <code>UpdateAttribute</code> processor. You will do the same here. Click on the plus sign and add an attribute named <code>filename</code>. Set the value to <code>${uuid}</code>.</li>
				<li>One way <a id="_idIndexMarker230"/>to modify JSON using NiFi is through <code>zip</code> field from the flowfile.</p></li>
				<li>Lastly, use the <code>PutFile</code> processor to write each row to disk. Configure the <strong class="bold">Directory</strong> and <strong class="bold">Conflict Resolution Strategy</strong> properties. By setting the <strong class="bold">Conflict Resolution Strategy</strong> property to <strong class="bold">ignore</strong>, the processor will not warn you if it has already processed a file with the same name.</li>
			</ol>
			<p>Create the connections and relationships between the processors:</p>
			<ul>
				<li>Connect <code>GetFile</code> to <code>SplitJson</code> for relationship success.</li>
				<li>Connect <code>SplitJson</code> to <code>EvaluateJsonPath</code> for relationship splits.</li>
				<li>Connect <code>EvaluateJsonPath</code> to <code>QueryRecord</code> for relationship matched.</li>
				<li>Connect <code>QueryRecord</code> to <code>AttributesToJSON</code> for relationship <code>over.40</code>.</li>
				<li>Connect <code>AttributesToJSON</code> to <code>UpdateAttribute</code> for relationship success.</li>
				<li>Connect <code>UpdateAttributes</code> to <code>JoltTransformJSON</code> for relationship success.</li>
				<li>Connect <code>JoltTransformJSON</code> to <code>PutFile</code> for relationship success.</li>
			</ul>
			<p>Run the <a id="_idIndexMarker232"/>data pipeline by starting each processor <a id="_idIndexMarker233"/>or clicking <strong class="bold">Run</strong> in the operate box. When complete, you will have a subset of 1,000 files – all people over 40 – on disk and named by their unique ID.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, you learned how to process CSV and JSON files using Python. Using this new skill, you have created a data pipeline in Apache Airflow by creating a Python function to process a CSV and transform it into JSON. You should now have a basic understanding of the Airflow GUI and how to run DAGs. You also learned how to build data pipelines in Apache NiFi using processors. The process for building more advanced data pipelines is the same, and you will learn the skills needed to accomplish this throughout the rest of this book.</p>
			<p>In the next chapter, you will learn how to use Python, Airflow, and NiFi to read and write data to databases. You will learn how to use PostgreSQL and Elasticsearch. Using both will expose you to standard relational databases that can be queried using SQL and NoSQL databases that allow you to store documents and use their own query languages.</p>
		</div>
	</body></html>