<html><head></head><body>
		<div id="_idContainer061">
			<h1 id="_idParaDest-38"><em class="italic"><a id="_idTextAnchor039"/>Chapter 3</em>: Reading and Writing Files</h1>
			<p>In the previous chapter, we looked at how to install various tools, such as NiFi, Airflow, PostgreSQL, and Elasticsearch. In this chapter, you will be learning how to use these tools. One of the most basic tasks in data engineering is moving data from a text file to a database. In this chapter, you will read data from and write data to several different text-based formats, such as CSV and JSON.  </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Reading and writing files in Python</li>
				<li>Processing files in Airflow</li>
				<li>NiFi processors for handling files</li>
				<li>Reading and writing data to databases in Python</li>
				<li>Databases in Airflow</li>
				<li>Database processors in NiFi</li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor040"/>Writing and reading files in Python</h1>
			<p>The title of <a id="_idIndexMarker123"/>this section may sound strange as you are probably used to seeing it written as reading and writing, but in this section, you will write data to files first, then <a id="_idIndexMarker124"/>read it. By writing it, you will understand the structure of the data and you will know what it is you are trying to read.</p>
			<p>To write <a id="_idIndexMarker125"/>data, you will use a library named <strong class="source-inline">faker</strong>. <strong class="source-inline">faker</strong> allows you to <a id="_idIndexMarker126"/>easily create fake data for common fields. You can generate an address by simply calling <strong class="source-inline">address()</strong>, or a female name using <strong class="source-inline">name_female()</strong>. This will simplify the creation of fake data while at the same time making it more realistic. </p>
			<p>To install <strong class="source-inline">faker</strong>, you can use <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip3 install faker</p>
			<p>With <strong class="source-inline">faker</strong> now installed, you are ready to start writing files. The next section will start with CSV files.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Writing and reading CSVs</h2>
			<p>The most <a id="_idIndexMarker127"/>common file type you will encounter is <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>). A CSV is a file made up of fields separated <a id="_idIndexMarker128"/>by commas. Because commas are fairly <a id="_idIndexMarker129"/>common in text, you need to be able to handle them in CSV files. This can be accomplished by using escape characters, usually a pair of quotes around text strings that could contain a comma that is not used to signify a new field. These quotes are called escape characters. The Python standard library for handling CSVs simplifies the process of handling CSV data. </p>
			<h3>Writing CSVs using the Python CSV Library</h3>
			<p>To <a id="_idIndexMarker130"/>write a CSV with <a id="_idIndexMarker131"/>the CSV library, you need to use the following steps:</p>
			<ol>
				<li>Open a file in writing mode. To open a file, you need to specify a filename and a mode. The mode for writing is <strong class="source-inline">w</strong>, but you can also open a file for reading with <strong class="source-inline">r</strong>, appending with <strong class="source-inline">a</strong>, or reading and writing with <strong class="source-inline">r+</strong>. Lastly, if you are handling files that are not text, you can add <strong class="source-inline">b</strong>, for binary mode, to any of the preceding modes to write in bytes; for example, <strong class="source-inline">wb</strong> will allow you to write in bytes:<p class="source-code">output = open('myCSV.CSV',mode='w')</p></li>
				<li>Create <strong class="source-inline">CSV_writer</strong>. At a minimum, you must specify a file to write to, but you can also pass additional parameters, such as a dialect. A dialect can be a defined CSV type, such as Excel, or it can be options such as the delimiter to use or the level of quoting. The defaults are usually what you will need; for example, the delimiter defaults to a comma (it is a CSV writer after all) and quoting defaults to <strong class="source-inline">QUOTE_MINIMAL</strong>, which will only add quotes when there are special characters or the delimiter within a field. So, you can create the writer as shown:<p class="source-code">mywriter=csv.writer(output) </p></li>
				<li>Include a header. You might be able to remember what the fields are in your CSV, but it is best to include a header. Writing a header is the same as writing any other row: define the values, then you will use <strong class="source-inline">writerow()</strong>, as shown:<p class="source-code">header=['name','age']</p><p class="source-code">mywriter.writerow(header)</p></li>
				<li>Write the data to a file. You can now write a data row by using <strong class="source-inline">writerow(0)</strong> and passing some data, as shown:<p class="source-code">data=['Bob Smith',40]</p><p class="source-code">mywriter.writerow(data)</p><p class="source-code">output.close()</p></li>
			</ol>
			<p>Now, if <a id="_idIndexMarker132"/>you look in the <a id="_idIndexMarker133"/>directory, you will have a CSV file named <strong class="source-inline">myCSV.CSV</strong> and the contents should look as in the following screenshot:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B15739_03_01.jpg" alt="Figure 3.1 – The contents of mycsv.csv&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – The contents of mycsv.csv</p>
			<p>Notice that when you used <strong class="source-inline">cat</strong> to view the file, the newlines were added. By default, <strong class="source-inline">CSV_writer</strong> uses a return and a newline (<strong class="source-inline">'\r\n'</strong>).</p>
			<p>The <a id="_idIndexMarker134"/>preceding example was very basic. However, if you are trying to write a lot of data, you would most likely <a id="_idIndexMarker135"/>want to loop through some condition or iterate through existing data. In the following example, you will use <strong class="source-inline">Faker</strong> to generate 1,000 records: </p>
			<p class="source-code">from faker import Faker</p>
			<p class="source-code">import csv</p>
			<p class="source-code">output=open('data.CSV','w')</p>
			<p class="source-code">fake=Faker()</p>
			<p class="source-code">header=['name','age','street','city','state','zip','lng','lat']</p>
			<p class="source-code">mywriter=csv.writer(output)</p>
			<p class="source-code">mywriter.writerow(header)</p>
			<p class="source-code">for r in range(1000):</p>
			<p class="source-code">    mywriter.writerow([fake.name(),fake.random_int(min=18, </p>
			<p class="source-code">    max=80, step=1), fake.street_address(), fake.city(),fake.</p>
			<p class="source-code">    state(),fake.zipcode(),fake.longitude(),fake.latitude()])</p>
			<p class="source-code">    output.close()</p>
			<p>You <a id="_idIndexMarker136"/>should now have a <strong class="source-inline">data.CSV</strong> file with 1,000 rows of names and ages.</p>
			<p>Now <a id="_idIndexMarker137"/>that you have written a CSV, the next section will walk you through reading it using Python.</p>
			<h3>Reading CSVs</h3>
			<p>Reading <a id="_idIndexMarker138"/>a CSV is somewhat similar to writing one. The same steps are followed with slight modifications: </p>
			<ol>
				<li value="1">Open a file using <strong class="source-inline">with</strong>. Using <strong class="source-inline">with</strong> has some additional benefits, but for now, the one you will reap is not having to use <strong class="source-inline">close()</strong> on the file. If you do not specify a mode, <strong class="source-inline">open</strong> defaults to read (<strong class="source-inline">r</strong>). After <strong class="source-inline">open</strong>, you will need to specify what to refer to the file as; in this case, you will open the <strong class="source-inline">data.CSV</strong> file and refer to it as <strong class="source-inline">f</strong>:<p class="source-code">with open('data.csv') as f:</p></li>
				<li>Create the reader. Instead of just using <strong class="source-inline">reader()</strong>, you will use <strong class="source-inline">DictReader()</strong>. By using the dictionary reader, you will be able to call fields in the data by name instead of position. For example, instead of calling the first item in a row as <strong class="source-inline">row[0]</strong>, you can now call it as <strong class="source-inline">row['name']</strong>. Just like the writer, the defaults are usually sufficient, and you will only need to specify a file to read. The following code opens <strong class="source-inline">data.CSV</strong> using the <strong class="source-inline">f</strong> variable name:<p class="source-code">myreader=CSV.DictReader(f)</p></li>
				<li>Grab the headers by reading a single line with <strong class="source-inline">next()</strong>:<p class="source-code">headers=next(myreader)</p></li>
				<li>Now, you can iterate through the rest of the rows using the following:<p class="source-code">for row in myreader:</p></li>
				<li>Lastly, you can print the names using the following:<p class="source-code">    print(row['name'])</p></li>
			</ol>
			<p>You should <a id="_idIndexMarker139"/>only see the 1,000 names scroll by. Now you have a Python dictionary that you can manipulate any way you need. There is another way to handle CSV data in Python and that requires <strong class="source-inline">pandas</strong>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Reading and writing CSVs using pandas DataFrames</h2>
			<p><strong class="source-inline">pandas</strong> DataFrames are a powerful tool not only for reading and writing data but also for the <a id="_idIndexMarker140"/>querying and <a id="_idIndexMarker141"/>manipulation of data. It does require a larger overhead than the built-in CSV library, but there <a id="_idIndexMarker142"/>are times when it may be worth the <a id="_idIndexMarker143"/>trade-off. You may already have <strong class="source-inline">pandas</strong> installed, depending on your Python environment, but if you do not, you can install it with the following:</p>
			<p class="source-code"><strong class="bold">pip3 install pandas</strong></p>
			<p>You can think of a <strong class="source-inline">pandas</strong> DataFrame as an Excel sheet or a table. You will have rows, columns, and an index. To load CSV data into a DataFrame, the following steps must be followed: </p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> (usually as <strong class="source-inline">pd</strong>):<p class="source-code">import pandas as pd</p></li>
				<li>Then, read the file using <strong class="source-inline">read_csv()</strong>. The <strong class="source-inline">read_csv()</strong> method takes several optional parameters, and one required parameter – the file or file-like buffer. The two optional parameters that may be of interest are <strong class="source-inline">header</strong>, which by defaultattempts to infer the headers. If you set <strong class="source-inline">header=0</strong>, then you can use the <strong class="source-inline">names</strong> parameter with an array of column names. If you have a large file and you just want to look at a piece of it, you can use <strong class="source-inline">nrows</strong> to specify the number of rows to read, so <strong class="source-inline">nrows=100</strong> means it will only read 100 rows for the data. In the following snippet, you will load the entire file using the defaults:<p class="source-code">df=pd.read_csv()('data.CSV')</p></li>
				<li>Let's <a id="_idIndexMarker144"/>now look at the first 10 records by using the following:<p class="source-code">df.head(10)</p></li>
			</ol>
			<p>Because <a id="_idIndexMarker145"/>you used <strong class="source-inline">Faker</strong> to <a id="_idIndexMarker146"/>generate data, you will have <a id="_idIndexMarker147"/>the same schema as in the following screenshot, but will have different values:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B15739_03_02.jpg" alt="Figure 3.2 – Reading a CSV into a DataFrame and printing head()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Reading a CSV into a DataFrame and printing head()</p>
			<p>You <a id="_idIndexMarker148"/>can create a DataFrame <a id="_idIndexMarker149"/>in Python with the following steps:</p>
			<ol>
				<li value="1">Create <a id="_idIndexMarker150"/>a dictionary of data. A dictionary <a id="_idIndexMarker151"/>is a data structure that stores data as a key:value pair. The value can be of any Python data type – for example, an array. Dictionaries have methods for finding <strong class="source-inline">keys()</strong>, <strong class="source-inline">values()</strong>, and <strong class="source-inline">items()</strong>. They also allow you to find the value of a key by using the key name in brackets – for example, <strong class="source-inline">dictionary['key']</strong> will return the value for that key:<p class="source-code">data={'Name':['Paul','Bob','Susan','Yolanda'],</p><p class="source-code">'Age':[23,45,18,21]}</p></li>
				<li>Pass <a id="_idIndexMarker152"/>the data to the DataFrame:<p class="source-code">df=pd.DataFrame(data)</p></li>
				<li>The <a id="_idIndexMarker153"/>columns are specified as the keys in the dictionary. Now that you have a DataFrame, you can write the contents to a CSV using <strong class="source-inline">to_csv()</strong> and passing a filename. In <a id="_idIndexMarker154"/>the example, we did not set an index, which means the row names will be a <a id="_idIndexMarker155"/>number from 0 to <em class="italic">n</em>, where <em class="italic">n</em> is the length of the DataFrame. When you export to CSV, these values will be written to the file, but the column name will be blank. So, in a case where you do not need the row names or index to be written to the file, pass the <strong class="source-inline">index</strong> parameter to <strong class="source-inline">to_csv()</strong>, as shown:<p class="source-code">df.to_csv('fromdf.CSV',index=False)</p></li>
			</ol>
			<p>You will now have a CSV file with the contents of the DataFrame. How we can use the contents of this DataFrame for executing SQL queries will be covered in the next chapter. They will become an important tool in your toolbox and the rest of the book will lean on them heavily. </p>
			<p>For now, let's move on to the next section, where you will learn about another common text format – <strong class="bold">JSON</strong>.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>Writing JSON with Python</h2>
			<p>Another <a id="_idIndexMarker156"/>common data format you will probably <a id="_idIndexMarker157"/>deal with is <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>). You <a id="_idIndexMarker158"/>will see <a id="_idIndexMarker159"/>JSON most often when making calls to <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>); however, it can exist as a file as well. How you handle the data is very similar no matter whether you read it from a file or an API. Python, as you learned with CSV, has a standard library for handling JSON data, not surprisingly named <strong class="source-inline">JSON–JSON</strong>. </p>
			<p>To write JSON using Python and the standard library, the following steps need to be observed:</p>
			<ol>
				<li value="1">Import the library and open the file you will write to. You also create the <strong class="source-inline">Faker</strong> object:<p class="source-code">from faker import Faker</p><p class="source-code">import json</p><p class="source-code">output=open('data.JSON','w')</p><p class="source-code">fake=Faker()</p></li>
				<li>We will create 1,000 records, just as we did in the CSV example, so you will need to create a dictionary to hold the data. As mentioned earlier, the value of a key can be any <a id="_idIndexMarker160"/>Python data type – including an array of values. After creating the dictionary to hold the records, add a <strong class="source-inline">'records'</strong> key and initialize it with a blank array, as shown:<p class="source-code">alldata={}</p><p class="source-code">alldata['records']=[]</p></li>
				<li>To <a id="_idIndexMarker161"/>write the records, you use <strong class="source-inline">Faker</strong> to create a dictionary, then append it to the array:<p class="source-code">for x in range(1000):</p><p class="source-code">	data={"name":fake.name(),"age":fake.random_int</p><p class="source-code">           (min=18, max=80, step=1),</p><p class="source-code">           "street":fake.street_address(),</p><p class="source-code">           "city":fake.city(),"state":fake.state(),</p><p class="source-code">           "zip":fake.zipcode(),</p><p class="source-code">           "lng":float(fake.longitude()),</p><p class="source-code">           "lat":float(fake.latitude())}</p><p class="source-code">	alldata['records'].append(data)	</p></li>
				<li>Lastly, to write the JSON to a file, use the <strong class="source-inline">JSON.dump()</strong> method. Pass the data that you want to write and a file to write to:<p class="source-code">json.dump(alldata,output)</p></li>
			</ol>
			<p>You now have a <strong class="source-inline">data.JSON</strong> file that has an array with 1,000 records. You can read this file by taking the following steps:</p>
			<ol>
				<li value="1">Open the file using the following:<p class="source-code">with open("data.JSON","r") as f:</p></li>
				<li>Use <strong class="source-inline">JSON.load()</strong> and pass the file reference to the method:<p class="source-code">data=json.load(f)</p></li>
				<li>Inspect the json by looking at the first record using the following:<p class="source-code">data['records'][0]</p><p>Or just use the name:</p><p class="source-code">data['records'][0]['name']</p></li>
			</ol>
			<p>When <a id="_idIndexMarker162"/>you <strong class="bold">load</strong> and <strong class="bold">dump</strong> JSON, make <a id="_idIndexMarker163"/>sure you do not add an <em class="italic">s</em> at the end of the JSON terms. <strong class="source-inline">loads</strong> and <strong class="source-inline">dumps</strong> are different than <strong class="source-inline">load</strong> and <strong class="source-inline">dump</strong>. Both are valid methods of the JSON library. The difference is that <strong class="source-inline">loads</strong> and <strong class="source-inline">dumps</strong> are for strings – they do not serialize the JSON.</p>
			<h3>pandas DataFrames</h3>
			<p>Reading <a id="_idIndexMarker164"/>and writing <a id="_idIndexMarker165"/>JSON with DataFrames is similar to what we did with CSV. The only difference is that you change <strong class="source-inline">to_csv</strong> to <strong class="source-inline">to_json()</strong> and <strong class="source-inline">read_csv()</strong> to <strong class="source-inline">read_json()</strong>. </p>
			<p>If you <a id="_idIndexMarker166"/>have a clean, well-formatted JSON file, you can read it using the following code:</p>
			<p class="source-code">df=pd.read_json('data.JSON')</p>
			<p>In the <a id="_idIndexMarker167"/>case of the <strong class="source-inline">data.JSON</strong> file, the records are nested in a <strong class="source-inline">records</strong> dictionary. So, loading the JSON is not as straightforward as the preceding code. You will need a few extra steps, which are as follows. To load JSON data from the file, do the following: </p>
			<ol>
				<li value="1">Use the <strong class="source-inline">pandas</strong> <strong class="source-inline">JSON</strong> library:<p class="source-code">import pandas.io.json as pd_JSON</p></li>
				<li>Open the file and load it with the <strong class="source-inline">pandas</strong> version of <strong class="source-inline">JSON.loads()</strong>:<p class="source-code">f=open('data.JSON','r')</p><p class="source-code">data=pd_JSON.loads(f.read())</p></li>
				<li>To create the DataFrame, you need to normalize the JSON. Normalizing is how you can flatten the JSON to fit in a table. In this case, you want to grab the individual JSON records held in the <strong class="source-inline">records</strong> dictionary. Pass that path – <strong class="source-inline">records</strong> – to the <strong class="source-inline">record_path</strong> parameter of <strong class="source-inline">json_normalize()</strong>:<p class="source-code">df=pd_JSON.json_normalize(data,record_path='records')</p></li>
			</ol>
			<p>You will now have a DataFrame that contains all the records in the <strong class="source-inline">data.JSON</strong> file. You can now write them back to JSON, or CSV, using DataFrames.</p>
			<p>When <a id="_idIndexMarker168"/>writing to JSON, you can pass the <strong class="source-inline">orient</strong> parameter, which determines the format of the JSON that is returned. The default <a id="_idIndexMarker169"/>is columns, which for the <strong class="source-inline">data.JSON</strong> file <a id="_idIndexMarker170"/>you created in the previous section would look like the following data:</p>
			<p class="source-code">&gt;&gt;&gt; df.head(2).to_json()</p>
			<p class="source-code">'{"name":{"0":"Henry Lee","1":"Corey Combs DDS"},"age":{"0":42,"1":43},"street":{"0":"57850 Zachary Camp","1":"60066 Ruiz Plaza Apt. 752"},"city":{"0":"Lake Jonathon","1":"East Kaitlin"},"state":{"0":"Rhode Island","1":"Alabama"},"zip":{"0":"93363","1":"16297"},"lng":{"0":-161.561209,"1":123.894456},"lat":</p>
			<p class="source-code">{"0":-72.086145,"1":-50.211986}}'</p>
			<p>By <a id="_idIndexMarker171"/>changing the <strong class="source-inline">orient</strong> value to <strong class="source-inline">records</strong>, you get each row as a record in the JSON, as shown:</p>
			<p class="source-code">&gt;&gt;&gt; df.head(2).to_JSON(orient='records')</p>
			<p class="source-code">'[{"name":"Henry Lee","age":42,"street":"57850, Zachary Camp","city":"Lake Jonathon","state":"Rhode Island", "zip":"93363","lng":-161.561209,"lat":72.086145},{"name":"Corey Combs DDS","age":43,"street":"60066 Ruiz Plaza Apt. 752","city":"EastKaitlin","state":"Alabama","zip":"16297","lng":123.894456, "lat":-50.211986}]'</p>
			<p>I find <a id="_idIndexMarker172"/>that working <a id="_idIndexMarker173"/>with JSON that is oriented around <strong class="source-inline">records</strong> makes processing it in tools such as Airflow much <a id="_idIndexMarker174"/>easier than JSON in other formats, such <a id="_idIndexMarker175"/>as split, index, columns, values, or table. Now that you know how to handle CSV and JSON files in Python, it is time to learn how to combine tasks into a data pipeline using Airflow and NiFi. In the next section, you will learn how to build pipelines in Apache Airflow.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor044"/>Building data pipelines in Apache Airflow</h1>
			<p>Apache <a id="_idIndexMarker176"/>Airflow uses Python functions, as well as <a id="_idIndexMarker177"/>Bash or other operators, to create tasks that can be combined into a <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) – meaning each task moves <a id="_idIndexMarker178"/>in one direction when completed. Airflow allows you to combine Python functions to create tasks. You can specify the order in which the tasks will run, and which tasks depend on others. This order and dependency are what make it a DAG. Then, you can schedule your DAG in Airflow to specify when, and how frequently, your DAG should run. Using the Airflow GUI, you can monitor and manage your DAG. By using what you learned in the preceding sections, you will now make a data pipeline in Airflow.</p>
			<p>Building a CSV to a JSON data pipeline</p>
			<p>Starting <a id="_idIndexMarker179"/>with a simple DAG will help you understand how Airflow works and will help you to add more functions <a id="_idIndexMarker180"/>to build a better data pipeline. The DAG you build will print out a message using Bash, then read the CSV and print a list of all the names. The following steps will walk you through building the data pipeline:</p>
			<ol>
				<li value="1">Open a new file using the Python IDE or any text editor. Import the required libraries, as shown:<p class="source-code">import datetime as dt</p><p class="source-code">from datetime import timedelta</p><p class="source-code">from airflow import DAG</p><p class="source-code">from airflow.operators.bash_operator import BashOperator</p><p class="source-code">from airflow.operators.python_operator import PythonOperator</p><p class="source-code">import pandas as pd</p><p>The first two imports bring in <strong class="source-inline">datetime</strong> and <strong class="source-inline">timedelta</strong>. These libraries are used for scheduling the DAG. The three Airflow imports bring in the required libraries for building the DAG and using the Bash and Python operators. These are the operators you will use to build tasks. Lastly, you import <strong class="source-inline">pandas</strong> so that you can easily convert between CSV and JSON.</p></li>
				<li>Next, write a function to read a CSV file and print out the names. By combining the steps for reading CSV data and writing JSON data from the previous sections, you can create a function that reads in the <strong class="source-inline">data.CSV</strong> file and writes it out to JSON, as shown in the following code:<p class="source-code">def CSVToJson():</p><p class="source-code">    df=pd.read_CSV('/home/paulcrickard/data.CSV')</p><p class="source-code">    for i,r in df.iterrows():</p><p class="source-code">        print(r['name'])</p><p class="source-code">    df.to_JSON('fromAirflow.JSON',orient='records')</p><p>This <a id="_idIndexMarker181"/>function opens the file in a DataFrame. Then, it iterates through the rows, printing only the names, and lastly, it writes the CSV to a JSON file. </p></li>
				<li>Now, you <a id="_idIndexMarker182"/>need to implement the Airflow portion of the pipeline. Specify the arguments that will be passed to <strong class="source-inline">DAG()</strong>. In this book, you will use a minimal set of parameters. The arguments in this example assign an owner, a start date, the number of retries in the event of a failure, and how long to wait before retrying. They are shown in the following dictionary:<p class="source-code">default_args = {</p><p class="source-code">    'owner': 'paulcrickard',</p><p class="source-code">    'start_date': dt.datetime(2020, 3, 18),</p><p class="source-code">    'retries': 1,</p><p class="source-code">    'retry_delay': dt.timedelta(minutes=5),</p><p class="source-code">}</p></li>
				<li>Next, pass the arguments dictionary to <strong class="source-inline">DAG()</strong>. Create the DAG ID, which is set to <strong class="source-inline">MyCSVDAG</strong>, the dictionary of arguments (the <strong class="source-inline">default_args</strong> variable in the preceding code), and the schedule interval (how often to run the data pipeline). The schedule interval can be set using <strong class="source-inline">timedelts</strong>, or you can use a crontab format with the following presets or crontab:<p>a) <strong class="source-inline">@once</strong></p><p>b) <strong class="source-inline">@hourly</strong> – <strong class="source-inline">0 * * * *</strong></p><p>c) <strong class="source-inline">@daily</strong> – <strong class="source-inline">0 0 * * *</strong></p><p>d) <strong class="source-inline">@weekly</strong> – <strong class="source-inline">0 0 * * 0</strong></p><p>e) <strong class="source-inline">@monthly</strong> – <strong class="source-inline">0 0 1 * *</strong></p><p>f) <strong class="source-inline">@yearly</strong> – <strong class="source-inline">0 0 1 1 *</strong></p><p>crontab uses the format minute, hour, day of month, month, day of week. The value for <strong class="source-inline">@yearly</strong> is <strong class="source-inline">0 0 1 1 *</strong>, which means run yearly on January 1 (<strong class="source-inline">1 1</strong>), at 0:0 (midnight), on any day of the week (<strong class="source-inline">*</strong>). </p><p class="callout-heading">Scheduling a DAG warning</p><p class="callout">The <strong class="source-inline">start_date</strong> variable of a DAG is <strong class="source-inline">start_date</strong> + <strong class="source-inline">the schedule_interval</strong>. This means if you schedule a DAG with a <strong class="source-inline">start_date</strong> value of today, and a <strong class="source-inline">schedule_interval</strong> value of daily, the DAG will not run until tomorrow.</p><p>The <a id="_idIndexMarker183"/>DAG is created with the following code:</p><p class="source-code">with DAG('MyCSVDAG',</p><p class="source-code">         default_args=default_args,</p><p class="source-code">         schedule_interval=timedelta(minutes=5),      </p><p class="source-code">         # '0 * * * *',</p><p class="source-code">         ) as dag:</p></li>
				<li>You <a id="_idIndexMarker184"/>can now create your tasks using operators. Airflow <a id="_idIndexMarker185"/>has several prebuilt operators. You can view them all in the documentation at <a href="https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html">https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html</a>. In this book, you will mostly use the Bash, Python, and Postgres operators. The operators allow you to remove most of the boilerplate code that is required to perform common tasks. In the following snippet, you will create two tasks using the Bash and Python operators:<p class="source-code">    print_starting = BashOperator(task_id='starting',</p><p class="source-code">                     bash_command='echo "I am reading the                       </p><p class="source-code">                     CSV now....."')</p><p class="source-code">    </p><p class="source-code">    CSVJson = PythonOperator(task_id='convertCSVtoJson',</p><p class="source-code">                             python_callable=CSVToJson)</p><p>The preceding snippet creates a task using the <strong class="source-inline">BashOperator</strong> operator, which prints out a statement to let you know it is running. This task serves no purpose other than to allow you to see how to connect multiple tasks together. The next task, <strong class="source-inline">CSVJson</strong>, uses the <strong class="source-inline">PythonOperator</strong> operator to call the function you defined at the beginning of the file (<strong class="source-inline">CSVToJson()</strong>). The function reads the <strong class="source-inline">data.CSV</strong> file and prints the <strong class="source-inline">name</strong> field in every row. </p></li>
				<li>With <a id="_idIndexMarker186"/>the tasks defined, you now need to make the connections between the tasks. You can do this <a id="_idIndexMarker187"/>using the <strong class="source-inline">set_upstream()</strong> and <strong class="source-inline">set_downstream()</strong> methods or with the bit shift operator. By using upstream and downstream, you can make the graph go from the Bash task to the Python task using either of two snippets; the following is the first snippet:<p class="source-code">print_starting .set_downstream(CSVJson)</p><p>The following is the second snippet:</p><p class="source-code">CSVJson.set_upstream(print_starting)</p><p>Using the bit shift operator, you can do the same; the following is the first option:</p><p class="source-code">print_starting &gt;&gt;  CSVJson</p><p>The following is the second option:</p><p class="source-code">CSVJson &lt;&lt; print_starting</p><p class="callout-heading">Note</p><p class="callout">Which method you choose is up to you; however, you should be consistent. In this book, you will see the bit shift operator setting the downstream.</p></li>
				<li>To use Airflow and Scheduler in the GUI, you first need to make a directory for your DAGs. During the install and configuration of Apache Airflow, in the previous chapter, we removed the samples and so the DAG directory is missing. If you look at <strong class="source-inline">airflow.cfg</strong>, you will see the setting for <strong class="source-inline">dags_folder</strong>. It is in the format of <strong class="source-inline">$AIRFLOW_HOME/dags</strong>. On my machine, <strong class="source-inline">$AIRFLOW_HOME</strong> is <strong class="source-inline">home/paulcrickard/airflow</strong>. This is the directory in which you will make the <strong class="source-inline">dags folder.e</strong> configuration file showing where the folder should be.</li>
				<li>Copy your DAG code to the folder, then run the following:<p class="source-code">airflow webserver</p><p class="source-code">airflow scheduler</p></li>
				<li>Launch <a id="_idIndexMarker188"/>the GUI by opening your web browser <a id="_idIndexMarker189"/>and going to <strong class="source-inline">http://localhost:8080</strong>. You will see your DAG, as shown in the following screenshot:<div id="_idContainer049" class="IMG---Figure"><img src="image/B15739_03_03.jpg" alt="Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG&#13;&#10;"/></div><p class="figure-caption">Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG</p></li>
				<li>Click on <strong class="bold">DAGs</strong> and select <strong class="bold">Tree View</strong>. Turn the DAG on, and then click <strong class="bold">Go</strong>. As the tasks start running, you will see the status of each run, as shown in the following screenshot:<div id="_idContainer050" class="IMG---Figure"><img src="image/B15739_03_04.jpg" alt="Figure 3.4 – Multiple runs of the DAG and the status of each task&#13;&#10;"/></div><p class="figure-caption">Figure 3.4 – Multiple runs of the DAG and the status of each task</p></li>
				<li>You will <a id="_idIndexMarker190"/>see that <a id="_idIndexMarker191"/>there have been successful runs – each task ran and did so successfully. But there is no output or results. To see the results, click on one of the completed squares, as shown in the following screenshot:<div id="_idContainer051" class="IMG---Figure"><img src="image/B15739_03_05.jpg" alt="Figure 3.5 – Checking results by hovering over the completed task&#13;&#10;"/></div><p class="figure-caption">Figure 3.5 – Checking results by hovering over the completed task</p></li>
				<li>You <a id="_idIndexMarker192"/>will see a popup <a id="_idIndexMarker193"/>with several options. Click the <strong class="bold">View Log</strong> button, as shown in the following screenshot:<div id="_idContainer052" class="IMG---Figure"><img src="image/B15739_03_06.jpg" alt="Figure 3.6 – Selecting View Log to see what happened in your task&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – Selecting View Log to see what happened in your task</p></li>
				<li>You <a id="_idIndexMarker194"/>will be redirected <a id="_idIndexMarker195"/>to the log screen for the task. Looking at a successful run of the CSV task, you should see a log file similar to the one in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B15739_03_07.jpg" alt="Figure 3.7 – Log of the Python task showing the names being printed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Log of the Python task showing the names being printed</p>
			<p>Congratulations! You have built a data pipeline with Python and ran it in Airflow. The result of your pipeline is a JSON file in your <strong class="source-inline">dags</strong> directory that was created from your <strong class="source-inline">data.CSV</strong> file. You can <a id="_idIndexMarker196"/>leave it running <a id="_idIndexMarker197"/>and it will continue to run at the specified <strong class="source-inline">schedule_interval</strong> time. Building more advanced pipelines will only require you to write more functions and connect them with the same process. But before you move on to more advanced techniques, you will need to learn how to use Apache NiFi to build data pipelines.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor045"/>Handling files using NiFi processors</h1>
			<p>In the <a id="_idIndexMarker198"/>previous sections, you learned how to read and write CSV and JSON files using Python. Reading files is such a common task that tools such as <a id="_idIndexMarker199"/>NiFi have prebuilt processors to handle it. In this section, you will learn how to handle files using NiFi processors.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor046"/>Working with CSV in NiFi</h2>
			<p>Working with <a id="_idIndexMarker200"/>files in NiFi requires many more <a id="_idIndexMarker201"/>steps than you had to use when doing the same tasks in Python. There are benefits to using more steps and using Nifi, including that someone who does not know code can look at your data pipeline and understand what it is you are doing. You may even find it easier to remember what it is you were trying to do when you come back to your pipeline in the future. Also, changes to the data pipeline do not require refactoring a lot of code; rather, you can reorder processors via drag and drop.</p>
			<p>In this section, you will create a data pipeline that reads in the <strong class="source-inline">data.CSV</strong> file you created in Python. It will run a query for people over the age of 40, then write out that record to a file.</p>
			<p>The result of this section is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15739_03_08.jpg" alt="Figure 3.8 – The data pipeline you will build in this section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – The data pipeline you will build in this section</p>
			<p>The <a id="_idIndexMarker202"/>following sections will walk you through building <a id="_idIndexMarker203"/>a data pipeline.</p>
			<h3>Reading a file with GetFile </h3>
			<p>The first <a id="_idIndexMarker204"/>step in your data pipeline is to read in the <strong class="source-inline">data.csv</strong> file. To do that, take the following steps:</p>
			<ol>
				<li value="1">Drag the <strong class="bold">Processor</strong> icon from the NiFi toolbar to the canvas. Search for <strong class="bold">GetFile</strong> and then select it.</li>
				<li>To configure the <strong class="source-inline">GetFile</strong> processor, you must specify the input directory. In the Python examples earlier in this chapter, I wrote the <strong class="source-inline">data.CSV</strong> file to my home directory, which is <strong class="source-inline">home/paulcrickard</strong>, so this is what I will use for the input directory. </li>
				<li>Next, you will need to specify a file filter. This field allows the NiFi expression language, so <a id="_idIndexMarker205"/>you could use <strong class="bold">regular expressions</strong> (<strong class="bold">regex</strong>) and specify any file ending with CSV – <strong class="source-inline">[^\.].*\.CSV</strong> – but for this example, you can just set the value to <strong class="source-inline">data.csv</strong>. </li>
				<li>Lastly, the <strong class="bold">Keep Source File</strong> property should be set to <strong class="bold">true</strong>. If you leave it as <strong class="bold">false</strong>, NiFi <a id="_idIndexMarker206"/>will delete the file once it has processed it. The complete configuration is shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15739_03_09.jpg" alt="Figure 3.9 – GetFile processor configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – GetFile processor configuration</p>
			<h3>Splitting records into distinct flowfiles</h3>
			<p>Now <a id="_idIndexMarker207"/>you can pass the success relationship <a id="_idIndexMarker208"/>from the <strong class="source-inline">GetFile</strong> processor to the <strong class="source-inline">SplitRecord</strong> processor:</p>
			<ol>
				<li value="1">The <strong class="source-inline">SplitRecord</strong> processor will allow you to separate each row into a separate flowfile. Drag <a id="_idIndexMarker209"/>and drop <a id="_idIndexMarker210"/>it on the canvas. You need to create a record reader and a record writer – NiFi already has several that you can configure. Click on the box next to <strong class="bold">Record Reader</strong> and select <strong class="bold">Create new service</strong>, as shown in the following screenshot: <div id="_idContainer056" class="IMG---Figure"><img src="image/B15739_03_10.jpg" alt="Figure 3.10 – A list of available readers&#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – A list of available readers</p></li>
				<li>You will need to choose the type of reader. Select <strong class="bold">CSVReader</strong> from the dropdown. Select the dropdown for <strong class="bold">Record Writer</strong> and choose <strong class="bold">CSVRecordSetWriter</strong>:<div id="_idContainer057" class="IMG---Figure"><img src="image/B15739_03_11.jpg" alt="Figure 3.11 – A list of available readers&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – A list of available readers</p></li>
				<li>To <a id="_idIndexMarker211"/>configure <strong class="bold">CSVReader</strong> and <strong class="bold">CSVRecordSetWriter</strong>, click the arrow to the right of either one. This <a id="_idIndexMarker212"/>will open the <strong class="bold">Files Configuration</strong> window on the <strong class="bold">CONTROLLER SERVICES</strong> tab. You will see the screen shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B15739_03_12.jpg" alt="Figure 3.12 – Configuring the reader and writer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Configuring the reader and writer</p>
			<p>The three <a id="_idIndexMarker213"/>icons to the right are as follows: </p>
			<ul>
				<li>A gear for settings </li>
				<li>A lightning bolt for enabling and disabling the service (it is currently disabled) </li>
				<li>A trash can to delete it </li>
			</ul>
			<p>Select the gear for <strong class="bold">CSVReader</strong>. The default configuration will work, except for the <strong class="bold">Treat First Line as Header</strong> property, which should be set to <strong class="bold">true</strong>. Click the gear for <strong class="bold">CSVRecordSetWriter</strong> and you can see the available properties. The defaults are sufficient in this example. Now, click the lightning bolt to enable the services.</p>
			<h3>Filtering records with the QueryRecord processor</h3>
			<p>You now <a id="_idIndexMarker214"/>have a pipeline that will read a CSV and split the rows into individual flowfiles. Now you can process each row with the <strong class="source-inline">QueryRecord</strong> processor. This processor will allow you to execute a SQL command against the flowfile. The contents of the new flowfile will be the results of the SQL query. In this example, you will select all records where the age of the person is over 40: </p>
			<ol>
				<li value="1">Drag and drop the <strong class="source-inline">QueryRecord</strong> processor to the canvas. To query the flowfile, you need to specify a record reader and writer. You have already created one of each of these and they are available in the dropdown now. The <strong class="bold">Include Zero Record FlowFiles</strong> property should be set to <strong class="bold">false</strong>. This property will route records that do not meet the criteria to the same relationship (which you do not want). </li>
				<li>Lastly, click the plus sign in the right-hand corner and specify a property name in the popup. The name of the property will become a relationship when you create a connection from this processor. Name the property <strong class="source-inline">over.40</strong>. Then, the value popup will appear. This is where you will enter the SQL query. The results of the query will become the contents of the flowfile. Since you want the records of people over 40 years of age, the query is as follows:<p class="source-code">Select * from FlowFile where age &gt; 40</p><p>The <strong class="source-inline">Select</strong> <strong class="source-inline">*</strong> query <a id="_idIndexMarker215"/>is what returns the entire flowfile. If you only wanted the name of the person and for the field to be <strong class="source-inline">full_name</strong>, you could run the following SQL:</p><p class="source-code">Select name as full_name from FlowFile where age &gt; 40</p></li>
			</ol>
			<p>The point I am attempting to drive home here is that you can execute SQL and modify the flowfile to something other than the contents of the row – for example, running and aggregation and a group by.</p>
			<h3>Extracting data from a flowfile</h3>
			<p>The next <a id="_idIndexMarker216"/>processor will extract a value from the flowfile. That processer is <strong class="source-inline">ExtractText</strong>. The processor can be used on any flowfile containing text and uses regex to pull any data from the flowfile and assign it to an attribute. </p>
			<p>To configure the processor, click the plus sign and name the property. You will extract the person name from the flowfile, so you can name the property name. The value will be regex and should be as follows:</p>
			<p class="source-code">\n([^,]*),</p>
			<p>Without a full tutorial on regex, the preceding regex statement looks for a newline and a comma – <strong class="source-inline">\n</strong> and the comma at the end – and grabs the text inside. The parentheses say to take the text and return any characters that are not <strong class="source-inline">^</strong> or a comma. This regex returns the person's name. The flowfile contains a header of field names in CSV, a new line, followed by values in CSV. The <strong class="source-inline">name</strong> field is the first field on the second line – after the newline and before <a id="_idIndexMarker217"/>the first comma that specifies the end of the <strong class="source-inline">name</strong> field. This is why the regex looks for the text between the newline and the comma.</p>
			<h3>Modifying flowfile attributes</h3>
			<p>Now that <a id="_idIndexMarker218"/>you have pulled out the person name as an attribute, you can use the <strong class="source-inline">UpdateAttribute</strong> processor to change the value of existing attributes. By using this processor, you will modify the default filename attribute that NiFi has provided the flowfile all the way at the beginning in the <strong class="source-inline">GetFile</strong> processor. Every flowfile will have the filename <strong class="source-inline">data.CSV</strong>. If you try to write the flowfiles out to CSV, they will all have the same name and will either overwrite or fail. </p>
			<p>Click the plus sign in the configuration for the <strong class="source-inline">UpdateAttribute</strong> processor and name the new property filename. The value will use the NiFi Expression Language. In the Expression Language, you can grab the value of an attribute using the format <strong class="source-inline">${attribute name}</strong>. So, to use the name attribute, set the value to <strong class="source-inline">${name}</strong>.</p>
			<h3>Saving a flowfile to disk</h3>
			<p>Using the <strong class="source-inline">PutFile</strong> processor, you can write the contents of a flowfile to disk. To configure the <a id="_idIndexMarker219"/>processor, you need to specify a directory in which to write the files. I will again use my home directory.  </p>
			<p>Next, you can specify a conflict resolution strategy. By default, it will be set to fail, but it allows you to overwrite an existing file. If you were running this data pipeline, aggregating data every hour and writing the results to files, maybe you would set the property to overwrite so that the file always holds the most current data. By default, the flowfile will write to a file on disk with the property filename as the filename.</p>
			<h3>Creating relationships between the processors</h3>
			<p>The <a id="_idIndexMarker220"/>last step is to make connections for specified relationships between the processors: </p>
			<ol>
				<li value="1">Grab the <strong class="source-inline">GetFile</strong> processor, drag the arrow to the <strong class="source-inline">SplitRecord</strong> processor, and check the relationship success in the popup.</li>
				<li>From the <strong class="source-inline">SplitRecord</strong> processor, make a connection to the <strong class="source-inline">QueryRecord</strong> processor and select the relationship splits. This means that any record that was split will be sent to the next processor. </li>
				<li>From <strong class="source-inline">QueryRecord</strong>, connect to the <strong class="source-inline">ExtractText</strong> processor. Notice the relationship you created is named <strong class="source-inline">over.40</strong>. If you added more SQL queries, you would get additional relationships. For this example, use the <strong class="source-inline">over.40</strong> relationship. </li>
				<li>Connect <strong class="source-inline">ExtractText</strong> to the <strong class="source-inline">UpdateAttribute</strong> processor for the relationship matched. </li>
				<li>Lastly, connect <strong class="source-inline">UpdateAttribute</strong> to the <strong class="source-inline">PutFile</strong> processor for the relationship success.</li>
			</ol>
			<p>The data pipeline is now complete. You can click on each processor and select <strong class="bold">Run</strong> to start it – or click the run icon in the operate window to start them all at once.</p>
			<p>When the pipeline is completed, you will have a directory with all the rows where the person was over 40. Of the 1,000 records, I have 635 CSVs named for each person. You will have different results based on what <strong class="source-inline">Faker</strong> used as the age value.</p>
			<p>This section showed you how to read in a CSV file. You also learned how you can split the file into rows and then run queries against them, as well as how to modify attributes of a flowfile and use it in another processor. In the next section, you will build another data <a id="_idIndexMarker221"/>pipeline using JSON. </p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor047"/>Working with JSON in NiFi</h2>
			<p>While <a id="_idIndexMarker222"/>having a different structure, working with JSON in NiFi is very similar to working with CSV. There are, however, a few processors <a id="_idIndexMarker223"/>for dealing exclusively with JSON. In this section, you will build a flow similar to the CSV example – read a file, split it into rows, and write each row to a file – but you will perform some more modifications of the data within the pipeline so that the rows you write to disk are different than what was in the original file. The following diagram shows the completed data pipeline:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B15739_03_13.jpg" alt="Figure 3.13 – The completed JSON data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – The completed JSON data pipeline</p>
			<p>To build the data pipeline, take the following steps:</p>
			<ol>
				<li value="1">Place the <strong class="source-inline">GetFile</strong> processor on to the canvas. To configure the processor, specify the <strong class="bold">Input Directory</strong> values as <strong class="source-inline">home/paulcrickard</strong> – and the <strong class="bold">File Filter</strong> value as <strong class="source-inline">data.JSON</strong>.</li>
				<li>In the <a id="_idIndexMarker224"/>CSV example, you used the <strong class="source-inline">SplitRecord</strong> processor. Here, for JSON, you can use the <strong class="source-inline">SplitJson</strong> processor. You will need to configure the <strong class="bold">JsonPath Expression</strong> property. This <a id="_idIndexMarker225"/>property is looking for an array that contains JSON elements. The JSON file is in the following format:<p class="source-code">{"records":[ { } ] }</p><p>Because each record is in an array, you can pass the following value to the <strong class="bold">JsonPath Expression</strong> property:</p><p class="source-code">$.records</p><p>This will split records inside of the array, which is the result you want.</p></li>
				<li>The records will now become individual flowfiles. You will pass the files to the <strong class="source-inline">EvaluateJsonPath</strong> processor. This processor allows you to extract values from the flowfile. You can either pass the results to the flowfile content or to an attribute. Set the value of the <strong class="bold">Destination</strong> property to <strong class="source-inline">flowfile-attribute</strong>. You can then select attributes to create using the plus sign. You will name the attribute, then specify the value. The value is the JSON path, and you use the format <strong class="source-inline">$.key</strong>. The configured processor is shown in the following screenshot:<div id="_idContainer060" class="IMG---Figure"><img src="image/B15739_03_14.jpg" alt="Figure 3.14 – Configuration for extracting values from the flowfile&#13;&#10;"/></div><p class="figure-caption">Figure 3.14 – Configuration for extracting values from the flowfile</p><p>These <a id="_idIndexMarker226"/>attributes will not be passed down the data pipeline with the flowfile.</p></li>
				<li>Now, you can use the <strong class="source-inline">QueryRecord</strong> processor, just like you did with the CSV example. The difference with JSON is that you need to create a new record reader <a id="_idIndexMarker227"/>and recordset writer. Select the option to create a new service. Select <strong class="bold">JsonTreeReader</strong> and <strong class="bold">JsonRecordsetWriter</strong>. Click the arrow to go to the <strong class="bold">Controller services</strong> tab and click the lightning bolt to activate the services. The default configurations will work in this example. In the processor, add a new property using the plus sign. Name it <strong class="source-inline">over.40</strong> and set the value to the following:<p class="source-code">Select * from FlowFile where age &gt; 40</p></li>
				<li>The next processor is the <strong class="source-inline">AttributesToJSON</strong> processor. This processor allows you to replace the flowfile content with the attributes you extracted in the <strong class="source-inline">EvaluateJsonPath</strong> processor shown in <em class="italic">step 3</em>. Set the <strong class="bold">Destination</strong> property to <strong class="source-inline">flowfile-content</strong>. This processor also allows you to specify a comma-separated list of attributes in the <strong class="bold">Attributes List</strong> property. This can come in handy if you only want certain attributes. In this example, you leave it blank and several attributes you do not extract will be added to the flowfile content. All of the metadata attributes that NiFi writes will now be a part of the flowfile. The flowfile will now look as in the following snippet:<p class="source-code">### Run it at night ###</p></li>
				<li>Using the <strong class="source-inline">EvalueJsonPath</strong> processor again, you will create an attribute named <strong class="source-inline">uuid</strong>. Now that the metadata from NiFi is in the flowfile, you have the unique ID of the flowfile. Make sure to set <strong class="bold">Destination</strong> to <strong class="source-inline">flowfile-attribute</strong>. You will extract it now so <a id="_idIndexMarker228"/>that you can pass it to the next processor – <strong class="source-inline">UpdateAttribute</strong>.</li>
				<li>In <a id="_idIndexMarker229"/>the CSV example, you updated the filename using the <strong class="source-inline">UpdateAttribute</strong> processor. You will do the same here. Click on the plus sign and add an attribute named <strong class="source-inline">filename</strong>. Set the value to <strong class="source-inline">${uuid}</strong>.</li>
				<li>One way <a id="_idIndexMarker230"/>to modify JSON using NiFi is through <strong class="bold">Jolt transformations</strong>. The <strong class="bold">JSON Language for Transform</strong> library allows you to modify JSON. A full <a id="_idIndexMarker231"/>tutorial on Jolt is beyond the scope of this book, but the processor allows you to select from several Jolt transformation DSLs. In this example, you will use a simple remove, which will delete a field. NiFi abbreviates the Jolt JSON because you have already specified what you are doing in the configuration. In the <strong class="bold">Jolt Specification</strong> property, enter the JSON, as shown in the following snippet:<p class="source-code">{</p><p class="source-code">    "zip": ""</p><p class="source-code">}</p><p>The preceding snippet will remove the <strong class="source-inline">zip</strong> field from the flowfile.</p></li>
				<li>Lastly, use the <strong class="source-inline">PutFile</strong> processor to write each row to disk. Configure the <strong class="bold">Directory</strong> and <strong class="bold">Conflict Resolution Strategy</strong> properties. By setting the <strong class="bold">Conflict Resolution Strategy</strong> property to <strong class="bold">ignore</strong>, the processor will not warn you if it has already processed a file with the same name.</li>
			</ol>
			<p>Create the connections and relationships between the processors:</p>
			<ul>
				<li>Connect <strong class="source-inline">GetFile</strong> to <strong class="source-inline">SplitJson</strong> for relationship success.</li>
				<li>Connect <strong class="source-inline">SplitJson</strong> to <strong class="source-inline">EvaluateJsonPath</strong> for relationship splits.</li>
				<li>Connect <strong class="source-inline">EvaluateJsonPath</strong> to <strong class="source-inline">QueryRecord</strong> for relationship matched.</li>
				<li>Connect <strong class="source-inline">QueryRecord</strong> to <strong class="source-inline">AttributesToJSON</strong> for relationship <strong class="source-inline">over.40</strong>.</li>
				<li>Connect <strong class="source-inline">AttributesToJSON</strong> to <strong class="source-inline">UpdateAttribute</strong> for relationship success.</li>
				<li>Connect <strong class="source-inline">UpdateAttributes</strong> to <strong class="source-inline">JoltTransformJSON</strong> for relationship success.</li>
				<li>Connect <strong class="source-inline">JoltTransformJSON</strong> to <strong class="source-inline">PutFile</strong> for relationship success.</li>
			</ul>
			<p>Run the <a id="_idIndexMarker232"/>data pipeline by starting each processor <a id="_idIndexMarker233"/>or clicking <strong class="bold">Run</strong> in the operate box. When complete, you will have a subset of 1,000 files – all people over 40 – on disk and named by their unique ID.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, you learned how to process CSV and JSON files using Python. Using this new skill, you have created a data pipeline in Apache Airflow by creating a Python function to process a CSV and transform it into JSON. You should now have a basic understanding of the Airflow GUI and how to run DAGs. You also learned how to build data pipelines in Apache NiFi using processors. The process for building more advanced data pipelines is the same, and you will learn the skills needed to accomplish this throughout the rest of this book.</p>
			<p>In the next chapter, you will learn how to use Python, Airflow, and NiFi to read and write data to databases. You will learn how to use PostgreSQL and Elasticsearch. Using both will expose you to standard relational databases that can be queried using SQL and NoSQL databases that allow you to store documents and use their own query languages.</p>
		</div>
	</body></html>