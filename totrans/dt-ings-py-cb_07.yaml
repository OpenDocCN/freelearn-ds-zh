- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingesting Analytical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analytical data is a bundle of data that serves various areas (such as finances,
    marketing, and sales) in a company, university, or any other institution, to facilitate
    decision-making, especially for strategic matters. When transposing analytical
    data to a data pipeline or a usual **Extract, Transform, and Load** (**ETL**)
    process, it corresponds to the final step, where data is already ingested, cleaned,
    aggregated, and has other transformations accordingly to business rules.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of scenarios where data engineers must retrieve data from a
    data warehouse or any other storage containing analytical data. The objective
    of this chapter is to learn how to read analytical data and its standard formats
    and cover practical use cases related to the reverse ETL concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting Parquet ﬁles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting Avro files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying schemas to analytical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering data and handling common issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting partitioned data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying reverse ETL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting analytical data for reverse ETL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like [*Chapter 6*](B19453_06.xhtml#_idTextAnchor195), in this chapter too,
    some recipes will need `SparkSession` initialized, and you can use the same session
    for all of them. You can use the following code to create your session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A `WARN` message as output is expected in some cases, especially if you are
    using WSL on Windows, so you don’t need to worry if you receive one.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also find the code from this chapter in its GitHub repository here:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting Parquet ﬁles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Parquet** is a columnar storage format that is open source and designed
    to support fast processing. It is available to any project in a **Hadoop ecosystem**
    and can be read in different programming languages.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to its compression and fastness, this is one of the most used formats when
    needing to analyze data in great volume. The objective of this recipe is to understand
    how to read a collection of Parquet files using **PySpark** in a real-world scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will need `SparkSession` to be initialized. You can use
    the code provided at the beginning of this chapter to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset for this recipe will be *Yellow Taxi Trip Records from New York*.
    You can download it by accessing the **NYC Government website** and selecting
    **2022** | **January** | **Yellow Taxi Trip Records** or using this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet)'
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to execute the code with a Jupyter notebook or a PySpark shell session.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting the Parquet file path**: To create a DataFrame based on the Parquet
    file, we have two options: pass the filename or the path of the Parquet file.
    In the following code block, you can see an example of passing only the name of
    the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the second option, we remove the Parquet filename, and Spark handles the
    rest. You can see how the code looks in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`.printSchema()` function to see whether the DataFrame was created successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Yellow taxi trip DataFrame schema](img/Figure_7.1_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Yellow taxi trip DataFrame schema
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualizing with pandas**: This is an optional step since it requires your
    local machine to have enough processing capacity and can freeze your kernel trying
    to process it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To take a better look at the DataFrame, let’s use `.toPandas()`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization](img/Figure_7.2_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can observe in the preceding code, reading Parquet files is straightforward.
    Like many Hadoop tools, PySpark natively supports reading and writing Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to JSON and CSV files, we used a function that derives from the `.read`
    method to inform PySpark that a Parquet file will be read, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We also saw two ways of reading, passing only the folder where the Parquet file
    is or passing the path with the Parquet filename. The best practice is to use
    the first case since there is usually more than one Parquet file, and reading
    just one may cause several errors. This is because each Parquet file inside the
    respective Parquet folder corresponds to a piece of the data.
  prefs: []
  type: TYPE_NORMAL
- en: After reading and transforming the dataset into a DataFrame, we printed its
    schema using the `.printSchema()` method. As the name suggests, it will print
    and show the schema of the DataFrame. Since we didn’t specify the schema we want
    for the DataFrame, Spark will infer it based on the data pattern inside the columns.
    Don’t worry about this now; we will cover this further in the *Applying schemas
    to analytical* *data* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `.printSchema()` method before doing any operations in the DataFrame
    is an excellent practice for understanding the best ways to handle the data inside.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as the last step of the recipe, we used the `.toPandas()` method to
    visualize our data better since the `.show()` Spark method is not intended to
    bring friendly visualizations like pandas. However, we must be cautious when using
    the `.toPandas()` method since it needs computational and memory power to translate
    the Spark DataFrame into a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s understand why `parquet` is an optimized file format for big data.
    Parquet files are column-oriented, stored in data blocks and in small chunks (a
    data fragment), allowing optimized reading and writing. In the following diagram,
    you can visually observe how `parquet` organizes data at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)](img/Figure_7.3_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)
  prefs: []
  type: TYPE_NORMAL
- en: 'Parquet files can frequently be found in a compressed form. This adds another
    layer of efficiency to improve data storage and transfer. A compressed Parquet
    file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `snappy` name informs us of the compression type and is crucial when creating
    a table in `gzip` format but more optimized for a massive volume of data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about Apache Parquet in the official documentation: [https://parquet.apache.org/docs/overview/motivation/](https://parquet.apache.org/docs/overview/motivation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to test other Parquet files and explore more data from *Open Targets
    Platform*, access this link: [http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/](http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting Avro files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like Parquet, **Apache Avro** is a widely used format to store analytical data.
    Apache Avro is a leading method of serialization to record data and relies on
    schemas. It also provides **Remote Procedure Calls** (**RPCs**), making transmitting
    data easier and resolving problems such as missing fields, extra fields, and naming
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will understand how to read an Avro file properly and later
    comprehend how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will require `SparkSession` with some different configurations
    from the previous *Ingesting Parquet ﬁles* recipe. If you are already running
    `SparkSession`, stop it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will create another session in the *How to do* *it…* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset used here can be found at this link: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files).'
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to execute the code in a Jupyter notebook or your PySpark shell session.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`avro` file, we need to import a `.jars` file in our `SparkSession` configuration,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When executed, this code will provide an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – SparkSession logs when downloading an Avro file package](img/Figure_7.4_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – SparkSession logs when downloading an Avro file package
  prefs: []
  type: TYPE_NORMAL
- en: It means the `avro` package was successfully downloaded and ready to use. We
    will later cover how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '`.jars` file configured, we will pass the file format to `.read` and add the
    file’s path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`.printSchema()`, let’s retrieve the schema of this DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will observe the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – DataFrame schema from the Avro file](img/Figure_7.5_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – DataFrame schema from the Avro file
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, this DataFrame contains the same data as the Parquet file
    covered in the last recipe, *Ingesting* *Parquet ﬁles*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can observe, we started this recipe slightly differently by creating
    `SparkSession` with a custom configuration. This is because, since version 2.4,
    Spark does not natively provide an internal API to read or write Avro files.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you try to read the file used here without downloading the `.jars` file,
    you will get the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Error message when Spark cannot find the Avro file package](img/Figure_7.6_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Error message when Spark cannot find the Avro file package
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading the error message, we can notice it is recommended to search for a
    third-party (or external) source called `avro` file. Check out the Spark third-parties
    documentation, which can be found here: [https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option](https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option).'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the documentation has some helpful information about how to set
    it for different languages, such as `org.apache.spark:spark-avro_2.12:3.3.1.jars`
    file incompatible with some PySpark versions, and so the recommendation is to
    use `org.apache.spark:spark-avro_2.12:2.4.4`.
  prefs: []
  type: TYPE_NORMAL
- en: '`.jars` file to be downloaded, but it is also incompatible with some versions
    of PySpark: `com.databricks:spark-avro_2.11:4.0.0`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the non-existence of an internal API to handle this file, we need to
    inform Spark of the format of the file, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We did the same thing when reading **MongoDB** collections, as seen in [*Chapter
    5*](B19453_05.xhtml#_idTextAnchor161), in the *Ingesting data from MongoDB using*
    *PySpark* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Once our file is converted into a DataFrame, all other functionalities and operations
    are identical without prejudice. As we saw, Spark will infer the schema and transform
    it into a columnar format.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know about both Apache Parquet and Apache Avro, you might wonder
    when to use each. Even though both are used to store analytical data, some key
    differences exist.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is how they store data. Parquet stores are in a columnar
    format, while Avro stores data in rows, which can be very efficient if you want
    to retrieve the entire row or dataset. However, columnar formats are much more
    optimized for aggregations or larger datasets, and `parquet` also supports more
    efficient queries using large-scale data and compression.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Columnar versus row format](img/Figure_7.7_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Columnar versus row format
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, Avro files are commonly used for data streaming. A good
    example is when using **Kafka** with **Schema Registry**, it will allow Kafka
    to verify the file’s expected schema in real time. You can see some example code
    in the Databricks documentation here: [https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml](https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Read more about how Apache Avro works and its functionalities on the official
    documentation page here: [https://avro.apache.org/docs/](https://avro.apache.org/docs/).'
  prefs: []
  type: TYPE_NORMAL
- en: Applying schemas to analytical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how to apply schemas to structured and unstructured
    data, but the application of a schema is not limited to raw files.
  prefs: []
  type: TYPE_NORMAL
- en: Even when working with already processed data, there will be cases when we need
    to cast the values of a column or change column names to be used by another department.
    In this recipe, we will learn how to apply a schema to Parquet files and how it
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need `SparkSession` for this recipe. Ensure you have a session that
    is up and running. We will use the same dataset as in the *Ingesting Parquet*
    *ﬁles* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to execute the code using a Jupyter notebook or your PySpark shell
    session.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking at our columns**: As seen in the *Ingesting Parquet ﬁles* recipe,
    we can list the columns and their inferred data types. You can see the list as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`VendorID`, to a more readable form. Refer to the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`parquet` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Printing the new DataFrame schema**: When printing the schema, we can see
    the name of some columns changed as we set them on the schema object in *step
    1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Executing the preceding code will provide the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – DataFrame with new schema applied](img/Figure_7.8_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – DataFrame with new schema applied
  prefs: []
  type: TYPE_NORMAL
- en: '`.toPandas()` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas](img/Figure_7.9_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, no numerical data has changed, and therefore the data integrity
    remains.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can observe in this exercise, defining and setting the schema for a DataFrame
    is not complex. However, it can be a bit laborious when we think about knowing
    the data types or declaring each column of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to start the schema definition is understanding the dataset we
    need to handle. This can be done by consulting a data catalog or even someone
    in more contact with the data. As a last option, you can create the DataFrame,
    let Spark infer the schema, and make adjustments when re-creating the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating the schema structure in Spark, there are a few items we need
    to pay attention to, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StructType` object, which represents the schema of a list of `StructField`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StructField` will define the name, data type, and whether the column allows
    null or empty fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data types**: The last thing to bear in mind is where we will define the
    column’s data type; as you can imagine, a few data types are available. You can
    always consult the documentation to see the supported data types here: [https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have defined the schema object, we can easily attribute it to the function
    that creates the DataFrame using the `.schema()` method, as we saw in *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: With the DataFrame created, all the following commands remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s do an experiment where instead of using `TimestampType()`, we will use
    `DateType()`. See the following portion of the changed code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we repeat the steps using the preceding code change, an error message will
    appear when we try to visualize the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Error reading the DataFrame when attributing an incompatible
    data type](img/Figure_7.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Error reading the DataFrame when attributing an incompatible data
    type
  prefs: []
  type: TYPE_NORMAL
- en: The reason behind this is the incompatibility of these two data types when formatting
    the data inside the column. `DateType()` uses the `yyyy-MM-dd` format, while `TimestampType()`
    uses `yyy-MM-dd HH:mm:ss.SSSS`.
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely at both columns, we see hour, minute, and second information.
    If we try to force it into another format, it could corrupt the data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learn more about the Spark data types here: [https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering data and handling common issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filtering data is a process of excluding or selecting only the necessary information
    to be used or stored. Even analytical data must be re-filtered to meet a specific
    need. An excellent example is **data marts** (we will cover them later in this
    recipe).
  prefs: []
  type: TYPE_NORMAL
- en: This recipe aims to understand how to create and apply filters to our data using
    a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires `SparkSession`, so ensure yours is up and running. You
    can use the code provided at the beginning of the chapter or create your own.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used here will be the same as in the *Ingesting Parquet* *ﬁles*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this exercise more practical, let’s imagine we want to analyze two
    scenarios: how many trips each vendor made and what hour of the day there are
    more pickups. We will create some aggregations and filter our dataset to carry
    out those analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading the Parquet file**: Let’s start by reading our Parquet file, as the
    following code shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`vendorId` instances and how many trips each vendor carried out in January
    (the timeframe of our dataset). We can use a `.groupBy()` function with `.count()`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what the vendor trip count looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – vendorId trips count](img/Figure_7.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – vendorId trips count
  prefs: []
  type: TYPE_NORMAL
- en: '`tpep_pickup_datetime` column, as shown in the following code. Then, we make
    a count and order it in an ascending flow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Count of trips per hour](img/Figure_7.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Count of trips per hour
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, at `14` hours and `19` hours, there is an increase in the
    number of pickups. We can think of some possible reasons for this, such as lunchtime
    and rush hour.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we are already very familiar with the reading operation to create a DataFrame,
    let’s go straight to *step 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can observe, the chain of functions here closely resembles SQL operations.
    This is because, behind the scenes, we are using the native SQL methods a DataFrame
    supports.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Like the SQL operations, the order of the methods in the chain will influence
    the result, and even whether it will result in a success or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 3*, we added a little bit more complexity by extracting the hour value
    from the `tpep_pickup_datetime` column. That was only possible because this column
    is of the timestamp data type. Also, we ordered by the count column this time,
    which was created once we invoked the `.count()` function, similar to the SQL.
    You can see this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'There are plenty of other functions, such as `.filter()` and `.select()`. You
    can find more PySpark functions here: [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analysis in this recipe was carried out using SQL functions natively supported
    by PySpark. However, these functions are not a good fit for more complex queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In those cases, the best practice is to use the **SQL API** of Spark. Let’s
    see how to do it in the code that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.createOrReplaceTempView()` method and pass a name to our temporary view,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`SparkSession` variable (`spark`), we will invoke `.sql()` and pass a multi-lined
    string containing the desired SQL code. To make it easier to visualize the results,
    let’s also attribute it to a variable called `vendor_groupby`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Observe we use the temporary view name to indicate where the query will be
    made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Executing this code will not generate an output.
  prefs: []
  type: TYPE_NORMAL
- en: '`.show()` method will work to bring the results, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – vendorId counts of trips using SQL code](img/Figure_7.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – vendorId counts of trips using SQL code
  prefs: []
  type: TYPE_NORMAL
- en: 'The downside of using the SQL API is that the error logs might sometimes be
    unclear. See the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Spark error when SQL does not have the right syntax](img/Figure_7.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Spark error when SQL does not have the right syntax
  prefs: []
  type: TYPE_NORMAL
- en: This screenshot shows the result of a query where the syntax was incorrect when
    grouping by the `tpep_pickup_datetime` column. In scenarios like this, the best
    approach is to debug using baby steps, executing the query operations and conditionals
    one by one. If your DataFrame comes from a table in a database, try to reproduce
    the query directly on the database and see whether there is a more intuitive error
    message.
  prefs: []
  type: TYPE_NORMAL
- en: Data marts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this recipe, one common use case for ingesting
    and re-filtering analytical data is to use it in a data mart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data marts are a smaller version of a data warehouse, with data concentrated
    on one subject, such as from a financial or sales department. The following diagram
    shows how they tend to be organized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Data marts diagram](img/Figure_7.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Data marts diagram
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the data mart concept has many benefits, such as reaching specific
    information or guaranteeing temporary access to a strict piece of data for a project
    without managing the security access of several users to the data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Find out more about data marts and data warehouse concepts on the Panoply.io
    blog: [https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/](https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/).'
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting partitioned data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The practice of partitioning data is not recent. It was implemented in databases
    to distribute data across multiple disks or tables. Actually, data warehouses
    can partition data according to the purpose and use of the data inside. You can
    read more here: [https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm](https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, partitioning data is related to how our data will be split into
    small chunks and processed.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to ingest data that is already partitioned
    and how it can affect the performance of our code.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires an initialized `SparkSession`. You can create your own
    or use the code provided at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data required to complete the steps can be found here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data).'
  prefs: []
  type: TYPE_NORMAL
- en: You can use a Jupyter notebook or a PySpark shell session to execute the code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating the DataFrame for the February data**: Let’s use the usual way for
    creating a DataFrame from a Parquet file, but this time passing only the month
    we want to read:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see no output from this execution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using pandas to show the results**: Once the DataFrame is created, we can
    better visualize the results using pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should observe this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned
    data](img/Figure_7.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned data
  prefs: []
  type: TYPE_NORMAL
- en: Observe that in the `tpep_pickup_datetime` column, there is only data from February,
    and now we don’t need to be very preoccupied with the processing capacity of our
    local machine.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a very simple recipe, but there are some important concepts that we
    need to understand a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can observe, all the magic happens during the creation of the DataFrame,
    where we pass not only the path where our Parquet files are stored but also the
    name of another subfolder containing the month reference. Let’s take a look at
    how this folder is organized in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Folder showing data partitioned by month](img/Figure_7.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Folder showing data partitioned by month
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `_SUCCESS` file indicates that the partitioning write process was successfully
    made.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the `chapter7_partitioned_files` folder, there are other subfolders with
    a number of references. Each of these subfolders represents a partition, in this
    case, categorized by month.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look inside a subfolder, we can observe one or more Parquet files. Refer
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Parquet file for February](img/Figure_7.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Parquet file for February
  prefs: []
  type: TYPE_NORMAL
- en: Partitions are an optimized form of reading or writing a specific amount of
    data from a dataset. That’s why using pandas to visualize the DataFrame was faster
    this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Partitioning also makes the execution of transformations faster since data
    will be processed using parallelism across the Spark internal worker nodes. You
    can visualize it better in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Partitioning parallelism diagram](img/Figure_7.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Partitioning parallelism diagram
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw, working with partitions to save data on a large scale brings several
    benefits. However, knowing how to partition your data is the key to reading and
    writing data in a performative way. Let’s list the three most important best practices
    when writing partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`month`, but it is possible to partition over any column and even to use a
    column with year, month, or day to bring more granularity. Normally, partitioning
    reflects what the best way to retrieve the data will be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Partition folders by month](img/Figure_7.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Partition folders by month
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` is configured or where it will write the final files, Spark
    can create small `parquet`/`avro` files. From a large data scale perspective,
    reading these small files can prejudice performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good practice is to use `coalesce()` while invoking the `write()` function
    to aggregate the files into a small amount. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find a good article about it here: [https://www.educba.com/pyspark-coalesce/](https://www.educba.com/pyspark-coalesce/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid over-partitioning**: This follows the same logic as the previous one.
    Over-partitioning will create small files since we split them using a granularity
    rule, and then Spark''s parallelism will be slowed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find more good practices here: [https://climbtheladder.com/10-spark-partitioning-best-practices/](https://climbtheladder.com/10-spark-partitioning-best-practices/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Related to the topic of partitioning, we also have the *database sharding*
    concept. It is a very interesting topic, and the MongoDB official documentation
    has a very nice post about it here: [https://www.mongodb.com/features/database-sharding-explained](https://www.mongodb.com/features/database-sharding-explained).'
  prefs: []
  type: TYPE_NORMAL
- en: Applying reverse ETL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, **reverse ETL** takes data from a data warehouse and inserts
    it into a business application such as **HubSpot** or **Salesforce**. The reason
    behind this is to make data more operational and use business tools to bring more
    insights to data that is already in a format ready for analysis or analytical
    format.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will teach us how to architect a reverse ETL pipeline and about
    the commonly used tools.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no technical requirements for this recipe. However, it is encouraged
    to use a whiteboard or a notepad to take notes.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will work with a scenario where we are ingesting data from an **e-learning
    platform**. Imagine we received a request from the marketing department to better
    understand user actions on the platform using the Salesforce system.
  prefs: []
  type: TYPE_NORMAL
- en: The objective here will be to create a diagram showing the data flow process
    from a source of data to the Salesforce platform.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make this exercise more straightforward, we will assume we already have
    data stored in the database for the e-learning platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingesting user action data from the website**: Let’s imagine we have a frontend
    API that sends useful information about our user’s actions and behavior on the
    e-learning platform to our backend databases. Refer to the following diagram to
    see what it looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 7.21 – Data flow from the frontend to an API in the backend](img/Figure_7.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Data flow from the frontend to an API in the backend
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing it using ETL**: With the available data, we can pick the necessary
    information that the marketing department needs and put it into the ETL process.
    We will ingest it from the backend database, apply any cleansing or transformations
    needed, and then load it into our data warehouse.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 7.22 – Diagram showing backend storage to the data warehouse](img/Figure_7.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Diagram showing backend storage to the data warehouse
  prefs: []
  type: TYPE_NORMAL
- en: '**Storing data in a data warehouse**: After the data is ready and transformed
    into an analytical format, it will be stored in the data warehouse. We don’t need
    to worry here about how data is modeled. Let’s assume a new analytical table will
    be created just for this processing purpose.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ETL to Salesforce**: Once data is populated in the data warehouse, we need
    to insert it into the Salesforce system. Let’s do this using PySpark, as you can
    see in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Data warehouse data flow to Salesforce](img/Figure_7.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Data warehouse data flow to Salesforce
  prefs: []
  type: TYPE_NORMAL
- en: With data inside Salesforce, we can advise the marketing team and automate this
    process to be triggered on a necessary schedule.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it seems complicated, the reverse ETL process is similar to an ingest
    job. In some cases, adding a few more transformations to fit the final application
    model might be necessary, but isn’t complex. Now, let’s take a closer look at
    what we did in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Reverse ETL diagram overview](img/Figure_7.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Reverse ETL diagram overview
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to understand whether we already have the requested raw data
    stored in our internal database to meet the marketing department’s needs. If we
    don’t, the data team is responsible for reaching out to the responsible developers
    to verify how to accomplish that.
  prefs: []
  type: TYPE_NORMAL
- en: Once this is checked, we proceed with the usual ETL pipeline. Normally, there
    will be SQL transformations to filter or group information based on the needs
    of the analysis. Then, we store it in a *source of truth*, such as a main data
    warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: It is in *step 4* that the reverse ETL occurs. The origin of this name is because,
    normally, the ETL process involves retrieving data from an application such as
    Salesforce and storing it in a data warehouse. However, in recent years, these
    tools have become another form of better understanding how users are behaving
    or interacting with our applications.
  prefs: []
  type: TYPE_NORMAL
- en: With user-centric feedback solutions with analytical data, we can get better
    insights into and access to specific results. Another example besides Salesforce
    can be a **machine learning** solution to predict whether some change in the e-learning
    platform would result in improved user retention.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To carry out reverse ETL, we can create our own solution or use a commercial
    one. Plenty of solutions on the market retrieve data from data warehouses and
    connect dynamically with business solutions. Some can also generate reports to
    provide feedback to the data warehouse again, improving the quality of information
    sent and even creating other analyses. The cons of these tools are that most are
    paid solutions, and free tiers tend to include one or few connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most used reverse ETL tools is **Hightouch**; you can find out more
    here: [https://hightouch.com/](https://hightouch.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about reverse ETL at *Astasia Myers’* Medium blog: [https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb](https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb).'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting analytical data for reverse ETL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what reverse ETL is, the next step is to understand which types
    of analytical data are a good use case to load into a Salesforce application,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe continues from the previous one, *Applying reverse ETL*, intending
    to illustrate a real scenario of deciding what data will be transferred into a
    Salesforce application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe has no technical requirements, but you can use a whiteboard or a
    notepad for annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Still using the example of a scenario where the marketing department requested
    data to be loaded into their Salesforce account, we will now go a little deeper
    to see what information is relevant for their analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We received a request from the marketing team to understand the user journey
    in the e-learning platform. They want to understand which courses are watched
    most and whether some need improvement. Currently, they don’t know what information
    we have in our backend databases.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s work on this scenario in small steps. The objective here will be to understand
    what data we need to accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consulting the data catalog**: To simplify our work, let’s assume our data
    engineers worked on creating a data catalog with the user information collected
    and stored in the backend databases. In the following diagram, we can better see
    how the information is stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Tables of interest for reverse ETL highlighted](img/Figure_7.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Tables of interest for reverse ETL highlighted
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, there are three tables with potentially relevant information
    to be retrieved: `user_data`, `course_information`, and `videos`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selecting the raw data**: We can see highlighted in the following diagram
    the columns that can provide the information needed for the analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Tables and respective columns highlighted as relevant for reverse
    ETL](img/Figure_7.26_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Tables and respective columns highlighted as relevant for reverse
    ETL
  prefs: []
  type: TYPE_NORMAL
- en: '**Transforming and filtering data**: Since we need a single table to load data
    into Salesforce, we can make a SQL filter and join the information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned at the beginning of the recipe, the marketing team wants to understand
    the user journey in the e-learning application. First, let’s understand what a
    user journey is.
  prefs: []
  type: TYPE_NORMAL
- en: A user journey is all the actions and interactions a user carries out on a system
    or application, from when they opt to use or buy a service until they log out
    or leave it. Information such as what type of content they have watched and for
    how long is very important in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the fields we collected and why they are important. The first six
    columns will give us an idea of the user and where they live. We can use these
    pieces of information later to see whether there are any patterns for the predilection
    of content.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – user_data relevant columns for reverse ETL](img/Figure_7.27_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – user_data relevant columns for reverse ETL
  prefs: []
  type: TYPE_NORMAL
- en: Then, the last columns provide information about the content this user is watching
    and whether there is any relationship between the types of content. For example,
    if they bought a Python course and a SQL course, we can use a tag (for example,
    `programming course`) from the content metadata to make a filer of correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – course_information and videos with columns highlighted](img/Figure_7.28_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – course_information and videos with columns highlighted
  prefs: []
  type: TYPE_NORMAL
- en: 'Feeding back all this information into Salesforce can help to answer the following
    questions about the user’s journey:'
  prefs: []
  type: TYPE_NORMAL
- en: Is there a tendency to finish one course before starting another?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do users tend to watch multiple courses at the same time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the educational team need to reformulate a course because of a high turnover?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Find more use cases here: [https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns).'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of websites you can refer to, to enhance your knowledge further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://segment.com/blog/reverse-etl/](https://segment.com/blog/reverse-etl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://hightouch.com/blog/reverse-etl](https://hightouch.com/blog/reverse-etl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.oracle.com/br/autonomous-database/what-is-data-mart/](https://www.oracle.com/br/autonomous-database/what-is-data-mart/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml](https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Structuring the Ingestion Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the book’s second part, you will be introduced to the monitoring practices
    and see how to create your very first data ingestion pipeline using the recommended
    tools on the market, all the while applying the best data engineering practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19453_08.xhtml#_idTextAnchor280), *Designing Monitored Data
    Workﬂows*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19453_09.xhtml#_idTextAnchor319), *Putting Everything Together
    with Airﬂow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19453_10.xhtml#_idTextAnchor364), *Logging and Monitoring Your
    Data Ingest in Airﬂow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19453_11.xhtml#_idTextAnchor402)*,* *Automating Your Data Ingestion
    Pipelines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19453_12.xhtml#_idTextAnchor433), *Using Data Observability
    for Debugging, Error Handling, and Preventing Downtime*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
