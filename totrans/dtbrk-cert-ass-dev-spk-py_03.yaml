- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Architecture and Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark approaches data processing differently than traditional tools and technologies.
    To understand Spark’s unique approach, we will have to understand its basic architecture.
    A deep dive into Spark’s architecture and its components will give you an idea
    of how Spark achieves its ground-breaking processing speeds for big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following broader topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture and execution hierarchy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different Spark components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The roles of the Spark driver and Spark executor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different deployment modes in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations and actions as Spark operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have valuable insights into Spark’s inner
    workings and know how to apply this knowledge effectively for your certification
    test.
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed that Apache Spark is an open source,
    distributed computing framework designed for big data processing and analytics.
    Its architecture is built to handle various workloads efficiently, offering speed,
    scalability, and fault tolerance. Understanding the architecture of Spark is crucial
    for comprehending its capabilities in processing large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The components of Spark architecture work in collaboration to process data
    efficiently. The following major components are involved:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkContext
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark executor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we talk about any of these components, it’s important to understand their
    execution hierarchy to know how each component interacts when a Spark program
    starts.
  prefs: []
  type: TYPE_NORMAL
- en: Execution hierarchy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s look at the execution flow of a Spark application with the help of the
    architecture depicted in *Figure 3**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Spark architecture](img/B19176_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Spark architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps outline the flow from submitting a Spark job to freeing up resources
    when the job is completed:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark executions start with a user submitting a `spark-submit` request to the
    Spark engine. This will create a Spark application. Once an action is performed,
    it will result in a **job** being created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This request will initiate communication with the cluster manager. In turn,
    the cluster manager initializes the Spark driver to execute the `main()` method
    of the Spark application. To execute this method, `SparkSession` is created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The driver starts communicating with the cluster manager and asks for resources
    to start planning for execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager then starts the executors, which can communicate with the
    driver directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The driver creates a logical plan, known as a **directed acyclic graph** (**DAG**),
    and physical plan for execution based on the total number of tasks required to
    be executed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The driver also divides data to be run on each executor, along with tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once each task finishes running, the driver gets the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the program finishes running, the `main()` method exits and Spark frees
    all executors and driver resources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you understand the execution hierarchy, let’s discuss each of Spark’s
    components in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Spark components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s dive into the inner workings of each Spark component to understand how
    each of them plays a crucial role in empowering efficient distributed data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Spark driver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark driver is the core of the intelligent and efficient computations in
    Spark. Spark follows an architecture that is commonly known as the **master-worker
    architecture** in network topology. Consider the Spark driver as a master and
    Spark executors as slaves. The driver has control and knowledge of all the executors
    at any given time. It is the responsibility of the driver to know how many executors
    are present and if any executor has failed so that it can fall back on its alternative.
    The Spark driver also maintains communication with executors all the time. The
    driver runs on the master node of a machine or cluster. When a Spark application
    starts running, the driver keeps up with all the required information that is
    needed to run the application successfully.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 3**.1*, the driver node contains `SparkSession`, which is
    the entry point of the Spark application. Previously, this was known as the `SparkContext`
    object, but in Spark 2.0, `SparkSession` handles all contexts to start execution.
    The application’s main method runs on the driver to coordinate the whole application.
    It runs on its own **Java Virtual Machine** (**JVM**). Spark driver can run as
    an independent process or it can run on one of the worker nodes, depending on
    the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark driver is responsible for dividing the application into smaller entities
    for execution. These entities are known as **tasks**. You will learn more about
    tasks in the upcoming sections of this chapter. The Spark driver also decides
    what data the executor will work on and what tasks are run on which executor.
    These tasks are scheduled to run on the executor nodes with the help of the cluster
    manager. This information that is driven by the driver enables fault tolerance.
    Since the driver has all the information about the number of available workers
    and the tasks that are running on each of them alongside data in case a worker
    fails, that task can be reassigned to a different cluster. Even if a task is taking
    too long to run, it can be assigned to another executor if that gets free. In
    that case, whichever executor returns the task earlier would prevail. The Spark
    driver also maintains metadata about the **Resilient Distributed Dataset** (**RDD**)
    and its partitions.
  prefs: []
  type: TYPE_NORMAL
- en: It is the responsibility of the Spark driver to design the complete execution
    map. It determines which tasks run on which executors, as well as how the data
    is distributed across these executors. This is done by creating RDDs internally.
    Based on this distribution of data, the operations that are required are determined,
    such as transformations and actions that are defined in the program. A DAG is
    created based on these decisions. The Spark driver optimizes the logical plan
    (DAG) and finds the best possible execution strategy for the DAG, in addition
    to determining the most optimal location for the execution of a particular task.
    These executions are done in parallel. The executors simply follow these commands
    without doing any optimization on their end.
  prefs: []
  type: TYPE_NORMAL
- en: For performance considerations, it is optimal to have the Spark driver work
    close to the executor. This reduces the latency by a great deal. This means that
    there would be less delay in the response time of the processes. Another point
    to note here is that this is true for the data as well. The executor reading the
    data close to it would have better performance than otherwise. Ideally, the driver
    and worker nodes should be run in the same **local area network** (**LAN**) for
    the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark driver also creates a web UI for the execution details. This UI is
    very helpful in determining the performance of the application. In cases where
    troubleshooting is required and some bottlenecks need to be identified in the
    Spark process, this UI is very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: SparkSession
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`SparkSession` is the main point of entry and interaction with Spark. As discussed
    earlier, in the previous versions of Spark, `SparkContext` used to play this role,
    but in Spark 2.0, `SparkSession` can be created for this purpose. The Spark driver
    creates a `SparkSession` object to interact with the cluster manager and get resource
    allocation through it.'
  prefs: []
  type: TYPE_NORMAL
- en: In the lifetime of the application, `SparkSession` is also used to interact
    with all the underlying Spark APIs. We talked about different Spark APIs in [*Chapter
    2*](B19176_02.xhtml#_idTextAnchor030) namely, SparkSQL, Spark Streaming, MLlib,
    and GraphX. All of these APIs use `SparkSession` from its core to interact with
    the Spark application.
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` keeps track of Spark executors throughout the application’s
    execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark is a distributed framework, which requires it to have access to computing
    resources. This access is governed and controlled by a process known as the cluster
    manager. It is the responsibility of the cluster manager to allocate computing
    resources for the Spark application when the application execution starts. These
    resources become available at the request of the application master. In the Apache
    Spark ecosystem, the **application master** plays a crucial role in managing and
    coordinating the execution of Spark applications within a distributed cluster
    environment. It’s an essential component that’s responsible for negotiating resources,
    scheduling tasks, and monitoring the application’s execution.
  prefs: []
  type: TYPE_NORMAL
- en: Once the resources are available, the driver is made aware of those resources.
    It’s the responsibility of the driver to manage these resources based on tasks
    that need to be executed by the Spark application. Once the application has finished
    execution, these resources are released back to the cluster manager.
  prefs: []
  type: TYPE_NORMAL
- en: Applications have their dedicated executor processes that parallelize how tasks
    are run. The advantage is that each application is independent of the other and
    runs on its own schedule. Data also becomes independent for each of these applications,
    so data sharing can only take place by writing data to disk so that it can be
    shared across applications.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cluster modes define how Spark applications utilize cluster resources, manage
    task execution, and interact with cluster managers for resource allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is more than one user sharing resources on the cluster, be it Spark
    applications or other applications that need cluster resources, they have to be
    managed based on different modes. There are two types of modes available for cluster
    managers – standalone client mode and cluster mode. The following table highlights
    some of the differences between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client Mode** | **Cluster Mode** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| In client mode, the driver program runs on the machine where the Spark application
    is submitted. | In cluster mode, the driver program runs within the cluster, on
    one of the worker nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| The driver program is responsible for orchestrating the execution of the
    Spark application, including creating `SparkContext` and coordinating tasks. |
    The cluster manager is responsible for launching the driver program and allocating
    resources for execution. |'
  prefs: []
  type: TYPE_TB
- en: '| The client machine interacts directly with the cluster manager to request
    resources and launch executors on worker nodes. | Once the driver program is launched,
    it coordinates with the cluster manager to request resources and distribute tasks
    to worker nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| It may not be suitable for production deployments with large-scale applications.
    | It is commonly used for production deployments as it allows for better resource
    utilization and scalability. It also ensures fault tolerance. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: Client mode versus cluster mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will talk about different deployment modes and their corresponding
    managers in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Built-in standalone mode** (**Spark’s native manager**): A simple cluster
    manager bundled with Spark that’s suitable for small to medium-scale deployments
    without external dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache YARN** (**Hadoop’s resource manager**): Integrated with Spark, YARN
    enables Spark applications to share Hadoop’s cluster resources efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mesos** (**resource sharing platform**): Mesos offers efficient resource
    sharing across multiple applications, allowing Spark to run alongside other frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will talk more about deployment modes later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Spark executors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark executors are the processes that run on the worker node and execute tasks
    sent by the driver. The data is stored in memory primarily but can also be written
    to disk storage closest to them. Driver launches the executors based on the DAG
    that Spark generates for its execution. Once the tasks have finished executing,
    executors send the results back to the driver.
  prefs: []
  type: TYPE_NORMAL
- en: Since the driver is the main controller of the Spark application, if an executor
    fails or takes too long to execute a task, the driver can choose to send that
    task over to other available executors. This ensures reliability and fault tolerance
    in Spark. We will read more about this later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It is the responsibility of the executor to read data from external sources
    that are needed to run the tasks. It can also write its partitioned data to the
    disk as needed. All processing for a task is done by the executor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key functions of an executor are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task execution**: Executors run tasks assigned by the Spark application,
    processing data stored in RDDs or DataFrames'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource allocation**: Each Spark application has a set of executors allocated
    by the cluster manager for managing resources such as CPU cores and memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Apache Spark, the concepts of job, stage, and task form the fundamental
    building blocks of its distributed computing framework. Understanding these components
    is essential to grasp the core workings of Spark’s parallel processing and task
    execution. See *Figure 3**.2* to understand the relationship between these concepts
    while we discuss them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Interaction between jobs, stages, and tasks](img/B19176_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Interaction between jobs, stages, and tasks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '`collect`). We will learn more about actions later. When an action (such as
    `collect` or `count`) is invoked on a dataset, it triggers the execution of one
    or more jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A job consists of several stages, each containing tasks that execute a set of
    transformations on data partitions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Stage**: Each job is divided into stages that may depend on other stages.
    Stages act as transformation boundaries – they are created at the boundaries of
    wide transformations that require data shuffling across partitions. If a stage
    is dependent on outputs from a previous stage, then this stage would not begin
    execution until the previous dependent stages have finished execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each stage is divided into a set of tasks to be executed on the cluster nodes,
    processing data in parallel.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Task**: A task is the smallest unit of execution in Spark. It is the smallest
    object compiled and run by Spark to perform a group of operations. It is executed
    on a Spark executor. Tasks are essentially a series of operations such as filter,
    groupBy, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks run in parallel across executors. They can be run on multiple nodes and
    are independent of each other. This is done with the help of slots. Each task
    processes a portion of the data partition. Occasionally, a group of these tasks
    has to finish execution to begin the next task’s execution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we understand these concepts, let’s see why they are significant in
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel processing**: Executors, jobs, stages, and tasks collaborate to
    enable parallel execution of computations, optimizing performance by leveraging
    distributed computing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task granularity and efficiency**: Tasks divide computations into smaller
    units, facilitating efficient resource utilization and parallelism across cluster
    nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will move on to discuss a significant concept that enhances efficiency
    in computation.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Apache Spark, partitioning is a critical concept that’s used to divide data
    across multiple nodes in a cluster for parallel processing. Partitioning improves
    data locality, enhances performance, and enables efficient computation by distributing
    data in a structured manner. Spark supports both static and dynamic partitioning
    strategies to organize data across the cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static partitioning of resources**: Static partitioning is available on all
    cluster managers. With static partitioning, maximum resources are allocated to
    each application and these resources remain dedicated to these applications during
    their lifetime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic sharing of resources**: Dynamic partitioning is only available on
    Mesos. When dynamically sharing resources, the Spark application gets fixed and
    independent memory allocation, such as static partitioning. The major difference
    is that when the tasks are not being run by an application, these cores can be
    used by other applications as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s discuss why partitioning is significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance optimization**: Effective partitioning strategies, whether static
    or dynamic, significantly impact Spark’s performance by improving data locality
    and reducing data shuffle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability and flexibility**: Dynamic partitioning provides adaptability
    to varying data sizes or distribution patterns without manual intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control and predictability**: Static partitioning offers control and predictability
    over data distribution, which can be advantageous in specific use cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, partitioning strategies – whether static or dynamic – in Spark play
    a crucial role in optimizing data distribution across cluster nodes, improving
    performance, and ensuring efficient parallel processing of data.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark offers different cluster and deployment modes to run applications
    across distributed computing environments. We’ll take a look at them in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment modes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different deployment modes available in Spark. These deployment modes
    define how Spark applications are launched, executed, and managed in diverse computing
    infrastructures. Based on these different deployment modes, it gets decided where
    the Spark driver, executor, and cluster manager will run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different deployment modes that are available in Spark are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local**: In local mode, the Spark driver and executor run on a single JVM
    and the cluster manager runs on the same host as the driver and executor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standalone**: In standalone mode, the driver can run on any node of the cluster
    and the executor will launch its own independent JVM. The cluster manager can
    remain on any of the hosts in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YARN (client)**: In this mode, the Spark driver runs on the client and YARN’s
    resource manager allocates containers for executors on NodeManagers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YARN (cluster)**: In this mode, the Spark driver runs with the YARN application
    master while YARN’s resource manager allocates containers for executors on NodeManagers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes**: In this mode, the driver runs in Kubernetes pods. Executors
    have their own pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at some points of significance regarding the different deployment
    modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource utilization**: Different deployment modes optimize resource utilization
    by determining where the driver program runs and how resources are allocated between
    the client and the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility and control**: Client mode offers easy accessibility to driver
    logs and outputs, facilitating development and debugging, while cluster mode utilizes
    cluster resources more efficiently for production workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with container orchestration**: Kubernetes deployment mode enables
    seamless integration with containerized environments, leveraging Kubernetes’ orchestration
    capabilities for efficient resource management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some considerations to keep in mind while choosing deployment modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Development versus production**: Client mode is suitable for development
    and debugging, while cluster mode is ideal for production workloads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource management**: Evaluate the allocation of resources between client
    and cluster nodes based on the application’s requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containerization needs**: Consider Kubernetes deployment for containerized
    environments, leveraging Kubernetes features for efficient container management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, deployment modes in Apache Spark provide flexibility in how Spark
    applications are launched and executed, catering to different development, production,
    and containerized deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at RDDs, which serve as foundational data abstractions in
    Apache Spark, enabling distributed processing, fault tolerance, and flexibility
    in handling large-scale data operations. While RDDs continue to be a fundamental
    concept, Spark’s DataFrame and Dataset APIs offer advancements in structured data
    processing and performance optimization.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark’s RDD stands as a foundational abstraction that underpins the distributed
    computing capabilities within the Spark framework. RDDs serve as the core data
    structure in Spark, enabling fault-tolerant and parallel operations on large-scale
    distributed datasets and they are immutable. This means that they cannot be changed
    over time. For any operations, a new RDD has to be generated from the existing
    RDD. When a new RDD originates from the original RDD, the new RDD has a pointer
    to the RDD it is generated from. This is the way Spark documents the lineage for
    all the transformations taking place on an RDD. This lineage enables **lazy evaluation**
    in Spark, which generates DAGs for different operations.
  prefs: []
  type: TYPE_NORMAL
- en: This immutability and lineage gives Spark the ability to reproduce any DataFrame
    in case of failure and it makes fault-tolerant by design. Since RDD is the lowest
    level of abstraction in Spark, all other datasets built on top of RDDs share these
    properties. The high-level DataFrame API is built on top of the low-level RDD
    API as well, so DataFrames also share the same properties.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are also partitioned by Spark and each partition is distributed to multiple
    nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key characteristics of Spark RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Immutable nature**: RDDs are immutable, ensuring that once created, they
    cannot be altered, allowing for a lineage of transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience through lineage**: RDDs store lineage information, enabling reconstruction
    of lost partitions in case of failures. Spark is designed to be fault-tolerant.
    Therefore, if an executor on a worker node fails while calculating an RDD, that
    RDD can be recomputed by another executor using the lineage that Spark has created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioned data**: RDDs divide data into partitions, distributed across
    multiple nodes in a cluster for parallel processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel execution**: Spark executes operations on RDDs in parallel across
    distributed partitions, enhancing performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss some more characteristics in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RDDs support lazy evaluation, deferring execution of transformations until an
    action is invoked. The way Spark achieves its efficiency in processing and fault
    tolerance is through lazy evaluation. Code execution in Spark is delayed. Unless
    an action is called an operation, Spark does not start code execution. This helps
    Spark achieve optimization as well. For all the transformations and actions, Spark
    keeps track of the steps in the code that need to be executed by creating a DAG
    for these operations. Because Spark creates the query plan before execution, it
    can make smart decisions about the hierarchy of execution as well. To achieve
    this, one of the features Spark uses is called **predicate pushdown**.
  prefs: []
  type: TYPE_NORMAL
- en: Predicate pushdown means that Spark can prioritize the operations to make them
    the most efficient. One example can be a filter operation. A filter operation
    would generally reduce the amount of data that the subsequent operations have
    to work with if the filter operation can be applied before other transformations.
    This is exactly how Spark operates. It will execute filters as early in the process
    as possible, thus making the next operations more performant.
  prefs: []
  type: TYPE_NORMAL
- en: This also implies that Spark jobs would fail only at execution time. Since Spark
    uses lazy evaluation, until an action is called, the code is not executed and
    certain errors can be missed. To catch these errors, Spark code would need to
    have an action for execution and hence error handling.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformations create new RDDs by applying functions to existing RDDs (for
    example, `map`, `filter`, and `reduce`). Transformations are operations that do
    not result in any code execution. These statements result in Spark creating a
    DAG for execution. Once that DAG is created, Spark would need an action operation
    in the end to run the code. Due to this, when certain developers try to time the
    code from Spark, they see that certain operations’ runtime is very fast. The reason
    could be that the code is only comprised of transformations until that point.
    Since no action is present, the code doesn’t run. To accurately measure the runtime
    of each operation, actions have to be called to force Spark to execute those statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the operations that can be classified as transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`orderBy()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupBy()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When these commands are executed, they are evaluated lazily. This means all
    these operations on DataFrames result in a new DataFrame, but they are not executed
    until an action is followed by them. This would return a DataFrame or RDD when
    it is triggered by an action.
  prefs: []
  type: TYPE_NORMAL
- en: Actions and computation execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Actions (for example, `collect`, `count`, and `saveAsTextFile`) prompt the execution
    of transformations on RDDs. Execution is triggered by actions only, not by transformations.
    When an action is called, this is when Spark starts execution on the DAG it created
    during the analysis phase of code. With the DAG created, Spark creates multiple
    query plans based on its internal optimizations. Then, it executes the plan that
    is the most efficient and cost-effective. We will discuss query plans later in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the operations that can be classified as actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`show()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`take()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collect()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`foreach()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`first()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these operations would result in Spark triggering code execution and
    thus operations are run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the following code to understand these concepts better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, until line 2, nothing would be executed. On line 3, an
    action is triggered and thus it triggers the whole code execution. Therefore,
    if you give the wrong data path in line 1 or the wrong column names in line 2,
    Spark will not detect this until it runs line 3\. This is a different paradigm
    than most other programming paradigms. This is what we call lazy evaluation in
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Actions bring about computation and collect results to be sent to the driver
    program.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the basics of transformations and actions in Spark, let’s
    move on to understanding the two types of transformations it offers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark’s transformations are broadly categorized into narrow and wide
    transformations, each serving distinct purposes in the context of distributed
    data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Narrow transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Narrow transformations, also known as local transformations, operate on individual
    partitions of data without shuffling or redistributing data across partitions.
    These transformations enable Spark to process data within a single partition independently.
    In narrow transformations, Spark will work with a single input partition and a
    single output partition. This means that these types of transformations would
    result in an operation that can be performed on a single partition. The data doesn’t
    have to be taken from multiple partitions or written back to multiple partitions.
    This results in operations that don’t require shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of their characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Partition-level operation**: Narrow transformations process data at the partition
    level, performing computations within each partition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence and local processing**: They do not require data movement or
    communication across partitions, allowing parallel execution within partitions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`, `filter`, and `flatMap` are typical examples of narrow transformations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at their significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency and speed**: Narrow transformations are efficient as they involve
    local processing within partitions, reducing communication overhead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: They facilitate maximum parallelism by operating on partitions
    independently, optimizing performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Wide transformations, also termed global or shuffle-dependent transformations,
    involve operations that require data shuffling and redistribution across partitions.
    These transformations involve dependencies between partitions, necessitating data
    exchange. With wide transformations, Spark will use the data present on multiple
    partitions and it could also write back the results to multiple partitions. These
    transformations would force a shuffle operation, so they are also referred to
    as shuffle transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Wide transformations are complex operations. They would need to write the results
    out in between operations if needed and they also have to aggregate data across
    different machines in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of their characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data shuffling**: Wide transformations reorganize data across partitions
    by reshuffling or aggregating data from multiple partitions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency on multiple partitions**: They depend on data from various partitions,
    leading to the exchange and reorganization of data across the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupBy`, `join`, and `sortByKey` are typical examples of wide transformations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at their significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network and disk overhead**: Wide transformations introduce network and disk
    overhead due to data shuffling, impacting performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage boundary creation**: They define stage boundaries within a Spark job,
    resulting in distinct stages during job execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the differences between narrow and wide transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data movement**: Narrow transformations process data within partitions locally,
    minimizing data movement, while wide transformations involve data shuffling and
    movement across partitions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance impact**: Narrow transformations typically offer higher performance
    due to reduced data movement, whereas wide transformations involve additional
    overhead due to data shuffling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism scope**: Narrow transformations enable maximum parallelism within
    partitions, while wide transformations might limit parallelism due to dependency
    on multiple partitions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Apache Spark, understanding the distinction between narrow and wide transformations
    is crucial. Narrow transformations excel in local processing within partitions,
    optimizing performance, while wide transformations, although necessary for certain
    operations, introduce overhead due to data shuffling and global reorganization
    across partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the significance of Spark RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed data processing**: RDDs enable distributed processing of large-scale
    data across a cluster of machines, promoting parallelism and scalability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance and reliability**: Their immutability and lineage-based recovery
    ensure fault tolerance and reliability in distributed environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility in operations**: RDDs support a wide array of transformations
    and actions, allowing diverse data manipulations and processing operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution and alternatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While RDDs remain fundamental, Spark’s DataFrame and Dataset APIs offer optimized,
    higher-level abstractions suitable for structured data processing and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Spark RDDs serve as the bedrock of distributed data processing within the Apache
    Spark framework, providing immutability, fault tolerance, and the foundational
    structure for performing parallel operations on distributed datasets. Although
    RDDs are fundamental, Spark’s DataFrame and Dataset APIs offer advancements in
    performance and structured data processing, catering to various use cases and
    preferences within the Spark ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about Spark’s architecture and its inner workings.
    This exploration of Spark’s distributed computing landscape covered different
    Spark components, such as the Spark driver and `SparkSession`. We also talked
    about the different types of cluster managers available in Spark. Then, we touched
    on different types of partitioning regarding Spark and its deployment modes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed Spark executors, jobs, stages, and tasks and highlighted
    the differences between them before learning about RDDs and their transformation
    types, learning more about narrow and wide transformations.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts form the foundation for harnessing Spark’s immense capabilities
    in distributed data processing and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss Spark DataFrames and their corresponding
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Sample questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Question 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: What’s true about Spark’s execution hierarchy?
  prefs: []
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, a job may reach multiple stage boundaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, manifests are one layer above jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, a stage comprises multiple jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, executors are the smallest unit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, tasks are one layer above slots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: What do executors do?
  prefs: []
  type: TYPE_NORMAL
- en: Executors host the Spark driver on a worker-node basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors are responsible for carrying out work that they get assigned by the
    driver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the start of the Spark application, executors are launched on a per-task
    basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors are located in slots inside worker nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The executors’ storage is ephemeral and as such it defers the task of caching
    data directly to the worker node thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Part 3: Spark Operations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will cover Spark DataFrames and their operations, emphasizing
    their role in structured data processing and analytics. This will include DataFrame
    creation, manipulation, and various operations such as filtering, aggregations,
    joins, and groupings, demonstrated through illustrative examples. Then, we will
    discuss advanced operations and optimization techniques, including broadcast variables,
    accumulators, and custom partitioning. This part also talks about performance
    optimization strategies, highlighting the significance of adaptive query execution
    and offering practical tips for enhancing Spark job performance. Furthermore,
    we will explore SQL queries in Spark, focusing on its SQL-like querying capabilities
    and interoperability with the DataFrame API. Examples will illustrate complex
    data manipulations and analytics through SQL queries in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19176_04.xhtml#_idTextAnchor071), *Spark DataFrames and their
    Operations*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19176_05.xhtml#_idTextAnchor115), *Advanced Operations and Optimizations
    in Spark*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19176_06.xhtml#_idTextAnchor164)*,* *SQL Queries in Spark*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
