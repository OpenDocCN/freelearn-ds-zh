<html><head></head><body><div><div><div></div>
		</div>
		<div><h1 id="_idParaDest-152"><a id="_idTextAnchor158"/>5. Getting Comfortable with Different Kinds of Data Sources</h1>
		</div>
		<div><p class="callout-heading">Overview</p>
			<p class="callout">This chapter will provide you with the skills to read CSV, Excel, and JSON files into pandas DataFrames. You will learn how to read PDF documents and HTML tables into pandas DataFrames and perform basic web scraping operations using powerful yet easy-to-use libraries such as Beautiful Soup. You will also see how to extract structured and textual information from portals. By the end of this chapter, you will be able to implement data wrangling techniques such as web scraping in the real world.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor159"/>Introduction</h1>
			<p>So far in this book, we have focused on studying pandas DataFrame objects as the main data structure for the application of wrangling techniques. In this chapter, we will learn about various techniques by which we can read data into a DataFrame from external sources. Some of these sources could be text-based (such as CSV, HTML, and JSON), whereas others could be binary (that is, not in ASCII format; for example, from Excel or PDFs). We will also learn how to deal with data that is present in web pages or HTML documents. </p>
			<p>Being able to deal with and extract meaningful data from various sources is of paramount interest to a data practitioner. Data can, and often does, come in various forms and flavors. It is essential to be able to get the data into a form that is useful for performing predictive or other kinds of downstream tasks.</p>
			<p>As we have gone through detailed examples of basic operations with NumPy and pandas, in this chapter, we will often skip trivial code snippets such as viewing a table, selecting a column, and plotting. Instead, we will focus on showing code examples for the new topics we aim to learn about here.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor160"/>Reading Data from Different Sources</h1>
			<p>One of the most valued and widely used skills of a data wrangling professional is the ability to extract and read data from a diverse array of sources into a structured format. Modern analytics pipelines depend on the ability and skills of those professionals to build a robust system that can scan and absorb a variety of data sources to build and analyze a pattern-rich model. Such kinds of feature-rich, multi-dimensional models will have high predictive and generalization accuracy. They will be valued by stakeholders and end users alike in any data-driven product. In the first part of this chapter, we will go through various data sources and how they can be imported into <code>pandas</code> DataFrames, thus imbuing data wrangling professionals with extremely valuable data ingestion knowledge.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor161"/>Data Files Provided with This Chapter</h2>
			<p>As this topic is about reading from various data sources, we will use small files of various types in the following exercises. All the data files are provided, along with the Jupyter notebook, in the code repository.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the data files can be accessed from the following link: <a href="https://packt.live/3fAWg3f">https://packt.live/3fAWg3f</a>.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor162"/>Libraries to Install for This Chapter</h2>
			<p>As this chapter deals with reading files of various formats, we need to have the support of additional libraries and software platforms to accomplish our goals.</p>
			<p>Before we install these libraries, ensure that <strong class="bold">Java Development Kit (JDK)</strong> is installed on your system. If not, go to the following link to install it:</p>
			<p><a href="https://www.oracle.com/in/java/technologies/javase-downloads.html">https://www.oracle.com/in/java/technologies/javase-downloads.html</a>.</p>
			<p>Once you are on the website, click the link that says <code>JDK Download</code>. Then, proceed to download and install JDK based on your operating system. Once installation completes, ensure that you restart your system, especially if you are using Windows. In case you face issues after installation, check if you've set the PATH system variable correctly. To learn how to do that, refer <a href="https://www.java.com/en/download/help/path.xml">https://www.java.com/en/download/help/path.xml</a>.</p>
			<p>Once JDK is installed, go ahead and install the necessary libraries. Execute the following command in your Jupyter Notebook cells:</p>
			<pre>!pip install tabula-py xlrd lxml</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Don't forget the <code>!</code> before each line of code. This little exclamation sign in front of each command lets the Jupyter runtime know that what is in the cell is a <code>bash</code> command and not a line of Python code.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor163"/>Reading Data Using Pandas</h2>
			<p>The <code>pandas</code> library provides a simple method called <code>read_csv</code> to read data in a tabular format from a comma-separated text file, or <code>.csv</code>. This is particularly useful because <code>.csv</code> is a lightweight yet extremely handy data exchange format for many applications, including such domains where machine-generated data is involved. It is not a proprietary format and therefore is universally used by a variety of data-generating sources.</p>
			<p>Generally, a <code>.csv</code> file has two sections. The first line of a <code>.csv</code> file is usually treated as a header line. So, each column (each word, or words, between two consecutive commas) in the first line should indicate the name of the column. This is very valuable information because without it, it would often be impossible to say what kind of data each of them represents. After the first line, we have data rows where each line represents one data point and each column represents values of those data points. </p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor164"/>Exercise 5.01: Working with Headers When Reading Data from a CSV File</h2>
			<p>In this exercise, you will see how to read data from a <code>.csv</code> file. The file can be found here <a href="https://packt.live/3fDMCNp">https://packt.live/3fDMCNp</a>. This exercise acts as a demonstration of how to work with headers and what to do when the headers are missing. At times, you will encounter situations where headers are not present, and you may have to add proper headers or column names of your own. Let's have a look at how this can be done:</p>
			<ol>
				<li>Open a new Jupyter Notebook and read the example <code>.csv</code> file (with a header) using the following code and examine the resulting DataFrame, as follows:<pre>import numpy as np
import pandas as pd
df1 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>")
df1</pre><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div><img src="img/B15780_05_01.jpg" alt="Figure 5.1: Output of the example CSV file&#13;&#10;" width="500" height="172"/></div><p class="figure-caption">Figure 5.1: Output of the example CSV file</p></li>
				<li>Read a <code>.csv</code> file with no header using a <code>pandas</code> DataFrame:<pre>df2 = pd.read_csv("<code>header=None</code> to avoid this.</p></li>
				<li>Read the <code>.csv</code> file by setting the <code>header</code> to <code>None</code>, as follows:<pre>df2 = pd.read_csv("<code>0</code>. This is how the <code>pandas</code> library treats a headerless CSV file when you ask it not to consider the first line (which is a data row in this case) as <code>header</code>:  </p><div><img src="img/B15780_05_03.jpg" alt="Figure 5.3: A CSV file with numeric column headers&#13;&#10;" width="514" height="184"/></div><p class="figure-caption">Figure 5.3: A CSV file with numeric column headers</p><p>This may be fine for data analysis purposes, but if you want the DataFrame to have meaningful headers, then you will have to add them using the <code>names</code> argument.</p></li>
				<li>Add the <code>names</code> argument to get the correct headers:<pre>df2 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_2.csv</strong>",\
                  header=None, names=['Bedroom','Sq.ft',\
                                      'Locality','Price($)'])
df2</pre><p>Finally, you will get a DataFrame that will look like this:</p><div><img src="img/B15780_05_04.jpg" alt="Figure 5.4: CSV file with correct column header&#13;&#10;" width="547" height="191"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.4: CSV file with correct column header</p>
			<p>As you can see in the preceding figure, the headers have been added in the right places.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hxmAgm">https://packt.live/3hxmAgm</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eaToda">https://packt.live/3eaToda</a>.</p>
			<p>Up until now, we've been comfortable reading from files where a comma acts as a delimiter. Let's look at the following exercise, where we will be reading from a CSV file where the values are not separated by commas.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor165"/>Exercise 5.02: Reading from a CSV File Where Delimiters Are Not Commas</h2>
			<p>It is fairly common to encounter raw data files where the separator/delimiter is a character and not a comma. This exercise will demonstrate how you can read data from a file in such a case. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The file can be found here: <a href="https://packt.live/2YPEJgO">https://packt.live/2YPEJgO</a>.</p>
			<p>Let's go through the following steps:</p>
			<ol>
				<li value="1">Read a <code>.csv</code> file using <code>pandas</code> DataFrames:<pre>import numpy as np
import pandas as pd
df3 = pd.read_csv("<code>;</code> separator was not expected, and the reading is flawed. A simple workaround is to specify the separator/delimiter explicitly in the <code>read</code> function.</p></li>
				<li>Specify the delimiter:<pre>df3 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_3.csv</strong>",sep=';')
df3</pre><p>The output is as follows:</p><div><img src="img/B15780_05_06.jpg" alt="Figure 5.6: Semicolons removed from the DataFrame&#13;&#10;" width="419" height="165"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.6: Semicolons removed from the DataFrame</p>
			<p>As we can see, it is fairly simple to read from a csv file when the delimiter is specified in the <code>read_csv</code> function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Na4oM0">https://packt.live/2Na4oM0</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fvdm2g">https://packt.live/3fvdm2g</a>.</p>
			<p>In the following exercise, we will see how to bypass the headers if your CSV file already comes with headers. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor166"/>Exercise 5.03: Bypassing and Renaming the Headers of a CSV File</h2>
			<p>This exercise will demonstrate how to bypass the headers of a CSV file and put in your own. To do that, you have to specifically set <code>header=0</code>. If you try to set the <code>names</code> variable to your <code>header</code> list, unexpected things can happen. Follow these steps:</p>
			<ol>
				<li value="1">Add <code>names</code> to a <code>.csv</code> file that has headers, as follows:<pre>import numpy as np
import pandas as pd
df4 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",\
                  names=['A','B','C','D'])
df4</pre><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div><img src="img/B15780_05_07.jpg" alt="Figure 5.7: A CSV file with headers overlapped&#13;&#10;" width="552" height="209"/></div><p class="figure-caption">Figure 5.7: A CSV file with headers overlapped</p></li>
				<li>To avoid this, set <code>header</code> to zero and provide a <code>names</code> list:<pre>df4 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",header=0,\
                  names=['A','B','C','D'])
df4</pre><p>The output is as follows: </p><div><img src="img/B15780_05_08.jpg" alt="Figure 5.8: A CSV file with defined headers&#13;&#10;" width="438" height="163"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.8: A CSV file with defined headers</p>
			<p>Keep in mind that this representation is just in memory at the moment and only available in the present session of the notebook; it is not reflected in the physical CSV file. The original file has not changed due to our manipulation.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QwEeA">https://packt.live/30QwEeA</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/315gOgr">https://packt.live/315gOgr</a>.</p>
			<p>We observed some operations that we can do on the headers in a file. However, some CSV files may have an even more complex structure than the simple ones that we have been using so far. In the following exercise, we will discover some tricks to deal with such complex structures.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor167"/>Exercise 5.04: Skipping Initial Rows and Footers When Reading a CSV File</h2>
			<p>In this exercise, we will skip the first few rows because, most of the time, the first few rows of a CSV data file are metadata about the data source or similar information, which is not read into the table. Also, we will go ahead and remove the footer of the file, which might sometimes contain information that's not very useful. Let's see how we can do that using the example shown in the following screenshot:</p>
			<div><div><img src="img/B15780_05_09.jpg" alt="Figure 5.9: Contents of the CSV file&#13;&#10;" width="1244" height="427"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9: Contents of the CSV file</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The first two lines in the CSV file are irrelevant data. The file can be found here <a href="https://packt.live/30SdvJh">https://packt.live/30SdvJh</a></p>
			<ol>
				<li value="1">Read the CSV file and examine the results:<pre>import numpy as np
import pandas as pd
df5 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_skiprows.csv</strong>")
df5</pre><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div><img src="img/B15780_05_10.jpg" alt="Figure 5.10: DataFrame with an unexpected error&#13;&#10;" width="619" height="253"/></div><p class="figure-caption">Figure 5.10: DataFrame with an unexpected error</p></li>
				<li>Skip the first two rows and read the file:<pre>df5 = pd.read_csv("<code>skipfooter</code> and the <code>engine='python'</code> option to enable this. There are two engines for these CSV reader functions, based on C or Python, of which only the Python engine supports the <code>skipfooter</code> option.</p></li>
				<li>Use the <code>skipfooter</code> option in Python:<pre>df6 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_skipfooter.csv</strong>",\
                  skiprows=2,skipfooter=1, engine='python')
df6</pre><p>The output is as follows:</p><div><img src="img/B15780_05_013.jpg" alt="Figure 5.13: DataFrame without a footer&#13;&#10;" width="463" height="166"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.13: DataFrame without a footer</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CbehGO">https://packt.live/2CbehGO</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ycw0Gp">https://packt.live/2Ycw0Gp</a>.</p>
			<p>We've now seen how to read values skipping the headers and footers from a file. It can very often be very handy while dealing with data collected from several different sources, especially in situations where a file contains unnecessary and junk information.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor168"/>Reading Only the First N Rows </h2>
			<p>In many situations, we may not want to read a whole data file but only the first few rows. This is particularly useful for extremely large data files, where we may just want to read the first couple of hundred rows to check an initial pattern and then decide to read the whole of the data afterward. Reading the entire file can take a long time and can slow down the entire data wrangling pipeline.</p>
			<p>A simple option, called <code>nrows</code>, in the <code>read_csv</code> function, enables us to do just that. We will specify the number of rows we want to read and pass it as an argument to <code>nrows</code> like so:</p>
			<pre>df7 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",nrows=2)
df7</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The path (highlighted) would need to be changed based on where the file is saved on your system.</p>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_05_014.jpg" alt="Figure 5.14: DataFrame with the first few rows of the CSV file&#13;&#10;" width="445" height="80"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14: DataFrame with the first few rows of the CSV file</p>
			<p>The ability to be able to read only a selected number of rows is useful, specifically if you are dealing with large CSV files. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor169"/>Exercise 5.05: Combining skiprows and nrows to Read Data in Small Chunks</h2>
			<p>This exercise will demonstrate how we can read from a very large data file. To do that, we can cleverly combine <code>skiprows</code> and <code>nrows</code> to read in a large file in small chunks of pre-determined sizes. We will read from the <code>Boston_housing.csv</code> file, which contains data about the pricing of houses in the Boston area in the US. It contains information such as per capita crime rate by town and the average number of rooms per dwelling. To do this, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Each exercise continues directly from the previous one. You do not need to open a new Jupyter Notebook each time.</p>
			<p class="callout">The dataset can be found here: <a href="https://packt.live/3fEIH2z">https://packt.live/3fEIH2z</a></p>
			<ol>
				<li value="1">Create a list where DataFrames will be stored:<pre>list_of_dataframe = []</pre></li>
				<li>Store the number of rows to be read into a variable:<pre>rows_in_a_chunk = 10</pre></li>
				<li>Create a variable to store the number of chunks to be read:<pre>num_chunks = 5</pre></li>
				<li>Create a dummy DataFrame to get the column names:<pre>import pandas as pd
df_dummy = pd.read_csv("<strong class="bold">../datasets/Boston_housing.csv</strong>",nrows=2)
colnames = df_dummy.columns</pre><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p></li>
				<li>Loop over the CSV file to read only a fixed number of rows at a time:<pre>for i in range(0,num_chunks*rows_in_a_chunk,rows_in_a_chunk):
    df = pd.read_csv("<strong class="bold">Boston_housing.csv</strong>", header=0,\
                     skiprows=i, nrows=rows_in_a_chunk,\
                     names=colnames)
    list_of_dataframe.append(df)</pre><p class="callout-heading">Note</p><p class="callout">This particular step will not show any output as the values are getting appended to the list. </p></li>
			</ol>
			<p>Note how the <code>iterator</code> variable is set up inside the <code>range</code> function to break it into chunks. Say the number of chunks is <code>5</code> and the rows per chunk is <code>10</code>, then the iterator will have a range of <code>(0,5*10,10)</code>, where the final <code>10</code> is step-size, that is, it will iterate with indices of <code>(0,9,19,29,39,49)</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fGmBwZ">https://packt.live/3fGmBwZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hDbVAJ">https://packt.live/3hDbVAJ</a>.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor170"/>Setting the skip_blank_lines Option</h2>
			<p>By default, <code>read_csv</code> ignores blank lines, which means if there are row entries with <code>NaN</code> values, the <code>read_csv</code> function will not read that data. However, in some situations, you may want to read them in as <code>NaN</code> so that you can count how many blank entries were present in the raw data file. In some situations, this is an indicator of the default data streaming quality and consistency. For this, you have to disable the <code>skip_blank_lines</code> option:</p>
			<pre>df9 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_blankline.csv</strong>",\
                  skip_blank_lines=False)
df9</pre>
			<p>The output is as follows:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The path (highlighted) would need to be changed based on where the file is located on your system.</p>
			<div><div><img src="img/B15780_05_015.jpg" alt="Figure 5.15: DataFrame of a .csv file that has blank rows&#13;&#10;" width="399" height="203"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15: DataFrame of a .csv file that has blank rows</p>
			<p>In the next section, we are going to read CSV data from a zip file. </p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor171"/>Reading CSV Data from a Zip File</h2>
			<p>This is an awesome feature of <code>pandas</code>, and it allows you to read directly from a compressed file, such as <code>.zip</code>, <code>.gz</code>, <code>.bz2</code>, or <code>.xz</code>. The only requirement is that the intended data file (<code>CSV</code>) should be the only file inside the compressed file. For example, we might need to compress a large csv file, and in that case, it will be the only file inside the <code>.zip</code> folder. </p>
			<p>In this example, we compressed the example CSV file with the <code>7-Zip</code> program and read from it directly using the <code>read_csv</code> method:</p>
			<pre>df10 = pd.read_csv('../datasets/CSV_EX_1.zip')
df10</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_05_016.jpg" alt="Figure 5.16: DataFrame of a compressed CSV file&#13;&#10;" width="420" height="159"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16: DataFrame of a compressed CSV file</p>
			<p>Next, we will turn our attention to a Microsoft Excel file. It turns out that most of the options and methods we learned about in the previous exercises with the CSV file apply directly to reading Excel files too.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor172"/>Reading from an Excel File Using sheet_name and Handling a Distinct sheet_name</h2>
			<p>In this section, we will focus on the differences between the methods of reading from an Excel file. An Excel file can consist of multiple worksheets, and we can read a specific sheet by passing in a particular argument, that is, <code>sheet_name</code>.</p>
			<p>For example, in the <code>Housing_data.xlsx</code> file, we have three worksheets. The following code reads them one by one into three separate DataFrames:</p>
			<pre>df11_1 = pd.read_excel("../datasets/Housing_data.xlsx",\
                       sheet_name='Data_Tab_1')
df11_2 = pd.read_excel("../datasets/Housing_data.xlsx",\
                       sheet_name='Data_Tab_2')
df11_3 = pd.read_excel("../datasets/Housing_data.xlsx",\
                       sheet_name='Data_Tab_3')</pre>
			<p>If the Excel file has multiple distinct worksheets but the <code>sheet_name</code> argument is set to <code>None</code>, then an ordered dictionary will be returned by the <code>read_excel</code> function. That ordered <code>dict</code> will have the data from all the worksheets, and the top-level keys will indicate the name of the worksheet. Thereafter, we can simply iterate over that dictionary or its keys to retrieve individual DataFrames.</p>
			<p>Let's consider the following example:</p>
			<pre>dict_df = pd.read_excel("../datasets/Housing_data.xlsx",\
                        sheet_name=None)
dict_df.keys()</pre>
			<p>The output is as follows:</p>
			<pre>odict_keys(['Data_Tab_1', 'Data_Tab_2', 'Data_Tab_3'])</pre>
			<p>Therefore, we can access these individual worksheets using the distinct keys. </p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor173"/>Exercise 5.06: Reading a General Delimited Text File</h2>
			<p>In this exercise, we will read from general delimited text files and see that this can be done as easily as reading from CSV files. However, we will have to use the right separator if it is anything other than a whitespace or a tab. To see this in action, let's go through the following steps:</p>
			<ol>
				<li value="1">Read the data from a <code>.txt</code> file using the <code>read_table</code> command:<pre>import pandas as pd
df13 = pd.read_table("<code>.txt</code> extension will result in the preceding DataFrame if read without explicitly setting the separator. As you can see, for each value read, there is a comma appended. In this case, we have to set the separator explicitly. </p></li>
				<li>Set the separator as a comma in the <code>sep</code> variable as follows:<pre>df13 = pd.read_table("<strong class="bold">../datasets/Table_EX_1.txt</strong>", sep=',')
df13</pre><p>The output is as follows:</p><div><img src="img/B15780_05_018.jpg" alt="Figure 5.18: A DataFrame read using a comma separator&#13;&#10;" width="437" height="157"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.18: A DataFrame read using a comma separator</p>
			<p>We can see in the figure that the data is read as expected from the <code>.txt</code> file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30UUdD8">https://packt.live/30UUdD8</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37F57ho">https://packt.live/37F57ho</a>.</p>
			<p>Now that we have seen the various ways of reading data from <code>csv</code> files, in the next section, let's focus on reading data directly from a URL.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor174"/>Reading HTML Tables Directly from a URL</h2>
			<p>The <code>pandas</code> library allows us to read HTML tables directly from a URL. This means that the library already has some kind of built-in HTML parser that processes the HTML content of a given page and tries to extract various tables from the page. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>read_html</code> method from the <code>pandas</code> library returns a list of DataFrames (even if the page has a single DataFrame) and you have to extract the relevant tables from the list.</p>
			<p>Consider the following example:</p>
			<pre>url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
list_of_df = pd.read_html(url)
df14 = list_of_df[0]
df14.head()</pre>
			<p>These results are shown in the following DataFrame:</p>
			<div><div><img src="img/B15780_05_019.jpg" alt="Figure 5.19: Results of reading HTML tables&#13;&#10;" width="1109" height="201"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19: Results of reading HTML tables</p>
			<p>In the following exercise, we'll explore some more wrangling techniques to get the data in the desired format. As discussed in the preceding exercise, <code>read_html</code>, the HTML-reading function, almost always returns more than one table for a given HTML page, and we have to further parse through the list to extract the particular table we are interested in.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor175"/>Exercise 5.07: Further Wrangling to Get the Desired Data</h2>
			<p>In this exercise, we will work with the table of the 2016 Summer Olympics medal tally (by nation). We can easily search to get a page on Wikipedia containing this data that we can pass on to <code>pandas</code>. We will apply a few wrangling techniques on this data to get the output. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Use the <code>read_html</code> command to read from the Wikipedia page containing Summer Olympics records from 2016:<pre>import pandas as pd
list_of_df = pd.read_html("https://en.wikipedia.org/wiki/"\
                          "2016_Summer_Olympics_medal_table",\
                          header=0)</pre></li>
				<li>Check the length of the list returned. We will see that it is <code>7</code>:<pre>len(list_of_df)</pre><p>The output is as follows:</p><pre>7</pre></li>
				<li>To look for the particular table, run a simple loop. We are using the <code>shape</code> property of a DataFrame to examine the number of rows and the number of columns of each of them:<pre>for t in list_of_df:
    print(t.shape)</pre><p>The output is as follows:</p><pre>(1, 1)
(87, 6)
(10, 9)
(0, 2)
(1, 2)
(4, 2)
(1, 2)</pre><p>It looks like the second element in this list is the table we are looking for.</p></li>
				<li>Extract the second element from the table:<pre>df15=list_of_df[1]
df15.head()</pre><p>The output is as follows:</p><div><img src="img/B15780_05_020.jpg" alt="Figure 5.20: Output of the data in the second table&#13;&#10;" width="1389" height="458"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.20: Output of the data in the second table</p>
			<p>As we can observe from the preceding table, data containing the records of the Wikipedia page has been read in a table format.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3de6LYw">https://packt.live/3de6LYw</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Bk9e6q">https://packt.live/2Bk9e6q</a>.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor176"/>Reading from a JSON file</h2>
			<p>Over the last 15 years, JSON has become ubiquitous for data exchange on the web. Today, it is the format of choice for almost every publicly available web API, and it is frequently used for private web APIs as well. It is a schema-less, text-based representation of structured data that is based on key-value pairs and ordered lists. The <code>pandas</code> library provides excellent support for reading data from a JSON file directly into a DataFrame.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor177"/>Exercise 5.08: Reading from a JSON File</h2>
			<p>In this exercise, we will read data from the <code>movies.json</code> file. This file contains the cast, genre, title, and year (of release) information for almost all major movies since <code>1900</code>. Let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>.json</code> file could be found at <a href="https://packt.live/3d7DO0l">https://packt.live/3d7DO0l</a>.</p>
			<ol>
				<li value="1">Extract the list of movies from the file into a DataFrame.<pre>import pandas as pd
df16 = pd.read_json("<strong class="bold">../datasets/movies.json</strong>")
df16.head()</pre><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the JSON file based on its location on your system.</p><p>The output is as follows:</p><div><img src="img/B15780_05_021.jpg" alt="Figure 5.21: DataFrame displaying the movie list&#13;&#10;" width="1603" height="612"/></div><p class="figure-caption">Figure 5.21: DataFrame displaying the movie list</p></li>
				<li>To look for the cast where the title is <code>Avengers</code>, use filtering:<pre>cast_of_avengers = df16[(df16['title']=="The Avengers") \
                   &amp; (df16['year']==2012)]['cast']
print(list(cast_of_avengers))</pre><p>The output will be as follows:</p><pre> [['Robert Downey, Jr.', 'Chris Evans', 'Mark Ruffalo', 
   'Chris Hemsworth', 'Scarlett Johansson', 'Jeremy Renner', 
   'Tom Hiddleston', 'Clark Gregg', 'Cobie Smulders', 
   'Stellan SkarsgÃyrd', 'Samuel L. Jackson']]</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37ISQJ8">https://packt.live/37ISQJ8</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2YeymVv">https://packt.live/2YeymVv</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor178"/>Reading a PDF File</h2>
			<p>Among the various types of data sources, the PDF format is probably the most difficult to parse in general. While there are some popular packages in Python for working with PDF files for general page formatting, the best library to use for table extraction from PDF files is <code>tabula-py</code>.</p>
			<p>From the GitHub page of this package, <code>tabula-py</code> is a simple Python wrapper of <code>tabula-java</code>, which can read a table from a PDF. You can read tables from PDFs and convert them into <code>pandas</code> DataFrames. The <code>tabula-py</code> library also enables you to convert a PDF file into a CSV/TSV/JSON file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure you've installed <code>tabula</code> based on the instructions detailed in the section titled <em class="italic">Libraries to Install for This Chapter</em>.</p>
			<p>You will also need the following packages installed on your system before you can run this, but they are free and easy to install; you can use <code>pip install</code> to install them from the notebook session as you did in the past: </p>
			<ul>
				<li><code>urllib3</code></li>
				<li><code>pandas</code></li>
				<li><code>pytest</code></li>
				<li><code>flake8</code></li>
				<li><code>distro</code></li>
				<li><code>pathlib</code></li>
			</ul>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor179"/>Exercise 5.09: Reading Tabular Data from a PDF File</h2>
			<p>In this exercise, we will first read from two different pages of a PDF file from <a href="https://packt.live/2Ygj4j7">https://packt.live/2Ygj4j7</a> in tabular format, and then we will perform a few simple operations to handle the headers of these files. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before proceeding, make sure you've installed <code>tabula</code> based on the instructions detailed in an earlier section titled <em class="italic">Libraries to Install for This Chapter</em>.</p>
			<p>Let's go through the following steps to do so:</p>
			<ol>
				<li value="1">The following code retrieves the tables from two pages and joins them to make one table:<pre>from tabula import read_pdf
df18_1 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',\
                  pages=[1], pandas_options={'header':None})
df18_1</pre><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the PDF based on its location on your system.</p><p>The output is as follows:</p><div><img src="img/B15780_05_022.jpg" alt="Figure 5.22: DataFrame with a table derived by merging a table flowing over &#13;&#10;two pages in a PDF&#13;&#10;" width="578" height="171"/></div><p class="figure-caption">Figure 5.22: DataFrame with a table derived by merging a table flowing over two pages in a PDF</p></li>
				<li>Retrieve the table from another page of the same PDF by using the following command:<pre>df18_2 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',\
                  pages=[2], pandas_options={'header':None})
df18_2</pre><p>The output is as follows:</p><div><img src="img/B15780_05_023.jpg" alt="Figure 5.23: DataFrame displaying a table from another page&#13;&#10;" width="506" height="168"/></div><p class="figure-caption">Figure 5.23: DataFrame displaying a table from another page</p></li>
				<li>To concatenate the tables that were derived from the first two steps, execute the following code:<pre>import pandas as pd
df1 = pd.DataFrame(df18_1)
df2 = pd.DataFrame(df18_2)
df18=pd.concat([df1,df2],axis=1)
df18.values.tolist()</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_05_024.jpg" alt="Figure 5.24: DataFrame derived by concatenating two tables&#13;&#10;" width="1311" height="410"/></div><p class="figure-caption">Figure 5.24: DataFrame derived by concatenating two tables</p><p>With PDF extraction, most of the time, headers will be difficult to extract automatically. </p></li>
				<li>Pass on the list of headers with the <code>names</code> argument in the <code>read-pdf</code> function set to <code>pandas_option</code>, as follows:<pre>names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS',\
         'RAD','TAX','PTRATIO','B','LSTAT','PRICE']
df18_1 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',pages = [1], \
                  pandas_options = {'header':None,\
                                    'names':names[:10]})
df18_2 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',pages = [2],\
                  pandas_options = {'header':None,\
                                    'names':names[10:]})
df1 = pd.DataFrame(df18_1)
df2 = pd.DataFrame(df18_2)
df18 = pd.concat([df1,df2],axis = 1)
df18.values.tolist()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/B15780_05_025.jpg" alt="Figure 5.25: DataFrame with the correct column headers for PDF data&#13;&#10;" width="1158" height="351"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25: DataFrame with the correct column headers for PDF data</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YcHz0v">https://packt.live/2YcHz0v</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<p>We will have a full activity on reading tables from a PDF report and processing them at the end of this chapter. Let's dive into web page scraping and the library used to do that, Beautiful Soup 4. </p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor180"/>Introduction to Beautiful Soup 4 and Web Page Parsing</h1>
			<p>The ability to read and understand web pages is of paramount interest to a person collecting and formatting data. For example, consider the task of gathering data about movies and then formatting it for a downstream system. Data from movie databases is best obtained from websites such as IMDb, and that data does not come pre-packaged in nice forms (such as CSV or JSON), so you need to know how to download and read a web page.</p>
			<p>You also need to be equipped with the knowledge of the structure of a web page so that you can design a system that can search for (query) a particular piece of information from a whole web page and get the value from it. This involves understanding the grammar of markup languages and being able to write something that can parse them. Doing this, and keeping all the edge cases in mind, for something like HTML is already incredibly complex, and if you extend the scope of the bespoke markup language to include XML as well, then it becomes full-time work for a team of people.</p>
			<p>Thankfully, we are using Python, and Python has a very mature and stable library that does all the complicated tasks for us. This library is called <code>BeautifulSoup</code> (it is, at present, in version 4, and thus we will call it <code>bs4</code> for short from now on). <code>bs4</code> is a library for getting data from HTML or XML documents, and it gives you a nice, normalized, idiomatic way of navigating and querying a document. It does not include a parser but it supports different ones.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor181"/>Structure of HTML</h2>
			<p>Before we jump into <code>bs4</code> and start working with it, we need to examine the structure of an HTML document. <strong class="bold">H</strong>yper <strong class="bold">T</strong>ext <strong class="bold">M</strong>arkup <strong class="bold">L</strong>anguage is a structured way of telling web browsers about the organization of a web page, meaning which kinds of elements (text, image, video, and so on) come from where, where inside the page they should appear, what they look like, what they contain, and how they will behave with user input. HTML5 is the latest version of HTML. An HTML document can be viewed as a tree, as we can see in the following diagram:</p>
			<div><div><img src="img/B15780_05_026.jpg" alt="Figure 5.26: HTML structure&#13;&#10;" width="740" height="476"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26: HTML structure</p>
			<p>Each node of the tree represents one element in the document. An element is anything that starts with <code>&lt;</code> and ends with <code>&gt;</code>. For example, <code>&lt;html&gt;</code>, <code>&lt;head&gt;</code>, <code>&lt;p&gt;</code>, <code>&lt;br&gt;</code>, <code>&lt;img&gt;</code>, and so on are various HTML elements. Some elements have a start and end element, where the end element begins with <code>&lt;/</code> and has the same name as the start element, such as <code>&lt;p&gt;</code> and <code>&lt;/p&gt;</code>, and they can contain an arbitrary number of elements of other types in them. Some elements do not have an ending part, such as the <code>&lt;br/&gt;</code> element, and they cannot contain anything within them.</p>
			<p>The only other thing that we need to know about an element at this point is the fact that elements can have attributes, which are there to modify the default behavior of an element. For example, an <code>&lt;a&gt;</code> anchor element requires a <code>href</code> attribute to tell the browser which website it should navigate to when that particular <code>&lt;a&gt;</code> is clicked, like this: <code>&lt;a href="http://cnn.com"&gt;</code>. <code>The CNN news channel</code>, <code>&lt;/a&gt;</code>, will take you to <a href="http://cnn.com">cnn.com</a> when clicked:</p>
			<div><div><img src="img/B15780_05_027.jpg" alt="Figure 5.27: CNN news channel hyperlink&#13;&#10;" width="845" height="69"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.27: CNN news channel hyperlink</p>
			<p>So, when you are at a particular element of the tree, you can visit all the children of that element to get their contents and attributes. </p>
			<p>Equipped with this knowledge, let's see how we can read and query data from an HTML document.</p>
			<p>In this topic, we will cover the reading and parsing of web pages, but we do not request them from a live website. Instead, we read them from disk. A section on reading them from the internet will follow in a future chapter. </p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor182"/>Exercise 5.10: Reading an HTML File and Extracting Its Contents Using Beautiful Soup</h2>
			<p>In this exercise, we will do the simplest thing possible. We will import the <code>Beautiful Soup</code> or <code>bs4</code> library and then use it to read an HTML document. Then, we will examine the different kinds of objects it returns. While doing the exercises for this topic, you should have the example HTML file (called <code>test.html</code>) open in a text editor so that you can check for the different tags and their attributes and contents:</p>
			<ol>
				<li value="1">Import the <code>bs4</code> library:<pre>from bs4 import BeautifulSoup</pre></li>
				<li>Please download the following test HTML file and save it on your disk, and then use <code>bs4</code> to read it from the disk:<pre>with open("<code>BeautifulSoup</code> object and it will read the contents from the file that the handler is attached to. We will see that the return type is an instance of <code>bs4.BeautifulSoup</code>. This class holds all the methods we need to navigate through the DOM tree that the document represents. </p></li>
				<li>Print the contents of the file in a nice way, by which we mean that the printing will keep some kind of nice indentation by using the <code>prettify</code> method from the class, like this:<pre>print(soup.prettify())</pre><p>The output is as follows:</p><div><img src="img/B15780_05_028.jpg" alt="Figure 5.28: Contents of the HTML file&#13;&#10;" width="553" height="344"/></div><p class="figure-caption">Figure 5.28: Contents of the HTML file</p><p>The same information can also be obtained by using the <code>soup.contents</code> member variable. The differences are: first, it won't print anything pretty and, second, it is essentially a list. </p><p>If we look carefully at the contents of the HTML file in a separate text editor, we will see that there are many paragraph tags, or <code>&lt;p&gt;</code> tags. Let's read content from one such <code>&lt;p&gt;</code> tag. We can do that using the simple <code>.</code> access modifier as we would have done for a normal member variable of a class. </p><p>The magic of <code>bs4</code> is the fact that it gives us this excellent way to dereference tags as member variables of the <code>BeautifulSoup</code> class instance. In the following few steps, we are going to read an HTML file and then pass the file handler returned by Python's <code>open</code> call directly to the constructor of the <code>BeautifulSoup</code> class. It does a lot of things (including reading the content and then parsing it) and returns an instance of the class that we can then use.</p></li>
				<li>Read the HTML file:<pre>with open("<code>&lt;p&gt;</code> tag. </p><p>We saw how to read a tag in the last exercise, but we can easily see the problem with this approach. When we look into our HTML document, we can see that we have more than one <code>&lt;p&gt;</code> tag there. How can we access all the <code>&lt;p&gt;</code> tags? It turns out that this is easy. </p></li>
				<li>Use the <code>findall</code> method to extract the content from the tag:<pre>with open("<code>6</code>, which is exactly the number of <code>&lt;p&gt;</code> tags in the document. </p><p>We have seen how to access all the tags of the same type. We have also seen how to get the content of the entire HTML document. </p></li>
				<li>Now we will see how to get the contents of a particular HTML tag: <pre>with open("<code>.</code> notation to get the contents of that tag. We saw in the previous step that we can access the entire content of a particular tag. However, HTML is represented as a tree, and we are able to traverse the children of a particular node. There are a few ways to do this.</p></li>
				<li>The first way is by using the <code>children</code> generator from any <code>bs4</code> instance, as follows:<pre>with open("<code>children</code> generator is that it only takes into account the immediate children of the tag. We have <code>&lt;tbody&gt;</code> under <code>&lt;table&gt;</code>, and our whole table structure is wrapped in it. That's why it was considered a single child of the <code>&lt;table&gt;</code> tag. </p><p>We have looked into how to browse the immediate children of a tag. We will see how we can browse all the possible children of a tag and not only the immediate one. </p></li>
				<li>To do that, we use the <code>descendants</code> generator from the <code>bs4</code> instance, as follows: <pre>with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:
    soup = BeautifulSoup(fd)
    table = soup.table
    children = table.children
    des = table.descendants
    print(len(list(children)), len(list(des)))</pre><p>The output is as follows:</p><pre>9 61</pre></li>
			</ol>
			<p>The comparison print at the end of the code block will show us the difference between <code>children</code> and <code>descendants</code>. The length of the list we got from <code>children</code> is only <code>9</code>, whereas the length of the list we got from <code>descendants</code> is <code>61</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N994l6">https://packt.live/2N994l6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UT2p2K">https://packt.live/2UT2p2K</a>.</p>
			<p>So far, we have seen some basic ways to navigate the tags inside an HTML document using <code>bs4</code>. Now, we are going to go one step further and use the power of <code>bs4</code> combined with the power of <code>pandas</code> to generate a DataFrame out of a plain HTML table. With the knowledge we will acquire now, it will be fairly easy for us to prepare a <code>pandas</code> DataFrame to perform <code>BeautifulSoup</code> library. </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor183"/>Exercise 5.11: DataFrames and BeautifulSoup</h2>
			<p>In this exercise, we will extract the data from the <code>test.html</code> page using the <code>BeautifulSoup</code> library. We will then perform a few operations for data preparation and display the data in an easily readable tabular format. To do that, let's go through the following steps:</p>
			<ol>
				<li value="1">Import <code>pandas</code> and read the document, as follows:<pre>import pandas as pd
from bs4 import BeautifulSoup
fd = open("<strong class="bold">../datasets/test.html</strong>", "r")
soup = BeautifulSoup(fd)
data = soup.findAll('tr')
print("Data is a {} and {} items long".format(type(data),\
      len(data)))
Data is a &lt;class 'bs4.element.ResultSet'&gt; and 4 items long</pre></li>
				<li>Check the original table structure in the HTML source. You will see that the first row is the column heading and all of the following rows are the data from the HTML source. We'll assign two different variables for the two sections, as follows:<pre>data_without_header = data[1:]
headers = data[0]
headers</pre><p>The output is as follows:</p><pre>&lt;tr&gt;
&lt;th&gt;Entry Header 1&lt;/th&gt;
&lt;th&gt;Entry Header 2&lt;/th&gt;
&lt;th&gt;Entry Header 3&lt;/th&gt;
&lt;th&gt;Entry Header 4&lt;/th&gt;
&lt;/tr&gt;</pre><p class="callout-heading">Note</p><p class="callout">Keep in mind that the art of scraping an HTML page goes hand in hand with an understanding of the source HTML structure. So, whenever you want to scrape a page, the first thing you need to do is right-click on it and then use <code>View Source</code> from the browser to see the source HTML.</p></li>
				<li>Once we have separated the two sections, we need two list comprehensions to make them ready to go in a DataFrame. For the header, this is easy:<pre>col_headers = [th.getText() for th in headers.findAll('th')]
col_headers</pre><p>The output is as follows:</p><pre>['Entry Header 1', 'Entry Header 2', 'Entry Header 3', 'Entry Header 4']</pre><p>Data preparation is a bit tricky for a <code>pandas</code> DataFrame. You need to have a two-dimensional list, which is a list of lists. We accomplish that in the following way, using the tricks we learned earlier about list comprehension.</p></li>
				<li>Use the <code>for…in</code> loop to iterate over the data:<pre>df_data = [[td.getText() for td in tr.findAll('td')] \
           for tr in data_without_header]
df_data</pre><p>The output is as follows:</p><div><img src="img/B15780_05_032.jpg" alt="Figure 5.32: Output as a two-dimensional list&#13;&#10;" width="562" height="164"/></div><p class="figure-caption">Figure 5.32: Output as a two-dimensional list</p></li>
				<li>Invoke the <code>pd.DataFrame</code> method and supply the right arguments by using the following code: <pre>df = pd.DataFrame(df_data, columns=col_headers)
df.head()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/B15780_05_033.jpg" alt="Figure 5.33: Output in tabular format with column headers&#13;&#10;" width="1074" height="251"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33: Output in tabular format with column headers</p>
			<p>Thus, we conclude our exercise on creating a data frame from an HTML table.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QyE6A">https://packt.live/30QyE6A</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hBPFY5">https://packt.live/3hBPFY5</a>.</p>
			<p>In the following exercise, we'll export a DataFrame as an Excel file.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor184"/>Exercise 5.12: Exporting a DataFrame as an Excel File</h2>
			<p>In this exercise, we will see how we can save a DataFrame as an Excel file. <code>Pandas</code> can do this natively, but it needs the help of the <code>openpyxl</code> library to achieve this goal. <code>openpyxl</code> is a Python library for reading/writing Excel 2010 <code>xlsx/xlsm/xltx/xltm</code> files. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is continued from the previous exercise. You'll need to continue in the same Jupyter Notebook.</p>
			<p>Let's perform the following steps:</p>
			<ol>
				<li value="1">Install the <code>openpyxl</code> library by using the following command:<pre>!pip install openpyxl</pre></li>
				<li>To save the DataFrame as an Excel file, use the following command from inside of the Jupyter notebook:<pre>writer = pd.ExcelWriter('<strong class="bold">../datasets/test_output.xlsx</strong>')
df.to_excel(writer, "Sheet1")
writer.save()
writer
&lt;pandas.io.excel._XlsxWriter at 0x24feb2939b0&gt;</pre></li>
			</ol>
			<p>This is the way in which we can export a <code>pandas</code> DataFrame to Excel. Given that Excel is a very popular format among many types of users, this is a very important trick you need to master.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YcSdV6">https://packt.live/2YcSdV6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YZTXjJ">https://packt.live/2YZTXjJ</a>.</p>
			<p>In the previous chapters, when we were discussing the stack, we explained how important it is to have a stack that we can push the URLs from a web page to so that we can pop them at a later time to follow each of them. The following exercise will demonstrate how to do that. </p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor185"/>Exercise 5.13: Stacking URLs from a Document Using bs4</h2>
			<p>In this exercise, we will append the URLs one after the other from the <code>test.html</code> web page. In that file, HTML file links or <code>&lt;a&gt;</code> tags are under a <code>&lt;ul&gt;</code> tag, and each of them is contained inside a <code>&lt;/li&gt;</code> tag. We are going to find all the <code>&lt;a&gt;</code> tags and create a stack with them. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is continued from the previous exercise. You'll need to continue in the same Jupyter Notebook.</p>
			<p>To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Find all the <code>&lt;a&gt;</code> tags by using the following command:<pre>d = open("<strong class="bold">../datasets/test.html</strong>", "r")
soup = BeautifulSoup(d)
lis = soup.find('ul').findAll('li')
stack = []
for li in lis:
    a = li.find('a', href=True)</pre><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the HTML file based on its location on your system.</p></li>
				<li>Define a stack before you start the loop. Then, inside the loop, use the <code>append</code> method to push the links in the stack:<pre>stack.append(a['href'])</pre></li>
				<li>Print the stack:<pre>print(stack)</pre><p>The output is as follows:</p><pre>['https://www.imdb.com/chart/top']</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hCCAOj">https://packt.live/3hCCAOj</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3fCYNd0">https://packt.live/3fCYNd0</a>.</p></li>
			</ol>
			<p>Let's put together everything we have learned so far in this chapter and get started with an activity.</p>
			<h2 id="_idParaDest-180">Activity 5.01: Reading Tabular Da<a id="_idTextAnchor186"/>ta from a Web Page and Creating DataFrames</h2>
			<p>In this activity, you have been g<a id="_idTextAnchor187"/>iven a Wikipedia page where you have the GDP of all countries listed. You have to create three <code>DataFrames</code> from the three sources mentioned on the page (<a href="https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal">https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal</a>)).</p>
			<p>You will have to do the following:</p>
			<ol>
				<li value="1">Open the page in a separate Chrome/Firefox tab and use something like an <code>Inspect Element</code> tool to view the source HTML and understand its structure.</li>
				<li>Read the page using <code>bs4</code>.</li>
				<li>Find the table structure you will need to deal with (how many tables are there?).</li>
				<li>Find the right table using <code>bs4</code>.</li>
				<li>Separate the source names and their corresponding data.</li>
				<li>Get the source names from the list of sources you have created.</li>
				<li>Separate the header and data from the data that you separated before for the first source only, and then create a DataFrame using that.</li>
				<li>Repeat the last task for the other two data sources.</li>
			</ol>
			<p>The output should look like this:</p>
			<div><div><img src="img/B15780_05_034.jpg" alt="Figure 5.34: Final output&#13;&#10;" width="1566" height="547"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.34: Final output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor317">this link</a>.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor188"/>Summary</h1>
			<p>In this chapter, we have looked into several different types of data formats and how to work with them. These formats include CSV, PDF, Excel, Plain Text, and HTML. HTML documents are the cornerstone of the World Wide Web and, given the amount of data that's contained in it, we can easily infer the importance of HTML as a data source.</p>
			<p>We learned about <code>bs4</code> (<code>BeautifulSoup 4</code>), a Python library that gives us Pythonic ways to read and query HTML documents. We used bs4 to load an HTML document and explored several different ways to navigate the loaded document.</p>
			<p>We also looked at how we can create a <code>pandas</code> DataFrame from an HTML document (which contains a table). Although there are some built-in ways to do this job in <code>pandas</code>, they fail as soon as the target table is encoded inside a complex hierarchy of elements. So, the knowledge we gathered in this topic to transform an HTML table into a <code>pandas</code> DataFrame in a step-by-step manner is invaluable. </p>
			<p>Finally, we looked at how we can create a stack in our code, where we push all the URLs that we encounter while reading the HTML file and then use them at a later time. In the next chapter, we will discuss list comprehensions, the <code>.zip</code> format, and outlier detection and cleaning.</p>
		</div>
	</div></body></html>