<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer164" class="Content">
			</div>
		</div>
		<div id="_idContainer165" class="Content">
			<h1 id="_idParaDest-152"><a id="_idTextAnchor158"/>5. Getting Comfortable with Different Kinds of Data Sources</h1>
		</div>
		<div id="_idContainer200" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will provide you with the skills to read CSV, Excel, and JSON files into pandas DataFrames. You will learn how to read PDF documents and HTML tables into pandas DataFrames and perform basic web scraping operations using powerful yet easy-to-use libraries such as Beautiful Soup. You will also see how to extract structured and textual information from portals. By the end of this chapter, you will be able to implement data wrangling techniques such as web scraping in the real world.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor159"/>Introduction</h1>
			<p>So far in this book, we have focused on studying pandas DataFrame objects as the main data structure for the application of wrangling techniques. In this chapter, we will learn about various techniques by which we can read data into a DataFrame from external sources. Some of these sources could be text-based (such as CSV, HTML, and JSON), whereas others could be binary (that is, not in ASCII format; for example, from Excel or PDFs). We will also learn how to deal with data that is present in web pages or HTML documents. </p>
			<p>Being able to deal with and extract meaningful data from various sources is of paramount interest to a data practitioner. Data can, and often does, come in various forms and flavors. It is essential to be able to get the data into a form that is useful for performing predictive or other kinds of downstream tasks.</p>
			<p>As we have gone through detailed examples of basic operations with NumPy and pandas, in this chapter, we will often skip trivial code snippets such as viewing a table, selecting a column, and plotting. Instead, we will focus on showing code examples for the new topics we aim to learn about here.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor160"/>Reading Data from Different Sources</h1>
			<p>One of the most valued and widely used skills of a data wrangling professional is the ability to extract and read data from a diverse array of sources into a structured format. Modern analytics pipelines depend on the ability and skills of those professionals to build a robust system that can scan and absorb a variety of data sources to build and analyze a pattern-rich model. Such kinds of feature-rich, multi-dimensional models will have high predictive and generalization accuracy. They will be valued by stakeholders and end users alike in any data-driven product. In the first part of this chapter, we will go through various data sources and how they can be imported into <strong class="source-inline">pandas</strong> DataFrames, thus imbuing data wrangling professionals with extremely valuable data ingestion knowledge.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor161"/>Data Files Provided with This Chapter</h2>
			<p>As this topic is about reading from various data sources, we will use small files of various types in the following exercises. All the data files are provided, along with the Jupyter notebook, in the code repository.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the data files can be accessed from the following link: <a href="https://packt.live/3fAWg3f">https://packt.live/3fAWg3f</a>.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor162"/>Libraries to Install for This Chapter</h2>
			<p>As this chapter deals with reading files of various formats, we need to have the support of additional libraries and software platforms to accomplish our goals.</p>
			<p>Before we install these libraries, ensure that <strong class="bold">Java Development Kit (JDK)</strong> is installed on your system. If not, go to the following link to install it:</p>
			<p><a href="https://www.oracle.com/in/java/technologies/javase-downloads.html">https://www.oracle.com/in/java/technologies/javase-downloads.html</a>.</p>
			<p>Once you are on the website, click the link that says <strong class="source-inline">JDK Download</strong>. Then, proceed to download and install JDK based on your operating system. Once installation completes, ensure that you restart your system, especially if you are using Windows. In case you face issues after installation, check if you've set the PATH system variable correctly. To learn how to do that, refer <a href="https://www.java.com/en/download/help/path.xml">https://www.java.com/en/download/help/path.xml</a>.</p>
			<p>Once JDK is installed, go ahead and install the necessary libraries. Execute the following command in your Jupyter Notebook cells:</p>
			<p class="source-code">!pip install tabula-py xlrd lxml</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Don't forget the <strong class="source-inline">!</strong> before each line of code. This little exclamation sign in front of each command lets the Jupyter runtime know that what is in the cell is a <strong class="source-inline">bash</strong> command and not a line of Python code.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor163"/>Reading Data Using Pandas</h2>
			<p>The <strong class="source-inline">pandas</strong> library provides a simple method called <strong class="source-inline">read_csv</strong> to read data in a tabular format from a comma-separated text file, or <strong class="source-inline">.csv</strong>. This is particularly useful because <strong class="source-inline">.csv</strong> is a lightweight yet extremely handy data exchange format for many applications, including such domains where machine-generated data is involved. It is not a proprietary format and therefore is universally used by a variety of data-generating sources.</p>
			<p>Generally, a <strong class="source-inline">.csv</strong> file has two sections. The first line of a <strong class="source-inline">.csv</strong> file is usually treated as a header line. So, each column (each word, or words, between two consecutive commas) in the first line should indicate the name of the column. This is very valuable information because without it, it would often be impossible to say what kind of data each of them represents. After the first line, we have data rows where each line represents one data point and each column represents values of those data points. </p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor164"/>Exercise 5.01: Working with Headers When Reading Data from a CSV File</h2>
			<p>In this exercise, you will see how to read data from a <strong class="source-inline">.csv</strong> file. The file can be found here <a href="https://packt.live/3fDMCNp">https://packt.live/3fDMCNp</a>. This exercise acts as a demonstration of how to work with headers and what to do when the headers are missing. At times, you will encounter situations where headers are not present, and you may have to add proper headers or column names of your own. Let's have a look at how this can be done:</p>
			<ol>
				<li>Open a new Jupyter Notebook and read the example <strong class="source-inline">.csv</strong> file (with a header) using the following code and examine the resulting DataFrame, as follows:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">df1 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>")</p><p class="source-code">df1</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div id="_idContainer166" class="IMG---Figure"><img src="Images/B15780_05_01.jpg" alt="Figure 5.1: Output of the example CSV file&#13;&#10;" width="500" height="172"/></div><p class="figure-caption">Figure 5.1: Output of the example CSV file</p></li>
				<li>Read a <strong class="source-inline">.csv</strong> file with no header using a <strong class="source-inline">pandas</strong> DataFrame:<p class="source-code">df2 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_2.csv</strong>")</p><p class="source-code">df2</p><p>The output is as follows:</p><div id="_idContainer167" class="IMG---Figure"><img src="Images/B15780_05_02.jpg" alt="Figure 5.2: Output of the .csv file being read using a DataFrame&#13;&#10;" width="538" height="160"/></div><p class="figure-caption">Figure 5.2: Output of the .csv file being read using a DataFrame</p><p class="callout-heading">Note</p><p class="callout">The file can be found here <a href="https://packt.live/30SEbJH">https://packt.live/30SEbJH</a>.</p><p>The top data row has been mistakenly read as the column header. You can specify <strong class="source-inline">header=None</strong> to avoid this.</p></li>
				<li>Read the <strong class="source-inline">.csv</strong> file by setting the <strong class="source-inline">header</strong> to <strong class="source-inline">None</strong>, as follows:<p class="source-code">df2 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_2.csv</strong>",header=None)</p><p class="source-code">df2</p><p>However, without any header information, you will get back the following output. The default headers will be just some default numeric indices starting from <strong class="source-inline">0</strong>. This is how the <strong class="source-inline">pandas</strong> library treats a headerless CSV file when you ask it not to consider the first line (which is a data row in this case) as <strong class="source-inline">header</strong>:  </p><div id="_idContainer168" class="IMG---Figure"><img src="Images/B15780_05_03.jpg" alt="Figure 5.3: A CSV file with numeric column headers&#13;&#10;" width="514" height="184"/></div><p class="figure-caption">Figure 5.3: A CSV file with numeric column headers</p><p>This may be fine for data analysis purposes, but if you want the DataFrame to have meaningful headers, then you will have to add them using the <strong class="source-inline">names</strong> argument.</p></li>
				<li>Add the <strong class="source-inline">names</strong> argument to get the correct headers:<p class="source-code">df2 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_2.csv</strong>",\</p><p class="source-code">                  header=None, names=['Bedroom','Sq.ft',\</p><p class="source-code">                                      'Locality','Price($)'])</p><p class="source-code">df2</p><p>Finally, you will get a DataFrame that will look like this:</p><div id="_idContainer169" class="IMG---Figure"><img src="Images/B15780_05_04.jpg" alt="Figure 5.4: CSV file with correct column header&#13;&#10;" width="547" height="191"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.4: CSV file with correct column header</p>
			<p>As you can see in the preceding figure, the headers have been added in the right places.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hxmAgm">https://packt.live/3hxmAgm</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eaToda">https://packt.live/3eaToda</a>.</p>
			<p>Up until now, we've been comfortable reading from files where a comma acts as a delimiter. Let's look at the following exercise, where we will be reading from a CSV file where the values are not separated by commas.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor165"/>Exercise 5.02: Reading from a CSV File Where Delimiters Are Not Commas</h2>
			<p>It is fairly common to encounter raw data files where the separator/delimiter is a character and not a comma. This exercise will demonstrate how you can read data from a file in such a case. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The file can be found here: <a href="https://packt.live/2YPEJgO">https://packt.live/2YPEJgO</a>.</p>
			<p>Let's go through the following steps:</p>
			<ol>
				<li value="1">Read a <strong class="source-inline">.csv</strong> file using <strong class="source-inline">pandas</strong> DataFrames:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">df3 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_3.csv</strong>")</p><p class="source-code">df3</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output will be as follows:</p><div id="_idContainer170" class="IMG---Figure"><img src="Images/B15780_05_05.jpg" alt="Figure 5.5: A DataFrame that has a semi-colon as a separator&#13;&#10;" width="513" height="185"/></div><p class="figure-caption">Figure 5.5: A DataFrame that has a semi-colon as a separator</p><p>Clearly, the <strong class="source-inline">;</strong> separator was not expected, and the reading is flawed. A simple workaround is to specify the separator/delimiter explicitly in the <strong class="source-inline">read</strong> function.</p></li>
				<li>Specify the delimiter:<p class="source-code">df3 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_3.csv</strong>",sep=';')</p><p class="source-code">df3</p><p>The output is as follows:</p><div id="_idContainer171" class="IMG---Figure"><img src="Images/B15780_05_06.jpg" alt="Figure 5.6: Semicolons removed from the DataFrame&#13;&#10;" width="419" height="165"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.6: Semicolons removed from the DataFrame</p>
			<p>As we can see, it is fairly simple to read from a csv file when the delimiter is specified in the <strong class="source-inline">read_csv</strong> function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Na4oM0">https://packt.live/2Na4oM0</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fvdm2g">https://packt.live/3fvdm2g</a>.</p>
			<p>In the following exercise, we will see how to bypass the headers if your CSV file already comes with headers. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor166"/>Exercise 5.03: Bypassing and Renaming the Headers of a CSV File</h2>
			<p>This exercise will demonstrate how to bypass the headers of a CSV file and put in your own. To do that, you have to specifically set <strong class="source-inline">header=0</strong>. If you try to set the <strong class="source-inline">names</strong> variable to your <strong class="source-inline">header</strong> list, unexpected things can happen. Follow these steps:</p>
			<ol>
				<li value="1">Add <strong class="source-inline">names</strong> to a <strong class="source-inline">.csv</strong> file that has headers, as follows:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">df4 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",\</p><p class="source-code">                  names=['A','B','C','D'])</p><p class="source-code">df4</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div id="_idContainer172" class="IMG---Figure"><img src="Images/B15780_05_07.jpg" alt="Figure 5.7: A CSV file with headers overlapped&#13;&#10;" width="552" height="209"/></div><p class="figure-caption">Figure 5.7: A CSV file with headers overlapped</p></li>
				<li>To avoid this, set <strong class="source-inline">header</strong> to zero and provide a <strong class="source-inline">names</strong> list:<p class="source-code">df4 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",header=0,\</p><p class="source-code">                  names=['A','B','C','D'])</p><p class="source-code">df4</p><p>The output is as follows: </p><div id="_idContainer173" class="IMG---Figure"><img src="Images/B15780_05_08.jpg" alt="Figure 5.8: A CSV file with defined headers&#13;&#10;" width="438" height="163"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.8: A CSV file with defined headers</p>
			<p>Keep in mind that this representation is just in memory at the moment and only available in the present session of the notebook; it is not reflected in the physical CSV file. The original file has not changed due to our manipulation.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QwEeA">https://packt.live/30QwEeA</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/315gOgr">https://packt.live/315gOgr</a>.</p>
			<p>We observed some operations that we can do on the headers in a file. However, some CSV files may have an even more complex structure than the simple ones that we have been using so far. In the following exercise, we will discover some tricks to deal with such complex structures.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor167"/>Exercise 5.04: Skipping Initial Rows and Footers When Reading a CSV File</h2>
			<p>In this exercise, we will skip the first few rows because, most of the time, the first few rows of a CSV data file are metadata about the data source or similar information, which is not read into the table. Also, we will go ahead and remove the footer of the file, which might sometimes contain information that's not very useful. Let's see how we can do that using the example shown in the following screenshot:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="Images/B15780_05_09.jpg" alt="Figure 5.9: Contents of the CSV file&#13;&#10;" width="1244" height="427"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9: Contents of the CSV file</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The first two lines in the CSV file are irrelevant data. The file can be found here <a href="https://packt.live/30SdvJh">https://packt.live/30SdvJh</a></p>
			<ol>
				<li value="1">Read the CSV file and examine the results:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">df5 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_skiprows.csv</strong>")</p><p class="source-code">df5</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p><p>The output is as follows:</p><div id="_idContainer175" class="IMG---Figure"><img src="Images/B15780_05_10.jpg" alt="Figure 5.10: DataFrame with an unexpected error&#13;&#10;" width="619" height="253"/></div><p class="figure-caption">Figure 5.10: DataFrame with an unexpected error</p></li>
				<li>Skip the first two rows and read the file:<p class="source-code">df5 = pd.read_csv("<strong class="bold">CSV_EX_skiprows.csv</strong>",skiprows=2)</p><p class="source-code">df5</p><p>The output is as follows:</p><div id="_idContainer176" class="IMG---Figure"><img src="Images/B15780_05_011.jpg" alt="Figure 5.11: Expected DataFrame after skipping two rows&#13;&#10;" width="423" height="160"/></div><p class="figure-caption">Figure 5.11: Expected DataFrame after skipping two rows</p><p>Similar to skipping the initial rows, it may be necessary to skip the footer of a file. For example, we do not want to read the data at the end of the following file:</p><div id="_idContainer177" class="IMG---Figure"><img src="Images/B15780_05_012.jpg" alt="Figure 5.12: Contents of the CSV file&#13;&#10;" width="564" height="218"/></div><p class="figure-caption">Figure 5.12: Contents of the CSV file</p><p>We have to use <strong class="source-inline">skipfooter</strong> and the <strong class="source-inline">engine='python'</strong> option to enable this. There are two engines for these CSV reader functions, based on C or Python, of which only the Python engine supports the <strong class="source-inline">skipfooter</strong> option.</p></li>
				<li>Use the <strong class="source-inline">skipfooter</strong> option in Python:<p class="source-code">df6 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_skipfooter.csv</strong>",\</p><p class="source-code">                  skiprows=2,skipfooter=1, engine='python')</p><p class="source-code">df6</p><p>The output is as follows:</p><div id="_idContainer178" class="IMG---Figure"><img src="Images/B15780_05_013.jpg" alt="Figure 5.13: DataFrame without a footer&#13;&#10;" width="463" height="166"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.13: DataFrame without a footer</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CbehGO">https://packt.live/2CbehGO</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ycw0Gp">https://packt.live/2Ycw0Gp</a>.</p>
			<p>We've now seen how to read values skipping the headers and footers from a file. It can very often be very handy while dealing with data collected from several different sources, especially in situations where a file contains unnecessary and junk information.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor168"/>Reading Only the First N Rows </h2>
			<p>In many situations, we may not want to read a whole data file but only the first few rows. This is particularly useful for extremely large data files, where we may just want to read the first couple of hundred rows to check an initial pattern and then decide to read the whole of the data afterward. Reading the entire file can take a long time and can slow down the entire data wrangling pipeline.</p>
			<p>A simple option, called <strong class="source-inline">nrows</strong>, in the <strong class="source-inline">read_csv</strong> function, enables us to do just that. We will specify the number of rows we want to read and pass it as an argument to <strong class="source-inline">nrows</strong> like so:</p>
			<p class="source-code">df7 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_1.csv</strong>",nrows=2)</p>
			<p class="source-code">df7</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The path (highlighted) would need to be changed based on where the file is saved on your system.</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="Images/B15780_05_014.jpg" alt="Figure 5.14: DataFrame with the first few rows of the CSV file&#13;&#10;" width="445" height="80"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14: DataFrame with the first few rows of the CSV file</p>
			<p>The ability to be able to read only a selected number of rows is useful, specifically if you are dealing with large CSV files. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor169"/>Exercise 5.05: Combining skiprows and nrows to Read Data in Small Chunks</h2>
			<p>This exercise will demonstrate how we can read from a very large data file. To do that, we can cleverly combine <strong class="source-inline">skiprows</strong> and <strong class="source-inline">nrows</strong> to read in a large file in small chunks of pre-determined sizes. We will read from the <strong class="source-inline">Boston_housing.csv</strong> file, which contains data about the pricing of houses in the Boston area in the US. It contains information such as per capita crime rate by town and the average number of rooms per dwelling. To do this, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Each exercise continues directly from the previous one. You do not need to open a new Jupyter Notebook each time.</p>
			<p class="callout">The dataset can be found here: <a href="https://packt.live/3fEIH2z">https://packt.live/3fEIH2z</a></p>
			<ol>
				<li value="1">Create a list where DataFrames will be stored:<p class="source-code">list_of_dataframe = []</p></li>
				<li>Store the number of rows to be read into a variable:<p class="source-code">rows_in_a_chunk = 10</p></li>
				<li>Create a variable to store the number of chunks to be read:<p class="source-code">num_chunks = 5</p></li>
				<li>Create a dummy DataFrame to get the column names:<p class="source-code">import pandas as pd</p><p class="source-code">df_dummy = pd.read_csv("<strong class="bold">../datasets/Boston_housing.csv</strong>",nrows=2)</p><p class="source-code">colnames = df_dummy.columns</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the CSV file based on its location on your system.</p></li>
				<li>Loop over the CSV file to read only a fixed number of rows at a time:<p class="source-code">for i in range(0,num_chunks*rows_in_a_chunk,rows_in_a_chunk):</p><p class="source-code">    df = pd.read_csv("<strong class="bold">Boston_housing.csv</strong>", header=0,\</p><p class="source-code">                     skiprows=i, nrows=rows_in_a_chunk,\</p><p class="source-code">                     names=colnames)</p><p class="source-code">    list_of_dataframe.append(df)</p><p class="callout-heading">Note</p><p class="callout">This particular step will not show any output as the values are getting appended to the list. </p></li>
			</ol>
			<p>Note how the <strong class="source-inline">iterator</strong> variable is set up inside the <strong class="source-inline">range</strong> function to break it into chunks. Say the number of chunks is <strong class="source-inline">5</strong> and the rows per chunk is <strong class="source-inline">10</strong>, then the iterator will have a range of <strong class="source-inline">(0,5*10,10)</strong>, where the final <strong class="source-inline">10</strong> is step-size, that is, it will iterate with indices of <strong class="source-inline">(0,9,19,29,39,49)</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fGmBwZ">https://packt.live/3fGmBwZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hDbVAJ">https://packt.live/3hDbVAJ</a>.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor170"/>Setting the skip_blank_lines Option</h2>
			<p>By default, <strong class="source-inline">read_csv</strong> ignores blank lines, which means if there are row entries with <strong class="source-inline">NaN</strong> values, the <strong class="source-inline">read_csv</strong> function will not read that data. However, in some situations, you may want to read them in as <strong class="source-inline">NaN</strong> so that you can count how many blank entries were present in the raw data file. In some situations, this is an indicator of the default data streaming quality and consistency. For this, you have to disable the <strong class="source-inline">skip_blank_lines</strong> option:</p>
			<p class="source-code">df9 = pd.read_csv("<strong class="bold">../datasets/CSV_EX_blankline.csv</strong>",\</p>
			<p class="source-code">                  skip_blank_lines=False)</p>
			<p class="source-code">df9</p>
			<p>The output is as follows:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The path (highlighted) would need to be changed based on where the file is located on your system.</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="Images/B15780_05_015.jpg" alt="Figure 5.15: DataFrame of a .csv file that has blank rows&#13;&#10;" width="399" height="203"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15: DataFrame of a .csv file that has blank rows</p>
			<p>In the next section, we are going to read CSV data from a zip file. </p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor171"/>Reading CSV Data from a Zip File</h2>
			<p>This is an awesome feature of <strong class="source-inline">pandas</strong>, and it allows you to read directly from a compressed file, such as <strong class="source-inline">.zip</strong>, <strong class="source-inline">.gz</strong>, <strong class="source-inline">.bz2</strong>, or <strong class="source-inline">.xz</strong>. The only requirement is that the intended data file (<strong class="source-inline">CSV</strong>) should be the only file inside the compressed file. For example, we might need to compress a large csv file, and in that case, it will be the only file inside the <strong class="source-inline">.zip</strong> folder. </p>
			<p>In this example, we compressed the example CSV file with the <strong class="source-inline">7-Zip</strong> program and read from it directly using the <strong class="source-inline">read_csv</strong> method:</p>
			<p class="source-code">df10 = pd.read_csv('../datasets/CSV_EX_1.zip')</p>
			<p class="source-code">df10</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="Images/B15780_05_016.jpg" alt="Figure 5.16: DataFrame of a compressed CSV file&#13;&#10;" width="420" height="159"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16: DataFrame of a compressed CSV file</p>
			<p>Next, we will turn our attention to a Microsoft Excel file. It turns out that most of the options and methods we learned about in the previous exercises with the CSV file apply directly to reading Excel files too.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor172"/>Reading from an Excel File Using sheet_name and Handling a Distinct sheet_name</h2>
			<p>In this section, we will focus on the differences between the methods of reading from an Excel file. An Excel file can consist of multiple worksheets, and we can read a specific sheet by passing in a particular argument, that is, <strong class="source-inline">sheet_name</strong>.</p>
			<p>For example, in the <strong class="source-inline">Housing_data.xlsx</strong> file, we have three worksheets. The following code reads them one by one into three separate DataFrames:</p>
			<p class="source-code">df11_1 = pd.read_excel("../datasets/Housing_data.xlsx",\</p>
			<p class="source-code">                       sheet_name='Data_Tab_1')</p>
			<p class="source-code">df11_2 = pd.read_excel("../datasets/Housing_data.xlsx",\</p>
			<p class="source-code">                       sheet_name='Data_Tab_2')</p>
			<p class="source-code">df11_3 = pd.read_excel("../datasets/Housing_data.xlsx",\</p>
			<p class="source-code">                       sheet_name='Data_Tab_3')</p>
			<p>If the Excel file has multiple distinct worksheets but the <strong class="source-inline">sheet_name</strong> argument is set to <strong class="source-inline">None</strong>, then an ordered dictionary will be returned by the <strong class="source-inline">read_excel</strong> function. That ordered <strong class="source-inline">dict</strong> will have the data from all the worksheets, and the top-level keys will indicate the name of the worksheet. Thereafter, we can simply iterate over that dictionary or its keys to retrieve individual DataFrames.</p>
			<p>Let's consider the following example:</p>
			<p class="source-code">dict_df = pd.read_excel("../datasets/Housing_data.xlsx",\</p>
			<p class="source-code">                        sheet_name=None)</p>
			<p class="source-code">dict_df.keys()</p>
			<p>The output is as follows:</p>
			<p class="source-code">odict_keys(['Data_Tab_1', 'Data_Tab_2', 'Data_Tab_3'])</p>
			<p>Therefore, we can access these individual worksheets using the distinct keys. </p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor173"/>Exercise 5.06: Reading a General Delimited Text File</h2>
			<p>In this exercise, we will read from general delimited text files and see that this can be done as easily as reading from CSV files. However, we will have to use the right separator if it is anything other than a whitespace or a tab. To see this in action, let's go through the following steps:</p>
			<ol>
				<li value="1">Read the data from a <strong class="source-inline">.txt</strong> file using the <strong class="source-inline">read_table</strong> command:<p class="source-code">import pandas as pd</p><p class="source-code">df13 = pd.read_table("<strong class="bold">../datasets/Table_EX_1.txt</strong>")</p><p class="source-code">df13</p><p>The output is as follows:</p><div id="_idContainer182" class="IMG---Figure"><img src="Images/B15780_05_017.jpg" alt="Figure 5.17: A DataFrame that has a comma-separated CSV file&#13;&#10;" width="519" height="194"/></div><p class="figure-caption">Figure 5.17: A DataFrame that has a comma-separated CSV file</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the text file based on its location on your system.</p><p>A comma-separated file saved with the <strong class="source-inline">.txt</strong> extension will result in the preceding DataFrame if read without explicitly setting the separator. As you can see, for each value read, there is a comma appended. In this case, we have to set the separator explicitly. </p></li>
				<li>Set the separator as a comma in the <strong class="source-inline">sep</strong> variable as follows:<p class="source-code">df13 = pd.read_table("<strong class="bold">../datasets/Table_EX_1.txt</strong>", sep=',')</p><p class="source-code">df13</p><p>The output is as follows:</p><div id="_idContainer183" class="IMG---Figure"><img src="Images/B15780_05_018.jpg" alt="Figure 5.18: A DataFrame read using a comma separator&#13;&#10;" width="437" height="157"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.18: A DataFrame read using a comma separator</p>
			<p>We can see in the figure that the data is read as expected from the <strong class="source-inline">.txt</strong> file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30UUdD8">https://packt.live/30UUdD8</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37F57ho">https://packt.live/37F57ho</a>.</p>
			<p>Now that we have seen the various ways of reading data from <strong class="source-inline">csv</strong> files, in the next section, let's focus on reading data directly from a URL.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor174"/>Reading HTML Tables Directly from a URL</h2>
			<p>The <strong class="source-inline">pandas</strong> library allows us to read HTML tables directly from a URL. This means that the library already has some kind of built-in HTML parser that processes the HTML content of a given page and tries to extract various tables from the page. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">read_html</strong> method from the <strong class="source-inline">pandas</strong> library returns a list of DataFrames (even if the page has a single DataFrame) and you have to extract the relevant tables from the list.</p>
			<p>Consider the following example:</p>
			<p class="source-code">url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'</p>
			<p class="source-code">list_of_df = pd.read_html(url)</p>
			<p class="source-code">df14 = list_of_df[0]</p>
			<p class="source-code">df14.head()</p>
			<p>These results are shown in the following DataFrame:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="Images/B15780_05_019.jpg" alt="Figure 5.19: Results of reading HTML tables&#13;&#10;" width="1109" height="201"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19: Results of reading HTML tables</p>
			<p>In the following exercise, we'll explore some more wrangling techniques to get the data in the desired format. As discussed in the preceding exercise, <strong class="source-inline">read_html</strong>, the HTML-reading function, almost always returns more than one table for a given HTML page, and we have to further parse through the list to extract the particular table we are interested in.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor175"/>Exercise 5.07: Further Wrangling to Get the Desired Data</h2>
			<p>In this exercise, we will work with the table of the 2016 Summer Olympics medal tally (by nation). We can easily search to get a page on Wikipedia containing this data that we can pass on to <strong class="source-inline">pandas</strong>. We will apply a few wrangling techniques on this data to get the output. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Use the <strong class="source-inline">read_html</strong> command to read from the Wikipedia page containing Summer Olympics records from 2016:<p class="callout-heading">Note</p><p class="callout">Watch out for the slashes in the string below. Remember that the backslashes ( <strong class="source-inline">\</strong> ) are used to split the code across multiple lines, while the forward slashes ( <strong class="source-inline">/</strong> ) are part of the URL.</p><p class="source-code">import pandas as pd</p><p class="source-code">list_of_df = pd.read_html("https://en.wikipedia.org/wiki/"\</p><p class="source-code">                          "2016_Summer_Olympics_medal_table",\</p><p class="source-code">                          header=0)</p></li>
				<li>Check the length of the list returned. We will see that it is <strong class="source-inline">7</strong>:<p class="source-code">len(list_of_df)</p><p>The output is as follows:</p><p class="source-code">7</p></li>
				<li>To look for the particular table, run a simple loop. We are using the <strong class="source-inline">shape</strong> property of a DataFrame to examine the number of rows and the number of columns of each of them:<p class="source-code">for t in list_of_df:</p><p class="source-code">    print(t.shape)</p><p>The output is as follows:</p><p class="source-code">(1, 1)</p><p class="source-code">(87, 6)</p><p class="source-code">(10, 9)</p><p class="source-code">(0, 2)</p><p class="source-code">(1, 2)</p><p class="source-code">(4, 2)</p><p class="source-code">(1, 2)</p><p>It looks like the second element in this list is the table we are looking for.</p></li>
				<li>Extract the second element from the table:<p class="source-code">df15=list_of_df[1]</p><p class="source-code">df15.head()</p><p>The output is as follows:</p><div id="_idContainer185" class="IMG---Figure"><img src="Images/B15780_05_020.jpg" alt="Figure 5.20: Output of the data in the second table&#13;&#10;" width="1389" height="458"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.20: Output of the data in the second table</p>
			<p>As we can observe from the preceding table, data containing the records of the Wikipedia page has been read in a table format.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3de6LYw">https://packt.live/3de6LYw</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Bk9e6q">https://packt.live/2Bk9e6q</a>.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor176"/>Reading from a JSON file</h2>
			<p>Over the last 15 years, JSON has become ubiquitous for data exchange on the web. Today, it is the format of choice for almost every publicly available web API, and it is frequently used for private web APIs as well. It is a schema-less, text-based representation of structured data that is based on key-value pairs and ordered lists. The <strong class="source-inline">pandas</strong> library provides excellent support for reading data from a JSON file directly into a DataFrame.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor177"/>Exercise 5.08: Reading from a JSON File</h2>
			<p>In this exercise, we will read data from the <strong class="source-inline">movies.json</strong> file. This file contains the cast, genre, title, and year (of release) information for almost all major movies since <strong class="source-inline">1900</strong>. Let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">.json</strong> file could be found at <a href="https://packt.live/3d7DO0l">https://packt.live/3d7DO0l</a>.</p>
			<ol>
				<li value="1">Extract the list of movies from the file into a DataFrame.<p class="source-code">import pandas as pd</p><p class="source-code">df16 = pd.read_json("<strong class="bold">../datasets/movies.json</strong>")</p><p class="source-code">df16.head()</p><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the JSON file based on its location on your system.</p><p>The output is as follows:</p><div id="_idContainer186" class="IMG---Figure"><img src="Images/B15780_05_021.jpg" alt="Figure 5.21: DataFrame displaying the movie list&#13;&#10;" width="1603" height="612"/></div><p class="figure-caption">Figure 5.21: DataFrame displaying the movie list</p></li>
				<li>To look for the cast where the title is <strong class="source-inline">Avengers</strong>, use filtering:<p class="source-code">cast_of_avengers = df16[(df16['title']=="The Avengers") \</p><p class="source-code">                   &amp; (df16['year']==2012)]['cast']</p><p class="source-code">print(list(cast_of_avengers))</p><p>The output will be as follows:</p><p class="source-code"> [['Robert Downey, Jr.', 'Chris Evans', 'Mark Ruffalo', </p><p class="source-code">   'Chris Hemsworth', 'Scarlett Johansson', 'Jeremy Renner', </p><p class="source-code">   'Tom Hiddleston', 'Clark Gregg', 'Cobie Smulders', </p><p class="source-code">   'Stellan SkarsgÃyrd', 'Samuel L. Jackson']]</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37ISQJ8">https://packt.live/37ISQJ8</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2YeymVv">https://packt.live/2YeymVv</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor178"/>Reading a PDF File</h2>
			<p>Among the various types of data sources, the PDF format is probably the most difficult to parse in general. While there are some popular packages in Python for working with PDF files for general page formatting, the best library to use for table extraction from PDF files is <strong class="source-inline">tabula-py</strong>.</p>
			<p>From the GitHub page of this package, <strong class="source-inline">tabula-py</strong> is a simple Python wrapper of <strong class="source-inline">tabula-java</strong>, which can read a table from a PDF. You can read tables from PDFs and convert them into <strong class="source-inline">pandas</strong> DataFrames. The <strong class="source-inline">tabula-py</strong> library also enables you to convert a PDF file into a CSV/TSV/JSON file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure you've installed <strong class="source-inline">tabula</strong> based on the instructions detailed in the section titled <em class="italic">Libraries to Install for This Chapter</em>.</p>
			<p>You will also need the following packages installed on your system before you can run this, but they are free and easy to install; you can use <strong class="source-inline">pip install</strong> to install them from the notebook session as you did in the past: </p>
			<ul>
				<li><strong class="source-inline">urllib3</strong></li>
				<li><strong class="source-inline">pandas</strong></li>
				<li><strong class="source-inline">pytest</strong></li>
				<li><strong class="source-inline">flake8</strong></li>
				<li><strong class="source-inline">distro</strong></li>
				<li><strong class="source-inline">pathlib</strong></li>
			</ul>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor179"/>Exercise 5.09: Reading Tabular Data from a PDF File</h2>
			<p>In this exercise, we will first read from two different pages of a PDF file from <a href="https://packt.live/2Ygj4j7">https://packt.live/2Ygj4j7</a> in tabular format, and then we will perform a few simple operations to handle the headers of these files. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before proceeding, make sure you've installed <strong class="source-inline">tabula</strong> based on the instructions detailed in an earlier section titled <em class="italic">Libraries to Install for This Chapter</em>.</p>
			<p>Let's go through the following steps to do so:</p>
			<ol>
				<li value="1">The following code retrieves the tables from two pages and joins them to make one table:<p class="source-code">from tabula import read_pdf</p><p class="source-code">df18_1 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',\</p><p class="source-code">                  pages=[1], pandas_options={'header':None})</p><p class="source-code">df18_1</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the PDF based on its location on your system.</p><p>The output is as follows:</p><div id="_idContainer187" class="IMG---Figure"><img src="Images/B15780_05_022.jpg" alt="Figure 5.22: DataFrame with a table derived by merging a table flowing over &#13;&#10;two pages in a PDF&#13;&#10;" width="578" height="171"/></div><p class="figure-caption">Figure 5.22: DataFrame with a table derived by merging a table flowing over two pages in a PDF</p></li>
				<li>Retrieve the table from another page of the same PDF by using the following command:<p class="source-code">df18_2 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',\</p><p class="source-code">                  pages=[2], pandas_options={'header':None})</p><p class="source-code">df18_2</p><p>The output is as follows:</p><div id="_idContainer188" class="IMG---Figure"><img src="Images/B15780_05_023.jpg" alt="Figure 5.23: DataFrame displaying a table from another page&#13;&#10;" width="506" height="168"/></div><p class="figure-caption">Figure 5.23: DataFrame displaying a table from another page</p></li>
				<li>To concatenate the tables that were derived from the first two steps, execute the following code:<p class="source-code">import pandas as pd</p><p class="source-code">df1 = pd.DataFrame(df18_1)</p><p class="source-code">df2 = pd.DataFrame(df18_2)</p><p class="source-code">df18=pd.concat([df1,df2],axis=1)</p><p class="source-code">df18.values.tolist()</p><p>The output is as follows:</p><p> </p><div id="_idContainer189" class="IMG---Figure"><img src="Images/B15780_05_024.jpg" alt="Figure 5.24: DataFrame derived by concatenating two tables&#13;&#10;" width="1311" height="410"/></div><p class="figure-caption">Figure 5.24: DataFrame derived by concatenating two tables</p><p>With PDF extraction, most of the time, headers will be difficult to extract automatically. </p></li>
				<li>Pass on the list of headers with the <strong class="source-inline">names</strong> argument in the <strong class="source-inline">read-pdf</strong> function set to <strong class="source-inline">pandas_option</strong>, as follows:<p class="source-code">names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS',\</p><p class="source-code">         'RAD','TAX','PTRATIO','B','LSTAT','PRICE']</p><p class="source-code">df18_1 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',pages = [1], \</p><p class="source-code">                  pandas_options = {'header':None,\</p><p class="source-code">                                    'names':names[:10]})</p><p class="source-code">df18_2 = read_pdf('<strong class="bold">../datasets/Housing_data.pdf</strong>',pages = [2],\</p><p class="source-code">                  pandas_options = {'header':None,\</p><p class="source-code">                                    'names':names[10:]})</p><p class="source-code">df1 = pd.DataFrame(df18_1)</p><p class="source-code">df2 = pd.DataFrame(df18_2)</p><p class="source-code">df18 = pd.concat([df1,df2],axis = 1)</p><p class="source-code">df18.values.tolist()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="Images/B15780_05_025.jpg" alt="Figure 5.25: DataFrame with the correct column headers for PDF data&#13;&#10;" width="1158" height="351"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25: DataFrame with the correct column headers for PDF data</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YcHz0v">https://packt.live/2YcHz0v</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<p>We will have a full activity on reading tables from a PDF report and processing them at the end of this chapter. Let's dive into web page scraping and the library used to do that, Beautiful Soup 4. </p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor180"/>Introduction to Beautiful Soup 4 and Web Page Parsing</h1>
			<p>The ability to read and understand web pages is of paramount interest to a person collecting and formatting data. For example, consider the task of gathering data about movies and then formatting it for a downstream system. Data from movie databases is best obtained from websites such as IMDb, and that data does not come pre-packaged in nice forms (such as CSV or JSON), so you need to know how to download and read a web page.</p>
			<p>You also need to be equipped with the knowledge of the structure of a web page so that you can design a system that can search for (query) a particular piece of information from a whole web page and get the value from it. This involves understanding the grammar of markup languages and being able to write something that can parse them. Doing this, and keeping all the edge cases in mind, for something like HTML is already incredibly complex, and if you extend the scope of the bespoke markup language to include XML as well, then it becomes full-time work for a team of people.</p>
			<p>Thankfully, we are using Python, and Python has a very mature and stable library that does all the complicated tasks for us. This library is called <strong class="source-inline">BeautifulSoup</strong> (it is, at present, in version 4, and thus we will call it <strong class="source-inline">bs4</strong> for short from now on). <strong class="source-inline">bs4</strong> is a library for getting data from HTML or XML documents, and it gives you a nice, normalized, idiomatic way of navigating and querying a document. It does not include a parser but it supports different ones.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor181"/>Structure of HTML</h2>
			<p>Before we jump into <strong class="source-inline">bs4</strong> and start working with it, we need to examine the structure of an HTML document. <strong class="bold">H</strong>yper <strong class="bold">T</strong>ext <strong class="bold">M</strong>arkup <strong class="bold">L</strong>anguage is a structured way of telling web browsers about the organization of a web page, meaning which kinds of elements (text, image, video, and so on) come from where, where inside the page they should appear, what they look like, what they contain, and how they will behave with user input. HTML5 is the latest version of HTML. An HTML document can be viewed as a tree, as we can see in the following diagram:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="Images/B15780_05_026.jpg" alt="Figure 5.26: HTML structure&#13;&#10;" width="740" height="476"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26: HTML structure</p>
			<p>Each node of the tree represents one element in the document. An element is anything that starts with <strong class="source-inline">&lt;</strong> and ends with <strong class="source-inline">&gt;</strong>. For example, <strong class="source-inline">&lt;html&gt;</strong>, <strong class="source-inline">&lt;head&gt;</strong>, <strong class="source-inline">&lt;p&gt;</strong>, <strong class="source-inline">&lt;br&gt;</strong>, <strong class="source-inline">&lt;img&gt;</strong>, and so on are various HTML elements. Some elements have a start and end element, where the end element begins with <strong class="source-inline">&lt;/</strong> and has the same name as the start element, such as <strong class="source-inline">&lt;p&gt;</strong> and <strong class="source-inline">&lt;/p&gt;</strong>, and they can contain an arbitrary number of elements of other types in them. Some elements do not have an ending part, such as the <strong class="source-inline">&lt;br/&gt;</strong> element, and they cannot contain anything within them.</p>
			<p>The only other thing that we need to know about an element at this point is the fact that elements can have attributes, which are there to modify the default behavior of an element. For example, an <strong class="source-inline">&lt;a&gt;</strong> anchor element requires a <strong class="source-inline">href</strong> attribute to tell the browser which website it should navigate to when that particular <strong class="source-inline">&lt;a&gt;</strong> is clicked, like this: <strong class="source-inline">&lt;a href="http://cnn.com"&gt;</strong>. <strong class="source-inline">The CNN news channel</strong>, <strong class="source-inline">&lt;/a&gt;</strong>, will take you to <a href="http://cnn.com">cnn.com</a> when clicked:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="Images/B15780_05_027.jpg" alt="Figure 5.27: CNN news channel hyperlink&#13;&#10;" width="845" height="69"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.27: CNN news channel hyperlink</p>
			<p>So, when you are at a particular element of the tree, you can visit all the children of that element to get their contents and attributes. </p>
			<p>Equipped with this knowledge, let's see how we can read and query data from an HTML document.</p>
			<p>In this topic, we will cover the reading and parsing of web pages, but we do not request them from a live website. Instead, we read them from disk. A section on reading them from the internet will follow in a future chapter. </p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor182"/>Exercise 5.10: Reading an HTML File and Extracting Its Contents Using Beautiful Soup</h2>
			<p>In this exercise, we will do the simplest thing possible. We will import the <strong class="source-inline">Beautiful Soup</strong> or <strong class="source-inline">bs4</strong> library and then use it to read an HTML document. Then, we will examine the different kinds of objects it returns. While doing the exercises for this topic, you should have the example HTML file (called <strong class="source-inline">test.html</strong>) open in a text editor so that you can check for the different tags and their attributes and contents:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">bs4</strong> library:<p class="source-code">from bs4 import BeautifulSoup</p></li>
				<li>Please download the following test HTML file and save it on your disk, and then use <strong class="source-inline">bs4</strong> to read it from the disk:<p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    print(type(soup))</p><p class="callout-heading">Note</p><p class="callout">Throughout this exercise, don't forget to change the path (highlighted) of the HTML file based on its location on your system.</p><p>The output is as follows:</p><p class="source-code">&lt;class 'bs4.BeautifulSoup'&gt;</p><p>You can pass a file handler directly to the constructor of the <strong class="source-inline">BeautifulSoup</strong> object and it will read the contents from the file that the handler is attached to. We will see that the return type is an instance of <strong class="source-inline">bs4.BeautifulSoup</strong>. This class holds all the methods we need to navigate through the DOM tree that the document represents. </p></li>
				<li>Print the contents of the file in a nice way, by which we mean that the printing will keep some kind of nice indentation by using the <strong class="source-inline">prettify</strong> method from the class, like this:<p class="source-code">print(soup.prettify())</p><p>The output is as follows:</p><div id="_idContainer193" class="IMG---Figure"><img src="Images/B15780_05_028.jpg" alt="Figure 5.28: Contents of the HTML file&#13;&#10;" width="553" height="344"/></div><p class="figure-caption">Figure 5.28: Contents of the HTML file</p><p>The same information can also be obtained by using the <strong class="source-inline">soup.contents</strong> member variable. The differences are: first, it won't print anything pretty and, second, it is essentially a list. </p><p>If we look carefully at the contents of the HTML file in a separate text editor, we will see that there are many paragraph tags, or <strong class="source-inline">&lt;p&gt;</strong> tags. Let's read content from one such <strong class="source-inline">&lt;p&gt;</strong> tag. We can do that using the simple <strong class="source-inline">.</strong> access modifier as we would have done for a normal member variable of a class. </p><p>The magic of <strong class="source-inline">bs4</strong> is the fact that it gives us this excellent way to dereference tags as member variables of the <strong class="source-inline">BeautifulSoup</strong> class instance. In the following few steps, we are going to read an HTML file and then pass the file handler returned by Python's <strong class="source-inline">open</strong> call directly to the constructor of the <strong class="source-inline">BeautifulSoup</strong> class. It does a lot of things (including reading the content and then parsing it) and returns an instance of the class that we can then use.</p></li>
				<li>Read the HTML file:<p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    print(soup.p)</p><p>The output is as follows:</p><div id="_idContainer194" class="IMG---Figure"><img src="Images/B15780_05_029.jpg" alt="Figure 5.29: Text from the &lt;p&gt; tag &#13;&#10;" width="1028" height="624"/></div><p class="figure-caption">Figure 5.29: Text from the &lt;p&gt; tag </p><p>As we can see, this is the content of the <strong class="source-inline">&lt;p&gt;</strong> tag. </p><p>We saw how to read a tag in the last exercise, but we can easily see the problem with this approach. When we look into our HTML document, we can see that we have more than one <strong class="source-inline">&lt;p&gt;</strong> tag there. How can we access all the <strong class="source-inline">&lt;p&gt;</strong> tags? It turns out that this is easy. </p></li>
				<li>Use the <strong class="source-inline">findall</strong> method to extract the content from the tag:<p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    all_ps = soup.find_all('p')</p><p class="source-code">    print("Total number of &lt;p&gt;  --- {}".format(len(all_ps)))</p><p>The output is as follows:</p><p class="source-code">Total number of &lt;p&gt;  --- 6</p><p>This will print <strong class="source-inline">6</strong>, which is exactly the number of <strong class="source-inline">&lt;p&gt;</strong> tags in the document. </p><p>We have seen how to access all the tags of the same type. We have also seen how to get the content of the entire HTML document. </p></li>
				<li>Now we will see how to get the contents of a particular HTML tag: <p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    table = soup.table</p><p class="source-code">    print(table.contents)</p><p>The output is as follows:</p><div id="_idContainer195" class="IMG---Figure"><img src="Images/B15780_05_030.jpg" alt="Figure 5.30: Contents of the &lt;table&gt; tag &#13;&#10;" width="558" height="451"/></div><p class="figure-caption">Figure 5.30: Contents of the &lt;table&gt; tag </p><p>Here, we are getting the (first) table from the document and then using the same <strong class="source-inline">.</strong> notation to get the contents of that tag. We saw in the previous step that we can access the entire content of a particular tag. However, HTML is represented as a tree, and we are able to traverse the children of a particular node. There are a few ways to do this.</p></li>
				<li>The first way is by using the <strong class="source-inline">children</strong> generator from any <strong class="source-inline">bs4</strong> instance, as follows:<p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    table = soup.table</p><p class="source-code">    for child in table.children:</p><p class="source-code">        print(child)</p><p class="source-code">        print("*****")</p><p>When we execute the code, we will see something like the following:</p><div id="_idContainer196" class="IMG---Figure"><img src="Images/B15780_05_031.jpg" alt="Figure 5.31: Traversing the children of a table node&#13;&#10;" width="1076" height="916"/></div><p class="figure-caption">Figure 5.31: Traversing the children of a table node</p><p>It seems that the loop has only been executed twice. Well, the problem with the <strong class="source-inline">children</strong> generator is that it only takes into account the immediate children of the tag. We have <strong class="source-inline">&lt;tbody&gt;</strong> under <strong class="source-inline">&lt;table&gt;</strong>, and our whole table structure is wrapped in it. That's why it was considered a single child of the <strong class="source-inline">&lt;table&gt;</strong> tag. </p><p>We have looked into how to browse the immediate children of a tag. We will see how we can browse all the possible children of a tag and not only the immediate one. </p></li>
				<li>To do that, we use the <strong class="source-inline">descendants</strong> generator from the <strong class="source-inline">bs4</strong> instance, as follows: <p class="source-code">with open("<strong class="bold">../datasets/test.html</strong>", "r") as fd:</p><p class="source-code">    soup = BeautifulSoup(fd)</p><p class="source-code">    table = soup.table</p><p class="source-code">    children = table.children</p><p class="source-code">    des = table.descendants</p><p class="source-code">    print(len(list(children)), len(list(des)))</p><p>The output is as follows:</p><p class="source-code">9 61</p></li>
			</ol>
			<p>The comparison print at the end of the code block will show us the difference between <strong class="source-inline">children</strong> and <strong class="source-inline">descendants</strong>. The length of the list we got from <strong class="source-inline">children</strong> is only <strong class="source-inline">9</strong>, whereas the length of the list we got from <strong class="source-inline">descendants</strong> is <strong class="source-inline">61</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N994l6">https://packt.live/2N994l6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UT2p2K">https://packt.live/2UT2p2K</a>.</p>
			<p>So far, we have seen some basic ways to navigate the tags inside an HTML document using <strong class="source-inline">bs4</strong>. Now, we are going to go one step further and use the power of <strong class="source-inline">bs4</strong> combined with the power of <strong class="source-inline">pandas</strong> to generate a DataFrame out of a plain HTML table. With the knowledge we will acquire now, it will be fairly easy for us to prepare a <strong class="source-inline">pandas</strong> DataFrame to perform <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) or modeling. We are going to show you how to create a DataFrame with the extracted data from HTML using the <strong class="source-inline">BeautifulSoup</strong> library. </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor183"/>Exercise 5.11: DataFrames and BeautifulSoup</h2>
			<p>In this exercise, we will extract the data from the <strong class="source-inline">test.html</strong> page using the <strong class="source-inline">BeautifulSoup</strong> library. We will then perform a few operations for data preparation and display the data in an easily readable tabular format. To do that, let's go through the following steps:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> and read the document, as follows:<p class="source-code">import pandas as pd</p><p class="source-code">from bs4 import BeautifulSoup</p><p class="source-code">fd = open("<strong class="bold">../datasets/test.html</strong>", "r")</p><p class="source-code">soup = BeautifulSoup(fd)</p><p class="source-code">data = soup.findAll('tr')</p><p class="source-code">print("Data is a {} and {} items long".format(type(data),\</p><p class="source-code">      len(data)))</p><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the HTML file based on its location on your system.</p><p>The output is as follows:</p><p class="source-code">Data is a &lt;class 'bs4.element.ResultSet'&gt; and 4 items long</p></li>
				<li>Check the original table structure in the HTML source. You will see that the first row is the column heading and all of the following rows are the data from the HTML source. We'll assign two different variables for the two sections, as follows:<p class="source-code">data_without_header = data[1:]</p><p class="source-code">headers = data[0]</p><p class="source-code">headers</p><p>The output is as follows:</p><p class="source-code">&lt;tr&gt;</p><p class="source-code">&lt;th&gt;Entry Header 1&lt;/th&gt;</p><p class="source-code">&lt;th&gt;Entry Header 2&lt;/th&gt;</p><p class="source-code">&lt;th&gt;Entry Header 3&lt;/th&gt;</p><p class="source-code">&lt;th&gt;Entry Header 4&lt;/th&gt;</p><p class="source-code">&lt;/tr&gt;</p><p class="callout-heading">Note</p><p class="callout">Keep in mind that the art of scraping an HTML page goes hand in hand with an understanding of the source HTML structure. So, whenever you want to scrape a page, the first thing you need to do is right-click on it and then use <strong class="source-inline">View Source</strong> from the browser to see the source HTML.</p></li>
				<li>Once we have separated the two sections, we need two list comprehensions to make them ready to go in a DataFrame. For the header, this is easy:<p class="source-code">col_headers = [th.getText() for th in headers.findAll('th')]</p><p class="source-code">col_headers</p><p>The output is as follows:</p><p class="source-code">['Entry Header 1', 'Entry Header 2', 'Entry Header 3', 'Entry Header 4']</p><p>Data preparation is a bit tricky for a <strong class="source-inline">pandas</strong> DataFrame. You need to have a two-dimensional list, which is a list of lists. We accomplish that in the following way, using the tricks we learned earlier about list comprehension.</p></li>
				<li>Use the <strong class="source-inline">for…in</strong> loop to iterate over the data:<p class="source-code">df_data = [[td.getText() for td in tr.findAll('td')] \</p><p class="source-code">           for tr in data_without_header]</p><p class="source-code">df_data</p><p>The output is as follows:</p><div id="_idContainer197" class="IMG---Figure"><img src="Images/B15780_05_032.jpg" alt="Figure 5.32: Output as a two-dimensional list&#13;&#10;" width="562" height="164"/></div><p class="figure-caption">Figure 5.32: Output as a two-dimensional list</p></li>
				<li>Invoke the <strong class="source-inline">pd.DataFrame</strong> method and supply the right arguments by using the following code: <p class="source-code">df = pd.DataFrame(df_data, columns=col_headers)</p><p class="source-code">df.head()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="Images/B15780_05_033.jpg" alt="Figure 5.33: Output in tabular format with column headers&#13;&#10;" width="1074" height="251"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33: Output in tabular format with column headers</p>
			<p>Thus, we conclude our exercise on creating a data frame from an HTML table.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QyE6A">https://packt.live/30QyE6A</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hBPFY5">https://packt.live/3hBPFY5</a>.</p>
			<p>In the following exercise, we'll export a DataFrame as an Excel file.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor184"/>Exercise 5.12: Exporting a DataFrame as an Excel File</h2>
			<p>In this exercise, we will see how we can save a DataFrame as an Excel file. <strong class="source-inline">Pandas</strong> can do this natively, but it needs the help of the <strong class="source-inline">openpyxl</strong> library to achieve this goal. <strong class="source-inline">openpyxl</strong> is a Python library for reading/writing Excel 2010 <strong class="source-inline">xlsx/xlsm/xltx/xltm</strong> files. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is continued from the previous exercise. You'll need to continue in the same Jupyter Notebook.</p>
			<p>Let's perform the following steps:</p>
			<ol>
				<li value="1">Install the <strong class="source-inline">openpyxl</strong> library by using the following command:<p class="source-code">!pip install openpyxl</p></li>
				<li>To save the DataFrame as an Excel file, use the following command from inside of the Jupyter notebook:<p class="source-code">writer = pd.ExcelWriter('<strong class="bold">../datasets/test_output.xlsx</strong>')</p><p class="source-code">df.to_excel(writer, "Sheet1")</p><p class="source-code">writer.save()</p><p class="source-code">writer</p><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the Excel file based the folder structure of your local system.</p><p>The output is as follows:</p><p class="source-code">&lt;pandas.io.excel._XlsxWriter at 0x24feb2939b0&gt;</p></li>
			</ol>
			<p>This is the way in which we can export a <strong class="source-inline">pandas</strong> DataFrame to Excel. Given that Excel is a very popular format among many types of users, this is a very important trick you need to master.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YcSdV6">https://packt.live/2YcSdV6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YZTXjJ">https://packt.live/2YZTXjJ</a>.</p>
			<p>In the previous chapters, when we were discussing the stack, we explained how important it is to have a stack that we can push the URLs from a web page to so that we can pop them at a later time to follow each of them. The following exercise will demonstrate how to do that. </p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor185"/>Exercise 5.13: Stacking URLs from a Document Using bs4</h2>
			<p>In this exercise, we will append the URLs one after the other from the <strong class="source-inline">test.html</strong> web page. In that file, HTML file links or <strong class="source-inline">&lt;a&gt;</strong> tags are under a <strong class="source-inline">&lt;ul&gt;</strong> tag, and each of them is contained inside a <strong class="source-inline">&lt;/li&gt;</strong> tag. We are going to find all the <strong class="source-inline">&lt;a&gt;</strong> tags and create a stack with them. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is continued from the previous exercise. You'll need to continue in the same Jupyter Notebook.</p>
			<p>To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Find all the <strong class="source-inline">&lt;a&gt;</strong> tags by using the following command:<p class="source-code">d = open("<strong class="bold">../datasets/test.html</strong>", "r")</p><p class="source-code">soup = BeautifulSoup(d)</p><p class="source-code">lis = soup.find('ul').findAll('li')</p><p class="source-code">stack = []</p><p class="source-code">for li in lis:</p><p class="source-code">    a = li.find('a', href=True)</p><p class="callout-heading">Note</p><p class="callout">Don't forget to change the path (highlighted) of the HTML file based on its location on your system.</p></li>
				<li>Define a stack before you start the loop. Then, inside the loop, use the <strong class="source-inline">append</strong> method to push the links in the stack:<p class="source-code">stack.append(a['href'])</p></li>
				<li>Print the stack:<p class="source-code">print(stack)</p><p>The output is as follows:</p><p class="source-code">['https://www.imdb.com/chart/top']</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hCCAOj">https://packt.live/3hCCAOj</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3fCYNd0">https://packt.live/3fCYNd0</a>.</p></li>
			</ol>
			<p>Let's put together everything we have learned so far in this chapter and get started with an activity.</p>
			<h2 id="_idParaDest-180">Activity 5.01: Reading Tabular Da<a id="_idTextAnchor186"/>ta from a Web Page and Creating DataFrames</h2>
			<p>In this activity, you have been g<a id="_idTextAnchor187"/>iven a Wikipedia page where you have the GDP of all countries listed. You have to create three <strong class="source-inline">DataFrames</strong> from the three sources mentioned on the page (<a href="https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal">https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal</a>)).</p>
			<p>You will have to do the following:</p>
			<ol>
				<li value="1">Open the page in a separate Chrome/Firefox tab and use something like an <strong class="source-inline">Inspect Element</strong> tool to view the source HTML and understand its structure.</li>
				<li>Read the page using <strong class="source-inline">bs4</strong>.</li>
				<li>Find the table structure you will need to deal with (how many tables are there?).</li>
				<li>Find the right table using <strong class="source-inline">bs4</strong>.</li>
				<li>Separate the source names and their corresponding data.</li>
				<li>Get the source names from the list of sources you have created.</li>
				<li>Separate the header and data from the data that you separated before for the first source only, and then create a DataFrame using that.</li>
				<li>Repeat the last task for the other two data sources.</li>
			</ol>
			<p>The output should look like this:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="Images/B15780_05_034.jpg" alt="Figure 5.34: Final output&#13;&#10;" width="1566" height="547"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.34: Final output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor317">this link</a>.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor188"/>Summary</h1>
			<p>In this chapter, we have looked into several different types of data formats and how to work with them. These formats include CSV, PDF, Excel, Plain Text, and HTML. HTML documents are the cornerstone of the World Wide Web and, given the amount of data that's contained in it, we can easily infer the importance of HTML as a data source.</p>
			<p>We learned about <strong class="source-inline">bs4</strong> (<strong class="source-inline">BeautifulSoup 4</strong>), a Python library that gives us Pythonic ways to read and query HTML documents. We used bs4 to load an HTML document and explored several different ways to navigate the loaded document.</p>
			<p>We also looked at how we can create a <strong class="source-inline">pandas</strong> DataFrame from an HTML document (which contains a table). Although there are some built-in ways to do this job in <strong class="source-inline">pandas</strong>, they fail as soon as the target table is encoded inside a complex hierarchy of elements. So, the knowledge we gathered in this topic to transform an HTML table into a <strong class="source-inline">pandas</strong> DataFrame in a step-by-step manner is invaluable. </p>
			<p>Finally, we looked at how we can create a stack in our code, where we push all the URLs that we encounter while reading the HTML file and then use them at a later time. In the next chapter, we will discuss list comprehensions, the <strong class="source-inline">.zip</strong> format, and outlier detection and cleaning.</p>
		</div>
	</div></body></html>