["```py\n    $ mkdir words\n\n    ```", "```py\n    $ cd words\n\n    ```", "```py\n    $ echo \"to be or not to be\" > sh.txt\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> val words = sc.textFile(\"file:///home/hduser/words\")\n\n    ```", "```py\n    scala> words.count\n\n    ```", "```py\n    scala> val wordsFlatMap = words.flatMap(_.split(\"\\\\W+\"))\n\n    ```", "```py\n    scala> val wordsMap = wordsFlatMap.map( w => (w,1))\n\n    ```", "```py\n    scala> val wordCount = wordsMap.reduceByKey( (a,b) => (a+b))\n\n    ```", "```py\n    scala> wordCount.collect.foreach(println)\n\n    ```", "```py\n    scala> sc.textFile(\"file:///home/hduser/ words\"). flatMap(_.split(\"\\\\W+\")).map( w => (w,1)). reduceByKey( (a,b) => (a+b)).foreach(println)\n\n    ```", "```py\n    $ mkdir words\n\n    ```", "```py\n    $ cd words\n\n    ```", "```py\n    $ echo \"to be or not to be\" > sh.txt\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> val words = sc.textFile(\"hdfs://localhost:9000/user/hduser/words\")\n\n    ```", "```py\n    scala> words.count\n\n    ```", "```py\n    scala> val wordsFlatMap = words.flatMap(_.split(\"\\\\W+\"))\n\n    ```", "```py\n    scala> val wordsMap = wordsFlatMap.map( w => (w,1))\n\n    ```", "```py\n    scala> val wordCount = wordsMap.reduceByKey( (a,b) => (a+b))\n\n    ```", "```py\n    scala> wordCount.collect.foreach(println)\n\n    ```", "```py\n    scala> sc.textFile(\"hdfs://localhost:9000/user/hduser/words\"). flatMap(_.split(\"\\\\W+\")).map( w => (w,1)). reduceByKey( (a,b) => (a+b)).foreach(println)\n\n    ```", "```py\n    $ wget -r ftp://ftp.ncdc.noaa.gov/pub/data/noaa/\n\n    ```", "```py\n    $ hdfs dfs -put ftp.ncdc.noaa.gov/pub/data/noaa weather/\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> val weatherFileRDD = sc.wholeTextFiles(\"hdfs://localhost:9000/user/hduser/weather/1901\")\n\n    ```", "```py\n    scala> val weatherRDD = weatherFileRDD.cache\n\n    ```", "```py\n    scala> weatherRDD.count\n\n    ```", "```py\n    scala> val firstElement = weatherRDD.first\n\n    ```", "```py\n    scala> val firstValue = firstElement._2\n\n    ```", "```py\n    scala> val firstVals = firstValue.split(\"\\\\n\")\n\n    ```", "```py\n    scala> firstVals.size\n\n    ```", "```py\n    scala> val windSpeed = firstVals.map(line => line.substring(65,69)\n\n    ```", "```py\n    $ mkdir currency\n    ```", "```py\n    $ cd currency\n    ```", "```py\n    $ vi na.txt\n    United States of America        US Dollar\n    Canada  Canadian Dollar\n    Mexico  Peso\n\n    ```", "```py\n    $ hdfs dfs -put currency /user/hduser/currency\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> import org.apache.hadoop.io.Text\n    scala> import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\n\n    ```", "```py\n    val currencyFile = sc.newAPIHadoopFile(\"hdfs://localhost:9000/user/hduser/currency\",classOf[KeyValueTextInputFormat],classOf[Text],classOf[Text])\n\n    ```", "```py\n    val currencyRDD = currencyFile.map( t => (t._1.toString,t._2.toString))\n\n    ```", "```py\n    scala> currencyRDD.count\n\n    ```", "```py\n    scala> currencyRDD.collect.foreach(println)\n\n    ```", "```py\n    $ echo \"to be or not to be\" > sh.txt\n\n    ```", "```py\n    scala>  val words = sc.textFile(\"s3n://com.infoobjects.wordcount/words\")\n\n    ```", "```py\n    cqlsh> CREATE KEYSPACE people WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };\n\n    ```", "```py\n    cqlsh> create columnfamily person(id int primary key,first_name varchar,last_name varchar);\n\n    ```", "```py\n    cqlsh> insert into person(id,first_name,last_name) values(1,'Barack','Obama');\n    cqlsh> insert into person(id,first_name,last_name) values(2,'Joe','Smith');\n\n    ```", "```py\n    \"com.datastax.spark\" %% \"spark-cassandra-connector\" % 1.2.0\n\n    ```", "```py\n    <dependency>\n      <groupId>com.datastax.spark</groupId>\n      <artifactId>spark-cassandra-connector_2.10</artifactId>\n      <version>1.2.0</version>\n    </dependency>\n    ```", "```py\n    $ wget http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.1.0/spark-cassandra-connector_2.10-1.2.0.jar\n\n    ```", "```py\n    scala> sc.getConf.set(\"spark.cassandra.connection.host\", \"localhost\")\n\n    ```", "```py\n    scala> import com.datastax.spark.connector._\n\n    ```", "```py\n    scala> val personRDD = sc.cassandraTable(\"people\",\"person\")\n\n    ```", "```py\n    scala> personRDD.count\n\n    ```", "```py\n    scala> personRDD.collect.foreach(println)\n\n    ```", "```py\n    scala> val firstRow = personRDD.first\n\n    ```", "```py\n    scala> firstRow.columnNames\n\n    ```", "```py\n    scala> val cc = new org.apache.spark.sql.cassandra.CassandraSQLContext(sc)\n\n    ```", "```py\n    scala> val p = cc.sql(\"select * from people.person\")\n\n    ```", "```py\n    scala> p.collect.foreach(println)\n\n    ```", "```py\n    $ mkdir uber\n\n    ```", "```py\n    $ cd uber\n\n    ```", "```py\n    $ sbt\n\n    ```", "```py\n    > set name := \"sc-uber\"\n\n    ```", "```py\n    > session save\n\n    ```", "```py\n    > exit\n\n    ```", "```py\n    $ vi buid.sbt\n\n    ```", "```py\n    addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.12.0\")\n\n    ```", "```py\n    $ sbt assembly\n\n    ```", "```py\n    $ mv thirdparty/sc-uber-assembly-0.1-SNAPSHOT.jar  thirdparty/sc-uber.jar\n\n    ```", "```py\n    $ spark-shell --jars thirdparty/sc-uber.jar\n\n    ```", "```py\n    $ spark-submit --jars thirdparty/sc-uber.jar\n\n    ```", "```py\n    CREATE TABLE 'person' (\n      'person_id' int(11) NOT NULL AUTO_INCREMENT,\n      'first_name' varchar(30) DEFAULT NULL,\n      'last_name' varchar(30) DEFAULT NULL,\n      'gender' char(1) DEFAULT NULL,\n      PRIMARY KEY ('person_id');\n    )\n    ```", "```py\n    Insert into person values('Barack','Obama','M');\n    Insert into person values('Bill','Clinton','M');\n    Insert into person values('Hillary','Clinton','F');\n    ```", "```py\n    $ spark-shell --jars /path-to-mysql-jar/mysql-connector-java-5.1.29-bin.jar\n\n    ```", "```py\n    scala> val url=\"jdbc:mysql://localhost:3306/hadoopdb\"\n    scala> val username = \"hduser\"\n    scala> val password = \"******\"\n\n    ```", "```py\n    scala> import org.apache.spark.rdd.JdbcRDD\n\n    ```", "```py\n    scala> import java.sql.{Connection, DriverManager, ResultSet}\n\n    ```", "```py\n    scala> Class.forName(\"com.mysql.jdbc.Driver\").newInstance\n\n    ```", "```py\n    scala> val myRDD = new JdbcRDD( sc, () =>\n    DriverManager.getConnection(url,username,password) ,\n    \"select first_name,last_name,gender from person limit ?, ?\",\n    1, 5, 2, r => r.getString(\"last_name\") + \", \" + r.getString(\"first_name\"))\n\n    ```", "```py\n    scala> myRDD.count\n    scala> myRDD.foreach(println)\n\n    ```", "```py\n    scala> myRDD.saveAsTextFile(\"hdfs://localhost:9000/user/hduser/person\")\n\n    ```", "```py\nJdbcRDD( SparkContext, getConnection: () => Connection,\nsql: String, lowerBound: Long, upperBound: Long,\nnumPartitions: Int,  mapRow: (ResultSet) => T =\n JdbcRDD.resultSetToObjectArray)\n\n```"]