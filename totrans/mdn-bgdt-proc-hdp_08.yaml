- en: Building Enterprise Search Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After learning data ingestions and data persistence approaches, let''s learn
    about searching the data. In this chapter, we will learn about the following important
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Data search techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building real-time search engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching real-time, full-text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data indexing techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a real-time data search pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data search concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our everyday life, we always keep on searching something. In the morning,
    we search for a toothbrush, newspaper, search stock prices, bus schedule, office
    bag, and so on. The list goes on and on. This search activity stops when we go
    to bed at the end of the day. We use a lot of tools and techniques to search these
    things to minimize the actual search time. We use Google to search most of the
    things such as news, stock prices, bus schedule, and anything and everything we
    need. To search a particular page of a book, we use the book's index. So, the
    point is that search is a very important activity of our life. There are two important
    concepts can be surfaced out of this, that is, search tool and search time. Just
    think of a situation where you want to know about a particular stock price of
    a company and it takes a few minutes to load that page. You will definitely get
    very annoyed. It is because the Search Time in this case is not acceptable to
    you. So then the question is, *How to reduce this search time?* We will learn
    that in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The need for an enterprise search engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like we all need a tool to search our own things, every company also needs
    a search engine to build so that internal and external entities can find what
    they want.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an employee has to search for his/her PTO balance, paystub of a
    particular month, and so on. The HR department may search for employees who are
    in finance group or so. In an e-commerce company, a product catalog is the most
    searchable object. It is a very sensitive object because it directly impacts the
    revenue of the company. If a customer wants to buy a pair of shoes, the first
    thing he/she can do is search the company product catalog. If the search time
    is more than a few seconds, the customer may lose interest in the product. It
    may also be possible that the same customer goes to another website to buy a pair
    of shoes, resulting in a loss of revenue.
  prefs: []
  type: TYPE_NORMAL
- en: 'It appears that even with all the tech and data in the world, we can''t do
    much without two crucial components:'
  prefs: []
  type: TYPE_NORMAL
- en: Data search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Companies such as Google, Amazon, and Apple have changed the word's expectations
    of search. We all expect them to search anything, anytime, and using any tool
    such as website, mobile, and voice-activated tools like Google Echo, Alexa, and
    HomePad. We expect these tools to answer all our questions, from *How's the weather
    today?* to give me a list of gas stations near me.
  prefs: []
  type: TYPE_NORMAL
- en: As these expectations are growing, the need to index more and more data is also
    growing.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for building an enterprise search engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some popular tools/products/technologies available:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Lucene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Solr
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom (in-house) search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, I will focus on Elasticsearch in detail. I will discuss Apache
    Solr on a conceptual level only.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is an open source search engine. It is based on Apache Lucene.
    It is distributed and supports multi-tenant capability. It uses schema-free JSON
    documents and has a built-in, HTTP-based web interface. It also supports analytical
    RESTful query workloads. It is a Java-based database server. Its main protocol
    is HTTP/JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Why Elasticsearch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch is the most popular data indexing tool as of today. It is because
    of its the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: It is **fast**. Data is indexed at a real-time speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is **scalable**. It scales horizontally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is **flexible**. It supports any data format, structured, semi-structured,
    or unstructured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is **distributed**. If one node fails, the cluster is still available for
    business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It supports data search query in any language: Java, Python Ruby, C#, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a **Hadoop connector,** which facilitates smooth communication between
    Elasticsearch and Hadoop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports robust data **aggregation** on huge datasets to find trends and
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Elastic stack** (Beats, Logstash, Elasticsearch, and Kibana) and X-Pack
    offers out-of-the-box support for data ingestion, data indexing, data visualization,
    data security, and monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we take a deep dive, let's understand a few important components of Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch index is a collection of JSON documents. Elasticsearch is a data
    store that may contain multiple indices. Each index may be divided into one or
    many types. A type is a group of similar documents. A type may contain multiple
    documents. In terms of database analogy, an index is a database and each of its
    types is a table. Each JSON document is a row in that table.
  prefs: []
  type: TYPE_NORMAL
- en: Indices created in Elasticsearch 6.0.0 or later may only contain a single mapping
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping types will be completely removed in Elasticsearch 7.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: Document
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Document in Elasticsearch means a JSON document. It is a basic unit of data
    to be stored in an index. An index comprises multiple documents. In the RDBMS
    world, a document is nothing but a row in a table. For example, a customer document
    may look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mapping is schema definition of an index. Just like a database, we have to define
    a data structure of a table. We have to create a table, its columns, and column
    data types. In Elasticsearch, we have define a structure of an index during its
    creation. We may have to define which field can be indexed, searchable, and storable.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that, Elasticsearch supports **dynamic mapping**. It means
    that mapping is not mandatory at index creation time. An index can be created
    without mapping. When a document is sent to Elasticsearch for indexing, Elasticsearch
    automatically defines the data structure of each field and make each field a searchable
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch is a collection of nodes (servers). Each node may store part
    of the data in index and provides federated indexing and search capabilities across
    all nodes. Each cluster has a unique name, `elasticsearch`, by default. A cluster
    is divided into multiple types of nodes, namely Master Node and Data Node. But
    an Elasticsearch cluster can be created using just one node having both Master
    and Data nodes installed on the same node:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master node**: This controls the entire cluster. There can be more than one
    master node in a cluster (three are recommended). Its main function is index creation
    or deletion and allocation of shards (partitions) to data nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data node**: This stores the actual index data in shards. They support all
    data-related operations such as aggregations, index search, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documents are divided into various logical types for example, order document,
    product document, customer document, and so on. Instead of creating three separate
    order, product, and customer indices, a single index can be logically divided
    into order, product and customer types. In RDBMS analogy, a type is nothing but
    a Table in a database. So, a type is a logical partition of an index.
  prefs: []
  type: TYPE_NORMAL
- en: Type is deprecated in Elasticsearch version 6.0.
  prefs: []
  type: TYPE_NORMAL
- en: How to index documents in Elasticsearch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's learn how Elasticsearch actually works by indexing these three sample
    documents. While learning this, we will touch upon a few important functions/concepts
    of Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the three sample JSON documents to be indexed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first. Let's install Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Please do the following steps to install Elasticsearch on your server. It is
    assumed that you are installing Elasticsearch using CentOS 7 on your server.
  prefs: []
  type: TYPE_NORMAL
- en: What minimum Hardware is required?
  prefs: []
  type: TYPE_NORMAL
- en: '**RAM**: 4 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which JDK needs to be installed? We need JDK 8\. If you don''t have JDK 8 installed
    on your server, do the following steps to install JDK 8:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change to home folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Download JDK RPM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Install RMP using YUM (it is assumed that you have `sudo` access):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since, we have installed JDK 8 on our server successfully, let's start installing
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For detailed installation steps, please refer to the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html**](https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RPM for Elasticsearch v6.2.3 can be downloaded from the website and installed
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To configure Elasticsearch to start automatically when the system boots up,
    run the following commands.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Elasticsearch can be started and stopped as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The main configuration file is located in the config folder called `elasticsearch.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do the following initial config changes in `elasticsearch.yml`. Find
    and replace the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now start Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Check whether Elasticsearch is running using the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, our Elasticsearch is working fine. Let's create an index to store our documents.
  prefs: []
  type: TYPE_NORMAL
- en: Create index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following `curl` command to create our first index named `my_index`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the index creation URL, we used settings, shards, and replica. Let's understand
    what is meant by shard and replica.
  prefs: []
  type: TYPE_NORMAL
- en: Primary shard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have created index with three shards. It means Elasticsearch will divide
    index into three partitions. Each partition is called a **shard**. Each shard
    is a full-fledged, independent Lucene index. The basic idea is that Elasticsearch
    will store each shard on a separate data node to increase the scalability. We
    have to mention how many shards we want at the time of index creation. Then, Elasticsearch
    will take care of it automatically. During document search, Elasticsearch will
    aggregate all documents from all available shards to consolidate the results so
    as to fulfill a user search request. It is totally transparent to the user. So
    the concept is that index can be divided into multiple shards and each shard can
    be hosted on each data node. The placement of shards will be taken care of by
    Elasticsearch itself. If we don't specify the number of shards in the index creation
    URL, Elasticsearch will create five shards per index by default.
  prefs: []
  type: TYPE_NORMAL
- en: Replica shard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have created index with one replica. It means Elasticsearch will create one
    copy (replica) of each shard and place each replica on separate data node other
    than the shard from which it is copied. So, now there are two shards, primary
    shard (the original shard) and replica shard (the copy of the primary shard).
    During a high volume of search activity, Elasticsearch can provide query results
    either from primary shards or from replica shards placed on different data nodes.
    This is how Elasticsearch increases the query throughput because each search query
    may go to different data nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the summary, both, primary shards and replica shards provide horizontal scalability
    and throughput. It scales out your search volume/throughput since searches can
    be executed on all replicas in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is a distributed data store. It means data can be divided into
    multiple data nodes. For example, assume if we have just one data node and we
    keep on ingesting and indexing documents on the same data node, it may possible
    that after reaching out the hardware capacity of that node, we will not be to
    ingest documents. Hence, in order to accommodate more documents, we have to add
    another data node to the existing Elasticsearch cluster. If we add another data
    node, Elasticsearch will re-balance the shards to the newly created data node.
    So now, user search queries can be accommodated to both the data nodes. If we
    created one replica shard, then two replicas per shard will be created and placed
    on these two data nodes. Now, if one of the data nodes goes down, then still user
    search queries will be executed using just one data node.
  prefs: []
  type: TYPE_NORMAL
- en: 'This picture shows how user search queries are executed from both the data
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8d46b2c-601b-444e-90d9-3b2912de3bd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following picture shows that even if data nodes **A** goes down, still,
    user queries are executed from data node **B**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29ff9261-0355-4bbb-8702-cc2758b6827d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s verify the newly created index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Health:** This means the overall cluster health is yellow. There are three
    statuses: green, yellow, and red. The status `Green`" means the cluster is fully
    functional and everything looks good. The status "Yellow" means cluster is fully
    available but some of the replicas are not allocated yet. In our example, since
    we are using just one node and 5 shards and 1 replica each, Elasticsearch will
    not allocate all the replicas of all the shards on just one data node. The cluster
    status "Red" means cluster is partially available and some datasets are not available.
    The reason may be that the data node is down or something else.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Status**: `Open`. It means the cluster is open for business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index** : Index name. In our example, the index name is `my_index`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uuid** : This is unique index ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pri** : Number of primary shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rep** : Number of replica shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**docs.count** : Total number of documents in an index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**docs.deleted** : Total number of documents deleted so far from an index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**store.size** : The store size taken by primary and replica shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pri.store.size** : The store size taken only by primary shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingest documents into index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following `curl` command can be used to ingest a single document in the
    `my_index` index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the previous command, we use a type called `customer`, which is a logical
    partition of an index. In a RDBMS analogy, a type is like a table in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we used the number `1` after the type customer. It is an ID of a customer.
    If we omit it, then Elasticsearch will generate an arbitrary ID for the document.
  prefs: []
  type: TYPE_NORMAL
- en: We have multiple documents to be inserted into the `my_index` index. Inserting
    documents one by one in the command line is very tedious and time consuming. Hence,
    we can include all the documents in a file and do a bulk insert into `my_index`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `sample.json` file and include all the three documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Bulk Insert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s ingest all the documents in the file `sample.json` at once using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s verify all the records using our favorite browser. It will show all
    the three records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Document search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we have documents in our `my_index` index, we can search these documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find out a document where `city = " Los Angeles?` and query is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we analyze the response, we can see that the source section gives a back
    the document we were looking for. The document is in the index `my_index`, `"_type"
    : "customer"`, `"_id" : "3"`. Elasticsearch searches all `three _shards` successfully.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `hits` section, there is a field called `_score`. Elasticsearch calculates
    the relevance frequency of each field within a document and stores it in index.
    It is called the weight of the document. This weight is calculated based on four
    important factors: term frequency, inverse frequency, document frequency, and
    field length frequency. This brings up another question, *How does Elasticsearch
    index a document?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have the following four documents to index in Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: I love Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch is a document store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase is key value data store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I love HBase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Term** | **Frequency** | **Document No.** |'
  prefs: []
  type: TYPE_TB
- en: '| a | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| index | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Elasticsearch | 2 | 1,2 |'
  prefs: []
  type: TYPE_TB
- en: '| HBase | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| I | 2 | 1,4 |'
  prefs: []
  type: TYPE_TB
- en: '| is | 2 | 2,3 |'
  prefs: []
  type: TYPE_TB
- en: '| Key | 1 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| love | 2 | 1,4 |'
  prefs: []
  type: TYPE_TB
- en: '| store | 2 | 2,3 |'
  prefs: []
  type: TYPE_TB
- en: '| value | 1 | 3 |'
  prefs: []
  type: TYPE_TB
- en: When we ingest three documents in Elasticsearch, an Inverted Index is created,
    like the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to query term Elasticsearch, then only two documents need to
    be searched: 1 and 2\. If we run another query to find *love Elasticsearch*, then
    three documents need to be searched (documents 1,2, and 4) before sending the
    results from only the first document.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, there is one more important concept we need to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Meta fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we ingest a document into index, Elasticsearch adds a few meta fields
    to each index document. The following is the list of meta fields with reference
    to our sample `my_index`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_index`: Name of the index. `my_index`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_type`: Mapping type. "customer" (deprecated in version 6.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_uid`: `_type + _id` (deprecated in version 6.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_id`: `document_id` (1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_all`: This concatenates all the fields of an index into a searchable string
    (deprecated in version 6.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_ttl`: Life a document before it can be automatically deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_timestamp`: Provides a timestamp for a document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_source`: This is an actual document, which is automatically indexed by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In RDBMS analogy, mapping means defining a table schema. We always define a
    table structure, that is, column data types. In Elasticsearch, we also need to
    define the data type for each field. But then comes another question. Why did
    we not define it before when we ingested three documents into the `my_index` index?
    The answer is simple. Elasticsearch doesn't care. It is claimed that *Elasticsearch
    is a schema-less data model*.
  prefs: []
  type: TYPE_NORMAL
- en: If we don't define a mapping, Elasticsearch dynamically creates a mapping for
    us by defining all fields as text. Elasticsearch is intelligent enough to find
    out date fields to assign the `date` data type to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find the existing dynamic mapping of index `my_index`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Elasticsearch supports two mapping types as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Static mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In static mapping, we always know our data and we define the appropriate data
    type for each field. Static mapping has to be defined at the time of index creation.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already used dynamic mapping for our documents in our example. Basically,
    we did not define any data type for any field. But when we ingested documents
    using `_Bulk` load, Elasticsearch transparently defined `text` and `date` data
    types appropriately for each field. Elasticsearch intelligently found our `Birthdate` as
    a date field and assigned the `date` data type to it.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch-supported data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following spreadsheet summarizes the available data types in Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Common** | **Complex** | **Geo** | **Specialized** |'
  prefs: []
  type: TYPE_TB
- en: '| String | Array | Geo_Point | `ip` |'
  prefs: []
  type: TYPE_TB
- en: '| Keyword | Object (single Json) | Geo_Shape | `completion` |'
  prefs: []
  type: TYPE_TB
- en: '| Date | Nested (Json array) |  | `token_count` |'
  prefs: []
  type: TYPE_TB
- en: '| Long |  |  | `join` |'
  prefs: []
  type: TYPE_TB
- en: '| Short |  |  | `percolator` |'
  prefs: []
  type: TYPE_TB
- en: '| Byte |  |  | `murmur3` |'
  prefs: []
  type: TYPE_TB
- en: '| Double |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Float |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Boolean |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Binary |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Integer_range |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Float_range |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Long_range |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Double_range |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Date_range |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Most of the data types need no explanation. But the following are a few explanations
    for specific data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Geo-Point**:You can define latitude and longitude points here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geo-Shape**:This is for defining shapes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completion**:This data type is for defining auto completion of words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Join**: To define parent/child relationships'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percolator**:This is for query-dsl'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Murmur3**:During index time, it is for calculations hash value and store
    it into index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s re-create another index, `second_index`, which is similar to our `first_index` with
    static mapping, where we will define the data type of each field separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let's understand the preceding mapping. We disable the `_source` field for the
    customer type. It means, we get rid of the default behavior, where Elasticsearch
    stores and indexes the document by default. Now, since we have disabled it, we
    will deal with each and every field separately to decide whether that field should
    be indexed stored or both.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the preceding example, we want to store only three fields, `name`, `state`
    and `zip`. Also, we don't want to index the `state` and `zip` fields. It means
    `state` and `zip` fields are not searchable.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already learned about an inverted index. We know that Elasticsearch
    stores a document into an inverted index. This transformation is known as analysis.
    This is required for a successful response of the index search query.
  prefs: []
  type: TYPE_NORMAL
- en: Also, many of the times, we need to use some kind of transformation before sending
    that document to Elasticsearch index. We may need to change the document to lowercase,
    stripping off HTML tags if any from the document, remove white space between two
    words, tokenize the fields based on delimiters, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch offers the following built-in analyzers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard analyzer**:It is a default analyzer. This uses standard tokenizer
    to divide text. It normalizes tokens, lowercases tokens, and also removes unwanted
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple analyzer**:This analyzer is composed of lowercase tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whitespace analyzer**: This uses the whitespace tokenizer to divide text
    at spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language analyzers**:Elasticsearch provides many language-specific analyzers
    such as English, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fingerprint analyzer**:The fingerprint analyzer is a specialist analyzer.
    It creates a fingerprint, which can be used for duplicate detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pattern analyzer**:The pattern analyzer uses a regular expression to split
    the text into terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop analyzer**:This uses letter tokenizer to divide text. It removes stop
    words from token streams. for example, all stop words like a, an, the, is and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keyword analyzer**:This analyzer tokenizes an entire stream as a single token.
    It can be used for zip code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Character filter**: Prepare a string before it is tokenize. Example: remove
    html tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenizer**: MUST have a single tokenizer. It''s used to break up the string
    into individual terms or tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token filter**: Change, add or remove tokens. Stemmer is a token filter,
    it is used to get base of word, for example: learned, learning => learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of atandard analyzer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of simple analyzer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch stack components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Elasticsearch stack consists of following
  prefs: []
  type: TYPE_NORMAL
- en: Beats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's study them in brief.
  prefs: []
  type: TYPE_NORMAL
- en: Beats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please refer to the following URL to know more about beats: [https://www.elastic.co/products/beats](https://www.elastic.co/products/beats).
  prefs: []
  type: TYPE_NORMAL
- en: Beats are lightweight data shippers. Beats are installed on to servers as agents.
    Their main function is collect the data and send it to either Logstash or Elasticsearch.
    We can configure beats to send data to Kafka topics also.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple beats. Each beat is meant for collecting specific datasets
    and metrics. The following are various types of Beats:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filebeat**:For collection of log files. They simplify the collection, parsing,
    and visualization of common log formats down to a single command. Filebeat comes
    with internal modules (auditd, Apache, nginx, system, and MySQL).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metricbeat**:For collection of metrics. They collect metrics from any systems
    and services, for example, memory, COU, and disk. Metricbeat is a lightweight
    way to send system and service statistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Packetbeat**: This is for collection of network data. Packetbeat is a lightweight
    network packet analyzer that sends data to Logstash or Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Winlogbeat**:For collection of Windows event data. Winlogbeat live-streams
    Windows event logs to Elasticsearch and Logstash.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auditbeat**:For collection of audit data. Auditbeat collects audit framework
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heartbeat**:For collection of uptime monitoring data. Heartbeat ships this
    information and response time to Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installation of Filebeat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logstash is a lightweight, open source data processing pipeline. It allows collecting
    data from a wide variety of sources, transforming it on the fly, and sending it
    to any desired destination.
  prefs: []
  type: TYPE_NORMAL
- en: It is most often used as a data pipeline for Elasticsearch, a popular analytics
    and search engine. Logstash is a popular choice for loading data into Elasticsearch
    because of its tight integration, powerful log processing capabilities, and over
    200 prebuilt open source plugins that can help you get your data indexed the way
    you want it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a structure of `Logstash.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Installation of Logstash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kibana is an open-source data visualization and exploration tool used for log
    and time series analytics, application monitoring, and operational intelligence
    use cases. Kibana offers tight integration with Elasticsearch, a popular analytics
    and search engine, which makes Kibana the default choice for visualizing data
    stored in Elasticsearch. Kibana is also popular due to its powerful and easy-to-use
    features such as histograms, line graphs, pie charts, heat maps, and built-in
    geospatial support**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we have an application deployed on an application server.
    That application is logging on to an access log. Then how can we analyze this
    access log using a dashboard? We would like to create a real-time visualization
    of the following info:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of various response codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of IPs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proposed technology stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filebeat**: To read access log and write to Kafka topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka:** Message queues and o buffer message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash:** To pull messages from Kafka and write to Elasticsearch index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch**: For indexing messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**: Dashboard visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to solve this problem, we install filebeat on Appserver. Filebeat will
    read each line from the access log and write to the kafka topic in real time.
    Messages will be buffered in Kafka. Logstash will pull messages from the Kafka
    topic and write to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana will create real-time streaming dashboard by reading messages from Elasticsearch
    index. The following is the architecture of our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/70bfafda-6da2-425d-bcbd-39bfe676906d.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the step-by-step code sample, `Acccss.log`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete `Filebeat.ymal`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Kafka output section, we have mentioned Kafka broker details. `output.kafka`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete `Filebeat.ymal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We have to create a `logs-topic` topic in Kafka before we start ingesting messages
    into it. It is assumed that we have already installed Kafka on the server. Please
    refer to [Chapter 2](b17d8b55-1716-471b-aa8c-daf6d590172e.xhtml), *Hadoop Life
    Cycle Management* to read more about Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create logs-topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the `Logstash.conf` (to read messages from Kafka and push
    them to Elasticseach):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Kafka section, we''ve mentioned the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the filter section**,** we are converting each message into JSON format.
    After that, we are parsing each message and dividing it into multiple fields such
    as `ip`, `timestamp`, and `status`. Also, we add the application name `myapp` field
    to each message.
  prefs: []
  type: TYPE_NORMAL
- en: In the output section, we are writing each message to Elasticsearch. The index
    name is `log_index-YYYY-MM-dd`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you looked at the basic concepts and components of an Elasticsearch
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we discussed how Elasticsearch indexes a document using inverted
    index. We also discussed mapping and analysis techniques. We learned how we can
    denormalize an event before ingesting into Elasticsearch. We discussed how Elasticsearch
    uses horizontal scalability and throughput. After learning about Elasticstack
    components such as Beats, Logstash, and Kibana, we handled a live use case, where
    we demonstrated how access log events can be ingested into Kafka using Filebeat.
    We developed a code to pull messages from Kafka and ingest into Elasticsearch
    using Logstash. At the end, we learned data visualization using Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to build analytics to design data visualization
    solutions that drive business decisions.
  prefs: []
  type: TYPE_NORMAL
