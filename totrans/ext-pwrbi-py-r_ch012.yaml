- en: '11 Adding Statistics Insights: Associations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you saw in the previous chapter, enriching your data can be done through
    the application of complex algorithms. In addition to distances and linear programming,
    statistics can often be the ultimate weapon for data analysis. We will explain
    the basic concepts of some statistical procedures that aim to extract relevant
    insights about the associations between variables from your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring associations between variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation between numeric variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation between categorical and numeric variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter requires you to have a working internet connection and **Power
    BI Desktop** already installed on your machine. You must have properly configured
    the R and Python engines and IDEs as outlined in *Chapter 2*, *Configuring R with
    Power BI*, and *Chapter 3*, *Configuring Python with Power BI*.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring associations between variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At first glance, you might wonder what the point of finding relationships between
    variables is. Being able to understand the behavior of a pair of variables and
    identify a pattern in their behavior helps business owners identify key factors
    to divert certain indicators of company health to their benefit. Knowing the pattern
    that binds the trend of two variables gives you the power to predict with some
    certainty one of them by knowing the other. So, knowing the tools to uncover these
    patterns gives you a kind of analytical superpower that is always appealing to
    business owners.
  prefs: []
  type: TYPE_NORMAL
- en: In general, two variables are **associated** with each other when the values
    of one of them are in some way related to the values of the other. When you can
    somehow measure the extent of the association of two variables, it is called **correlation**.
    The concept of correlation is immediately applicable in a case where the two variables
    are numerical. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation between numeric variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we generally do to understand whether there is an association
    between two numeric variables is to represent them on the two Cartesian axes in
    order to obtain a **scatterplot**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – A simple scatterplot](img/file266.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – A simple scatterplot
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a scatterplot, it is possible to identify three important characteristics
    of a possible association:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direction**: It can be *positive* (increasing), *negative* (decreasing),
    or *not defined* (no association found or increasing and decreasing). If the increment
    of a variable corresponds to the increment of the other, the direction is positive;
    otherwise, it is negative:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Direction types of the association](img/file267.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.2 – Direction types of the association
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Form**: It describes the general form that association takes in its simplest
    sense. Obviously, there are many possible forms, but there are some that are more
    common, such as *linear* and *curvilinear* (nonlinear) forms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Shapes of the association](img/file268.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.3 – Shapes of the association
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Strength**: The strength of an association is determined by how close the
    points in the scatterplot follow the line that draws the generic shape of the
    association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Strength of the association](img/file269.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Strength of the association
  prefs: []
  type: TYPE_NORMAL
- en: As these visual patterns become measurable through the application of mathematical
    concepts, we can define different types of correlations between numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: Karl Pearson’s correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pearson’s correlation coefficient** (**r**) measures the degree of linearity
    of the association between the variables under analysis. This means that if *r
    = 1* or *r = -1*, then the two variables are in a *perfect linear relationship*.
    The sign of the coefficient determines the direction of the association. If instead
    *r is close to 0* indifferently on the negative or positive side, it means that
    the association between the variables is *very weak*. The coefficient cannot take
    values outside the range [-1, 1].'
  prefs: []
  type: TYPE_NORMAL
- en: The square of Pearson's coefficient (*R²*) is called the **coefficient of determination**
    and measures the percentage of **variance** (which measures the dispersion of
    observations from their mean value) in one variable due to the variance of the
    other variable, assuming the association is linear. For example, if the correlation
    coefficient *r* between a person's weight and height variables measures 0.45,
    it can be said that about 20% (0.45 x 0.45) of the change (variance) in a person's
    weight is due to the change in their height.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the correlation coefficient r is rarely done by hand, as any data
    management platform provides a function that easily calculates it. If you are
    curious, given a dataset of *n* entities, identify the variables *x* and *y* for
    which you want to calculate the correlation coefficient – this is the formula
    that allows you to calculate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Formula for the Pearson correlation coefficient](img/file270.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Formula for the Pearson correlation coefficient
  prefs: []
  type: TYPE_NORMAL
- en: The *x̄* and *ȳ* values correspond to the mean values of the homonymous variables
    in the dataset. Keep in mind that Pearson’s correlation function is **symmetric**,
    meaning that the order of the columns for which it is calculated does not matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of the correlation coefficient *r* calculated for some specific associations
    (**Boigelot distributions**) are shown in *Figure 11.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Pearson’s correlation calculated over Boigelot distributions](img/file271.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Pearson’s correlation calculated over Boigelot distributions
  prefs: []
  type: TYPE_NORMAL
- en: The first row of scatterplots in *Figure 11.6* gives an idea of how the magnitude
    of the correlation measures the strength of the association tending toward a linear
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: The second row shows that, regardless of the angle of the linear relationship,
    the correlation coefficient *r* is always equal to 1 in absolute value, that is,
    it always correctly identifies the linear relationship and its direction. The
    only exception is the case of the horizontal linear relationship at the center,
    for which the coefficient *r* is undefined, since for all values of *x* there
    is a single value of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third row shows one of the most important limitations of the correlation
    coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pearson's correlation coefficient is not capable of detecting a pattern in an
    association between two variables as much as the latter are nonlinear.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'These are the other limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: With a very small dataset size (say, 3-6 observations), it might appear that
    an association is present even though it does not exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The correlation coefficient is very sensitive to outliers (observations located
    far from most others). It can also happen that outliers give a false idea of the
    existence of association:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Wrong correlation values due to outliers](img/file272.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.7 – Wrong correlation values due to outliers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will cover outliers in more detail in *Chapter 12*, *Adding Statistics Insights,
    Outliers, and Missing Values*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If the observations in the dataset are divided into different clusters, this
    may induce a false sense of association:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Wrong correlation value due to clusters](img/file273.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.8 – Wrong correlation value due to clusters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The variables used in the calculation of the correlation coefficient must be
    defined on a continuous scale. For variables based on a discrete scale, such as
    the 1-10 rating of a service, you must use Spearman's rank correlation, which
    we will look at in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If in an association there is a variable that has unequal variability with
    a range of values of a second variable, we are faced with a case of **heteroscedasticity**.
    The scatterplot assumes the typical shape of a cone, like the one in *Figure 11.1*.
    In this case, the correlation coefficient could identify an incorrect linear relationship
    (more than one linear relationship would satisfy the conic form of the scatterplot):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Wrong correlation value due to heteroscedasticity](img/file274.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.9 – Wrong correlation value due to heteroscedasticity
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The variables involved in calculating the Pearson correlation coefficient should
    not be highly **skewed**, that is, the distribution of the variables must not
    be distorted or asymmetrical with respect to a symmetrical bell curve (normal
    distribution, where the mean, median, and mode are equal), otherwise a reduction
    in the true size of the correlation magnitude could occur:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Distribution skewness types](img/file275.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Distribution skewness types
  prefs: []
  type: TYPE_NORMAL
- en: 'Having clarified these limitations, the first question that comes up is: how
    can I calculate the correlation between two numeric variables when the association
    is nonlinear, when the variables are based on an ordinal scale, or when they have
    a skewed distribution? One of the possible solutions to this problem was provided
    by Charles Spearman.'
  prefs: []
  type: TYPE_NORMAL
- en: Charles Spearman’s correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Charles Spearman introduced a nonparametric alternative to Pearson''s correlation.
    A **nonparametric statistical method** refers to the fact that it does not assume
    that the data on which it is calculated follows specific models described by a
    handful of parameters. This method is summarized in a new correlation coefficient
    called **Spearman’s rank-order correlation coefficient** (denoted by **ρ**, *rho*).
    Spearman''s coefficient measures the strength and direction of the association
    between two variables once their observations have been ranked according to their
    value. The formula that calculates it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Spearman’s rank-order correlation coefficient formula](img/file276.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Spearman’s rank-order correlation coefficient formula
  prefs: []
  type: TYPE_NORMAL
- en: The value *D* is the difference between the two ranks of each observation. To
    learn more details about the calculation, take a look at the references. Keep
    in mind that Spearman’s correlation function is also symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: Spearman's correlation coefficient ranges from -1 to +1\. The sign of the coefficient
    indicates whether it is a positive or negative monotone association.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Features of Spearman''s Correlation**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because Spearman's correlation applies to ranks, it provides a measure of a
    **monotonic association** between two continuous random variables and is the best-fitting
    correlation coefficient for ordinal variables. Because of the way it is calculated,
    as opposed to Pearson's, Spearman's correlation is *robust to outliers*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Remember that an association is said to be monotonic if it is increasing over
    its entire domain or decreasing over its entire domain (not a combination of the
    two):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Monotonic and non-monotonic associations](img/file277.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Monotonic and non-monotonic associations
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s important to check the monotonicity of the association, since in general
    the
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: correlation coefficients are not able to accurately describe non-monotonic relationships.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Calculating the Spearman correlation coefficient for the Boigelot distributions,
    we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Spearman’s correlation calculated over Boigelot distributions](img/file278.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Spearman’s correlation calculated over Boigelot distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'Even Spearman''s correlation fails to capture the nonlinear patterns you observe
    in the third row of distributions in *Figure 11.13*. And this is due not to the
    nonlinearity of the above associations, but to their non-monotonicity. In contrast,
    for monotonic nonlinear associations, Spearman''s correlation is better suited
    than Pearson''s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Pearson’s and Spearman’s correlation over a nonlinear monotonic
    association](img/file279.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Pearson’s and Spearman’s correlation over a nonlinear monotonic
    association
  prefs: []
  type: TYPE_NORMAL
- en: Spearman's correlation is not the only one that uses data ranking in its calculation.
    Kendall's also uses this strategy. Let's take a look at its features.
  prefs: []
  type: TYPE_NORMAL
- en: Maurice Kendall’s correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kendall’s rank correlation coefficient** (**τ**, tau) is also a nonparametric
    method of detecting associations between two variables. Its calculation is based
    on the concept of **concordant pairs** and **discordant pairs** (check the *References*
    section for more details). The formula useful in calculating Kendall''s correlation
    coefficient is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Kendall’s correlation coefficient formula](img/file280.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Kendall’s correlation coefficient formula
  prefs: []
  type: TYPE_NORMAL
- en: The *n[c]* value represents the number of concordant pairs, and the *n[d]* value
    represents the number of discordant pairs. Like the other correlation functions,
    Kendall’s is symmetric too.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the assumptions made for Spearman''s correlation also apply to Kendall''s.
    Here is a comparison between the two correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: Both correlations handle ordinal data and nonlinear continuous monotonic data
    very well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both correlations are robust to outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall correlation is preferred to Spearman correlation when the sample size
    is small and when it has many tied ranks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall’s coefficient is usually smaller than Spearman’s.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall's correlation has more computational complexity than Spearman's.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That said, let's use all three correlation coefficients in a real case.
  prefs: []
  type: TYPE_NORMAL
- en: Description of a real case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your boss has asked you to carry out a world population analysis requested by
    a client. Specifically, you need to understand whether there is a relationship
    and of what magnitude between life expectancy and per capita gross domestic product
    (GDP) over the years.
  prefs: []
  type: TYPE_NORMAL
- en: Looking around the web for useful data for the purpose, you realize that there
    is a portal that can help your case named **Gapminder** ([https://www.gapminder.org/](https://www.gapminder.org/)).
    It fights devastating misconceptions and promotes a fact-based worldview everyone
    can understand combining data from multiple sources into unique coherent time
    series that can’t be found elsewhere. The portal precisely exposes data on life
    expectancy ([http://bit.ly/life-expectancy-data](http://bit.ly/life-expectancy-data))
    and data on GDP per capita ([http://bit.ly/gdp-per-capita-data](http://bit.ly/gdp-per-capita-data)).
    In addition, there is someone who has transformed the data to collect it in a
    more suitable form for our purposes and has shared his work with the community
    in a CSV file ([http://bit.ly/gdp-life-expect-data](http://bit.ly/gdp-life-expect-data)).
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! You have everything you need to start your analysis in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run the code in this section, you need to install the Seaborn module in
    your `pbi_powerquery_env` environment. As you''ve probably learned by now, proceed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Anaconda prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the command `conda activate pbi_powerquery_env`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the command `pip install seaborn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code you'll find in this section is available in the file `01-gdp-life-expectancy-analysis-in-python.py`
    in the `Chapter11\Python` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, let''s take a quick look at the data in the above CSV file,
    while also importing the forms needed for the rest of the operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – A sample of the dataset about GDP and life expectancy](img/file281.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – A sample of the dataset about GDP and life expectancy
  prefs: []
  type: TYPE_NORMAL
- en: 'The variables we are interested in are `lifeExp` and `gdpPercap`. Before drawing
    a scatterplot of the two, let''s also take a look at the distribution of each
    of them. So let''s define the functions we will use for the plots and draw the
    distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The plots created are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Distributions of life expectancy and GDP variables](img/file282.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Distributions of life expectancy and GDP variables
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, while the life expectancy distribution more or less approximates
    the normal distribution, the GDP distribution is totally positive-skewed. Should
    there be a significant association between the two variables, you should then
    expect a likely non-linear scatter plot. Let''s check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Scatterplot between life expectancy and GDP per capita](img/file283.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Scatterplot between life expectancy and GDP per capita
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 11.18*, you can see that the association between the two variables
    is obvious and, as expected, is nonlinear (background arrow). What is more, outliers
    are present that are highlighted at the top. You are therefore faced with two
    assumptions that invalidate Pearson's correlation. Fortunately, the association
    is monotonically increasing, so the Spearman and Kendall correlations should detect
    the pattern more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python implementation of the three correlation coefficients is already
    made available by pandas. Therefore, correlation analysis is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `corr()` function of a pandas dataframe returns the correlation calculation
    for each pair of its numeric columns. Since the correlation functions are symmetric
    (that is, the order of the columns for which it is computed does not matter),
    the three calls return three dataframes each having all the numeric features as
    rows and columns, and the correlations between the features as values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Correlations between life expectancy and GDP per capita](img/file284.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Correlations between life expectancy and GDP per capita
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all three correlations identify a positive association between
    the two variables. As expected, the strength detected by Pearson's correlation
    is the weakest (*r = 0.58*). In contrast, the Spearman and Kendall correlations
    have a higher magnitude, *ρ = 0.83* and *τ = 0.64*, respectively. Especially from
    Spearman's correlation, it is evident that the two variables are strongly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'It often happens that in a data science project, you have to select the most
    predictive variables towards a target variable to predict when you have a large
    number of columns. The technique of correlation can certainly help us in this:
    assuming that the target variable is GDP per capita, we could decide to keep all
    those variables as predictors that have a correlation with the target variable
    greater than 0.7\. You understand that in this case, it is important to consider
    the right correlation method, otherwise, you would risk rejecting a variable that
    is strongly correlated to the target one.'
  prefs: []
  type: TYPE_NORMAL
- en: Great! Let's see how to replicate this analysis using R as well.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the packages we recommend using in R for calculating correlation coefficients
    is **corrr** ([https://github.com/tidymodels/corrr](https://github.com/tidymodels/corrr)).
    It allows you to explore and rearrange the tibbles returned by the `correlate()`
    function very easily according to the practices suggested by Tidyverse. Therefore,
    you need to install the `corrr` package in the most recent version of CRAN R you
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2
    in our case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''corrr'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code you'll find in this section is available in the file `01-gdp-life-expectancy-analysis-in-r.R`
    in the `Chapter11\R` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s proceed to import the necessary libraries, load the data from the
    CSV file on the web into a tibble, and display the first few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see this in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – First rows of the population tibble](img/file285.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – First rows of the population tibble
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, in this case, we define the functions necessary to draw a distribution
    plot and a scatterplot and use them to generate the plots of the distributions
    of the variables `lifeExp` and `gdpPercap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the plots you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – Distribution plots of Life Expectancy and GDP per capita](img/file286.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – Distribution plots of Life Expectancy and GDP per capita
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the scatterplot between the two variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the graph you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – Scatterplot between Life Expectancy and GDP per capita](img/file287.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – Scatterplot between Life Expectancy and GDP per capita
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation matrices (persisted in tibbles) are obtained straightforwardly
    via the `corrr` package’s `correlate()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the console results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.23 – Correlation coefficients in tibbles](img/file288.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 – Correlation coefficients in tibbles
  prefs: []
  type: TYPE_NORMAL
- en: Pretty simple, right!? Now that you know how to get correlation coefficients
    in both Python and R, let's implement what you learned in Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in Power BI with Python and R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Power BI has the ability to introduce a minimum of statistical analysis for
    the data that has been loaded into the data model thanks to DAX. You can find
    a list of statistical functions you can use at this link: `http://bit.ly/dax-stats-func`.
    As for the simple *Pearson correlation*, you can use it for columns in an already
    loaded table thanks to the predefined quick measures, which behind the scenes
    add some sometimes non-trivial DAX code for you. For more details, you can click
    on this link: [http://bit.ly/power-bi-corr-coef](http://bit.ly/power-bi-corr-coef).
    However, there is no easy way to implement the Spearman and Kendall correlation
    coefficients.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, you have probably heard of the **correlation plot** available in Microsoft's
    AppSource ([https://bit.ly/power-bi-corr-plot](https://bit.ly/power-bi-corr-plot)).
    You might think it would be possible to use that to get the correlation coefficients.
    First of all, the plot only displays Pearson correlation coefficients and not
    Spearman and Kendall ones. Also, you cannot extract coefficients from a visual
    or custom visual to persist them in a table in order to use them for later calculations
    (for example, feature selection). So the correlation plot idea is totally off
    the mark in this case.
  prefs: []
  type: TYPE_NORMAL
- en: The only way to proceed is to use Python or R. Let's see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, due to the simplicity of the code, we will implement the correlation
    coefficients in both Python and R in one project.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure that Power BI Desktop references the correct versions of Python
    and R in **Options**. Then follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Get Data**, select **Web**, and click on **Connect**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.24 – Get data from the web](img/file289.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.24 – Get data from the web
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Enter the `http://bit.ly/gdp-life-expect-data` string into the **URL** textbox
    and click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see a preview of the data. Then click **Transform Data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run Python script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following Python script and then click **OK**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can find this code in the file `02-gdp-life-expectancy-analysis-in-power-bi-with-python.py`
    in the `Chapter11\Python` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are only interested in the data in `corr_df`. So, click on its **Table**
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the preview of the Pearson correlation coefficients between all the
    numeric columns of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run R script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following R script and then click **OK**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can find this code in the file `02-gdp-life-expectancy-analysis-in-power-bi-with-r.R`
    in the `Chapter11\R` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are only interested in the data in `corr_tbl`. So, click its **Table** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the preview of the Spearman correlation coefficients between all
    the numeric columns of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Awesome! You have just calculated the correlation coefficients according to
    Pearson and Spearman for the numeric columns of a source dataset in Power BI with
    Python and R. Simple, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: You're probably wondering, *Okay, all clear on the numeric variables. What if
    I had categorical (non-numeric) variables? How can I calculate the correlation
    between them? And what about the correlation between a numeric variable and a
    categorical one?* Let's take a look at how to approach this type of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation between categorical and numeric variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have shown that, in the case of two numeric variables, you can get a sense
    of the association between them by taking a look at their scatterplot. Clearly,
    this strategy cannot be used when one or both variables are categorical. Note
    that a variable is **categorical** (or qualitative, or nominal) when it takes
    on values that are names or labels. For example, smartphone operating systems
    (iOS, Android, Linux, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to analyze the case of two categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Considering both variables categorical
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, is there a graphical representation that helps us understand whether there
    is a significant association between two categorical variables? The answer is
    yes and its name is a **mosaic plot**. In this section, we will take the Titanic
    disaster dataset as a reference dataset. In order to have an idea of what a mosaic
    plot looks like, let''s take into consideration the variables `Survived` (which
    takes values `1` and `0`) and `Pclass` (*passenger class*, which takes values
    `1`, `2`, and `3`). Since we want to study the association between these two variables,
    we consider the following mosaic plot generated by them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.25 – Mosaic plot of the variables Survived and Pclass](img/file290.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.25 – Mosaic plot of the variables Survived and Pclass
  prefs: []
  type: TYPE_NORMAL
- en: In short, the objective of the mosaic plot is to provide, at a glance, the strength
    of the association between the individual elements of each variable through the
    color of the tiles that represent the pairs of elements in question.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Basically, the more the color of the tile tends toward *dark blue*, the more
    there are observations represented by that tile compared to the expected quantity
    in the case of variables independent of each other.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the other hand, the more the color of the tile tends toward *dark red*, the
    less there are observations represented by that tile compared to the expected
    quantity in the case of variables independent of each other.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Looking at the top row of tiles in *Figure 11.25* associated with *Survived
    = 0*, we can say that, among the non-survivors, more than 50% of the people belong
    to the third class and the number is larger than expected if there was no association
    between survivors and classes. So, in general, we can say that there is a *positive
    association* of medium strength between the non-survivors and the third-class
    people. In other words, it is quite likely to find third-class people among non-survivors.
    In contrast, the number of first-class people among non-survivors is far lower
    than would be expected if there were no association between survivors and classes.
    So there is a strong negative association between the non-survivors and the first-class
    people. Thus, it is very likely that first-class people are not present among
    non-survivors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Focusing instead on the bottom row of tiles in *Figure 11.25*, we have confirmation
    of what we have already said: it is very likely to find first-class people among
    the survivors and very unlikely to find third-class people.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The numbers behind the plot in *Figure 11.25* are based on conditional probabilities
    and can be found by calculating the **contingency table** (also called **crosstab**)
    generated by the various elements of each of the categorical variables under analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.26 – Mosaic plot of the variables Survived and Pclass](img/file291.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.26 – Mosaic plot of the variables Survived and Pclass
  prefs: []
  type: TYPE_NORMAL
- en: See the *Reference* section for more details about contingency tables and mosaic
    plots.
  prefs: []
  type: TYPE_NORMAL
- en: But in a nutshell, how do we numerically determine the global strength of the
    association between two categorical variables? The answer lies in the coefficient
    highlighted in *Figure 11.26*, namely *Cramér's V*. Let's see what this is all
    about.
  prefs: []
  type: TYPE_NORMAL
- en: Harald Cramér’s correlation coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Cramér’s correlation coefficient** (**V**) measures the strength of the association
    between two categorical variables and it ranges from 0 to +1 (it doesn’t admit
    negative values, contrary to the coefficients seen until now). This coefficient
    is based on **Pearson''s chi-square (χ2) statistic**, which is used to test whether
    two categorical variables are independent. Cramér''s V formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.27 – Cramér’s V formula](img/file292.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.27 – Cramér’s V formula
  prefs: []
  type: TYPE_NORMAL
- en: In the formula of *Figure 11.27*, the value *N* is the sample size and *k* is
    the smallest number of distinct elements among those of each categorical variable
    in the association.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cramér’s V coefficient is a symmetric function and the guidelines of *Figure
    11.28* can be used to determine the magnitude of its effect size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.28 – Cramér’s V effect size ranges](img/file293.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.28 – Cramér’s V effect size ranges
  prefs: []
  type: TYPE_NORMAL
- en: The fact that *V* is a symmetric function leads to an important loss of information.
    Let's see why.
  prefs: []
  type: TYPE_NORMAL
- en: Henri Theil’s uncertainty coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have the two categorical variables `IsFraudster` and `Hobby` in
    the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.29 – Sample dataset of categorical variables](img/file294.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.29 – Sample dataset of categorical variables
  prefs: []
  type: TYPE_NORMAL
- en: Each value of the `Hobby` variable can be associated with a unique value of
    the `IsFraudster` variable (for example, *Chess → Fraudster*). However, the reverse
    is not true (for example, *Fraudster → [Chess, Body building]*). Therefore, the
    strength of the *Hobby → Fraudster* association (I know `Hobby` and need to determine
    `IsFraudster`) is of higher magnitude than the *Fraudster → Hobby* association
    (I know `IsFraudster` and need to determine `Hobby`). Unfortunately, the use of
    Cramér's coefficient *V* causes this distinction to be lost, since it is a symmetric
    function. To maintain the asymmetric nature of the relation, we must introduce
    Theil's uncertainty coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theil''s uncertainty coefficient**, also called the **entropy coefficient**,
    between two variables, *X* and *Y*, has a range of [0, 1] and it is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.30 – Theil''s uncertainty coefficient formula](img/file295.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.30 – Theil's uncertainty coefficient formula
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the concept of **entropy**, which provides information about
    the variation or diversity (and therefore uncertainty) of the information contained
    in one variable, given by *H(X)*. Then it’s also based on the **conditional entropy**
    (or **joint entropy**) *H(X|Y)*, which measures data diversity associated with
    the two variables *X* and *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, always considering the Titanic disaster dataset, the coefficient
    *U(Survived|Pcalss)* is 0.06; conversely, *U(Pcalss|Survived)* is 0.09.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see which coefficient to consider when dealing with associations in which
    one variable is numeric and the other is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: Considering a numeric variable and a categorical one
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to graphically represent an association between a numeric variable
    and a categorical variable, the boxplot or the violin plot are the types of graphic
    representation for you. If you have already come across the problem of having
    to represent the distribution of a variable by highlighting key statistics, then
    you should be familiar with a **boxplot**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.31 – Graphical explanation of a boxplot](img/file296.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.31 – Graphical explanation of a boxplot
  prefs: []
  type: TYPE_NORMAL
- en: 'A **violin plot** is nothing but a combination of a histogram/distribution
    plot and a boxplot for the same variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.32 – Graphical explanation of a violin plot](img/file297.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.32 – Graphical explanation of a violin plot
  prefs: []
  type: TYPE_NORMAL
- en: You will find more details about boxplots and violin plots in the references.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you need to relate a numeric variable to a categorical one, you can build
    a violin plot for each element of the categorical variable. To go back to the
    Titanic disaster dataset example, taking the `Pclass` (categorical) and `Age`
    (numeric) variables into account, you get this multiple violin plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.33 – Graphical explanation of a violin plot](img/file298.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.33 – Graphical explanation of a violin plot
  prefs: []
  type: TYPE_NORMAL
- en: If you look carefully inside each violin plot, you will see a white dot for
    each thin black boxplot plotted within it. Those dots represent the mean of each
    distribution of the `Age` variable for each element of the `Pclass` variable.
    Since they are arranged at fairly distinct heights from each other, it is possible
    that the `Pclass` variable is a good predictor for the `Age` variable. But how
    do we measure the strength of the association between numeric and categorical
    variables? The answer is given to us by the *correlation ratio*.
  prefs: []
  type: TYPE_NORMAL
- en: Karl Pearson’s correlation ratio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s again thanks to Karl Pearson that we have a tool to calculate the degree
    of nonlinear association between a categorical and a numerical variable. This
    is the **correlation ratio** (**η**, eta), which was introduced during the study
    of analysis of variance (**ANOVA**), and it ranges from 0 to +1.
  prefs: []
  type: TYPE_NORMAL
- en: Just as *r²* can be interpreted as the percentage of variance in one variable
    explained linearly by the other, *η²* (also called the **intraclass correlation
    coefficient**) represents the percentage of variance in the dependent (target)
    variable explained *linearly or nonlinearly* by the independent (predictor) variable.
    For this interpretation to be valid, it requires that the dependent variable be
    numeric and the independent variable be categorical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation ratio is an asymmetric function only if the categorical variable
    is an ordinal one (that is, days of week can be transformed into integers), otherwise,
    it only makes sense in one way. The interclass correlation coefficient formula
    (from which we immediately derive the correlation ratio by applying the square
    root) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.34 – Correlation ratio’s formula](img/file299.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.34 – Correlation ratio’s formula
  prefs: []
  type: TYPE_NORMAL
- en: The value is the mean of *y* broken down by category *x* and is the mean of
    the whole *y* (cross-category). Since *σ* represents the *variance* of a variable,
    we can read *η²* as the ratio of the dispersion of the variable *y* weighted for
    each category of *x* to the full dispersion of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The more *η²* tends to 1, the less the observations disperse around their mean
    value for each individual category. Therefore, the total dispersion of the numerical
    variable is all due to the breakdown in categories, and not to the individual
    dispersions for each category. That is why in this case we can say that there
    is a strong association between the numerical variable and the categorical variable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To better understand the concept laid out above, consider analyzing the grades
    taken in three subjects. If the grades are dispersed for each subject, you have
    a certain eta. If, on the other hand, the grades coincide for each subject, then
    eta equals 1\. This statement is represented in *Figure 11.35*, where each observation
    is plotted as a point in the violin plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.35 – Differences in η after changing grade distributions per topic](img/file300.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.35 – Differences in η after changing grade distributions per topic
  prefs: []
  type: TYPE_NORMAL
- en: So, let's now implement the correlation coefficients described in this section
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Python community is very lucky because Shaked Zychlinski developed a library
    with many data analysis tools, including a function that takes into account the
    data type of the columns of a pandas dataframe and generates a dataframe with
    the appropriate correlations. This library is **Dython** ([http://shakedzy.xyz/dython/](http://shakedzy.xyz/dython/))
    and can be installed via `pip`. Moreover, if you want to create mosaic plots,
    you must also install **statsmodels** ([https://www.statsmodels.org/](https://www.statsmodels.org/))
    in your `pbi_powerquery_env` environment. As you''ve probably learned by now,
    proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Anaconda prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the command `conda activate pbi_powerquery_env`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the command `pip install dython`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the command `pip install statsmodels`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code you'll find in this section is available in the file `03-titanic-disaster-analysis-in-python.py`
    in the `Chapter11\Python` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Of all the utilities in Dython, we need the ones in the `nominal` module. After
    loading the main libraries, we also create a helper function to draw a violin
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you can load the Titanic disaster data from a CSV file exposed
    on the web and transform the numerical columns `Survived` and `Pclass` (passenger
    class) into strings, since they are categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It is then possible to calculate *Cramér''s V* coefficient between the above
    two categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It returns a value of `0.34`, indicating a medium association strength.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also calculate *Theil''s U* uncertainty coefficient of the `Survived`
    variable given the `Pclass` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned value is `0.087`. Conversely, you can calculate the same coefficient
    for the `Pclass` variable given the `Survived` one, in order to show the asymmetry
    of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This one returns `0.058`. So it’s clear that the association *Pclass → Survived*
    is stronger than the opposite one.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about calculating the *correlation ratio η* between the variables `Age`
    (passenger age) and `Pclass`? Let''s do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `0.366`. Now, what if you want to get a correlation value for
    each couple of columns regardless of their data type? In this case, the `associations()`
    function is our friend. You just have to specify whether you want to use Theil’s
    U coefficient (`nom_nom_assoc = ''theil''`) or Cramér’s V one (`nom_nom_assoc
    = ''cramer''`) for categorical variables and that’s it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you''ll get a beautiful heatmap that helps you understand at a
    glance which columns have the strongest correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.36 – Correlation heatmap](img/file301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.36 – Correlation heatmap
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, you can also select the type of correlation to be used
    between numeric variables (Pearson, Spearman, Kendall) via the `num_num_assoc`
    parameter. Moreover, you can access the coefficient dataframe using the code `ass['corr']`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how to implement the same things in R.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is no R package on CRAN similar to Dython that allows you to calculate
    correlations between columns in a tibble regardless of their data type. Nor is
    there a package in which the correlation ratio as previously defined is defined.
    Therefore, based on the source code of the Dython package, we created them from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we used some CRAN packages that expose some of the correlation functions
    introduced in the previous sections. In particular, these packages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**rstatix** ([https://github.com/kassambara/rstatix](https://github.com/kassambara/rstatix)),
    an intuitive pipe-friendly framework for basic statistical tests. We used it for
    the `cramer_v()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DescTools** ([https://andrisignorell.github.io/DescTools/](https://andrisignorell.github.io/DescTools/)),
    a collection of miscellaneous basic statistics functions. We used it for the `UncertCoef()`
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vcd** ([https://cran.r-project.org/web/packages/vcd/index.html](https://cran.r-project.org/web/packages/vcd/index.html)),
    a collection of visualization techniques and tools for categorical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sjPlot** ([https://strengejacke.github.io/sjPlot/](https://strengejacke.github.io/sjPlot/)),
    a collection of plotting and table output functions for data visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, you need to install these packages in the most recent version of CRAN R
    you have:'
  prefs: []
  type: TYPE_NORMAL
- en: Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2
    in our case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''rstatix'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''DescTools'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''vcd'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''sjPlot'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code you'll find in this section is available in the file `03-titanic-survive-class-analysis-in-r.R`
    in the `Chapter11\R` folder.
  prefs: []
  type: TYPE_NORMAL
- en: After loading the most important libraries, again we created a helper function
    to graph a violin plot. After that, we defined the functions `correlation_ratio()`
    to calculate eta and `calc_corr()` to calculate the correlation of a tibble regardless
    of the data type of the given columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you can load the Titanic disaster data from a CSV file exposed
    on the web and transform the numerical columns `Survived` and `Pclass` (passenger
    class) into factors, since they are categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It is then possible to calculate *Cramér''s V* coefficient between the above
    two categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It returns a value of `0.34`, indicating a medium association strength.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also calculate *Theil''s U* uncertainty coefficient of the `Survived`
    variable given the `Pclass` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned value is `0.087`. Also, in this case, we can calculate the *correlation
    ratio η* between the numeric variable `Age` and the categorical `Pclass` one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `0.366`. Now you can get the correlation value for each couple
    of columns regardless of their data type. We implemented the `calc_corr()` function
    for this purpose. You just have to specify whether you want to use Theil’s U coefficient
    (`theil_uncert=True`) or Cramér’s V one (`theil_uncert=False`) for categorical
    variables and that’s it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see something like this as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.37 – Correlation tibble you get from the Titanic one](img/file302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.37 – Correlation tibble you get from the Titanic one
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also plot a heatmap of the correlation tibble just created using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.38 – Heatmap of the correlation tibble](img/file303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.38 – Heatmap of the correlation tibble
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! You have just implemented all of the correlation coefficients studied
    in this chapter in R as well. Let's now see how to implement them in Power BI,
    both with Python and with R.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing correlation coefficients in Power BI with Python and R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Obviously, Power BI was not born as an advanced statistical analysis tool, so
    you will not find, for example, all you need to implement all the correlation
    coefficients seen in the previous sections. In this case, the support given by
    Python and R is of fundamental importance for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in this case, due to the simplicity of the code, we will implement the
    correlation coefficients in both Python and R in one project.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure that Power BI Desktop references the correct versions of Python
    and R in **Options**. Then follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Get Data**, select **Web**, and click on **Connect**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `http://bit.ly/titanic-dataset-csv` URL into the **URL** textbox and
    click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see a preview of the data. Then click **Transform Data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run Python script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the script you can find in the file `04-correlation-analysis-in-power-bi-with-python.py`
    into the `Chapter11\Python` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are only interested in the data in `result_df`. So, click its **Table** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the preview of all correlation coefficients between all the columns
    of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run R script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the script you can find in the file `04-correlation-analysis-in-power-bi-with-r.R`
    in the `Chapter11\R` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are only interested in the data in `corr_tbl`. So, click its **Table** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the preview of all correlation coefficients between all the columns
    of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazing! You have just calculated the correlation coefficients for all the numeric
    columns of a source dataset in Power BI with both Python and R!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned how to calculate the correlation coefficient for
    two numerical variables according to Pearson, Spearman, and Kendall. Then you
    also learned how to calculate it for two categorical variables thanks to Cramér's
    V and Theil's uncertainty coefficient. Finally, you also learned how to calculate
    it for one numeric and one categorical variable thanks to the correlation ratio.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how statistics are really important for determining
    outliers and imputing missing values in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For additional reading, check out the following books and articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spearman''s Rank-Order Correlation* ([https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php](https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Concordant Pairs and Discordant Pairs* ([https://www.statisticshowto.com/concordant-pairs-discordant-pairs/](https://www.statisticshowto.com/concordant-pairs-discordant-pairs/))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mosaic Plot and Chi-Square Test* ([https://towardsdatascience.com/mosaic-plot-and-chi-square-test-c41b1a527ce4](https://towardsdatascience.com/mosaic-plot-and-chi-square-test-c41b1a527ce4))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Understanding Boxplots* ([https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Violin plots explained* ([https://towardsdatascience.com/violin-plots-explained-fb1d115e023d](https://towardsdatascience.com/violin-plots-explained-fb1d115e023d))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The Search for Categorical Correlation* ([https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
