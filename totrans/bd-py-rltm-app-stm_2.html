<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 2. The Storm Anatomy"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. The Storm Anatomy</h1></div></div></div><p>This chapter gives a detailed view of the internal structure and processes of the Storm technology. We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storm processes</li><li class="listitem" style="list-style-type: disc">Storm-topology-specific terminologies</li><li class="listitem" style="list-style-type: disc">Interprocess communication</li><li class="listitem" style="list-style-type: disc">Fault tolerance in Storm</li><li class="listitem" style="list-style-type: disc">Guaranteed tuple processing</li><li class="listitem" style="list-style-type: disc">Parallelism in Storm—scaling a distributed computation</li></ul></div><p>As we advance through the chapter, you will understand Storm's processes and their role in detail. In this chapter, various Storm-specific terminologies will be explained. You will learn how Storm achieves fault tolerance for different types of failure. We will see what guaranteed message processing is and, most importantly, how to configure parallelism in Storm to achieve fast and reliable processing.</p><div class="section" title="Storm processes"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec11"/>Storm processes</h1></div></div></div><p>We will start with <a id="id47" class="indexterm"/>Nimbus first, which is actually the entry-point daemon in Storm. Just to compare with Hadoop, Nimbus is actually the job tracker of Storm. Nimbus's job is to distribute code to all supervisor daemons of a cluster. So, when topology code<a id="id48" class="indexterm"/> is submitted, it actually reaches all physical machines in the cluster. Nimbus also monitors failure of supervisors. If a supervisor continues to fail, then Nimbus reassigns those workers' jobs to other workers of a different physical machine. The current version of Storm allows only one instance of the Nimbus daemon to run. Nimbus is also responsible for assigning tasks to supervisor nodes. If you lose Nimbus, the workers will still continue to compute. Supervisors will continue to restart workers as and when they die. Without Nimbus, a worker's task won't be reassigned to another machine worker within the cluster.</p><p>There is no alternative Storm process that will take over if Nimbus dies, and no process will even try to restart it. There is nothing to worry about, however, since it can be restarted anytime. In a production environment, alerts can also be set when Nimbus dies. In future, we may<a id="id49" class="indexterm"/> see highly available Nimbus.</p><div class="section" title="Supervisor"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Supervisor</h2></div></div></div><p>A supervisor manages all the workers of the respective machine. Distributed computation in Storm is possible due to the supervisor daemon, as there is one supervisor per machine in your cluster. The<a id="id50" class="indexterm"/> supervisor daemon listens for the work assigned by Nimbus to the machine that it runs, and distributes it among workers. Due to any runtime exception, workers can die anytime, and the supervisor restarts them when there is no heartbeat<a id="id51" class="indexterm"/> from dead workers. Each worker process executes a part of a topology. Similar to the Hadoop ecosystem, supervisor is a task tracker of Storm. It tracks the tasks of workers of the same machine. The maximum number of possible workers depends on the number of ports defined in <code class="literal">storm.yaml</code>.</p></div><div class="section" title="Zookeeper"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Zookeeper</h2></div></div></div><p>In addition to its own <a id="id52" class="indexterm"/>components, Storm relies on a Zookeeper cluster (one or more Zookeeper servers) to perform the coordination job between Nimbus and the <a id="id53" class="indexterm"/>supervisors. Apart from using Zookeeper for coordination purposes, Nimbus and the supervisors also store all their states in Zookeeper, and Zookeeper stores them on a local disk where it is running. Having more than one Zookeeper daemon increases the reliability of the system, because if one daemon goes down, another becomes the leader.</p></div><div class="section" title="The Storm UI"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>The Storm UI</h2></div></div></div><p>Storm is also equipped<a id="id54" class="indexterm"/> with a web-based user interface. It should be started on a machine that also runs Nimbus. The Storm UI provides a report of the entire cluster, such as the sum of all active supervisor machines, the total number of workers<a id="id55" class="indexterm"/> available, allotted to each topology and how many remaining, and topology-level diagnostics such as tuples stats (how many tuples were emitted, and the ACK between spout to bolt or bolt to bolt). The Storm UI also shows the total number of workers, which is actually sum of all workers available of all supervisors' machines. </p><p>The following screenshot shows a sample screen of the Storm UI:</p><div class="mediaobject"><img src="images/B03471_02_01.jpg" alt="The Storm UI"/></div><p>Following is the explanation of Storm UI:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Topology stats</strong></span>: Under <span class="strong"><strong>Topology stats</strong></span>, you can click and see the stats of the last 10 minutes, 3 hours, or all time.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spouts (All time)</strong></span>: This <a id="id56" class="indexterm"/>displays the number of executors and tasks assigned for this spout, along with the stats of emitted tuples and other latency stats.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bolts (All time)</strong></span>: This<a id="id57" class="indexterm"/> displays a list of all bolts, along with the assigned executors/tasks. When you are doing performance tuning, keep the <span class="strong"><strong>Capacity</strong></span> column close to <code class="literal">1</code>. In the preceding example for <span class="strong"><strong>aggregatorBolt</strong></span>, it is <code class="literal">1.500</code>, so instead of <code class="literal">200</code> executors/tasks, we can use <code class="literal">300</code>. The <span class="strong"><strong>Capacity</strong></span> column helps us decide the right degree of parallelism. The idea is very simple; if the <span class="strong"><strong>Capacity</strong></span> column reads more than <code class="literal">1</code>, try increasing the executors and tasks in the same ratio. If the value of executors/tasks is high and the <span class="strong"><strong>Capacity</strong></span> column is close to zero, try reducing the number of executors/tasks. You can do this until you get the best configuration.</li></ul></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Storm-topology-specific terminologies"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Storm-topology-specific terminologies</h1></div></div></div><p>A topology is a logical separation of programming work into many small-scale processing units called spout and bolt, which is similar to MapReduce in Hadoop. A topology can be written<a id="id58" class="indexterm"/> in many languages, including Java, Python, and lot more supported languages. In visual depictions, a topology is shown as a graph of connecting spouts and bolts. Spouts and bolts execute tasks across the cluster. Storm has two modes of operation, called local mode and distributed mode:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In local mode, all processes<a id="id59" class="indexterm"/> of Storm and workers run within your code development environment. This is good for testing and development of topologies.</li><li class="listitem" style="list-style-type: disc">In distributed mode, Storm<a id="id60" class="indexterm"/> operates as a cluster of machines. When you submit topology code to the Nimbus, Nimbus takes care of distributing the code and allocating workers to run your topology based on your configuration.</li></ul></div><p>In the following figure, we have purple bolts; these receive a tuple or records from the spout above them. A tuple supports most of the data types available in the programming language in which the topology code is being written. It flows as an independent unit from a spout to a bolt or a bolt to another bolt. An unbounded flow of tuples is called a stream. In a single tuple, you can have many key-value pairs to pass together.</p><p>The next figure illustrates streams in more detail. A spout is connected to a source of tuples and generates continuous tuples for the topology as a stream. What you emit from the spout as a key-value pair can be received by the bolt using the same key.</p><div class="mediaobject"><img src="images/B03471_02_02.jpg" alt="Storm-topology-specific terminologies"/></div><div class="section" title="The worker process, executor, and task"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>The worker process, executor, and task</h2></div></div></div><p>Storm distinguishes between<a id="id61" class="indexterm"/> the following three main entities, which are used to actually run a topology in a Storm cluster:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Worker</li><li class="listitem" style="list-style-type: disc">Executor</li><li class="listitem" style="list-style-type: disc">Task</li></ul></div><p>Let's say we have decided to<a id="id62" class="indexterm"/> keep two workers, one spout executor, three <span class="strong"><strong>Bolt1</strong></span> executors, and two <span class="strong"><strong>Bolt2</strong></span> executors. Assume that the ratio of the <a id="id63" class="indexterm"/>number of executors and tasks is the same. The total sum of executors is six for<a id="id64" class="indexterm"/> spout and bolt. Out <a id="id65" class="indexterm"/>of six executors, some <a id="id66" class="indexterm"/>will run within the scope of worker 1, and some will be in control of worker 2; this decision is taken by the supervisor. This is explained in the following figure:</p><div class="mediaobject"><img src="images/B03471_02_03.jpg" alt="The worker process, executor, and task"/></div><p>The next figure explains the position of the workers and executors within the scope of the supervisor that is running on a machine:</p><div class="mediaobject"><img src="images/B03471_02_04.jpg" alt="The worker process, executor, and task"/></div><p>The number of executors<a id="id67" class="indexterm"/> and tasks is set while building the topology code. In the pr<a id="id68" class="indexterm"/>eceding figure, we have two workers (1 and 2), run and managed by the supervisor of that machine. Assume that <span class="strong"><strong>Executor 1</strong></span> is running one task, because the ratio of executors<a id="id69" class="indexterm"/> to tasks is the same (for example, 10 executors means 10 tasks, which makes the ratio 1:1). But <span class="strong"><strong>Executor 2</strong></span> is running two<a id="id70" class="indexterm"/> tasks sequentially, so the ratio of tasks to executors is 2:1 (for example, 10 executors means 20 tasks, which makes the ratio 2:1). Having more tasks never means higher processing speed, but this is true for more executors, as tasks run sequentially.</p></div><div class="section" title="Worker processes"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Worker processes</h2></div></div></div><p>A single worker process <a id="id71" class="indexterm"/>executes a portion of a topology and runs on its own JVM. Workers are allocated during topology submission. A worker process is linked to a specific topology and can run one or more executors for one or more spouts or bolts of that topology. A running topology consists of many such workers running on many machines within a Storm cluster.</p></div><div class="section" title="Executors"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Executors</h2></div></div></div><p>An executor is a thread run within the scope of a worker's JVM. An executor may run one or more tasks for<a id="id72" class="indexterm"/> a spout or bolt sequentially.</p><p>An executor always runs <a id="id73" class="indexterm"/>on one thread for all its tasks, which means that tasks run serially on an executor. The number of executors can be changed after the topology has been started without shutdown, using the <code class="literal">rebalance</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>storm rebalance &lt;topology name&gt; -n &lt;number of workers&gt; -e &lt;spout&gt;=&lt;number of executors&gt; -e &lt;bolt1 name&gt;=&lt;number of executors&gt; -e &lt;bolt2 name&gt;=&lt;number of executors&gt;</strong></span></pre></div></div><div class="section" title="Tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Tasks</h2></div></div></div><p>A task performs data processing and runs within its parent executor's thread of execution. The default value of the number of tasks is the same as the number of executors. While building the topology, we can keep a higher number of tasks as well. It can help to increase the number of executors in the future, which keeps the scope of scaling open. Initially, we can have 10 executors and 20 tasks, so the ratio is 2:1. This means two tasks per executor. A future rebalancing action can make 20 executors and 20 tasks, which will make the ratio 1:1.</p></div><div class="section" title="Interprocess communication"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Interprocess communication</h2></div></div></div><p>The following figure illustrates communication between the Storm submitter (client), the Nimbus thrift server, Zookeeper, supervisors, workers of supervisors, executors, and tasks. Each worker process runs as a separate JVM.</p><div class="mediaobject"><img src="images/B03471_02_05.jpg" alt="Interprocess communication"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="A physical view of a Storm cluster"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>A physical view of a Storm cluster</h1></div></div></div><p>The next figure explains the physical position of each process. There can be only one Nimbus. However, more than one Zookeeper is there to support failover, and per machine, there is one supervisor.</p><div class="mediaobject"><img src="images/B03471_02_06.jpg" alt="A physical view of a Storm cluster"/></div><div class="section" title="Stream grouping"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Stream grouping</h2></div></div></div><p>A stream grouping controls the flow of tuples between from spout to bolt or bolt to bolt. In Storm, we have four types of groupings. Shuffle and field grouping are most commonly used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Shuffle grouping</strong></span>: Tuple flow between two random tasks in this grouping</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Field grouping</strong></span>: A tuple with a particular field key is always delivered to the same task of the downstream bolt</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>All grouping</strong></span>: Sends the same tuple to all tasks of the downstream bolt</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Global grouping</strong></span>: Tuples from all tasks reach one task</li></ul></div><p>The subsequent figure gives a diagrammatic explanation of all the four types of groupings:</p><div class="mediaobject"><img src="images/B03471_02_07.jpg" alt="Stream grouping"/></div></div><div class="section" title="Fault tolerance in Storm"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Fault tolerance in Storm</h2></div></div></div><p>Supervisor runs a synchronization thread to get assignment information (what part of topology I am supposed to run) from Zookeeper and write to the local disk. This local filesystem information helps keep the worker up to date:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Case 1</strong></span>: This is the ideal case for most of the times. When the cluster works normally, the worker's heartbeat goes back to the supervisors and Nimbus via Zookeeper.<div class="mediaobject"><img src="images/B03471_02_08.jpg" alt="Fault tolerance in Storm"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Case 2</strong></span>: If a supervisor dies, processing still continues, but the assignment is never synchronized. Nimbus will reassign the work to another supervisor of a different machine. Those workers will be running, but will not receive any new tuples. Do set an alert to restart the supervisor or use a Unix tool that can restart the supervisor.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Case 3</strong></span>: If Nimbus dies, the topologies will continue to function normally. Processing will still continue, but topology life cycle operations and reassigning to another machine will not be possible.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Case 4</strong></span>: If a worker dies (as the heartbeat stops arriving), the supervisor will try to restart the worker process and processing will continue. If a worker dies repeatedly, Nimbus will reassign the work to other nodes in the cluster.</li></ul></div></div><div class="section" title="Guaranteed tuple processing in Storm"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Guaranteed tuple processing in Storm</h2></div></div></div><p>As Storm is already equipped to deal with various process-level failures, another important feature is the ability to deal with failure of tuples that occurs when a worker dies. This is just to give an idea of bitwise XOR: the XOR of two sets of the same bits is 0. This is called XOR magic, and it can help us know whether the delivery of a tuple to the next bolt is successful or not. Storm uses 64 bits to track tuples. Every tuple gets a 64-bit tuple ID. This 64-bit ID, along with the task ID, is kept at ACKer.</p><p>In the next figure, ACKing and a replay case is explained:</p><div class="mediaobject"><img src="images/B03471_02_09.jpg" alt="Guaranteed tuple processing in Storm"/></div><div class="section" title="XOR magic in acking"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec08"/>XOR magic in acking</h3></div></div></div><p>A spout tuple is not fully processed until all the tuples in the linked tuple tree are completed. If the tuple tree is not completed within a configured timeout (the default value is <code class="literal">topology.message.timeout.secs: 30</code>), the spout tuple is replayed.</p><p>In the preceding diagram, the first acker gets <code class="literal">10101</code> (for simplicity of explanation, we are keeping 5 bits) for tuple 1 from the spout. Once <span class="strong"><strong>Bolt 1</strong></span> receives the same tuple, it also ACK to acker. From both sources, acker gets <code class="literal">10101</code>. This means <code class="literal">10101</code> XOR <code class="literal">10101 = 0</code>. Tuple 1 is successfully received by <span class="strong"><strong>Bolt 1</strong></span>. The same process repeats between bolts 1 and 2. At last, <span class="strong"><strong>Bolt 2</strong></span> sends ack to acker, and the tuple tree is completed. This creates a signal to call the spout's <code class="literal">success</code> function. Any failure in tuple processing can trigger the spout's <code class="literal">fail</code> function call, which gives an indication to send the tuple back for processing again.</p><p>Storm's acker tracks the completion of the tuple tree by performing XOR between the sender's tuple and the receiver's tuple. Each time a tuple is sent, its value is XORed into the checksum maintained by acker, and each time a tuple is acked, its value is XORed in again at acker.</p><p>If all tuples have been successfully acked, the checksum will be zero. Ackers are system-level executors.</p><p>In the spout, we have a choice of two emit functions.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">emit([tuple])</code>: This is a simple emit</li><li class="listitem" style="list-style-type: disc"><code class="literal">storm.emit([tuple], id=the_value)</code>: This creates a reliable spout, but only if you can re-emit a tuple using <code class="literal">the_value</code></li></ul></div><p>In the Spout, we also have two ACK functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">fail(the_value)</code>: This function is called when a timeout occurs or the tuple fails</li><li class="listitem" style="list-style-type: disc"><code class="literal">ack(the_value)</code>: This function is called when the last bolt of the topology ACK the tuple tree</li></ul></div><p>This ID field should be a random and unique value to replay from the spout's <code class="literal">fail</code> function. Using this ID, we can re-emit it from the <code class="literal">fail</code> function. If successful, the jn <code class="literal">success</code> function will call and it can remove successful tuples from the global list or recreate from the source.</p><p>You will be able to recreate the same tuple if you have a reliable spout in the topology. To create a reliable spout, emit a unique message ID (<code class="literal">the_value</code>) from the spout's next tuple function along with the tuple:</p><div class="informalexample"><pre class="programlisting">storm.emit([tuple], id=the_value)</pre></div><p>Whether a tuple is not ACKed within a configured period of time, or the programming code fails a tuple due to some error condition, both are valid cases of replay.</p><p>When the <code class="literal">fail</code> function is called, the code can read from the source of the spout using the same message ID, and when the <code class="literal">success</code> function is called, an action such as removing a message from the queue can be taken.</p><p>The message ID is an application-specific key that can help you recreate a tuple and emit it back from the spout. An example of a message ID can be a queue message ID, or a primary key of a table. A tuple is considered failed if a timeout occurs or due to any other reason.</p><p>Storm has a fault tolerance mechanism that guarantees at-least-once processing for all tuples emitted only from a reliable spout.</p><p>Once you have a reliable spout in place, you can make the bolt do the linking between the input and output tuples, which creates a tuple tree. Once a tuple tree is established, acker knows any failure in the linked tree, and the original message ID is used to create the entire tuple tree again.</p><p>In the bolt, there are two functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">emit([tuple])</code>: There is no tuple tree linking. We can't track which original message ID was used.</li><li class="listitem" style="list-style-type: disc"><code class="literal">storm.emit([tuple], anchors=[message_key])</code>: With linking in place, the original tuple can now be replayed.</li></ul></div><p>The following figure explains how tuple B is generated from tuple A:</p><div class="mediaobject"><img src="images/B03471_02_10.jpg" alt="XOR magic in acking"/></div><p>The next figure illustrates the bolt performing <span class="strong"><strong>ACK</strong></span>:</p><div class="mediaobject"><img src="images/B03471_02_11.jpg" alt="XOR magic in acking"/></div><p>The following figure illustrates the failure condition, where the signal reaches the spout upon failure:</p><div class="mediaobject"><img src="images/B03471_02_12.jpg" alt="XOR magic in acking"/></div><p>A successful <span class="strong"><strong>ACK</strong></span> is demonstrated as follows:</p><div class="mediaobject"><img src="images/B03471_02_13.jpg" alt="XOR magic in acking"/></div><p>The following figure illustrates a condition of a big tuple tree without a bolt, and there is no failure:</p><div class="mediaobject"><img src="images/B03471_02_14.jpg" alt="XOR magic in acking"/></div><p>The next figure demonstrates an example of failure in a tuple tree—in the middle of the tuple tree:</p><div class="mediaobject"><img src="images/B03471_02_15.jpg" alt="XOR magic in acking"/></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Tuning parallelism in Storm – scaling a distributed computation"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Tuning parallelism in Storm – scaling a distributed computation</h1></div></div></div><p>To explain parallelism of Storm, we will configure three parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The number of workers </li><li class="listitem" style="list-style-type: disc">The number of executors</li><li class="listitem" style="list-style-type: disc">The number of tasks</li></ul></div><p>The following figure gives a diagrammatic explanation of an example where we have a topology with just one spout and one bolt. In this case, we will set different values for the numbers of workers, executors, and tasks at the spout and bolt levels, and see how parallelism works in each case:</p><div class="mediaobject"><img src="images/B03471_02_16.jpg" alt="Tuning parallelism in Storm – scaling a distributed computation"/></div><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>// assume we have two workers in total for topology.</strong></span>
<span class="strong"><strong>topology.workers: 2</strong></span>
<span class="strong"><strong> // just one executor of spout.</strong></span>
<span class="strong"><strong>builder.setSpout("spout-sentence", TwitterStreamSpout(),1)</strong></span>

<span class="strong"><strong>// two executors of bolt.</strong></span>
<span class="strong"><strong>builder.setBolt("bolt-split", SplitSentenceBolt(),2)</strong></span>
<span class="strong"><strong> // four tasks for bolts.</strong></span>
<span class="strong"><strong>.setNumTasks(4)</strong></span>
<span class="strong"><strong>.shuffleGrouping("spout-sentence");</strong></span></pre></div><p>For this configuration, we will have two workers, which will run in separate JVMs (worker 1 and worker 2).</p><p>For the spout, there is one executor, and the default number of tasks is one, which makes the ratio 1:1 (one task per executor).</p><p>For the bolt, there are two executors and four tasks, which makes it 4/2 = two tasks per executor. These two executors run under worker 2, with each having two tasks, while the executor of worker 1 gets only one task.</p><p>This can be illustrated nicely using the following figure:</p><div class="mediaobject"><img src="images/B03471_02_17.jpg" alt="Tuning parallelism in Storm – scaling a distributed computation"/></div><p>Let's change the configuration in the bolt to two executors and two tasks:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>builder.setBolt("bolt-split", SplitSentenceBolt(),2)</strong></span>
<span class="strong"><strong> // 2 tasks for bolts.</strong></span>
<span class="strong"><strong>.setNumTasks(2)</strong></span>
<span class="strong"><strong>.shuffleGrouping("spout-sentence");</strong></span></pre></div><p>This can be illustrated well here:</p><div class="mediaobject"><img src="images/B03471_02_18.jpg" alt="Tuning parallelism in Storm – scaling a distributed computation"/></div><p>The number of workers is two again. As the bolt has two executors and two tasks, that makes it 2/2, or one task per executor. Now you can see that both executors get one task each. In terms of performance, both cases are exactly the same, as the tasks run sequentially within the executor thread. More executors means a higher degree of parallelism, and more workers means using resources such as CPU and RAM more effectively. Memory allocation is done at the worker level using the <code class="literal">worker</code>.<code class="literal">childopts</code> setting. We should also monitor the maximum amount of memory a particular worker process is holding. This plays an important role in deciding the total number of workers. It can be seen using the <code class="literal">ps -ef</code> option. Always keep the tasks and executors in the same ratio, and derive the correct value for the number of executors using the capacity column of the Storm UI. As an important note, we should keep the shorter duration transaction in the bolt and try to tune it via splitting code into more bolts or reducing the batch size tuple. The batch size is the number of records received by the bolt in a single tuple delivery. Also, don't block the <code class="literal">nextTuple</code> method of the spout due to the longer holding transaction.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Summary</h1></div></div></div><p>As this chapter approaches its end, you must have got a brief idea about the Nimbus, supervisor, UI, and Zookeeper processes. This chapter also taught you how to tune parallelism in Storm by playing with the number of workers, executors, and tasks. You became familiar with the important problem of distributing computation, that is, failures and overcoming failures by different kinds of fault tolerance available in the system. And most importantly, you learned how to write a "reliable" spout to achieve guaranteed message processing and linking in bolts.</p><p>The next chapter will give you information about how to build a simple topology using a Python library called Petrel. Petrel addresses some limitations of Storm's built-in Python support, providing simpler and more streamlined development.</p></div></div>
</body></html>