["```py\nscala> import org.apache.spark._\n  import org.apache.spark._\n    scala> import org.apache.spark.graphx._\n\n\timport org.apache.spark.graphx._\n\tscala> import org.apache.spark.rdd.RDD\n\timport org.apache.spark.rdd.RDD\n  scala> //Create an RDD of users containing tuple values with a mandatory\n  Long and another String type as the property of the vertex\n  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L,\n  \"Thomas\"), (2L, \"Krish\"),(3L, \"Mathew\")))\n  users: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[0]\n  at parallelize at <console>:31\n  scala> //Created an RDD of Edge type with String type as the property of the edge\n  scala> val userRelationships: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, \"Follows\"),    Edge(1L, 2L, \"Son\"),Edge(2L, 3L, \"Follows\")))\nuserRelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at parallelize at <console>:31\n    scala> //Create a graph containing the vertex and edge RDDs as created beforescala> val userGraph = Graph(users, userRelationships)\n\tuserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@ed5cf29\n\n\tscala> //Number of edges in the graph\n\tscala> userGraph.numEdges\n      res3: Long = 3\n    scala> //Number of vertices in the graph\n\tscala> userGraph.numVertices\n      res4: Long = 3\n\t  scala> //Number of edges coming to each of the vertex. \n\t  scala> userGraph.inDegrees\nres7: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[19] at RDD at\n VertexRDD.scala:57\nscala> //The first element in the tuple is the vertex id and the second\n element in the tuple is the number of edges coming to that vertex\n scala> userGraph.inDegrees.foreach(println)\n      (3,1)\n\n      (2,2)\n    scala> //Number of edges going out of each of the vertex. scala> userGraph.outDegrees\n\tres9: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[23] at RDD at VertexRDD.scala:57\n    scala> //The first element in the tuple is the vertex id and the second\n\telement in the tuple is the number of edges going out of that vertex\n\tscala> userGraph.outDegrees.foreach(println)\n      (1,2)\n\n      (2,1)\n    scala> //Total number of edges coming in and going out of each vertex. \n\tscala> userGraph.degrees\nres12: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[27] at RDD at\n VertexRDD.scala:57\n    scala> //The first element in the tuple is the vertex id and the second \n\telement in the tuple is the total number of edges coming in and going out of that vertex.\n\tscala> userGraph.degrees.foreach(println)\n      (1,2)\n\n      (2,3)\n\n      (3,1)\n    scala> //Get the vertices of the graph\n\tscala> userGraph.vertices\nres11: org.apache.spark.graphx.VertexRDD[String] = VertexRDDImpl[11] at RDD at VertexRDD.scala:57\n    scala> //Get all the vertices with the vertex number and the property as a tuplescala> userGraph.vertices.foreach(println)\n      (1,Thomas)\n\n      (3,Mathew)\n\n      (2,Krish)\n    scala> //Get the edges of the graph\n\tscala> userGraph.edges\nres15: org.apache.spark.graphx.EdgeRDD[String] = EdgeRDDImpl[13] at RDD at\n EdgeRDD.scala:41\n    scala> //Get all the edges properties with source and destination vertex numbers\n\tscala> userGraph.edges.foreach(println)\n      Edge(1,2,Follows)\n\n      Edge(1,2,Son)\n\n      Edge(2,3,Follows)\n    scala> //Get the triplets of the graph\n\tscala> userGraph.triplets\nres18: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[String,String]]\n = MapPartitionsRDD[32] at mapPartitions at GraphImpl.scala:48\n    scala> userGraph.triplets.foreach(println)\n\t((1,Thomas),(2,Krish),Follows)\n\t((1,Thomas),(2,Krish),Son)\n\t((2,Krish),(3,Mathew),Follows)\n\n```", "```py\nscala> import org.apache.spark._\nimport org.apache.spark._\nscala> import org.apache.spark.graphx._\nimport org.apache.spark.graphx._\nscala> import org.apache.spark.rdd.RDD\nimport org.apache.spark.rdd.RDD\nscala> //Create the vertices with the stops\nscala> val stops: RDD[(Long, String)] = sc.parallelize(Array((1L, \"Manchester\"), (2L, \"London\"),(3L, \"Colombo\"), (4L, \"Bangalore\")))\nstops: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[33] at parallelize at <console>:38\nscala> //Create the edges with travel legs\nscala> val legs: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, \"air\"),    Edge(2L, 3L, \"air\"),Edge(3L, 4L, \"air\"))) \nlegs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[34] at parallelize at <console>:38 \nscala> //Create the onward journey graph\nscala> val onwardJourney = Graph(stops, legs)onwardJourney: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@190ec769scala> onwardJourney.triplets.map(triplet => (triplet.srcId, (triplet.srcAttr, triplet.dstAttr))).sortByKey().collect().foreach(println)\n(1,(Manchester,London))\n(2,(London,Colombo))\n(3,(Colombo,Bangalore))\nscala> val returnJourney = onwardJourney.reversereturnJourney: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@60035f1e\nscala> returnJourney.triplets.map(triplet => (triplet.srcId, (triplet.srcAttr,triplet.dstAttr))).sortByKey(ascending=false).collect().foreach(println)\n(4,(Bangalore,Colombo))\n(3,(Colombo,London))\n(2,(London,Manchester))\n\n```", "```py\nscala> returnJourney.triplets.map(triplet => (triplet.srcId,triplet.dstId,triplet.attr,triplet.srcAttr,triplet.dstAttr)).foreach(println) \n(2,1,air,London,Manchester) \n(3,2,air,Colombo,London) \n(4,3,air,Bangalore,Colombo) \n\n```", "```py\n scala> // Create the vertices \nscala> val stops: RDD[(Long, String)] = sc.parallelize(Array((1L,\n \"Manchester\"), (2L, \"London\"),(3L, \"Colombo\"), (4L, \"Bangalore\"))) \nstops: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[66] at parallelize at <console>:38 \nscala> //Create the edges \nscala> val legs: RDD[Edge[Long]] = sc.parallelize(Array(Edge(1L, 2L, 50L),    Edge(2L, 3L, 100L),Edge(3L, 4L, 80L))) \nlegs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Long]] = ParallelCollectionRDD[67] at parallelize at <console>:38 \nscala> //Create the graph using the vertices and edges \nscala> val journey = Graph(stops, legs) \njourney: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@8746ad5 \nscala> //Convert the stop names to upper case \nscala> val newStops = journey.vertices.map {case (id, name) => (id, name.toUpperCase)} \nnewStops: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[80] at map at <console>:44 \nscala> //Get the edges from the selected journey and add 10% price to the original price \nscala> val newLegs = journey.edges.map { case Edge(src, dst, prop) => Edge(src, dst, (prop + (0.1*prop))) } \nnewLegs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[81] at map at <console>:44 \nscala> //Create a new graph with the original vertices and the new edges \nscala> val newJourney = Graph(newStops, newLegs) \nnewJourney: org.apache.spark.graphx.Graph[String,Double]\n = org.apache.spark.graphx.impl.GraphImpl@3c929623 \nscala> //Print the contents of the original graph \nscala> journey.triplets.foreach(println) \n((1,Manchester),(2,London),50) \n((3,Colombo),(4,Bangalore),80) \n((2,London),(3,Colombo),100) \nscala> //Print the contents of the transformed graph \nscala>  newJourney.triplets.foreach(println) \n((2,LONDON),(3,COLOMBO),110.0) \n((3,COLOMBO),(4,BANGALORE),88.0) \n((1,MANCHESTER),(2,LONDON),55.0) \n\n```", "```py\nscala> import org.apache.spark._\n  import org.apache.spark._    scala> import org.apache.spark.graphx._\n  import org.apache.spark.graphx._    scala> import org.apache.spark.rdd.RDD\n  import org.apache.spark.rdd.RDD    scala> //Create an RDD of users containing tuple values with a mandatory\n  Long and another String type as the property of the vertex\n  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L,\n  \"Thomas\"), (2L, \"Krish\"),(3L, \"Mathew\")))\nusers: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[104]\n at parallelize at <console>:45\n    scala> //Created an RDD of Edge type with String type as the property of\n\tthe edge\n\tscala> val userRelationships: RDD[Edge[String]] =\n\tsc.parallelize(Array(Edge(1L, 2L, \"Follows\"), Edge(1L, 2L,\n\t\"Son\"),Edge(2L, 3L, \"Follows\"), Edge(1L, 4L, \"Follows\"), Edge(3L, 4L, \"Follows\")))\n\tuserRelationships:\n\torg.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] =\n\tParallelCollectionRDD[105] at parallelize at <console>:45\n    scala> //Create a vertex property object to fill in if an invalid vertex id is given in the edge\n\tscala> val missingUser = \"Missing\"\nmissingUser: String = Missing\n    scala> //Create a graph containing the vertex and edge RDDs as created\n\tbefore\n\tscala> val userGraph = Graph(users, userRelationships, missingUser)\nuserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@43baf0b9\n    scala> //List the graph triplets and find some of the invalid vertex ids given and for them the missing vertex property is assigned with the value \"Missing\"scala> userGraph.triplets.foreach(println)\n      ((3,Mathew),(4,Missing),Follows)  \n      ((1,Thomas),(2,Krish),Son)    \n      ((2,Krish),(3,Mathew),Follows)    \n      ((1,Thomas),(2,Krish),Follows)    \n      ((1,Thomas),(4,Missing),Follows)\n    scala> //Since the edges with the invalid vertices are invalid too, filter out\n\tthose vertices and create a valid graph. The vertex predicate here can be any valid filter condition of a vertex. Similar to vertex predicate, if the filtering is to be done on the edges, instead of the vpred, use epred as the edge predicate.\n\tscala> val fixedUserGraph = userGraph.subgraph(vpred = (vertexId, attribute) => attribute != \"Missing\")\nfixedUserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@233b5c71 \n scala> fixedUserGraph.triplets.foreach(println)\n  ((2,Krish),(3,Mathew),Follows)\n  ((1,Thomas),(2,Krish),Follows)\n  ((1,Thomas),(2,Krish),Son)\n\n```", "```py\nscala> // Import the partition strategy classes \nscala> import org.apache.spark.graphx.PartitionStrategy._ \nimport org.apache.spark.graphx.PartitionStrategy._ \nscala> // Partition the user graph. This is required to group the edges \nscala> val partitionedUserGraph = fixedUserGraph.partitionBy(CanonicalRandomVertexCut) \npartitionedUserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@5749147e \nscala> // Generate the graph without parallel edges and combine the properties of duplicate edges \nscala> val graphWithoutParallelEdges = partitionedUserGraph.groupEdges((e1, e2) => e1 + \" and \" + e2) \ngraphWithoutParallelEdges: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@16a4961f \nscala> // Print the details \nscala> graphWithoutParallelEdges.triplets.foreach(println) \n((1,Thomas),(2,Krish),Follows and Son) \n((2,Krish),(3,Mathew),Follows) \n\n```", "```py\nscala> import org.apache.spark._\n  import org.apache.spark._    \n  scala> import org.apache.spark.graphx._\n  import org.apache.spark.graphx._    \n  scala> import org.apache.spark.rdd.RDD\n  import org.apache.spark.rdd.RDD\n    scala> //Define a property class that is going to hold all the properties of the vertex which is nothing but player information\n\tscala> case class Player(name: String, country: String)\n      defined class Player\n    scala> // Create the player vertices\n\tscala> val players: RDD[(Long, Player)] = sc.parallelize(Array((1L, Player(\"Novak Djokovic\", \"SRB\")), (3L, Player(\"Roger Federer\", \"SUI\")),(5L, Player(\"Tomas Berdych\", \"CZE\")), (7L, Player(\"Kei Nishikori\", \"JPN\")), (11L, Player(\"Andy Murray\", \"GBR\")),(15L, Player(\"Stan Wawrinka\", \"SUI\")),(17L, Player(\"Rafael Nadal\", \"ESP\")),(19L, Player(\"David Ferrer\", \"ESP\"))))\nplayers: org.apache.spark.rdd.RDD[(Long, Player)] = ParallelCollectionRDD[145] at parallelize at <console>:57\n    scala> //Define a property class that is going to hold all the properties of the edge which is nothing but match informationscala> case class Match(matchType: String, points: Int, head2HeadCount: Int)\n      defined class Match\n    scala> // Create the match edgesscala> val matches: RDD[Edge[Match]] = sc.parallelize(Array(Edge(1L, 5L, Match(\"G1\", 1,1)), Edge(1L, 7L, Match(\"G1\", 1,1)), Edge(3L, 1L, Match(\"G1\", 1,1)), Edge(3L, 5L, Match(\"G1\", 1,1)), Edge(3L, 7L, Match(\"G1\", 1,1)), Edge(7L, 5L, Match(\"G1\", 1,1)), Edge(11L, 19L, Match(\"G2\", 1,1)), Edge(15L, 11L, Match(\"G2\", 1, 1)), Edge(15L, 19L, Match(\"G2\", 1, 1)), Edge(17L, 11L, Match(\"G2\", 1, 1)), Edge(17L, 15L, Match(\"G2\", 1, 1)), Edge(17L, 19L, Match(\"G2\", 1, 1)), Edge(3L, 15L, Match(\"S\", 5, 1)), Edge(1L, 17L, Match(\"S\", 5, 1)), Edge(1L, 3L, Match(\"F\", 11, 1))))\nmatches: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Match]] = ParallelCollectionRDD[146] at parallelize at <console>:57\n    scala> //Create a graph with the vertices and edges\n\tscala> val playGraph = Graph(players, matches)\nplayGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@30d4d6fb \n\n```", "```py\nscala> //Print the match details\n\tscala> playGraph.triplets.foreach(println)\n((15,Player(Stan Wawrinka,SUI)),(11,Player(Andy Murray,GBR)),Match(G2,1,1))    \n((15,Player(Stan Wawrinka,SUI)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))    \n((7,Player(Kei Nishikori,JPN)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    \n((1,Player(Novak Djokovic,SRB)),(7,Player(Kei Nishikori,JPN)),Match(G1,1,1))    \n((3,Player(Roger Federer,SUI)),(1,Player(Novak Djokovic,SRB)),Match(G1,1,1))    \n((1,Player(Novak Djokovic,SRB)),(3,Player(Roger Federer,SUI)),Match(F,11,1))    \n((1,Player(Novak Djokovic,SRB)),(17,Player(Rafael Nadal,ESP)),Match(S,5,1))    \n((3,Player(Roger Federer,SUI)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    \n((17,Player(Rafael Nadal,ESP)),(11,Player(Andy Murray,GBR)),Match(G2,1,1))    \n((3,Player(Roger Federer,SUI)),(7,Player(Kei Nishikori,JPN)),Match(G1,1,1))    \n((1,Player(Novak Djokovic,SRB)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    \n((17,Player(Rafael Nadal,ESP)),(15,Player(Stan Wawrinka,SUI)),Match(G2,1,1))    \n((11,Player(Andy Murray,GBR)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))    \n((3,Player(Roger Federer,SUI)),(15,Player(Stan Wawrinka,SUI)),Match(S,5,1))    \n((17,Player(Rafael Nadal,ESP)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))\n    scala> //Print matches with player names and the match type and the resultscala> playGraph.triplets.map(triplet => triplet.srcAttr.name + \" won over \" + triplet.dstAttr.name + \" in  \" + triplet.attr.matchType + \" match\").foreach(println)\n      Roger Federer won over Tomas Berdych in  G1 match    \n      Roger Federer won over Kei Nishikori in  G1 match    \n      Novak Djokovic won over Roger Federer in  F match    \n      Novak Djokovic won over Rafael Nadal in  S match    \n      Roger Federer won over Stan Wawrinka in  S match    \n      Rafael Nadal won over David Ferrer in  G2 match    \n      Kei Nishikori won over Tomas Berdych in  G1 match    \n      Andy Murray won over David Ferrer in  G2 match    \n      Stan Wawrinka won over Andy Murray in  G2 match    \n      Stan Wawrinka won over David Ferrer in  G2 match    \n      Novak Djokovic won over Kei Nishikori in  G1 match    \n      Roger Federer won over Novak Djokovic in  G1 match    \n      Rafael Nadal won over Andy Murray in  G2 match    \n      Rafael Nadal won over Stan Wawrinka in  G2 match    \n      Novak Djokovic won over Tomas Berdych in  G1 match \n\n```", "```py\nscala> //Group 1 winners with their group total points\nscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"G1\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)\n      (Kei Nishikori,1)    \n      (Roger Federer,1)    \n      (Roger Federer,1)    \n      (Novak Djokovic,1)    \n      (Novak Djokovic,1)    \n      (Roger Federer,1)\n    scala> //Find the group total of the players\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"G1\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)\n      (Roger Federer,3)    \n      (Novak Djokovic,2)    \n      (Kei Nishikori,1)\n    scala> //Group 2 winners with their group total points\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"G2\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)\n      (Rafael Nadal,1)    \n      (Rafael Nadal,1)    \n      (Andy Murray,1)    \n      (Stan Wawrinka,1)    \n      (Stan Wawrinka,1)    \n      (Rafael Nadal,1) \n\n```", "```py\nscala> //Find the group total of the players\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"G2\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)\n      (Stan Wawrinka,2)    \n      (Andy Murray,1)    \n      (Rafael Nadal,3)\n    scala> //Semi final winners with their group total points\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"S\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)\n      (Novak Djokovic,5)    \n      (Roger Federer,5)\n    scala> //Find the group total of the players\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"S\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)\n      (Novak Djokovic,5)    \n      (Roger Federer,5)\n    scala> //Final winner with the group total points\n\tscala> playGraph.triplets.filter(triplet => triplet.attr.matchType == \"F\").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)\n      (Novak Djokovic,11)\n    scala> //Tournament total point standing\n\tscala> playGraph.triplets.map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)\n      (Stan Wawrinka,2)\n\n      (Rafael Nadal,3)    \n      (Kei Nishikori,1)    \n      (Andy Murray,1)    \n      (Roger Federer,8)    \n      (Novak Djokovic,18)\n    scala> //Find the winner of the tournament by finding the top scorer of the tournament\n\tscala> playGraph.triplets.map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).map{ case (k,v) => (v,k)}.sortByKey(ascending=false).take(1).map{ case (k,v) => (v,k)}.foreach(println)\n      (Novak Djokovic,18)\n    scala> //Find how many head to head matches held for a given set of players in the descending order of head2head count\n\tscala> playGraph.triplets.map(triplet => (Set(triplet.srcAttr.name , triplet.dstAttr.name) , triplet.attr.head2HeadCount)).reduceByKey(_+_).map{case (k,v) => (k.mkString(\" and \"), v)}.map{ case (k,v) => (v,k)}.sortByKey().map{ case (k,v) => v + \" played \" + k + \" time(s)\"}.foreach(println)\n      Roger Federer and Novak Djokovic played 2 time(s)    \n      Roger Federer and Tomas Berdych played 1 time(s)    \n      Kei Nishikori and Tomas Berdych played 1 time(s)    \n      Novak Djokovic and Tomas Berdych played 1 time(s)    \n      Rafael Nadal and Andy Murray played 1 time(s)    \n      Rafael Nadal and Stan Wawrinka played 1 time(s)    \n      Andy Murray and David Ferrer played 1 time(s)    \n      Rafael Nadal and David Ferrer played 1 time(s)    \n      Stan Wawrinka and David Ferrer played 1 time(s)    \n      Stan Wawrinka and Andy Murray played 1 time(s)    \n      Roger Federer and Stan Wawrinka played 1 time(s)    \n      Roger Federer and Kei Nishikori played 1 time(s)    \n      Novak Djokovic and Kei Nishikori played 1 time(s)    \n      Novak Djokovic and Rafael Nadal played 1 time(s) \n\n```", "```py\n scala> //List of players who have won at least one match\n\tscala> val winners = playGraph.triplets.map(triplet => triplet.srcAttr.name).distinct\nwinners: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[201] at distinct at <console>:65\n    scala> winners.foreach(println)\n      Kei Nishikori    \n      Stan Wawrinka    \n      Andy Murray    \n      Roger Federer    \n      Rafael Nadal    \n      Novak Djokovic\n    scala> //List of players who have lost at least one match\n\tscala> val loosers = playGraph.triplets.map(triplet => triplet.dstAttr.name).distinct\nloosers: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[205] at distinct at <console>:65\n    scala> loosers.foreach(println)\n      Novak Djokovic    \n      Kei Nishikori    \n      David Ferrer    \n      Stan Wawrinka    \n      Andy Murray    \n      Roger Federer    \n      Rafael Nadal    \n      Tomas Berdych\n    scala> //List of players who have won at least one match and lost at least one match\n\tscala> val wonAndLost = winners.intersection(loosers)\nwonAndLost: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[211] at intersection at <console>:69\n    scala> wonAndLost.foreach(println)\n      Novak Djokovic    \n      Rafael Nadal    \n      Andy Murray    \n      Roger Federer    \n      Kei Nishikori    \n      Stan Wawrinka \n    scala> //List of players who have no wins at all\n\tscala> val lostAndNoWins = loosers.collect().toSet -- wonAndLost.collect().toSet\nlostAndNoWins: \nscala.collection.immutable.Set[String] = Set(David Ferrer, Tomas Berdych)\n    scala> lostAndNoWins.foreach(println)\n      David Ferrer    \n      Tomas Berdych\n    scala> //List of players who have no loss at all\n\tscala> val wonAndNoLosses = winners.collect().toSet -- loosers.collect().toSet\n wonAndNoLosses: \n\t  scala.collection.immutable.Set[String] = Set() \nscala> //The val wonAndNoLosses returned an empty set which means that there is no single player in this tournament who have only wins\nscala> wonAndNoLosses.foreach(println)\n\n```", "```py\nscala> import org.apache.spark._\n  import org.apache.spark._ \n  scala> import org.apache.spark.graphx._\n  import org.apache.spark.graphx._    \n  scala> import org.apache.spark.rdd.RDD\n  import org.apache.spark.rdd.RDD\n    scala> //Define a property class that is going to hold all the properties of the vertex which is nothing but player informationscala> case class Player(name: String, country: String)\n      defined class Player\n    scala> // Create the player verticesscala> val players: RDD[(Long, Player)] = sc.parallelize(Array((1L, Player(\"Novak Djokovic\", \"SRB\")), (3L, Player(\"Roger Federer\", \"SUI\")),(5L, Player(\"Tomas Berdych\", \"CZE\")), (7L, Player(\"Kei Nishikori\", \"JPN\")), (11L, Player(\"Andy Murray\", \"GBR\")),(15L, Player(\"Stan Wawrinka\", \"SUI\")),(17L, Player(\"Rafael Nadal\", \"ESP\")),(19L, Player(\"David Ferrer\", \"ESP\"))))\nplayers: org.apache.spark.rdd.RDD[(Long, Player)] = ParallelCollectionRDD[212] at parallelize at <console>:64\n    scala> //Define a property class that is going to hold all the properties of the edge which is nothing but match informationscala> case class Match(matchType: String, points: Int, head2HeadCount: Int)\n      defined class Match\n    scala> // Create the match edgesscala> val matches: RDD[Edge[Match]] = sc.parallelize(Array(Edge(1L, 5L, Match(\"G1\", 1,1)), Edge(1L, 7L, Match(\"G1\", 1,1)), Edge(3L, 1L, Match(\"G1\", 1,1)), Edge(3L, 5L, Match(\"G1\", 1,1)), Edge(3L, 7L, Match(\"G1\", 1,1)), Edge(7L, 5L, Match(\"G1\", 1,1)), Edge(11L, 19L, Match(\"G2\", 1,1)), Edge(15L, 11L, Match(\"G2\", 1, 1)), Edge(15L, 19L, Match(\"G2\", 1, 1)), Edge(17L, 11L, Match(\"G2\", 1, 1)), Edge(17L, 15L, Match(\"G2\", 1, 1)), Edge(17L, 19L, Match(\"G2\", 1, 1)), Edge(3L, 15L, Match(\"S\", 5, 1)), Edge(1L, 17L, Match(\"S\", 5, 1)), Edge(1L, 3L, Match(\"F\", 11, 1))))\nmatches: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Match]] = ParallelCollectionRDD[213] at parallelize at <console>:64\n    scala> //Create a graph with the vertices and edgesscala> val playGraph = Graph(players, matches)\nplayGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@263cd0e2\n    scala> //Reverse this graph to have the winning player coming in the destination vertex\n\tscala> val rankGraph = playGraph.reverse\nrankGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@7bb131fb\n    scala> //Run the PageRank algorithm to calculate the rank of each vertex\n\tscala> val rankedVertices = rankGraph.pageRank(0.0001).vertices\nrankedVertices: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[1184] at RDD at VertexRDD.scala:57\n    scala> //Extract the vertices sorted by the rank\n\tscala> val rankedPlayers = rankedVertices.join(players).map{case \n\t(id,(importanceRank,Player(name,country))) => (importanceRank,\n\tname)}.sortByKey(ascending=false)\n\n\trankedPlayers: org.apache.spark.rdd.RDD[(Double, String)] = ShuffledRDD[1193] at sortByKey at <console>:76\n\n\tscala> rankedPlayers.collect().foreach(println)\n      (3.382662570589846,Novak Djokovic)    \n      (3.266079758089846,Roger Federer)    \n      (0.3908953124999999,Rafael Nadal)    \n      (0.27431249999999996,Stan Wawrinka)    \n      (0.1925,Andy Murray)    \n      (0.1925,Kei Nishikori)    \n      (0.15,David Ferrer)    \n      (0.15,Tomas Berdych) \n\n```", "```py\n\t scala> import org.apache.spark._\n\n  import org.apache.spark._    \n  scala> import org.apache.spark.graphx._\n\n  import org.apache.spark.graphx._    \n  scala> import org.apache.spark.rdd.RDD\n\n  import org.apache.spark.rdd.RDD    \n\n  scala> // Create the RDD with users as the vertices\n  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L, \"Thomas\"), (2L, \"Krish\"),(3L, \"Mathew\"), (4L, \"Martin\"), (5L, \"George\"), (6L, \"James\")))\n\nusers: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[1194] at parallelize at <console>:69\n\n\tscala> // Create the edges connecting the users\n\tscala> val userRelationships: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, \"Follows\"),Edge(2L, 3L, \"Follows\"), Edge(4L, 5L, \"Follows\"), Edge(5L, 6L, \"Follows\")))\n\nuserRelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1195] at parallelize at <console>:69\n\n\tscala> // Create a graph\n\tscala> val userGraph = Graph(users, userRelationships)\n\nuserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@805e363\n\n\tscala> // Find the connected components of the graph\n\tscala> val cc = userGraph.connectedComponents()\n\ncc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,String] = org.apache.spark.graphx.impl.GraphImpl@13f4a9a9\n\n\tscala> // Extract the triplets of the connected components\n\tscala> val ccTriplets = cc.triplets\n\nccTriplets: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,String]] = MapPartitionsRDD[1263] at mapPartitions at GraphImpl.scala:48\n\n\tscala> // Print the structure of the tripletsscala> ccTriplets.foreach(println)\n      ((1,1),(2,1),Follows)    \n\n      ((4,4),(5,4),Follows)    \n\n      ((5,4),(6,4),Follows)    \n\n      ((2,1),(3,1),Follows)\n\n\tscala> //Print the vertex numbers and the corresponding connected component id. The connected component id is generated by the system and it is to be taken only as a unique identifier for the connected component\n\tscala> val ccProperties = ccTriplets.map(triplet => \"Vertex \" + triplet.srcId + \" and \" + triplet.dstId + \" are part of the CC with id \" + triplet.srcAttr)\n\nccProperties: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1264] at map at <console>:79\n\n\tscala> ccProperties.foreach(println)\n\n      Vertex 1 and 2 are part of the CC with id 1    \n\n      Vertex 5 and 6 are part of the CC with id 4    \n\n      Vertex 2 and 3 are part of the CC with id 1    \n\n      Vertex 4 and 5 are part of the CC with id 4\n\n\tscala> //Find the users in the source vertex with their CC id\n\tscala> val srcUsersAndTheirCC = ccTriplets.map(triplet => (triplet.srcId, triplet.srcAttr))\n\nsrcUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1265] at map at <console>:79\n\n\tscala> //Find the users in the destination vertex with their CC id\n\tscala> val dstUsersAndTheirCC = ccTriplets.map(triplet => (triplet.dstId, triplet.dstAttr))\n\ndstUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1266] at map at <console>:79\n\n\tscala> //Find the union\n\tscala> val usersAndTheirCC = srcUsersAndTheirCC.union(dstUsersAndTheirCC)\n\nusersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = UnionRDD[1267] at union at <console>:83\n\n\tscala> //Join with the name of the users\n\tscala> val usersAndTheirCCWithName = usersAndTheirCC.join(users).map{case (userId,(ccId,userName)) => (ccId, userName)}.distinct.sortByKey()\n\nusersAndTheirCCWithName: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = ShuffledRDD[1277] at sortByKey at <console>:85\n\n\tscala> //Print the user names with their CC component id. If two users share the same CC id, then they are connected\n\tscala> usersAndTheirCCWithName.collect().foreach(println)\n\n      (1,Thomas)    \n\n      (1,Mathew)    \n\n      (1,Krish)    \n\n      (4,Martin)    \n\n      (4,James)    \n\n      (4,George) \n\n```", "```py\n\t $ cd $SPARK_1.6__HOME \n\t$ ./bin/spark-shell --packages graphframes:graphframes:0.1.0-spark1.6 \n\tIvy Default Cache set to: /Users/RajT/.ivy2/cache \n\tThe jars for the packages stored in: /Users/RajT/.ivy2/jars \n\t:: loading settings :: url = jar:file:/Users/RajT/source-code/spark-source/spark-1.6.1\n\t/assembly/target/scala-2.10/spark-assembly-1.6.2-SNAPSHOT-hadoop2.2.0.jar!\n\t/org/apache/ivy/core/settings/ivysettings.xml \n\tgraphframes#graphframes added as a dependency \n\t:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 \n\tconfs: [default] \n\tfound graphframes#graphframes;0.1.0-spark1.6 in list \n\t:: resolution report :: resolve 153ms :: artifacts dl 2ms \n\t:: modules in use: \n\tgraphframes#graphframes;0.1.0-spark1.6 from list in [default] \n   --------------------------------------------------------------------- \n   |                  |            modules            ||   artifacts   | \n   |       conf       | number| search|dwnlded|evicted|| number|dwnlded| \n   --------------------------------------------------------------------- \n   |      default     |   1   |   0   |   0   |   0   ||   1   |   0   | \n   --------------------------------------------------------------------- \n   :: retrieving :: org.apache.spark#spark-submit-parent \n   confs: [default] \n   0 artifacts copied, 1 already retrieved (0kB/5ms) \n   16/07/31 09:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \n   Welcome to \n      ____              __ \n     / __/__  ___ _____/ /__ \n    _\\ \\/ _ \\/ _ `/ __/  '_/ \n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1 \n       /_/ \n\n\t  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) \n\t  Type in expressions to have them evaluated. \n\t  Type :help for more information. \n\t  Spark context available as sc. \n\t  SQL context available as sqlContext. \n\t  scala> import org.graphframes._ \n\t  import org.graphframes._ \n\t  scala> import org.apache.spark.rdd.RDD \n\t  import org.apache.spark.rdd.RDD \n\t  scala> import org.apache.spark.sql.Row \n\t  import org.apache.spark.sql.Row \n\t  scala> import org.apache.spark.graphx._ \n\t  import org.apache.spark.graphx._ \n\t  scala> //Create a DataFrame of users containing tuple values with a mandatory Long and another String type as the property of the vertex \n\t  scala> val users = sqlContext.createDataFrame(List((1L, \"Thomas\"),(2L, \"Krish\"),(3L, \"Mathew\"))).toDF(\"id\", \"name\") \n\t  users: org.apache.spark.sql.DataFrame = [id: bigint, name: string] \n\t  scala> //Created a DataFrame for Edge with String type as the property of the edge \n\t  scala> val userRelationships = sqlContext.createDataFrame(List((1L, 2L, \"Follows\"),(1L, 2L, \"Son\"),(2L, 3L, \"Follows\"))).toDF(\"src\", \"dst\", \"relationship\") \n\t  userRelationships: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint, relationship: string] \n\t  scala> val userGraph = GraphFrame(users, userRelationships) \n\t  userGraph: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, name: string], e:[src: bigint, dst: bigint, relationship: string]) \n\t  scala> // Vertices in the graph \n\t  scala> userGraph.vertices.show() \n\t  +---+------+ \n\t  | id|  name| \n\t  +---+------+ \n\t  |  1|Thomas| \n\t  |  2| Krish| \n\t  |  3|Mathew| \n\t  +---+------+ \n\t  scala> // Edges in the graph \n\t  scala> userGraph.edges.show() \n\t  +---+---+------------+ \n\t  |src|dst|relationship| \n\t  +---+---+------------+ \n\t  |  1|  2|     Follows| \n\t  |  1|  2|         Son| \n\t  |  2|  3|     Follows| \n\t  +---+---+------------+ \n\t  scala> //Number of edges in the graph \n\t  scala> val edgeCount = userGraph.edges.count() \n\t  edgeCount: Long = 3 \n\t  scala> //Number of vertices in the graph \n\t  scala> val vertexCount = userGraph.vertices.count() \n\t  vertexCount: Long = 3 \n\t  scala> //Number of edges coming to each of the vertex.  \n\t  scala> userGraph.inDegrees.show() \n\t  +---+--------+ \n\t  | id|inDegree| \n\t  +---+--------+ \n\t  |  2|       2| \n\t  |  3|       1| \n\t  +---+--------+ \n\t  scala> //Number of edges going out of each of the vertex.  \n\t  scala> userGraph.outDegrees.show() \n\t  +---+---------+ \n\t  | id|outDegree| \n\t  +---+---------+ \n\t  |  1|        2| \n\t  |  2|        1| \n\t  +---+---------+ \n\t  scala> //Total number of edges coming in and going out of each vertex.  \n\t  scala> userGraph.degrees.show() \n\t  +---+------+ \n\t  | id|degree| \n\t  +---+------+ \n\t  |  1|     2| \n\t  |  2|     3| \n\t  |  3|     1| \n\t  +---+------+ \n\t  scala> //Get the triplets of the graph \n\t  scala> userGraph.triplets.show() \n\t  +-------------+----------+----------+ \n\t  |         edge|       src|       dst| \n\t  +-------------+----------+----------+ \n\t  |[1,2,Follows]|[1,Thomas]| [2,Krish]| \n\t  |    [1,2,Son]|[1,Thomas]| [2,Krish]| \n\t  |[2,3,Follows]| [2,Krish]|[3,Mathew]| \n\t  +-------------+----------+----------+ \n\t  scala> //Using the DataFrame API, apply filter and select only the needed edges \n\t  scala> val numFollows = userGraph.edges.filter(\"relationship = 'Follows'\").count() \n\t  numFollows: Long = 2 \n\t  scala> //Create an RDD of users containing tuple values with a mandatory Long and another String type as the property of the vertex \n\t  scala> val usersRDD: RDD[(Long, String)] = sc.parallelize(Array((1L, \"Thomas\"), (2L, \"Krish\"),(3L, \"Mathew\"))) \n\t  usersRDD: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[54] at parallelize at <console>:35 \n\t  scala> //Created an RDD of Edge type with String type as the property of the edge \n\t  scala> val userRelationshipsRDD: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, \"Follows\"),    Edge(1L, 2L, \"Son\"),Edge(2L, 3L, \"Follows\"))) \n\t  userRelationshipsRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[55] at parallelize at <console>:35 \n\t  scala> //Create a graph containing the vertex and edge RDDs as created before \n\t  scala> val userGraphXFromRDD = Graph(usersRDD, userRelationshipsRDD) \n\t  userGraphXFromRDD: org.apache.spark.graphx.Graph[String,String] = \n\t  org.apache.spark.graphx.impl.GraphImpl@77a3c614 \n\t  scala> //Create the GraphFrame based graph from Spark GraphX based graph \n\t  scala> val userGraphFrameFromGraphX: GraphFrame = GraphFrame.fromGraphX(userGraphXFromRDD) \n\t  userGraphFrameFromGraphX: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, attr: string], e:[src: bigint, dst: bigint, attr: string]) \n\t  scala> userGraphFrameFromGraphX.triplets.show() \n\t  +-------------+----------+----------+\n\t  |         edge|       src|       dst| \n\t  +-------------+----------+----------+ \n\t  |[1,2,Follows]|[1,Thomas]| [2,Krish]| \n\t  |    [1,2,Son]|[1,Thomas]| [2,Krish]| \n\t  |[2,3,Follows]| [2,Krish]|[3,Mathew]| \n\t  +-------------+----------+----------+ \n\t  scala> // Convert the GraphFrame based graph to a Spark GraphX based graph \n\t  scala> val userGraphXFromGraphFrame: Graph[Row, Row] = userGraphFrameFromGraphX.toGraphX \n\t  userGraphXFromGraphFrame: org.apache.spark.graphx.Graph[org.apache.spark.sql.Row,org.apache.spark.sql.Row] = org.apache.spark.graphx.impl.GraphImpl@238d6aa2 \n\n```", "```py\n $ cd $SPARK_1.6_HOME \n\t  $ ./bin/spark-shell --packages graphframes:graphframes:0.1.0-spark1.6 \n\t  Ivy Default Cache set to: /Users/RajT/.ivy2/cache \n\t  The jars for the packages stored in: /Users/RajT/.ivy2/jars \n\t  :: loading settings :: url = jar:file:/Users/RajT/source-code/spark-source/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.2-SNAPSHOT-hadoop2.2.0.jar!/org/apache/ivy/core/settings/ivysettings.xml \n\t  graphframes#graphframes added as a dependency \n\t  :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 \n\t  confs: [default] \n\t  found graphframes#graphframes;0.1.0-spark1.6 in list \n\t  :: resolution report :: resolve 145ms :: artifacts dl 2ms \n\t  :: modules in use: \n\t  graphframes#graphframes;0.1.0-spark1.6 from list in [default] \n\t  --------------------------------------------------------------------- \n\t  |                  |            modules            ||   artifacts   | \n\t  |       conf       | number| search|dwnlded|evicted|| number|dwnlded| \n\t  --------------------------------------------------------------------- \n\t  |      default     |   1   |   0   |   0   |   0   ||   1   |   0   | \n\t  --------------------------------------------------------------------- \n\t  :: retrieving :: org.apache.spark#spark-submit-parent \n\t  confs: [default] \n\t  0 artifacts copied, 1 already retrieved (0kB/5ms) \n\t  16/07/29 07:09:08 WARN NativeCodeLoader: \n\t  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \n\t  Welcome to \n      ____              __ \n     / __/__  ___ _____/ /__ \n    _\\ \\/ _ \\/ _ `/ __/  '_/ \n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1 \n      /_/ \n\n\t  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) \n\t  Type in expressions to have them evaluated. \n\t  Type :help for more information. \n\t  Spark context available as sc. \n\t  SQL context available as sqlContext. \n\t  scala> import org.graphframes._ \n\t  import org.graphframes._ \n\t  scala> import org.apache.spark.rdd.RDD \n\t  import org.apache.spark.rdd.RDD \n\t  scala> import org.apache.spark.sql.Row \n\t  import org.apache.spark.sql.Row \n\t  scala> import org.apache.spark.graphx._ \n\t  import org.apache.spark.graphx._ \n\t  scala> //Create a DataFrame of users containing tuple values with a mandatory String field as id and another String type as the property of the vertex. Here it can be seen that the vertex identifier is no longer a long integer. \n\t  scala> val users = sqlContext.createDataFrame(List((\"1\", \"Thomas\"),(\"2\", \"Krish\"),(\"3\", \"Mathew\"))).toDF(\"id\", \"name\") \n\t  users: org.apache.spark.sql.DataFrame = [id: string, name: string] \n\t  scala> //Create a DataFrame for Edge with String type as the property of the edge \n\t  scala> val userRelationships = sqlContext.createDataFrame(List((\"1\", \"2\", \"Follows\"),(\"2\", \"1\", \"Follows\"),(\"2\", \"3\", \"Follows\"))).toDF(\"src\", \"dst\", \"relationship\") \n\t  userRelationships: org.apache.spark.sql.DataFrame = [src: string, dst: string, relationship: string] \n\t  scala> //Create the GraphFrame \n\t  scala> val userGraph = GraphFrame(users, userRelationships) \n\t  userGraph: org.graphframes.GraphFrame = GraphFrame(v:[id: string, name: string], e:[src: string, dst: string, relationship: string]) \n\t  scala> // Search for pairs of users who are following each other \n\t  scala> // In other words the query can be read like this. Find the list of users having a pattern such that user u1 is related to user u2 using the edge e1 and user u2 is related to the user u1 using the edge e2\\. When a query is formed like this, the result will list with columns u1, u2, e1 and e2\\. When modelling real-world use cases, more meaningful variables can be used suitable for the use case. \n\t  scala> val graphQuery = userGraph.find(\"(u1)-[e1]->(u2); (u2)-[e2]->(u1)\") \n\t  graphQuery: org.apache.spark.sql.DataFrame = [e1: struct<src:string,dst:string,relationship:string>, u1: struct<\n\t  d:string,name:string>, u2: struct<id:string,name:string>, e2: struct<src:string,dst:string,relationship:string>] \n\t  scala> graphQuery.show() \n\t  +-------------+----------+----------+-------------+\n\n\t  |           e1|        u1|        u2|           e2| \n\t  +-------------+----------+----------+-------------+ \n\t  |[1,2,Follows]|[1,Thomas]| [2,Krish]|[2,1,Follows]| \n\t  |[2,1,Follows]| [2,Krish]|[1,Thomas]|[1,2,Follows]| \n\t  +-------------+----------+----------+-------------+\n\n```"]