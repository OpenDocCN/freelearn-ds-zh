- en: Chapter 1. Using OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Querying OpenCL platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying OpenCL devices on your platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying for OpenCL device extensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying OpenCL contexts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying an OpenCL program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating OpenCL kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating command queues and enqueuing OpenCL kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start the journey by looking back into the history of computing and why
    OpenCL is important from the respect that it aims to unify the software programming
    model for heterogeneous devices. The goal of OpenCL is to develop a royalty-free
    standard for cross-platform, parallel programming of modern processors found in
    personal computers, servers, and handheld/embedded devices. This effort is taken
    by "The Khronos Group" along with the participation of companies such as Intel,
    ARM, AMD, NVIDIA, QUALCOMM, Apple, and many others. OpenCL allows the software
    to be written once and then executed on the devices that support it. In this way
    it is akin to Java, this has benefits because software development on these devices
    now has a uniform approach, and OpenCL does this by exposing the hardware via
    various data structures, and these structures interact with the hardware via **Application
    Programmable Interfaces** (**APIs**). Today, OpenCL supports CPUs that includes
    x86s, ARM and PowerPC and GPUs by AMD, Intel, and NVIDIA.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can definitely appreciate the fact that we need to develop software
    that is cross-platform compatible, since it allows the developers to develop an
    application on whatever platform they are comfortable with, without mentioning
    that it provides a coherent model in which we can express our thoughts into a
    program that can be executed on any device that supports this standard. However,
    what cross-platform compatibility also means is the fact that heterogeneous environments
    exists, and for quite some time, developers have to learn and grapple with the
    issues that arise when writing software for those devices ranging from execution
    model to memory systems. Another task that commonly arose from developing software
    on those heterogeneous devices is that developers were expected to express and
    extract parallelism from them as well. Before OpenCL, we know that various programming
    languages and their philosophies were invented to handle the aspect of expressing
    parallelism (for example, Fortran, OpenMP, MPI, VHDL, Verilog, Cilk, Intel TBB,
    Unified parallel C, Java among others) on the device they executed on. But these
    tools were designed for the homogeneous environments, even though a developer
    may think that it's to his/her advantage, since it adds considerable expertise
    to their resume. Taking a step back and looking at it again reveals that is there
    is no unified approach to express parallelism in heterogeneous environments. We
    need not mention the amount of time developers need to be productive in these
    technologies, since parallel decomposition is normally an involved process as
    it's largely hardware dependent. To add salt to the wound, many developers only
    have to deal with homogeneous computing environments, but in the past few years
    the demand for heterogeneous computing environments grew.
  prefs: []
  type: TYPE_NORMAL
- en: The demand for heterogeneous devices grew partially due to the need for high
    performance and highly reactive systems, and with the "power wall" at play, one
    possible way to improve more performance was to add specialized processing units
    in the hope of extracting every ounce of parallelism from them, since that's the
    only way to reach power efficiency. The primary motivation for this shift to hybrid
    computing could be traced to the research headed entitled *Optimizing power using
    Transformations* by *Anantha P. Chandrakasan*. It brought out a conclusion that
    basically says that many-core chips (which run at a slightly lower frequency than
    a contemporary CPU) are actually more power-efficient. The problem with heterogeneous
    computing without a unified development methodology, for example, OpenCL, is that
    developers need to grasp several types of ISA and with that the various levels
    of parallelism and their memory systems are possible. CUDA, the GPGPU computing
    toolkit, developed by NVIDIA deserves a mention not only because of the remarkable
    similarity it has with OpenCL, but also because the toolkit has a wide adoption
    in academia as well as industry. Unfortunately CUDA can only drive NVIDIA's GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to extract parallelism from an environment that's heterogeneous
    is an important one simply because the computation should be parallel, otherwise
    it would defeat the entire purpose of OpenCL. Fortunately, major processor companies
    are part of the consortium led by The Khronos Group and actively realizing the
    standard through those organizations. Unfortunately the story doesn't end there,
    but the good thing is that we, developers, realized that a need to understand
    parallelism and how it works in both homogeneous and heterogeneous environments.
    OpenCL was designed with the intention to express parallelism in a heterogeneous
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: For a long time, developers have largely ignored the fact that their software
    needs to take advantage of the multi-core machines available to them and continued
    to develop their software in a single-threaded environment, but that is changing
    (as discussed previously). In the many-core world, developers need to grapple
    with the concept of concurrency, and the advantage of concurrency is that when
    used effectively, it maximizes the utilization of resources by providing progress
    to others while some are stalled.
  prefs: []
  type: TYPE_NORMAL
- en: 'When software is executed concurrently with multiple processing elements so
    that threads can run simultaneously, we have parallel computation. The challenge
    that the developer has is to discover that concurrency and realize it. And in
    OpenCL, we focus on two parallel programming models: task parallelism and data
    parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: Task parallelism means that developers can create and manipulate concurrent
    tasks. When developers are developing a solution for OpenCL, they would need to
    decompose a problem into different tasks and some of those tasks can be run concurrently,
    and it is these tasks that get mapped to **processing elements** (**PEs**) of
    a parallel environment for execution. On the other side of the story, there are
    tasks that cannot be run concurrently and even possibly interdependent. An additional
    complexity is also the fact that data can be shared between tasks.
  prefs: []
  type: TYPE_NORMAL
- en: When attempting to realize data parallelism, the developer needs to readjust
    the way they think about data and how they can be read and updated concurrently.
    A common problem found in parallel computation would be to compute the sum of
    all the elements given in an arbitrary array of values, while storing the intermediary
    summed value and one possible way to do this is illustrated in the following diagram
    and the operator being applied there, that is, ![Introduction](img/4520OT_01_07.jpg)
    is any binary associative operator. Conceptually, the developer could use a task
    to perform the addition of two elements of that input to derive the summed value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/4520OT_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Whether the developer chooses to embody task/data parallelism is dependent on
    the problem, and an example where task parallelism would make sense will be by
    traversing a graph. And regardless of which model the developer is more inclined
    with, they come with their own sets of problems when you start to map the program
    to the hardware via OpenCL. And before the advent of OpenCL, the developer needs
    to develop a module that will execute on the desired device and communication,
    and I/O with the driver program. An example example of this would be a graphics
    rendering program where the CPU initializes the data and sets everything up, before
    offloading the rendering to the GPU. OpenCL was designed to take advantage of
    all devices detected so that resource utilization is maximized, and hence in this
    respect it differs from the "traditional" way of software development.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have established a good understanding of OpenCL, we should spend
    some time understanding how a developer can learn it. And not to fret, because
    every project you embark with, OpenCL will need you to understand the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Discover the makeup of the heterogeneous system you are developing for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the properties of those devices by probing it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start the parallel program decomposition using either or all of task parallelism
    or data parallelism, by expressing them into instructions also known as kernels
    that will run on the platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up data structures for the computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate memory objects for the computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the kernels in the order that's desired on the proper device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collate the results and verify for correctness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we need to solidify the preceding points by taking a deeper look into
    the various components of OpenCL. The following components collectively make up
    the OpenCL architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Platform Model**: A platform is actually a host that is connected to one
    or more OpenCL devices. Each device comprises possibly multiple **compute units**
    (**CUs**) which can be decomposed into one or possibly multiple processing elements,
    and it is on the processing elements where computation will run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution Model**: Execution of an OpenCL program is such that the host program
    would execute on the host, and it is the host program which sends kernels to execute
    on one or more OpenCL devices on that platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a kernel is submitted for execution, an index space is defined such that
    a work item is instantiated to execute each point in that space. A work item would
    be identified by its global ID and it executes the same code as expressed in the
    kernel. Work items are grouped into work groups and each work group is given an
    ID commonly known as its work group ID, and it is the work group's work items
    that get executed concurrently on the PEs of a single CU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That index space we mentioned earlier is known as NDRange describing an N-dimensional
    space, where N can range from one to three. Each work item has a global ID and
    a local ID when grouped into work groups, that is distinct from the other and
    is derived from NDRange. The same can be said about work group IDs. Let's use
    a simple example to illustrate how they work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Given two arrays, A and B, of 1024 elements each, we would like to perform
    the computation of vector multiplication also known as dot product, where each
    element of A would be multiplied by the corresponding element in B. The kernel
    code would look something as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this scenario, let's assume we have 1024 processing elements and we would
    assign one work item to perform exactly one multiplication, and in this case our
    work group ID would be zero (since there's only one group) and work items IDs
    would range from {0 … 1023}. Recall what we discussed earlier, that it is the
    work group's work items that can executed on the PEs. Hence reflecting back, this
    would not be a good way of utilizing the device.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this same scenario, let''s ditch the former assumption and go with this:
    we still have 1024 elements but we group four work items into a group, hence we
    would have 256 work groups with each work group having an ID ranging from {0 …
    255}, but it is noticed that the work item''s global ID still would range from
    {0 … 1023} simply because we have not increased the number of elements to be processed.
    This manner of grouping work items into their work groups is to achieve scalability
    in these devices, since it increases execution efficiency by ensuring all PEs
    have something to work on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The NDRange can be conceptually mapped into an N-dimensional grid and the following
    diagram illustrates how a 2DRange works, where WG-X denotes the length in rows
    for a particular work group and WG-Y denotes the length in columns for a work
    group, and how work items are grouped including their respective IDs in a work
    group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Introduction](img/4520OT_01_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Before the execution of the kernels on the device(s), the host program plays
    an important role and that is to establish context with the underlying devices
    and laying down the order of execution of the tasks. The host program does the
    context creation by establishing the existence (creating if necessary) of the
    following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All devices to be used by the host program
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCL kernels, that is, functions and their abstractions that will run
    on those devices
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory objects that encapsulated the data to be used / shared by the OpenCL
    kernels.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once that is achieved, the host needs to create a data structure called a command
    queue that will be used by the host to coordinate the execution of the kernels
    on the devices and commands are issued to this queue and scheduled onto the devices.
    A command queue can accept: kernel execution commands, memory transfer commands,
    and synchronization commands. Additionally, the command queues can execute the
    commands in-order, that is, in the order they''ve been given, or out-of-order.
    If the problem is decomposed into independent tasks, it is possible to create
    multiple command queues targeting different devices and scheduling those tasks
    onto them, and then OpenCL will run them concurrently.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Model**: So far, we have understood the execution model and it''s
    time to introduce the memory model that OpenCL has stipulated. Recall that when
    the kernel executes, it is actually the work item that is executing its instance
    of the kernel code. Hence the work item needs to read and write the data from
    memory and each work item has access to four types of memories: global, constant,
    local, and private. These memories vary from size as well as accessibilities,
    where global memory has the largest size and is most accessible to work items,
    whereas private memory is possibly the most restrictive in the sense that it''s
    private to the work item. The constant memory is a read-only memory where immutable
    objects are stored and can be shared with all work items. The local memory is
    only available to all work items executing in the work group and is held by each
    compute unit, that is, CU-specific.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application running on the host uses the OpenCL API to create memory objects
    in global memory and will enqueue memory commands to the command queue to operate
    on them. The host's responsibility is to ensure that data is available to the
    device when the kernel starts execution, and it does so by copying data or by
    mapping/unmapping regions of memory objects. During a typical data transfer from
    the host memory to the device memory, OpenCL commands are issued to queues which
    may be blocking or non-blocking. The primary difference between a blocking and
    non-blocking memory transfer is that in the former, the function calls return
    only once (after being queued) it is deemed safe, and in the latter the call returns
    as soon as the command is enqueued.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Memory mapping in OpenCL allows a region of memory space to be available for
    computation and this region can be blocking or non-blocking and the developer
    can treat this space as readable or writeable or both.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hence forth, we are going to focus on getting the basics of OpenCL by letting
    our hands get dirty in developing small OpenCL programs to understand a bit more,
    programmatically, how to use the platform and execution model of OpenCL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The OpenCL specification Version 1.2 is an open, royalty-free standard for
    general purpose programming across various devices ranging from mobile to conventional
    CPUs, and lately GPUs through an API and the standard at the time of writing supports:'
  prefs: []
  type: TYPE_NORMAL
- en: Data and task based parallel programming models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements a subset of ISO C99 with extensions for parallelism with some restrictions
    such as recursion, variadic functions, and macros which are not supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical operations comply to the IEEE 754 specification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting to handheld and embedded devices can be accomplished by establishing
    configuration profiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interoperability with OpenGL, OpenGL ES, and other graphics APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, we are going to show you how you can become proficient
    in programming OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: As you go through the book, you'll discover not only how to use the API to perform
    all kinds of operations on your OpenCL devices, but you'll also learn how to model
    a problem and transform it from a serial program to a parallel program. More often
    than not, the techniques you'll learn can be transferred to other programming
    toolsets.
  prefs: []
  type: TYPE_NORMAL
- en: In the toolsets, I have worked with OpenCL^(TM), CUDA^(TM), OpenMP^(TM), MPI^(TM),
    Intel thread building blocks^(TM), Cilk^(TM), CilkPlus^(TM), which allows the
    developer to express parallelism in a homogeneous environment and find the entire
    process of learning the tools to application of knowledge to be classified into
    four parts. These four phases are rather common and I find it extremely helpful
    to remember them as I go along. I hope you will be benefited from them as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding concurrency**: The programmer works in the problem domain to identify
    the available concurrency and expose it to use in the algorithm design'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm structure**: The programmer works with high-level structures for
    organizing a parallel algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supporting Structures**: This refers to how the parallel program will be
    organized and the techniques used to manage shared data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation mechanisms**: The final step is to look at specific software
    constructs for implementing a parallel program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't worry about these concepts, they'll be explained as we move through the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: The next few recipes we are going to examine have to do with understanding the
    usage of OpenCL APIs, by focusing our efforts in understanding the platform model
    of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Querying OpenCL platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you start coding, ensure that you have installed the appropriate OpenCL
    development toolkit for the platform you are developing for. In this recipe, we
    are going to demonstrate how you can use OpenCL to query its platform to retrieve
    simple information about the compliant devices it has detected and its various
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this first OpenCL application, you'll get to query your computer for the
    sort of OpenCL platform that's installed. In the setup of your computer, you could
    have a configuration where both NVIDIA and AMD graphic cards are installed, and
    in this case you might have installed both the AMD APP SDK and NVIDIA's OpenCL
    toolkit. And hence you would have both the platforms installed.
  prefs: []
  type: TYPE_NORMAL
- en: The following code listing is extracted from `Ch1/platform_details/platform_details.c`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pay attention to the included comments, as they would help you to understand
    each individual function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile it on the UNIX platform, you would run a compile command similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When that happens, you would have a binary executable named `platform_details`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the program, simply execute the `platform_details` program, and a sample
    output will be an OSX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you first learn to program OpenCL, it can be a daunting task but it does
    get better as we move along. So, let''s decipher the source code that we''ve just
    seen. The file is a C source file and what you''ll notice is that it''s arranged
    such that the system header files are almost always placed right near the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is what the C programmers would call as the platform-dependent code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The OpenCL header files are needed for the program to be compiled because they
    contain the method signatures. Now, we will try to understand what the rest of
    the code is doing. In OpenCL, one of the code conventions is to have data types
    be prefixed by `cl_` and you'll find data types for each of the platform, device
    and context as `cl_platform_XX`, `cl_device_XX`, `cl_context_XX`, and APIs prefixed
    in a similar fashion by `cl` and one such API is `clGetPlatformInfo`.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCL, the APIs do not assume that you know exactly how many resources (for
    example platforms, devices, and contexts) are present or are needed when you write
    the OpenCL code. And in order to write portable code, the developers of the language
    have figured out a clever way to present the API such that you use the same API
    to pose a general question and based on the results of that question, request
    more information via the same API. Let me illustrate with an example.
  prefs: []
  type: TYPE_NORMAL
- en: In the code, you will notice that `clGetPlatformInfo()` was invoked twice. The
    first invocation was to query the number of platforms that were installed on the
    machine. Based on the results of that query, we invoked `clGetPlatformInfo` again,
    but this time we passed in context-sensitive information, for example, obtaining
    the name of the vendor. You'll find this pattern recurring when programming with
    OpenCL and the cons, I can think of is that it makes the API rather cryptic at
    times, but the nice thing about it is that it prevents the proliferation of APIs
    in the language.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, this is rather trivial when it comes to the entire ecosystem of
    programming OpenCL, but subsequent chapters will show how you can transform sequential
    code to parallel code in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's build on the code and query OpenCL for the devices that are attached
    to the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Querying OpenCL devices on your platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll now query OpenCL devices that are installed on your platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code listing discussed in the *How to do it…* section presents an abbreviated
    portion of the code in `Ch1/device_details/device_details.c`. This code demonstrates
    how you can obtain the types of devices installed on your platform via `clGetDeviceIDs`.
    You'll use that information to retrieve detailed data about the device by passing
    it to `clGetDeviceInfo`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, you need to completely reference the appropriate chapter code.
    Pay attention to the included comments, as they would help you understand each
    individual function. We''ve included the main part of this recipe with highlighted
    commentary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On UNIX platforms, you can compile `device_details.c` by running this command
    on your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And a binary executable named `device_details` should be deposited locally on
    your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you execute the binary executable depending on your machine''s setup,
    you will see varying results. But on my OSX platform here is the output when executed
    on a machine with Intel Core i5 processor with a NVIDIA mobile GPU GT330m (extensions
    are highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Don't worry too much if the information doesn't seem to make sense right now,
    the subsequent chapters will reveal all.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Leveraging the work we did in the previous section, now we have made use of
    the platform via `clGetPlatformInfo`, that was detected to query for the devices
    attached. This time, we used new API functions, `clGetDeviceIDs` and `clGetDeviceInfo`.
    The former attempts to uncover all the basic information about the devices attached
    to the given platform, and we use `clGetDeviceInfo` to iterate through the results
    to understand more about their capabilities. This information is valuable when
    you are crafting your algorithm and is not very sure about what device it's going
    to be run on. Considering that OpenCL supports various processors, it's a good
    way to write portable code.
  prefs: []
  type: TYPE_NORMAL
- en: There is actually a lot more information you can derive from your device and
    I'd strongly suggest you to refer [http://www.khronos.org/registry/cl/sdk/2.0/docs/man/xhtml/](http://www.khronos.org/registry/cl/sdk/2.0/docs/man/xhtml/)
    and look at the main page for `clGetDeviceInfo`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've understood how to query the platform and the attached devices,
    we should take a look at how to query OpenCL extensions. The extensions allow
    the vendor to define additional capabilities that's delivered with the OpenCL
    compliant device, which in turn allows you, the programmer, to utilize them.
  prefs: []
  type: TYPE_NORMAL
- en: Querying for OpenCL device extensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The extensions in OpenCL allow the programmer to leverage on additional capabilities
    provided by the vendor of the device, and hence they're optional. However, there
    are extensions that are recognized by OpenCL and purportedly supported by major
    vendors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a partial list of the approved and supported extensions in OpenCL 1.2\.
    If you wish to discover the entire list of extensions that adopters of OpenCL
    have made public (some are given in the table), please refer to the PDF document
    via this link: [http://www.khronos.org/registry/cl/specs/opencl-1.2-extensions.pdf](http://www.khronos.org/registry/cl/specs/opencl-1.2-extensions.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Extension name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_fp64` | This expression gives a double precision floating-point |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_int64_base_atomics` | This expression gives 64-bit integer base atomic
    operations, provides atomic operations for addition, subtraction, exchange, increment/decrement,
    and CAS |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_int64_extended_atomics` | This expression gives 64-bit integer extended
    atomic operations, provides atomic operations for finding the minimum, maximum,
    and boolean operations such as and, or, and xor |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_3d_image_writes` | This expression writes to 3D image objects |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_fp16` | This expression gives a halfly precised floating point |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_global_int32_base_atomics` | This expression gives atomics for 32-bit
    operands |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_global_int32_extended_atomics` | This expression gives more atomic
    functionality for 32-bit operands |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_local_int32_base_atomics` | This expression gives atomics for 32-bit
    operands in shared memory space |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_local_int32_extended_atomics` | This expression gives more atomic
    functionality for 32-bit operands in shared memory space |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_byte_addressable_store` | This expression allows memory writes to
    bytes less than a 32-bit word |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_APPLE_gl_sharing` | This expression provides MacOSX OpenGL sharing, and
    also allows applications to use the OpenGL buffer, texture, and render buffer
    objects as OpenCL memory objects |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_gl_sharing` | This expression provides OpenGL sharing |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_gl_event` | This expression retrieves CL event objects from GL sync
    objects |'
  prefs: []
  type: TYPE_TB
- en: '| `cl_khr_d3d10_sharing` | This expression shares memory objects with Direct3D
    10 |'
  prefs: []
  type: TYPE_TB
- en: Next, let's find out how we can determine what extensions are supported and
    available on your platform by leveraging the previous code we've worked on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The listing below only shows the interesting portion of the code found in `Ch1/device_extensions/device_extensions.c`.
    Various devices that are OpenCL compliant will have different capabilities, and
    during your application development you definitely want to make sure certain extensions
    are present prior to making use of them. The code discussed in the *How to do
    it…* section of this recipe shows you how to retrieve those extensions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve included the main querying function, which allows you to implement this
    particular recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile the code, do as you did before by running a similar command on your
    terminal like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On a UNIX platform, here''s what we got when executed on an Intel Core i5 processor
    with an NVIDIA mobile GPU GT330m (extensions are highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we examine the work we just did, we simply leveraged on the existing code
    and added the needed functionality where it was required, namely by adding code
    to handle the case where `CL_DEVICE_EXTENSIONS` was being passed in. We created
    an array of a fixed size on the stack and passed that array to `clGetDeviceInfo`,
    where the API will eventually store the information into the array. Extracting
    the information is as simple as printing out the array. For advanced usage, you
    might want to deposit that information into a global table structure where the
    other parts of the application can make use of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what those extensions mean and how you can take advantage of
    them, I''d suggest that you refer to the Khronos register for OpenCL: [http://www.khronos.org/registry/cl/](http://www.khronos.org/registry/cl/).'
  prefs: []
  type: TYPE_NORMAL
- en: We won't dwell too much on each extension that we've seen so far. Let's move
    on to understanding the OpenCL contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Querying OpenCL contexts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An OpenCL context is created with one or more devices. Contexts are used by
    the OpenCL runtime for managing objects such as command queues (the object that
    allows you to send commands to the device), memory, program, and kernel objects,
    and for executing kernels on one or more devices specified in the context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In more detail, OpenCL contexts can be created by associating a collection
    of devices that are available for the platform via `clCreateContext` or by associating
    it with a particular type of device, for example, CPU, GPUs, and so on, via `clCreateContextFromType`.
    However, in either way you cannot create contexts that are associated with more
    than one platform. Let''s use the example of vector multiplication in the *Introduction*
    section to demonstrate these concepts. The problem of vector multiplication or
    dot product can be solved using: pen and paper, CPU, GPU, or GPU + CPU. Obviously,
    the first option doesn''t quite scale when we have a little more than 20 elements
    and with OpenCL you have more options. The first thing you need to decide is which
    platform it should be run, and in OpenCL it means deciding whether to use the
    AMD, NVIDIA, Intel, and so on. And what comes next is to decide whether to run
    the dot product on all of the devices listed for that platform or only some of
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: So, let's assume that the platform reports one Intel Core i7 and 3 AMD GPUs
    and the developer could use the `clCreateContextFromType` to restrict execution
    to either CPUs or GPUs, but when you use `clCreateContext`, you can list all the
    four devices to be executed against, theoretically speaking (however, in practice
    it's hard to use all CPUs and GPUs effectively because the GPU can push more threads
    for execution than the CPU). The following diagram illustrates the options available
    to the developer to create contexts assuming the host environment is installed
    with both Intel and AMD's OpenCL platform software. The configuration gets a little
    more interesting when you consider the Ivy Bridge Intel processor, which includes
    an HD Graphics co-processor that allows a context that's both CPU and GPU aware.
  prefs: []
  type: TYPE_NORMAL
- en: '![Querying OpenCL contexts](img/4520OT_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Contexts have another interesting property, that is, it retains a reference
    count so that third-party libraries can refer to it and hence utilize the devices.
    For example, if the `cl_khr_d3d10_sharing` extension is available on your device,
    you can actually interoperate between OpenCL and Direct3D 10, and treat Direct3D
    10 resources similar to memory objects as OpenCL memory objects that you can read
    from or write to. However, we will not demonstrate the capability with this extension
    in this book and will instead leave it to the reader to engage themselves in further
    exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code listing given in the *How to do it…* section is extracted from `Ch1/context_query/context_details.c`,
    and it illustrates how to create and release OpenCL contexts.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To query an OpenCL context, you need to include a function similar to the following
    in your code. You should reference the full code listing alongside this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: On UNIX platforms, you can compile and build the program by typing the following
    command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'On the test machine, we have two OpenCL compliant devices. The first is the
    Intel Core i5 CPU, and the second is the NVIDIA mobile GT330m GPU. And the following
    is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have been following the book, you should realize that we didn't do anything
    special other than leverage on the previous exercises where we discover the sort
    of platforms installed, and with that uncover the devices and finally use that
    information to create the relevant contexts. Finally, with those relevant contexts
    we can query them. What you will notice is that the context's reference count
    is one in both cases, which indicates that a memory object is currently referencing
    it and the fact that we passed in `CL_CONTEXT_REFERENCE_COUNT` reflects this.
    This counter is only good when you want to detect if the application is experiencing
    a context leak, which actually means a memory leak. For OpenCL devices such as
    the CPU or GPU, the problem might not sound as a big deal. But for mobile processors,
    it would pose quite a serious problem since memory leaks, in general, wastes resources
    and the ultimately depleting battery life.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are actually more details where you can query the context via `clGetContextInfo`
    by passing in various `cl_context_info` types. Here''s a list of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| cl_context_info | Return type | Information returned in `param_name` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_CONTEXT_REFERENCE COUNT` | `cl_uint` | This variable returns the context
    reference count |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_CONTEXT_NUM_DEVICES` | `cl_uint` | This variable returns the number of
    devices in context |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_CONTEXT_DEVICES` | `cl_device_id[]` | This variable returns a list of
    devices in context |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_CONTEXT_PROPERTIES` | `cl_context_properties` | This variable returns
    the properties argument specified in `clCreateContext` or `clCreateContextFromType`
    |'
  prefs: []
  type: TYPE_TB
- en: Now that we've understood the basics of querying the platform, devices, extensions,
    and contexts I think it's time to take a look at OpenCL kernels and how you can
    program them.
  prefs: []
  type: TYPE_NORMAL
- en: Querying an OpenCL program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In OpenCL, kernels refer to a function declared in a program. A program in OpenCL
    consists of a set of kernels that are functions declared with the `__kernel` qualifier
    in the code. Such a program encapsulates a context, a program source or binary,
    and the number of kernels attached. The following sections explain how to build
    the OpenCL program and finally load the kernels for execution on the devices.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to run OpenCL kernels, you need to have a program (source or binary).
    Currently, there are two ways to build a program: from source files and other
    from binary objects via `clCreateProgramWithSource` and `clCreateProgramWithBinary`
    respectively (clever names). These two APIs return a program object represented
    by the OpenCL type, `cl_program` when successful. Let''s examine the method signatures
    to understand it better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you read the signature carefully, you''ll notice that the OpenCL context
    needs to be created prior to build our program from source. Next the `strings`
    and `lengths` arguments hold the various (kernel) filenames and their respective
    file lengths, and the last argument, `errcode_ret` reflects the presence of errors
    while building the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Examine the signature and you can quickly realize that the `binaries` and `lengths`
    arguments hold the pointers to the program binaries and their respective lengths.
    All the binaries are loaded into the devices represented by the `device_list`
    argument through the context. Whether the program was loaded onto the device successfully
    is reflected in the `binary_status` argument. The developer would find this manner
    of program creation useful when the binary is the only artifact that can be exposed
    to customers or even during system integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: For a developer to be able to create a valid OpenCL program by pulling offline
    binaries using `clCreateProgramWithBinary`, he needs to generate the offline binaries
    in the first place using the platform's compiler and this process is unfortunately
    vendor specific. If you are using the AMD APP SDK, then you would need to enable
    the `cl_amd_offline_devices` AMD extension, and when you create the context, you
    need to pass in the `CL_CONTEXT_OFFLINE_DEVICES_AMD` property. If you are developing
    for the Intel or Apple OpenCL platforms, we would recommend you to consult the
    documentation at their websites.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to build the program by invoking `clBuildProgram` passing it the
    created `cl_program` object from `clCreateProgramFromSource` and during program
    creation, the developer can provide additional compiler options to it (just as
    you perform when compiling C/C++ programs). Let's see an example of how you might
    do this in the code given in the *How to do it…* section, abbreviated from `Ch1/build_opencl_program/build_opencl_program.c`
    and the OpenCL kernel files are listed in `Ch1/build_opencl_program/{simple.cl,
    simple_2.cl`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To query an OpenCL program, you need to include a function similar to the following
    in your code. You should refer to the complete code listing alongside this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to what you did previously, the compilation command won''t be too far
    off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You'll find the executable file named `build_opencl_program` deposited on the
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to run the program, depending on how you compile it. If
    you reexamine the code snippet shown earlier, you would notice that the compiler
    options is defined in the source code and hence it''s static, but there''s another
    dynamic way in which the compiler options can be passed during compilation and
    the following are those two simple approaches are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you chose the option of defining the build options statically, that is,
    if you have the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL will simply build the program based on those build options you provided.
    This is rather suitable as the shipped application will have consistent results
    when running across different customer's setups.
  prefs: []
  type: TYPE_NORMAL
- en: To run the program, simply click on the `build_opencl_program` executable.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you chose the other option of allowing your users to pass in options
    of their choice (largely depending on your algorithm design), that is, if you
    have something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In place of options, we have the array of pointers to strings, traditionally
    used to pass in arguments to the program via the command line (conveniently known
    to the C programmer as `argv`), then you would have allowed the user to pass in
    multiple build options.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the program, you would enter a command similar to this where you quote
    the multiple options (enclosed with quotes) you wish to pass to the program via
    `–D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code example in this section is a little more involved than what we''ve
    been doing so far. What we did was to build an OpenCL program with two files:
    `simple.cl` and `simple_2.cl` which contains two simple OpenCL kernels via this
    (earlier) code snippet.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We demonstrated on to create the necessary data structures to store the contents
    of both files and the length of their program in two variables, `buffer` and `sizes`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we demonstrated how you built an OpenCL program using the `cl_program`
    object that's returned by `clCreateProgramWithSource` with build options that
    are either pre or user defined. We've also learnt how to use the `clGetProgramInfo`
    to query the program object for the result of the build. Also, the host application
    has the capability to dump any build errors from this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we released the data structures associated with the program and contexts
    in reverse order of their creation. In OpenCL 1.2, there is another new manner
    in which you can build a OpenCL program object but you would need to use both
    of the new APIs: `clCompileProgram` and `clLinkProgram`. The rationale behind
    them is to facilitate separation, compilation, and linkage.'
  prefs: []
  type: TYPE_NORMAL
- en: The build options deserved a further mention here, as there are in general four
    groups of options available to the OpenCL programmer. Go through the following
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, in general, three groups of options available when you wish to build
    the OpenCL program: options to control behavior in math, optimizations, and miscellaneous.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table presents the math options available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `-cl-single-precision-constant` | This option treats double precision floating
    point as a single precision constant. |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-denorms-are-zero` | This option controls how single and double precision
    denormalized numbers are handled. The compiler can choose to flush these numbers
    to zero. See [http://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/](http://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/).
    |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-fp32-correctly-rounded-divide-sqrt` | This option can be passed to `clBuildProgram`
    or `clCompileProgram`, which allows an application to specify that a single precision
    floating point divide (x / y and 1 / x) and sqrt used in the program source are
    correctly rounded. |'
  prefs: []
  type: TYPE_TB
- en: 'The following table highlights the optimization options available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `-cl-opt-disable` | This option disables all optimizations. Optimizations
    are enabled by default |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-mad-enable` | This option allows a * b + c to be computed with reduced
    accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-unsafe-math-optimizations` | This option combines the `–cl-mad-enable`
    and `–cl-no-signed-zeros` options |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-no-signed-zeros` | This option allows floating point arithmetic to ignore
    the signedness of zero, since according to IEEE 754, there''s a difference between
    +0.0 and -0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-finite-math-only` | This option allows optimizations to assume no floating
    point argument to take a NaN or an infinite value |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-fast-relaxed-math` | This option combines the `–cl-unsafe-math-`optimizations
    and the `–cl-finite-math-only` options |'
  prefs: []
  type: TYPE_TB
- en: 'The following table here highlights the miscellaneous options available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `-w` | This option prevents all warning messages |'
  prefs: []
  type: TYPE_TB
- en: '| `-Werror` | This option turns all warning messages into errors |'
  prefs: []
  type: TYPE_TB
- en: '| `-cl-std=VERSION` | This option builds the program based on the version of
    the OpenCL compiler (VERSION={CL1.1}) |'
  prefs: []
  type: TYPE_TB
- en: Let's move on to a bigger example where we create and query OpenCL kernels and
    eventually place them on a command queue for a device.
  prefs: []
  type: TYPE_NORMAL
- en: Creating OpenCL kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve managed to create a program from the source files. These source
    files are actually the OpenCL kernel code. Here''s an example of how they look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The kernels are identified by `__kernel` qualified to the C-like function. The
    `__global` qualifiers refer to the memory space in which the variables reside.
    We'll have more to say about this in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this program cannot execute on the device even though we have created the
    program objects, as described previously. Recall that a program can reference
    several kernels and we need to hold on to those kernels, because it is the kernel
    that gets scheduled for execution on the devices and not the program object. OpenCL
    gives us the function to extract those kernels via `clCreateKernel` or `clCreateKernelsInProgram`.
    Let''s take a close look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: By looking at this code, you'll notice that in order to create the kernel we
    first need to create the program object, the name of the kernel function plus
    the capture of the return status. This API returns a `cl_kernel`, which represents
    the kernel object when successful. This API provides the programmer with an option
    of not transforming every kernel function in the program into actual OpenCL kernel
    objects ready for execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if you wish to simply transform all kernel functions in the program into
    kernel objects, then `clCreateKernelsInProgram` is the API to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You use this API to ask OpenCL to create and load the kernels into the `kernels`
    argument, and you hint to the OpenCL compiler how many kernels you're expecting
    with the `num_kernels` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete code can be found in `ch1/kernel_query/kernel_query.c`. An abbreviated
    code is shown in the code snippet discussed in the *How to do it...* section of
    this recipe to keep us focused on the key APIs. This code requires one or more
    OpenCL source files, that is, `*.cl` and once you've placed them together you
    need to change the program's variables, `file_names` and `NUMBER_OF_FILES` to
    reflect the files accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it …
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To query an OpenCL kernel, you''ll need to include a function similar to the
    following in your code. You should reference the full code listing alongside this
    recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The compilation is very similar to that of `build_opencl_program.c` illustrated
    in the previous section, so we''re skipping this step. When this application is
    run with two OpenCL source files, the output we will get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The two source files, each defined a simple kernel function that adds its two
    arguments and stores the result into the third argument; and hence the `arity`
    of the function is `3`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code invokes `clCreateKernelsInProgram` twice. If you recall, this pattern
    recurs for many of the OpenCL APIs, where the first call would query the platform
    for certain details, which in this case is the number of kernels detected in the
    program. The subsequent calls would ask OpenCL to deposit the kernel objects into
    the storage referenced by kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we invoke `clGetKernelInfo`, passing to it the retrieved kernel objects,
    and printing out some information about the kernel functions, such as the kernel
    function's name and the arity of the function through the `CL_KERNEL_FUNCTION_NAME`
    and `CL_KERNEL_NUM_ARGS` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'A complete list of details that can be queried from the kernel objects is reflected
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| cl_kernel_info | Return Type | Information returned in param_value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_KERNEL_FUNCTION_NAME` | `char[]` | This variable returns the kernel function''s
    name |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_KERNEL_NUM_ARGS` | `cl_uint` | This variable returns the number of arguments
    to kernel |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_KERNEL_REFERENCE_COUNT` | `cl_uint` | This variable returns the kernel
    reference count |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_KERNEL_CONTEXT` | `cl_context` | This variable returns the associated
    context for this kernel |'
  prefs: []
  type: TYPE_TB
- en: '| `CL_KERNEL_PROGRAM` | `cl_program` | This variable returns the program object,
    that will be bound to the kernel object |'
  prefs: []
  type: TYPE_TB
- en: Now that we've figured out how to create kernel objects, we should take a look
    at how to create command queues and start enqueuing our kernel objects and data
    for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Creating command queues and enqueuing OpenCL kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will show you how to enqueue OpenCL kernel objects on the device.
    Before we do that, let's recall that we can create kernels without specifying
    an OpenCL device and the kernels can be executed on the device via the command
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we probably should spend some time talking about in-order execution
    and how they can be compared with out-of-order execution, though this subject
    is complex but intriguing as well. When a program is to be executed, the processor
    has the option of processing the instructions in the program in-order or out-of-order;
    a key difference between these two schemes is that in-order results in an execution
    order that is static, while out-of-order allows instructions to be scheduled dynamically.
    Out-of-order execution typically involves reordering the instructions, so that
    all computation units in the processors are utilized and driven by the goal of
    minimizing the stalling of the computation.
  prefs: []
  type: TYPE_NORMAL
- en: However, kernels are not the only objects that can be queued on the command
    queue. A kernel needs data so that it can perform its operations and data needs
    to transferred to the device for consumption, and these data could be OpenCL buffer
    / sub-buffer or image objects. The memory objects that encapsulate the data need
    to be transported into the device and you have to issue memory commands to the
    command queue for that to occur; and in many use cases, it is common to hydrate
    the device with data prior to computation.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram highlights this use case where a kernel is scheduled for
    in-order execution, assuming that the kernel needs the data to be copied explicitly
    or memory-mapped, and upon completion of computation, the data is copied from
    the device's memory to host memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating command queues and enqueuing OpenCL kernels](img/4520OT_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also multiple command queues can be created and enqueued with commands and the
    reason for their existence is because the problem you wish to solve might involve
    some, if not all of the heterogeneous devices in the host. And they could represent
    independent streams of computation where no data is shared, or dependent streams
    of computation where each subsequent task depends on the previous task (often,
    data is shared). Take care that these command queues will execute on the device
    without synchronization, provided that no data is shared. If data is shared, then
    the programmer needs to ensure synchronization of the data through synchronization
    commands provided by the OpenCL specification.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of independent streams of computation, the following diagram assumes
    that three independent tasks have been identified and they need to execute on
    a device. Three command queues (in-order execution only) with tasks enqueued in
    each of them and a pipeline can be formed, such that the device executes the kernel
    code while I/O is being performed to achieve better utilization by not having
    the device sit idle waiting for data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating command queues and enqueuing OpenCL kernels](img/4520OT_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that even though by default, commands enqueued in the command queue
    execute in-order, you can enable out-of-order execution by passing the `CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE`
    flag when creating the command queue.
  prefs: []
  type: TYPE_NORMAL
- en: An example of out-of-order execution is shown in the following diagram, and
    let's assume that our problem is decomposed into three interdependent kernels,
    where each kernel will consume and process the data and then pass it to the next
    phase. Let's assume further that the execution of the kernels is out-of-order.
    What would happen next is mayhem and that's probably why this option is never
    the default.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating command queues and enqueuing OpenCL kernels](img/4520OT_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, the reader should be aware about CPUs from AMD and Intel.
  prefs: []
  type: TYPE_NORMAL
- en: When you start working on the kernels, you might discover that certain kernels
    seem to have better performance than others. And you can profile the kernel while
    you are fine-tuning it by passing the `CL_QUEUE_PROFILING_ENABLE` flag when creating
    the command queue.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without repeating too much of the previous code, here's the relevant code that
    is derived from `ch1/kernel_queue/kernel_queue.c`. This code listing would need
    valid OpenCL kernel file(s) with distinct kernel function names (function overloading
    is disallowed) and valid function parameters. In `ch1/kernel_queue/hello_world.cl`
    you can see an example of such a function or kernel otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should reference the full code listing alongside this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, the compilation steps are similar to that in `kernel_query.c` with
    a command like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the sample output when I execute the application on my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: From the output, you can tell that the task has been enqueued onto a command
    queue successfully!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following from the previous section where we successfully queried the OpenCL
    kernel objects for information, we leverage on that code to create a command queue
    via `clCreateCommandQueue`, enqueue the kernel into the queue via `clEnqueueTask`,
    but not before setting the data needed for the kernel via `clSetKernelArg` and
    `clCreateBuffer`. You can ignore these two APIs for now, until we explain them
    in a later chapter.
  prefs: []
  type: TYPE_NORMAL
