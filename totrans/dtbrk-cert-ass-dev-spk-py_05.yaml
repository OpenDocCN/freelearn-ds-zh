- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Advanced Operations and Optimizations in Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的高级操作和优化
- en: In this chapter, we will delve into the advanced capabilities of Apache Spark,
    equipping you with the knowledge and techniques necessary to optimize your data
    processing workflows. From the inner workings of the Catalyst optimizer to the
    intricacies of different types of joins, we will explore advanced Spark operations
    that empower you to harness the full potential of this powerful framework.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨Apache Spark的高级功能，为您提供优化数据处理工作流程所需的知识和技术。从Catalyst优化器的内部工作原理到不同类型连接的复杂性，我们将探索高级Spark操作，让您能够充分利用这个强大框架的全部潜力。
- en: 'The chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Different options to group data in Spark DataFrames.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark DataFrame中对数据进行分组的不同选项。
- en: Various types of joins in Spark, including inner join, left join, right join,
    outer join, cross join, broadcast join, and shuffle join, each with its unique
    use cases and implications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的各种连接类型，包括内连接、左连接、右连接、外连接、交叉连接、广播连接和洗牌连接，每种连接都有其独特的用例和影响
- en: Shuffle and broadcast joins, with a focus on broadcast hash joins and shuffle
    sort-merge joins, along with their applications and optimization strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuffle和广播连接，重点关注广播哈希连接和洗牌排序合并连接，以及它们的应用和优化策略
- en: Reading and writing data to disk in Spark using different data formats, such
    as CSV, Parquet, and others.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的数据格式（如CSV、Parquet等）在Spark中读取和写入数据
- en: Using Spark SQL for different operations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行不同操作
- en: The Catalyst optimizer, a pivotal component in Spark’s query execution engine
    that employs rule-based and cost-based optimizations to enhance query performance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst优化器，Spark查询执行引擎中的一个关键组件，它采用基于规则和基于成本的优化来提高查询性能
- en: The distinction between narrow and wide transformations in Spark and when to
    use each type to achieve optimal parallelism and resource efficiency
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中窄变换和宽变换的区别以及何时使用每种类型以实现最佳并行性和资源效率
- en: Data persistence and caching techniques to reduce recomputation and expedite
    data processing, with best practices for efficient memory management
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据持久化和缓存技术以减少重复计算并加速数据处理，以及高效内存管理的最佳实践
- en: Data partitioning through repartition and coalesce, and how to use these operations
    to balance workloads and optimize data distribution
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过repartition和coalesce进行数据分区，以及如何使用这些操作来平衡工作负载和优化数据分布
- en: '**User-defined functions** (**UDFs**) and custom functions, which allow you
    to implement specialized data processing logic, as well as when and how to leverage
    them effectively'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户定义函数**（**UDFs**）和自定义函数，允许您实现专门的数据处理逻辑，以及何时以及如何有效地利用它们'
- en: Performing advanced optimizations in Spark using the Catalyst optimizer and
    **Adaptive Query** **Execution** (**AQE**)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Catalyst优化器和**自适应查询执行**（**AQE**）进行Spark的高级优化
- en: Data-based optimization techniques and their benefits
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据的优化技术及其好处
- en: Each section will provide in-depth insights, practical examples, and best practices,
    ensuring you are well-equipped to handle complex data processing challenges in
    Apache Spark. By the end of this chapter, you will possess the knowledge and skills
    needed to harness the advanced capabilities of Spark and unlock its full potential
    for your data-driven endeavors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分都将提供深入见解、实际示例和最佳实践，确保您能够应对Apache Spark中的复杂数据处理挑战。到本章结束时，您将掌握利用Spark高级功能的知识和技能，并解锁其在数据驱动项目中的全部潜力。
- en: Grouping data in Spark and different Spark joins
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中对数据进行分组和不同的Spark连接
- en: 'We will start with one of the most important data manipulation techniques:
    grouping and joining data. When we are doing data exploration, grouping data based
    on different criteria becomes essential to data analysis. We will look at how
    we can group different data using `groupBy`.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最重要的数据处理技术之一开始：分组和连接数据。当我们进行数据探索时，根据不同标准对数据进行分组成为数据分析的关键。我们将探讨如何使用`groupBy`对不同的数据进行分组。
- en: Using groupBy in a DataFrame
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在DataFrame中使用groupBy
- en: We can group data in a DataFrame based on different criteria – for example,
    we can group data based on different columns in a DataFrame. We can also apply
    different aggregations, such as `sum` or `average`, to this grouped data to get
    a holistic view of data slices.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据不同的标准在DataFrame中对数据进行分组——例如，我们可以根据DataFrame中的不同列对数据进行分组。我们还可以应用不同的聚合，如`sum`或`average`，来获取数据切片的整体视图。
- en: For this purpose, in Spark, we have the `groupBy` operation. The `groupBy` operation
    is similar to `groupBy` in SQL in that we can do group-wise operations on these
    grouped datasets. Moreover, we can specify multiple `groupBy` criteria in a single
    `groupBy` statement. The following example shows how to use `groupBy` in PySpark.
    We will use the DataFrame salary data we created in the previous chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，在Spark中，我们有`groupBy`操作。`groupBy`操作与SQL中的`groupBy`类似，因为我们可以在这些分组数据集上执行分组操作。此外，我们可以在一个`groupBy`语句中指定多个`groupBy`标准。以下示例展示了如何在PySpark中使用`groupBy`。我们将使用上一章中创建的DataFrame薪水数据。
- en: 'In the following `groupBy` statement, we are grouping the salary data based
    on the `Department` column:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下`groupBy`语句中，我们根据`Department`列对薪水数据进行分组：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a result, this operation returns a grouped data object that has been grouped
    by the `Department` column:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个操作返回了一个按`Department`列分组的分组数据对象：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This can be assigned to a separate DataFrame and more operations can be done
    on this data. All the aggregate operations can also be used for different groups
    of a DataFrame.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以分配给一个单独的DataFrame，并且可以对这组数据执行更多操作。所有聚合操作也可以用于DataFrame的不同组。
- en: 'We will use the following statement to get the average salary across different
    departments in our `salary_data` DataFrame:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下语句来获取`salary_data` DataFrame中不同部门的平均薪水：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the result:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we can see that each department’s average salary is calculated
    based on the `salary` column of the `salary_data` DataFrame. All four departments,
    including `null` (since we had null values in our DataFrame), are included in
    the resulting DataFrame.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到每个部门的平均薪水是基于`salary_data` DataFrame中的`salary`列计算的。包括`null`（因为我们DataFrame中有空值），所有四个部门都包含在结果DataFrame中。
- en: Now, let’s take a look at how we can apply complex `groupBy` operations to data
    in PySpark DataFrames.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将复杂的`groupBy`操作应用于PySpark DataFrame中的数据。
- en: A complex groupBy statement
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个复杂的groupBy语句
- en: '`groupBy` can be used in complex data operations, such as multiple aggregations
    within a single `groupBy` statement.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy`可以在复杂的数据操作中使用，例如在单个`groupBy`语句中进行多个聚合。'
- en: 'In the following code snippet, we are going to use `groupBy` by taking a sum
    of the salary column for each department. Then, we will round off the `sum(Salary)`
    column that we just created to two digits after a decimal. After, we will rename
    the `sum(Salary)` column back to `Salary`. All of these operations are being done
    in a single `groupBy` statement:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们将使用`groupBy`操作，通过对每个部门的薪水列求和。然后，我们将刚刚创建的`sum(Salary)`列四舍五入到小数点后两位。之后，我们将`sum(Salary)`列重命名为`Salary`。所有这些操作都在一个`groupBy`语句中完成：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As a result, we will see the following DataFrame showing the aggregated sum
    of the `Salary` column based on each department:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将看到以下DataFrame，它显示了基于每个部门的`Salary`列的聚合总和：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we can see that each department’s total salary is calculated
    in a new column named `sum(Salary)`, after which we round this total up to two
    decimal places. In the next statement, we rename the `sum(Salary)` column back
    to `Salary` and then sort this resulting DataFrame based on `Department`. In the
    resulting DataFrame, we can see that each department’s sum of salaries is calculated
    in the new column.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到每个部门的总薪水被计算在一个名为`sum(Salary)`的新列中，之后我们将这个总数四舍五入到两位小数。在下一个语句中，我们将`sum(Salary)`列重命名为`Salary`，然后根据`Department`对结果DataFrame进行排序。在结果DataFrame中，我们可以看到每个部门的薪水总和被计算在新列中。
- en: Now that we know how to group data using different aggregations, let’s take
    a look at how we can join two DataFrames together in Spark.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用不同的聚合函数来分组数据，让我们看看如何在Spark中将两个DataFrame合并在一起。
- en: Joining DataFrames in Spark
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的DataFrame连接
- en: Join operations are fundamental in data processing tasks and are a core component
    of Apache Spark. Spark provides several types of joins to combine data from different
    DataFrames or datasets. In this section, we will explore different Spark join
    operations and when to use each type.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 连接操作是数据处理任务的基本操作，也是Apache Spark的核心组件之一。Spark提供了多种类型的连接操作，用于将来自不同DataFrame或数据集的数据合并在一起。在本节中，我们将探讨不同的Spark连接操作以及何时使用每种类型。
- en: Join operations are used to combine data from two or more DataFrames based on
    a common column. These operations are essential for tasks such as merging datasets,
    aggregating information, and performing relational operations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 连接操作用于根据公共列将两个或多个数据帧中的数据合并在一起。这些操作对于合并数据集、聚合信息和执行关系操作等任务至关重要。
- en: 'In Spark, the primary syntax for performing joins is using the `.join()` method,
    which takes the following parameters:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，执行连接的主要语法是使用 `.join()` 方法，该方法接受以下参数：
- en: '`other`: The other DataFrame to join with'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`other`: 要与之连接的其他数据帧'
- en: '`on`: The column(s) on which to join the DataFrames'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on`: 要连接数据帧的列'
- en: '`how`: The type of join to perform (inner, outer, left, or right)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how`: 要执行的连接类型（内部、外部、左连接或右连接）'
- en: '`suffixes`: Suffixes to add to columns with the same name in both DataFrames'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffixes`: 在两个数据帧中具有相同名称的列中添加的后缀'
- en: 'These parameters are used in the main syntax of the join operation, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数在连接操作的主要语法中使用，如下所示：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `Dataframe1` would be on the left-hand side of the join and `Dataframe2`
    would be on the right-hand side of the join.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Dataframe1` 将位于连接的左侧，而 `Dataframe2` 将位于连接的右侧。
- en: DataFrames or datasets can be joined based on common columns within a DataFrame,
    and the result of a join query is a new DataFrame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据帧或数据集可以根据数据帧内的公共列进行连接，连接查询的结果是一个新的数据帧。
- en: 'We will demonstrate the join operation on two new DataFrames. First, let’s
    create these DataFrames. The first DataFrame is called `salary_data_with_id`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个新的数据帧上演示连接操作。首先，让我们创建这些数据帧。第一个数据帧称为 `salary_data_with_id`：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting DataFrame, named `salary_data_with_id`, looks like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据帧，命名为 `salary_data_with_id`，看起来如下：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we’ll create another DataFrame named `employee_data`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建另一个名为 `employee_data` 的数据帧：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting DataFrame, named `employee_data`, looks like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据帧，命名为 `employee_data`，看起来如下：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, let’s suppose we want to join these two DataFrames together based on the
    `ID` column.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要根据 `ID` 列将这两个数据帧连接在一起。
- en: As we mentioned earlier, Spark offers different types of join operations. We
    will explore some of them in this chapter. Let’s start with inner joins.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，Spark 提供了不同类型的连接操作。我们将在本章中探索其中的一些。让我们从内部连接开始。
- en: Inner joins
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内部连接
- en: An **inner join** is used when we want to join two DataFrames based on values
    that are common in both DataFrames. Any value that doesn’t exist in any one of
    the DataFrames would not be part of the resulting DataFrame. By default, the join
    type is an inner join in Spark.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要根据两个数据帧中共同存在的值来连接两个数据帧时，使用**内部连接**。任何在任何一个数据帧中不存在的值都不会是结果数据帧的一部分。默认情况下，Spark
    中的连接类型是内部连接。
- en: Use case
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: Inner joins are useful for merging data when you are interested in common elements
    in both DataFrames – for example, joining sales data with customer data to see
    which customers made a purchase.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 内部连接在合并数据时非常有用，当你对两个数据帧中的共同元素感兴趣时——例如，将销售数据与客户数据连接起来，以查看哪些客户进行了购买。
- en: 'The following code illustrates how we can use an inner join with the DataFrames
    we created earlier:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用我们之前创建的数据帧进行内部连接：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting DataFrame now contains all the columns of both DataFrames – `salary_data_with_id`
    and `employee_data` – joined together in a single DataFrame. It only includes
    rows that are common in both DataFrames. Here’s what it looks like:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据帧现在包含两个数据帧的所有列——`salary_data_with_id` 和 `employee_data`——在单个数据帧中连接在一起。它只包括两个数据帧中都有的行。下面是它的样子：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You will notice that the `how` parameter defines the type of join that is being
    done in this statement. Currently, it says `inner` because we wanted the DataFrames
    to join based on an inner join. We can also see that IDs `7` and `8` are missing.
    The reason is that the `employee_data` DataFrame did not contain IDs `7` and `8`.
    Since we’re using an inner join, it only joins data based on common data elements
    in both DataFrames. Any data that is not present in either one of the DataFrames
    will not be part of the resulting DataFrame.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，`how` 参数定义了此语句中正在进行的连接类型。目前，它显示为 `inner`，因为我们想要数据帧根据内部连接进行连接。我们还可以看到 ID
    `7` 和 `8` 缺失。原因是 `employee_data` 数据帧不包含 ID `7` 和 `8`。由于我们使用的是内部连接，它只基于两个数据帧中共同的数据元素进行连接。任何在任一数据帧中不存在的数据都不会是结果数据帧的一部分。
- en: Next, we will explore outer joins.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索外部连接。
- en: Outer joins
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 外部连接
- en: An `null`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `null`。
- en: We should use an outer join when we want to join two DataFrames based on values
    that exist in both DataFrames, regardless of whether they don’t exist in the other
    DataFrame. Any values that exist in any one of the DataFrames would be part of
    the resulting DataFrame.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要根据两个 DataFrame 中都存在的值来连接两个 DataFrame，无论这些值是否存在于另一个 DataFrame 中时，我们应该使用外连接。任何存在于任何一个
    DataFrame 中的值都将成为结果 DataFrame 的一部分。
- en: Use case
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: Outer joins are suitable for situations where you want to include all records
    from both DataFrames while accommodating unmatched values – for example, when
    merging employee data with project data to see which employees are assigned to
    which projects, including those not currently assigned to any.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 外连接适用于需要包含两个 DataFrame 中的所有记录，同时容纳不匹配值的情况——例如，当合并员工数据与项目数据以查看哪些员工被分配到哪些项目时，包括那些目前没有被分配到任何项目的员工。
- en: 'The following code illustrates how we use an outer join with the DataFrames
    we created earlier:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了我们如何使用之前创建的 DataFrame 进行外连接：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting DataFrame contains data for all the employees in the `salary_data_with_id`
    and `employee_data` DataFrames. Here’s what it looks like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 包含 `salary_data_with_id` 和 `employee_data` DataFrame 中所有员工的资料。它看起来是这样的：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You will notice that the `how` parameter has changed and says `outer`. In the
    resulting DataFrame, IDs `7` and `8` are now present. However, also notice that
    the `ID`, `State`, and `Gender` columns for IDs `7` and `8` are `null`. The reason
    is that the `employee_data` DataFrame did not contain IDs `7` and `8`. Any data
    not present in either of the DataFrames would be part of the resulting DataFrame,
    but the corresponding columns would be `null` for the DataFrame that this was
    not present, as shown in the case of employee IDs `7` and `8`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 `how` 参数已更改，并显示为 `outer`。在结果 DataFrame 中，现在存在 ID `7` 和 `8`。然而，也请注意，ID
    `7` 和 `8` 的 `ID`、`State` 和 `Gender` 列是 `null`。原因是 `employee_data` DataFrame 不包含
    ID `7` 和 `8`。任何在两个 DataFrame 中都不存在的数据将成为结果 DataFrame 的一部分，但对应列将显示为 `null`，正如员工
    ID `7` 和 `8` 的情况所示。
- en: Next, we will explore left joins.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨左连接。
- en: Left joins
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 左连接
- en: A left join returns all the rows from the left DataFrame and the matched rows
    from the right DataFrame. If there is no match in the right DataFrame, the result
    will contain `null` values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 左连接返回左侧 DataFrame 的所有行和右侧 DataFrame 的匹配行。如果右侧 DataFrame 中没有匹配项，则结果将包含 `null`
    值。
- en: Use case
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: Left joins are handy when you want to keep all records from the left DataFrame
    and only the matching records from the right DataFrame – for instance, when merging
    customer data with transaction data to see which customers have made a purchase.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要保留左侧 DataFrame 的所有记录，并且只保留右侧 DataFrame 的匹配记录时，左连接很有用——例如，当合并客户数据与交易数据以查看哪些客户进行了购买时。
- en: 'The following code illustrates how we can use a left join with the DataFrames
    we created earlier:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了我们如何使用之前创建的 DataFrame 进行左连接：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting DataFrame contains all the data from the left DataFrame – that
    is, `salary_data_with_id`. It looks like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 包含来自左侧 DataFrame 的所有数据——即 `salary_data_with_id`。它看起来是这样的：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the `how` parameter has changed and says `left`. Now, IDs `7` and
    `8` are present. However, also notice that the `ID`, `State`, and `Gender` columns
    for IDs `7` and `8` are `null`. The reason is that the `employee_data` DataFrame
    did not contain IDs `7` and `8`. Since `salary_data_with_id` is the left DataFrame
    in the join statement, its values take priority in the join.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`how` 参数已更改，并显示为 `left`。现在，ID `7` 和 `8` 存在。然而，也请注意，ID `7` 和 `8` 的 `ID`、`State`
    和 `Gender` 列是 `null`。原因是 `employee_data` DataFrame 不包含 ID `7` 和 `8`。由于 `salary_data_with_id`
    是连接语句中的左侧 DataFrame，其值在连接中具有优先权。
- en: All records from the left DataFrame are present in the resulting DataFrame,
    and matching records from the right DataFrame are included. Non-matching entries
    in the right DataFrame are filled with `null` values in the result.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有来自左侧 DataFrame 的记录都包含在结果 DataFrame 中，并且包含来自右侧 DataFrame 的匹配记录。右侧 DataFrame
    中的不匹配条目在结果中用 `null` 值填充。
- en: Next, we will explore right joins.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨右连接。
- en: Right joins
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 右连接
- en: A right join is similar to a left join, but it returns all the rows from the
    right DataFrame and the matched rows from the left DataFrame. Non-matching rows
    from the left DataFrame contain null values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 右连接与左连接类似，但它返回右侧 DataFrame 的所有行和左侧 DataFrame 的匹配行。左侧 DataFrame 中的不匹配行包含 `null`
    值。
- en: Use case
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: Right joins are the opposite of left joins and are used when you want to keep
    all records from the right DataFrame while including matching records from the
    left DataFrame.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 右连接是左连接的相反，当您想保留右侧 DataFrame 的所有记录并包含来自左侧 DataFrame 的匹配记录时使用。
- en: 'The following code illustrates how to use a right join with the DataFrames
    we created earlier:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用我们之前创建的 DataFrame 进行右连接：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting DataFrame contains all the data from the right-hand DataFrame
    – that is, `employee_data`. It looks like this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 包含右侧 DataFrame 的所有数据——即 `employee_data`。它看起来如下：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice the `how` parameter has changed and now says `right`. The resulting DataFrame
    shows that IDs `7` and `8` are not present. The reason is that the `employee_data`
    DataFrame does not contain IDs `7` and `8`. Since `employee_data` is the right
    DataFrame in the join statement, its values take priority in the join.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 `how` 参数已更改，现在显示为 `right`。结果 DataFrame 显示 IDs `7` 和 `8` 不存在。原因是 `employee_data`
    DataFrame 不包含 IDs `7` 和 `8`。由于 `employee_data` 是连接语句中的右侧 DataFrame，其值在连接中具有优先权。
- en: All records from the right DataFrame are present in the resulting DataFrame,
    and matching records from the left DataFrame are included. Non-matching entries
    in the left DataFrame are filled with `null` values in the result.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所有来自右侧 DataFrame 的记录都包含在结果 DataFrame 中，并且包含来自左侧 DataFrame 的匹配记录。左侧 DataFrame
    中的不匹配条目在结果中用 `null` 值填充。
- en: Next, we will explore cross joins.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索交叉连接。
- en: Cross joins
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉连接
- en: A **cross join**, also known as a **Cartesian join**, combines each row from
    the left DataFrame with every row from the right DataFrame. This results in a
    large, Cartesian product DataFrame.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉连接**，也称为**笛卡尔连接**，将左侧 DataFrame 的每一行与右侧 DataFrame 的每一行组合。这导致了一个大型的笛卡尔积
    DataFrame。'
- en: Use case
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用例
- en: Cross joins should be used with caution due to their potential for generating
    massive datasets. They are typically used when you want to explore all possible
    combinations of data, such as when generating test data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们可能生成大量数据集，因此应谨慎使用交叉连接。它们通常用于探索所有可能的数据组合，例如在生成测试数据时。
- en: Next, we will explore the union option to join two DataFrames.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索并集选项以连接两个 DataFrame。
- en: Union
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并集
- en: 'Union is used to join two DataFrames that have a similar schema. To illustrate
    this, we will create another DataFrame called `salary_data_with_id_2` that contains
    some more values. The schema of this DataFrame is the same as the one for `salary_data_with_id`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 并集用于连接具有相似模式的两个 DataFrame。为了说明这一点，我们将创建另一个名为 `salary_data_with_id_2` 的 DataFrame，其中包含一些额外的值。这个
    DataFrame 的模式与 `salary_data_with_id` 的模式相同：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As a result, you will see the schema of the DataFrame first, after which you
    will see the actual DataFrame and its values:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你将首先看到 DataFrame 的模式，然后是实际的 DataFrame 及其值：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once we have this DataFrame, we can use the `union()` function to join the
    `salary_data_with_id` and `salary_data_with_id_2` DataFrames together. The following
    example illustrates this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个 DataFrame，我们就可以使用 `union()` 函数将 `salary_data_with_id` 和 `salary_data_with_id_2`
    DataFrame 连接在一起。以下示例说明了这一点：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting DataFrame, named `unionDF`, looks like this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame，命名为 `unionDF`，看起来如下：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, both DataFrames are joined together and as a result, new rows
    are added to the resulting DataFrame. The last four rows are from `salary_data_with_id_2`
    and were added to the rows of `salary_data_with_id`. This is another way to join
    two DataFrames together.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，两个 DataFrame 已连接在一起，因此结果 DataFrame 中添加了新行。最后四行来自 `salary_data_with_id_2`
    并添加到 `salary_data_with_id` 的行中。这是将两个 DataFrame 连接在一起的一种方法。
- en: In this section, we explored different types of Spark joins and their appropriate
    use cases. Choosing the right join type is crucial to ensure efficient data processing
    in Spark, and understanding the implications of each type will help you make informed
    decisions in your data analysis and processing tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了不同类型的 Spark 连接及其适用场景。选择正确的连接类型对于确保 Spark 中高效的数据处理至关重要，并且理解每种类型的含义将帮助你在数据分析和处理任务中做出明智的决定。
- en: Now, let’s look at how we can read and write data in Spark.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 Spark 中读取和写入数据。
- en: Reading and writing data
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入数据
- en: When we work with Spark and do all the operations in Spark for data manipulation,
    one of the most important things that we need to do is read and write data to
    disk. Remember, Spark is an in-memory framework, which means that all the operations
    take place in the memory of the compute or cluster. Once these operations are
    completed, we’ll want to write that data to disk. Similarly, before we manipulate
    any data, we’ll likely need to read data from disk as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 Spark 并在 Spark 中进行数据操作的所有操作时，我们需要做的最重要的事情之一是将数据读写到磁盘上。记住，Spark 是一个内存框架，这意味着所有操作都在计算或集群的内存中执行。一旦这些操作完成，我们就会希望将数据写入磁盘。同样，在我们操作任何数据之前，我们可能还需要从磁盘读取数据。
- en: There are several data formats that Spark supports for reading and writing different
    types of data files. We will discuss the following formats in this chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持多种数据格式用于读取和写入不同类型的数据文件。在本章中，我们将讨论以下格式。
- en: '**Comma Separated** **Values** (**CSV**)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逗号分隔值** (**CSV**)'
- en: '**Parquet**'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parquet**'
- en: '**Optimized Row** **Columnar** (**ORC**)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化行** **列式** (**ORC**)'
- en: Please note that these are not the only formats that Spark supports but this
    is a very popular subset of formats. A lot of other formats are also supported
    by Spark, such as Avro, text, JDBC, Delta, and others.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些并不是 Spark 支持的唯一格式，但这是一个非常流行的格式子集。Spark 还支持许多其他格式，例如 Avro、文本、JDBC、Delta
    等。
- en: In the next section, we will discuss the CSV file format and how to read and
    write CSV format data files.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论 CSV 文件格式以及如何读取和写入 CSV 格式的数据文件。
- en: Reading and writing CSV files
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和写入 CSV 文件
- en: In this section, we will discuss how to read and write data from the CSV file
    format. In this file format, data is separated by commas. This is a very popular
    data format because of its ease of use and simplicity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何从 CSV 文件格式读取和写入数据。在这个文件格式中，数据由逗号分隔。这是一个非常流行的数据格式，因为它易于使用且简单。
- en: 'Let’s look at how to write CSV files with Spark by running the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来查看如何使用 Spark 写入 CSV 文件：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The resulting DataFrame, named `salary_data_with_id`, looks like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 DataFrame，命名为 `salary_data_with_id`，如下所示：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There are certain parameters in the `dataframe.write.csv()` function that we
    can see here. The first parameter is the dataframe name that we need to write
    to disk. The second parameter, `header`, specifies whether the file that we need
    to write should be written with a header row or not.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到 `dataframe.write.csv()` 函数中的一些参数。第一个参数是我们需要写入磁盘的 DataFrame 名称。第二个参数
    `header` 指定我们需要写入的文件是否应该带有标题行。
- en: There are certain parameters in the `dataframe.read.csv()` function that we
    should discuss. The first parameter is the `path/name` value of the file that
    we need to read. The second parameter, `header`, specifies whether the file has
    a header row to be read.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `dataframe.read.csv()` 函数中，有一些参数我们需要讨论。第一个参数是我们需要读取的文件的 `path/name` 值。第二个参数
    `header` 指定文件是否有标题行需要读取。
- en: In the first statement, we’re writing the `salary_data` DataFrame to a CSV file
    named `salary_data.csv`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个语句中，我们将 `salary_data` DataFrame 写入名为 `salary_data.csv` 的 CSV 文件。
- en: In the next statement, we’re reading back the same file that we wrote to see
    its contents. We can see that the resulting file contains the same data that we
    wrote.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含我们写入的相同数据。
- en: 'Let’s look at another function that can be used to read CSV files with Spark:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个可以用于使用 Spark 读取 CSV 文件的函数：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The resulting DataFrame, named `read_data`, looks like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 DataFrame，命名为 `read_data`，如下所示：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There are certain parameters in the `spark.read.format()` function. First, we
    specify the format of the file that needs to be read. Then, we can perform different
    function calls for different options. In the next call, we specify that the file
    has a header, so the DataFrame expects to have a header. Then, we specify that
    we need to have a schema for this data, which is defined in the `schema` variable.
    Finally, in the `load` function, we define the path of the file to be loaded.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `spark.read.format()` 函数中，有一些参数。首先，我们指定需要读取的文件格式。然后，我们可以为不同的选项执行不同的函数调用。在下一个调用中，我们指定文件有标题，因此
    DataFrame 预期会有标题。然后，我们指定我们需要为这些数据有一个模式，该模式在 `schema` 变量中定义。最后，在 `load` 函数中，我们定义要加载的文件的路径。
- en: Next, we will learn how to read and write Parquet files with Spark.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用 Spark 读取和写入 Parquet 文件。
- en: Reading and writing Parquet files
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和写入 Parquet 文件
- en: In this section, we will discuss the Parquet file format. Parquet is a columnar
    file format that makes data reading and writing very efficient. It is also a compact
    file format that facilitates faster reads and writes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Parquet文件格式。Parquet是一种列式文件格式，使得数据读取和写入非常高效。它也是一种紧凑的文件格式，有助于加快读取和写入速度。
- en: 'Let’s learn how to write Parquet files with Spark by running the following
    code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来学习如何使用Spark写入Parquet文件：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The resulting DataFrame, named `salary_data_with_id`, looks like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame，命名为`salary_data_with_id`，看起来如下：
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There are certain parameters in the `dataframe.write()` function that we can
    see here. The first call is to the `parquet` function to define the file type.
    Then, as the next parameter, we specify the path where this Parquet file needs
    to be written.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dataframe.write()`函数中，有一些参数我们可以在这里看到。第一个调用是`parquet`函数，用于定义文件类型。然后，作为下一个参数，我们指定了需要将这个Parquet文件写入的路径。
- en: In the next statement, we’re reading the same file that we wrote, to see its
    contents. We can see that the resulting file contains the data we wrote.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含了我们写入的数据。
- en: Next, we will look at how we can read and write ORC files with Spark.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用Spark读取和写入ORC文件。
- en: Reading and writing ORC files
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和写入ORC文件
- en: In this section, we will discuss the ORC file format. Like Parquet, ORC is also
    a columnar and compact file format that makes data reading and writing very efficient.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论ORC文件格式。与Parquet类似，ORC也是一种列式和紧凑的文件格式，使得数据读取和写入非常高效。
- en: 'Let’s learn how to write ORC files with Spark by running the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来学习如何使用Spark写入ORC文件：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The resulting DataFrame, named `salary_data_with_id`, looks like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame，命名为`salary_data_with_id`，看起来如下：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are certain parameters in the `dataframe.write()` function that we can
    see. The first call is to the `orc` function to define the file type. Then, as
    the next parameter, we specify the path where this Parquet file needs to be written.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dataframe.write()`函数中，有一些参数我们可以看到。第一个调用是`orc`函数，用于定义文件类型。然后，作为下一个参数，我们指定了需要将这个Parquet文件写入的路径。
- en: In the next statement, we’re reading back the same file that we wrote to see
    its contents. We can see that the resulting file contains the same data that we
    wrote.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含了我们写入的相同数据。
- en: Next, we will look at how we can read and write Delta files with Spark.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用Spark读取和写入Delta文件。
- en: Reading and writing Delta files
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和写入Delta文件
- en: The Delta file format is an open format that is more optimized than Parquet
    and other columnar formats. When the data is stored in Delta format, you will
    notice that the underlying files are in Parquet. The Delta format adds a transactional
    log on top of Parquet files to make data reads and writes a lot more efficient.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Delta文件格式是一种比Parquet和其他列式格式更优化的开放格式。当数据以Delta格式存储时，你会注意到底层文件是Parquet格式的。Delta格式在Parquet文件之上添加了一个事务日志，使得数据读取和写入更加高效。
- en: 'Let’s learn how to read and write Delta files with Spark by running the following
    code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来学习如何使用Spark读取和写入Delta文件：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The resulting DataFrame, named `salary_data_with_id`, looks like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame，命名为`salary_data_with_id`，看起来如下：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this example, we’re writing `salary_data_with_id` to a Delta file. We added
    the `delta` parameter to the `format` function, after which we saved the file
    to a location.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将`salary_data_with_id`写入一个Delta文件。我们在`format`函数中添加了`delta`参数，之后将文件保存到某个位置。
- en: In the next statement, we are reading the same Delta file we wrote into a DataFrame
    called `df`. The contents of the file remain the same as the DataFrame we used
    to write it with.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个语句中，我们正在将我们写入的相同Delta文件读取到一个名为`df`的DataFrame中。文件的内容与用于写入它的DataFrame保持一致。
- en: Now that we know how to manipulate and join data with advanced operations in
    Spark, we will look at how we can use SQL with Spark DataFrames interchangeably
    to switch between Python and SQL as languages. This gives a lot of power to Spark
    users since this allows them to use multiple languages, depending on the use case
    and their knowledge of different languages.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何在Spark中使用高级操作来操作和合并数据，我们将探讨如何使用SQL与Spark DataFrames进行交互，以在Python和SQL之间切换语言。这为Spark用户提供了很大的权力，因为它允许他们根据用例和他们对不同语言的知识使用多种语言。
- en: Using SQL in Spark
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中使用SQL
- en: In [*Chapter 2*](B19176_02.xhtml#_idTextAnchor030), we talked about Spark Core
    and how it’s shared across different components of Spark. DataFrames and Spark
    SQL can also be used interchangeably. We can also use data stored in DataFrames
    with Spark SQL queries.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B19176_02.xhtml#_idTextAnchor030)中，我们讨论了Spark Core以及它是如何跨Spark的不同组件共享的。DataFrame和Spark
    SQL也可以互换使用。我们还可以使用DataFrame中存储的数据进行Spark SQL查询。
- en: 'The following code illustrates how we can make use of this feature:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何利用此功能：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The resulting DataFrame looks like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结果DataFrame看起来像这样：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `createOrReplaceTempView` function is used to convert a DataFrame into a
    table named `SalaryTable`. Once this conversion is made, we can run regular SQL
    queries on top of this table. We are running a `count *` query to count the total
    number of elements in a table.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`createOrReplaceTempView`函数用于将DataFrame转换为名为`SalaryTable`的表。一旦完成此转换，我们就可以在此表上运行常规SQL查询。我们正在运行一个`count
    *`查询来计算表中的元素总数。'
- en: In the next section, we will see what a UDF is and how we use that in Spark.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解UDF是什么以及如何在Spark中使用它。
- en: UDFs in Apache Spark
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的UDFs
- en: UDFs are a powerful feature in Apache Spark that allows you to extend the functionality
    of Spark by defining custom functions. UDFs are essential for transforming and
    manipulating data in ways not directly supported by built-in Spark functions.
    In this section, we’ll delve into the concepts, implementation, and best practices
    for using UDFs in Spark.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs（用户定义函数）是Apache Spark中的一个强大功能，它允许你通过定义自定义函数来扩展Spark的功能。UDFs对于以Spark内置函数不支持的方式转换和操作数据至关重要。在本节中，我们将深入探讨UDFs在Spark中的概念、实现和最佳实践。
- en: What are UDFs?
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是UDFs？
- en: UDFs are custom functions that are created by users to perform specific operations
    on data within Spark. UDFs extend the range of transformations and operations
    you can apply to your data, making Spark more versatile for diverse use cases.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs是由用户创建的用于在Spark中对数据进行特定操作的定制函数。UDFs扩展了你可以应用于数据转换和操作的范围，使Spark在多样化的用例中更加灵活。
- en: 'Here are some of the key characteristics of UDFs:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是UDFs的一些关键特性：
- en: '**User-customized logic**: UDFs allow you to apply user-specific logic or custom
    algorithms to your data'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户自定义逻辑**：UDFs允许你将用户特定的逻辑或自定义算法应用于你的数据'
- en: '**Support for various languages**: Spark supports UDFs written in various programming
    languages, including Scala, Python, Java, and R'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持多种语言**：Spark支持用Scala、Python、Java和R等多种编程语言编写的UDFs'
- en: '**Compatibility with DataFrames and resilient distributed datasets (RDDs)**:
    UDFs can be used with both DataFrames and RDDs'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与DataFrame和弹性分布式数据集（RDD）的兼容性**：UDFs可以与DataFrame和RDD一起使用'
- en: '**Leverage external libraries**: You can use external libraries within your
    UDFs to perform advanced operations'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用外部库**：你可以在UDFs中使用外部库来执行高级操作'
- en: Let’s see how UDFs are created.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建UDFs。
- en: Creating and registering UDFs
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和注册UDFs
- en: To use UDFs in Spark, you need to create and register them. The process involves
    defining a function and registering it with Spark. You can define UDFs for both
    SQL and DataFrame operations. In this section, you will see the basic syntax of
    defining a UDF in Spark and then registering that UDF with Spark. You can write
    any custom Python code in your UDF for your application’s logic. The first example
    is in Python; the next example is in Scala.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Spark中使用UDFs，你需要创建并注册它们。这个过程涉及定义一个函数并将其注册到Spark中。你可以为SQL和DataFrame操作定义UDFs。在本节中，你将看到在Spark中定义UDF的基本语法，然后将其注册到Spark中。你可以在UDF中编写任何自定义Python代码以用于你的应用程序逻辑。第一个例子是Python；下一个例子是Scala。
- en: Creating UDFs in Python
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中创建UDFs
- en: 'We can use the following code to create a UDF in Python:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在Python中创建一个UDF：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Creating UDFs in Scala
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Scala中创建UDFs
- en: 'We can use the following code to create a UDF in Scala:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在Scala中创建一个UDF：
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Use cases for UDFs
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UDFs的用例
- en: 'UDFs are versatile and can be used in a wide range of scenarios, including,
    but not limited to, the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs非常灵活，可以在广泛的应用场景中使用，包括但不限于以下内容：
- en: '**Data transformation**: Applying custom logic to transform data, such as data
    enrichment, cleansing, and feature engineering'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：应用自定义逻辑以转换数据，例如数据丰富、清洗和特征工程'
- en: '**Complex calculations**: Implementing complex mathematical or statistical
    operations not available in Spark’s standard functions'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂计算**：实现Spark标准函数中不可用的复杂数学或统计操作'
- en: '**String manipulation**: Parsing and formatting strings, regular expressions,
    and text processing'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符串操作**：解析和格式化字符串、正则表达式和文本处理'
- en: '**Machine learning**: Creating custom functions for feature extraction, preprocessing,
    or post-processing in machine learning workflows'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：在机器学习工作流程中创建用于特征提取、预处理或后处理的自定义函数'
- en: '**Domain-specific logic**: Implementing specific domain-related logic that
    is unique to your use case'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定领域逻辑**：实现特定于您用例的特定领域相关逻辑'
- en: Best practices for using UDFs
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用UDF的最佳实践
- en: 'When working with UDFs in Spark, consider the following best practices:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Spark中使用UDF时，请考虑以下最佳实践：
- en: '**Avoid performance bottlenecks**: UDFs can impact performance, especially
    when used with large datasets. Profile and monitor your application to identify
    performance bottlenecks.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免性能瓶颈**：UDF可能会影响性能，尤其是在与大数据集一起使用时。分析并监控您的应用程序以识别性能瓶颈。'
- en: '**Minimize UDF complexity**: Keep UDFs simple and efficient to avoid slowing
    down your Spark application. Complex operations can lead to longer execution times.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化UDF复杂性**：保持UDF简单和高效，以避免减慢您的Spark应用程序。复杂的操作可能导致更长的执行时间。'
- en: '**Check for data type compatibility**: Ensure that the UDF’s output data type
    matches the column data type to avoid errors and data type mismatches.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查数据类型兼容性**：确保UDF的输出数据类型与列数据类型匹配，以避免错误和数据类型不匹配。'
- en: '**Optimize data processing**: Consider using built-in Spark functions whenever
    possible as they are highly optimized for distributed data processing.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化数据处理**：尽可能使用内置的Spark函数，因为它们针对分布式数据处理进行了高度优化。'
- en: '**Use vectorized UDFs**: In some Spark versions, vectorized UDFs are available,
    which can significantly improve UDF performance by processing multiple values
    at once.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用矢量化UDF**：在某些Spark版本中，可用的矢量化UDF可以通过一次处理多个值来显著提高UDF性能。'
- en: '**Test and validate**: Test your UDFs thoroughly on small subsets of data before
    applying them to the entire dataset. Ensure they produce the desired results.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试和验证**：在将UDF（用户定义函数）应用于整个数据集之前，先在数据的小子集上彻底测试它们。确保它们产生预期的结果。'
- en: '**Document UDFs**: Document your UDFs with comments and descriptions to make
    your code more maintainable and understandable to others.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记录UDF**：使用注释和描述记录您的UDF，使您的代码更易于维护和理解他人。'
- en: In this section, we explored the concept of UDFs in Apache Spark. UDFs are powerful
    tools for extending Spark’s capabilities and performing custom data transformations
    and operations. When used judiciously and efficiently, UDFs can help you address
    a wide range of data processing challenges in Spark.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Apache Spark中UDF（用户定义函数）的概念。UDF是扩展Spark功能并执行自定义数据转换和操作的强大工具。当谨慎且高效地使用时，UDF可以帮助您解决Spark中广泛的数据处理挑战。
- en: Now that we’ve covered the advanced operations in Spark, we will dive into the
    concept of Spark optimization.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了Spark的高级操作，我们将深入探讨Spark优化的概念。
- en: Optimizations in Apache Spark
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark的优化
- en: Apache Spark, renowned for its distributed computing capabilities, offers a
    suite of advanced optimization techniques that are crucial for maximizing performance,
    improving resource utilization, and enhancing the efficiency of data processing
    jobs. These techniques go beyond basic optimizations, allowing users to fine-tune
    and optimize Spark applications for optimal execution.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以其分布式计算能力而闻名的Apache Spark提供了一套高级优化技术，这些技术对于最大化性能、提高资源利用率和增强数据处理作业的效率至关重要。这些技术超越了基本优化，使用户能够针对最佳执行对Spark应用程序进行微调和优化。
- en: Understanding optimization in Spark
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Spark中的优化
- en: Optimization in Spark aims to fine-tune the execution of jobs to improve speed,
    resource utilization, and overall performance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的优化旨在微调作业的执行以提高速度、资源利用率和整体性能。
- en: Apache Spark is well-known for its powerful optimization capabilities, which
    significantly enhance the performance of distributed data processing tasks. At
    the heart of this optimization framework lies the Catalyst optimizer, an integral
    component that plays a pivotal role in enhancing query execution efficiency. This
    is achieved before the query is executed.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以其强大的优化能力而闻名的Apache Spark，显著提高了分布式数据处理任务的性能。在这个优化框架的核心是Catalyst优化器，这是一个关键组件，在提高查询执行效率方面发挥着至关重要的作用。这是在查询执行之前完成的。
- en: The Catalyst optimizer works primarily on static optimization plans that are
    generated during query compilation. However, AQE, which was introduced in Spark
    3.0, is a dynamic and adaptive approach to optimizing query plans at runtime based
    on the actual data characteristics and execution environment. We will learn more
    about both these paradigms in the next section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器主要在查询编译期间生成的静态优化计划上工作。然而，AQE（在Spark 3.0中引入）是一种动态和自适应的方法，在运行时根据实际数据特性和执行环境优化查询计划。我们将在下一节中了解更多关于这两种范例的信息。
- en: Catalyst optimizer
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst优化器
- en: The Catalyst optimizer is an essential part of Apache Spark’s query execution
    engine. It is a powerful tool that uses advanced techniques to optimize query
    plans, thus improving the performance of Spark applications. The term “*catalyst*”
    refers to its ability to spark transformations in the query plan and make it more
    efficient.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器是Apache Spark查询执行引擎的一个关键部分。它是一个强大的工具，使用高级技术来优化查询计划，从而提高Spark应用程序的性能。术语“*催化剂*”指的是它激发查询计划中的转换并使其更高效的能力。
- en: 'Let’s look at some of the key characteristics of the Catalyst optimizer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Catalyst优化器的一些关键特性：
- en: '**Rule-based optimization**: The Catalyst optimizer employs a set of rules
    and optimizations to transform and enhance query plans. These rules cover a wide
    range of query optimization scenarios.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的优化**：Catalyst优化器采用一组规则和优化来转换和增强查询计划。这些规则涵盖了广泛的查询优化场景。'
- en: '**Logical and physical query plans**: It works with both logical and physical
    query plans. The logical plan represents the abstract structure of a query, while
    the physical plan outlines how to execute it.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑和物理查询计划**：它同时适用于逻辑和物理查询计划。逻辑计划表示查询的抽象结构，而物理计划概述了如何执行它。'
- en: '**Extensibility**: Users can define custom rules and optimizations. This extensibility
    allows you to tailor the optimizer to your specific use case.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：用户可以定义自定义规则和优化。这种可扩展性允许你根据特定的用例定制优化器。'
- en: '**Cost-based optimization**: The Catalyst optimizer can evaluate the cost of
    different query plans and choose the most efficient one based on cost estimates.
    This is particularly useful when dealing with complex queries.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于成本的优化**：Catalyst优化器可以评估不同查询计划的成本，并根据成本估计选择最有效的一个。这在处理复杂查询时特别有用。'
- en: Let’s take a look at the different components that make up the Catalyst optimizer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看构成Catalyst优化器的不同组件。
- en: Catalyst optimizer components
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Catalyst优化器组件
- en: To gain a deeper understanding of the Catalyst optimizer, it’s essential to
    examine its core components.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解Catalyst优化器，必须检查其核心组件。
- en: Logical query plan
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逻辑查询计划
- en: The logical query plan represents the high-level, abstract structure of a query.
    It defines what you want to accomplish without specifying how to achieve it. Spark’s
    Catalyst optimizer works with this logical plan to determine the optimal physical
    plan.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑查询计划表示查询的高级、抽象结构。它定义了你想要完成的事情，而不指定如何实现它。Spark的Catalyst优化器与这个逻辑计划一起工作，以确定最佳物理计划。
- en: Rule-based optimization
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于规则的优化
- en: Rule-based optimization is the backbone of the Catalyst optimizer. It comprises
    a set of rules that transform the logical query plan into a more efficient version.
    Each rule focuses on a specific aspect of optimization, such as predicate pushdown,
    constant folding, or column pruning.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的优化是Catalyst优化器的核心。它包括一组规则，将逻辑查询计划转换为一个更高效的版本。每个规则都专注于优化的一个特定方面，例如谓词下沉、常量折叠或列剪枝。
- en: Physical query plan
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 物理查询计划
- en: The physical query plan defines how to execute the query. Once the logical plan
    is optimized using rule-based techniques, it’s converted into a physical plan,
    taking into account the available resources and the execution environment. This
    phase ensures that the plan is executable in a distributed and parallel manner.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 物理查询计划定义了如何执行查询。一旦逻辑计划使用基于规则的技巧优化，它就被转换为一个物理计划，考虑到可用的资源和执行环境。这一阶段确保计划可以在分布式和并行方式下执行。
- en: Cost-based optimization
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于成本的优化
- en: In addition to rule-based optimization, the Catalyst optimizer can use cost-based
    optimization. It estimates the cost of different execution plans, taking into
    account factors such as data distribution, join strategies, and available resources.
    This approach helps Spark choose the most efficient plan based on actual execution
    characteristics.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于规则的优化外，Catalyst优化器还可以使用基于成本的优化。它估计不同执行计划的成本，考虑数据分布、连接策略和可用资源等因素。这种方法有助于Spark根据实际的执行特征选择最有效的计划。
- en: Catalyst optimizer in action
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Catalyst优化器在行动
- en: To witness the Catalyst optimizer in action, let’s consider a practical example
    using Spark’s SQL API.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了见证Catalyst优化器的实际应用，让我们考虑一个使用Spark SQL API的实际示例。
- en: 'In this code example, we’re loading data from a CSV file, applying a selection
    operation to pick specific columns, and filtering rows based on a condition. By
    calling `explain()` on the resulting DataFrame, we can see the optimized query
    plan that was generated by the Catalyst optimizer. The output provides insights
    into the physical execution steps Spark will perform:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们正在从CSV文件加载数据，应用选择操作以选择特定列，并根据条件过滤行。通过在生成的DataFrame上调用`explain()`，我们可以看到由Catalyst优化器生成的优化查询计划。输出提供了关于Spark将执行的物理执行步骤的见解：
- en: '[PRE37]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This explanation from the `explain()` method often includes details about the
    physical execution plan, the use of specific optimizations, and the chosen strategies
    for query execution.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个来自`explain()`方法的解释通常包括关于物理执行计划、特定优化使用以及查询执行所选策略的详细信息。
- en: By examining the query plan and understanding how the Catalyst optimizer enhances
    it, you can gain valuable insights into the inner workings of Spark’s optimization
    engine.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查查询计划并理解Catalyst优化器如何增强它，你可以获得关于Spark优化引擎内部工作原理的宝贵见解。
- en: This section provided a solid introduction to the Catalyst optimizer, its components,
    and a practical example. You can expand on this foundation by delving deeper into
    rule-based and cost-based optimization techniques, as well as discussing real-world
    scenarios where the Catalyst optimizer can have a substantial impact on query
    performance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对Catalyst优化器、其组件和实际示例的坚实基础介绍。你可以在此基础上进一步深入研究基于规则和基于成本的优化技术，并讨论Catalyst优化器在查询性能上产生重大影响的实际场景。
- en: Next, we will see how AQE takes optimizations to the next level in Spark.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到AQE如何将优化提升到Spark的下一个层次。
- en: Adaptive Query Execution (AQE)
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应查询执行（AQE）
- en: Apache Spark, a powerful distributed computing framework, offers a multitude
    of optimization techniques to enhance the performance of data processing jobs.
    One such advanced optimization feature is AQE, a dynamic approach that significantly
    improves query processing efficiency.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark，一个强大的分布式计算框架，提供了多种优化技术来增强数据处理作业的性能。其中一项高级优化功能是AQE，这是一种动态方法，可以显著提高查询处理效率。
- en: AQE dynamically adjusts execution plans during runtime based on actual data
    statistics and hardware conditions. It collects and utilizes runtime statistics
    to optimize join strategies, partitioning methods, and broadcast operations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: AQE根据实际数据统计信息和硬件条件在运行时动态调整执行计划。它收集并利用运行时统计信息来优化连接策略、分区方法和广播操作。
- en: 'Let’s look at its key components:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的关键组件：
- en: '**Runtime statistics collection**: AQE collects runtime statistics, such as
    data size, skewness, and partitioning, during query execution'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时统计信息收集**：AQE在查询执行期间收集运行时统计信息，如数据大小、偏差和分区。'
- en: '**Adaptive optimization rules**: It utilizes collected statistics to adjust
    and optimize join strategies, partitioning methods, and broadcast operations dynamically'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应优化规则**：它利用收集到的统计信息动态调整和优化连接策略、分区方法和广播操作。'
- en: 'Now, let’s consider its benefits and significance:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑它的好处和重要性：
- en: '**Improved performance**: AQE significantly enhances performance by optimizing
    execution plans dynamically, leading to better resource utilization and reduced
    execution time'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升**：AQE通过动态优化执行计划，显著提高了性能，从而实现了更好的资源利用和减少的执行时间。'
- en: '**Handling variability**: It efficiently handles variations in data sizes,
    skewed data distributions, and changing hardware conditions during query execution'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理可变性**：它在查询执行期间有效地处理数据大小、数据分布偏差和硬件条件的变化。'
- en: '**Efficient resource utilization**: It optimizes query plans in real time,
    leading to better resource utilization and reduced execution time'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效资源利用**：它实时优化查询计划，从而提高资源利用率和减少执行时间'
- en: AQE workflow
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AQE工作流程
- en: 'Let’s look at how AQE optimizes workflows in Spark 3.0:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看AQE如何在Spark 3.0中优化工作流程：
- en: '**Runtime statistics collection**: During query execution, Spark collects statistics
    related to data distribution, partition sizes, and join keys’ cardinality'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时统计收集**：在查询执行期间，Spark收集与数据分布、分区大小和连接键基数相关的统计信息'
- en: '**Adaptive optimization**: Utilizing the collected statistics, Spark dynamically
    adjusts the query execution plan, optimizing join strategies, partitioning methods,
    and data redistribution techniques'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应优化**：利用收集到的统计信息，Spark动态调整查询执行计划，优化连接策略、分区方法和数据重分布技术'
- en: '**Enhanced performance**: The adaptive optimization ensures that Spark adapts
    to changing data and runtime conditions, resulting in improved query performance
    and resource utilization'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强性能**：自适应优化确保Spark适应不断变化的数据和运行时条件，从而提高查询性能和资源利用率'
- en: AQE in Apache Spark represents a significant advancement in query optimization,
    moving beyond static planning to adapt to runtime conditions and data characteristics.
    By dynamically adjusting execution plans based on real-time statistics, it optimizes
    query performance, ensuring efficient and scalable processing of large-scale datasets.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: AQE在Apache Spark中代表了查询优化的重要进步，它超越了静态规划，以适应运行时条件和数据特征。通过根据实时统计信息动态调整执行计划，它优化查询性能，确保大规模数据集的高效和可扩展处理。
- en: Next, we will see how Spark does cost-based optimizations.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到Spark如何进行基于成本的优化。
- en: Cost-based optimization
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于成本的优化
- en: Spark estimates the cost of executing different query plans based on factors
    such as data size, join operations, and shuffle stages. It utilizes cost estimates
    to select the most efficient query execution plan.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Spark根据数据大小、连接操作和洗牌阶段等因素估计执行不同查询计划的成本。它利用成本估计来选择最有效的查询执行计划。
- en: 'Here are the benefits:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是其好处：
- en: '**Optimal plan selection**: Cost-based optimization chooses the most cost-effective
    execution plan while considering factors such as join strategies and data distribution'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最优计划选择**：基于成本的优化在选择最经济的执行计划时考虑因素，如连接策略和数据分布'
- en: '**Performance improvement**: Minimizing unnecessary shuffling and computations
    improves query performance'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升**：最小化不必要的洗牌和计算提高查询性能'
- en: Next, we will see how Spark utilizes memory management and tuning for optimizations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到Spark如何利用内存管理和调整进行优化。
- en: Memory management and tuning
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存管理和调整
- en: Spark also applies efficient memory allocation strategies, including storage
    and execution memory, to avoid unnecessary spills and improve processing. It fine-tunes
    garbage collection settings to minimize interruptions and improve overall job
    performance.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还应用了有效的内存分配策略，包括存储和执行内存，以避免不必要的溢出并提高处理效率。它微调垃圾收集设置以最小化中断并提高整体作业性能。
- en: 'Here are its benefits:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是其好处：
- en: '**Reduced overheads**: Optimized memory usage minimizes unnecessary spills
    to disk, reducing overheads and improving job performance'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低开销**：优化的内存使用最小化不必要的磁盘溢出，降低开销并提高作业性能'
- en: '**Stability and reliability**: Tuned garbage collection settings enhance stability
    and reduce pauses, ensuring more consistent job execution'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性和可靠性**：调整垃圾收集设置提高稳定性并减少暂停，确保更一致的作业执行'
- en: Advanced Spark optimization techniques, including AQE, cost-based optimization,
    the Catalyst optimizer, and memory management, play a vital role in improving
    Spark job performance, resource utilization, and overall efficiency. By leveraging
    these techniques, users can optimize Spark applications to meet varying data processing
    demands and enhance their scalability and performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 包括AQE、基于成本的优化、Catalyst优化器和内存管理在内的高级Spark优化技术，在提高Spark作业性能、资源利用率和整体效率方面发挥着至关重要的作用。通过利用这些技术，用户可以优化Spark应用程序以满足不同的数据处理需求，并提高其可扩展性和性能。
- en: So far, we have seen how Spark optimizes its query plans internally. However,
    there are other optimizations that users can implement to make Spark’s performance
    even better. We will discuss some of these optimizations next.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Spark如何内部优化其查询计划。然而，还有其他用户可以实施以进一步提高Spark性能的优化。接下来，我们将讨论一些这些优化。
- en: Data-based optimizations in Apache Spark
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的基于数据的优化
- en: In addition to Spark’s inner optimizations, there are certain things we can
    take care of in terms of implementation to make Spark more efficient. These are
    user-controlled optimizations. If we are aware of these challenges and how to
    handle them in real-world data applications, we can utilize Spark’s distributed
    architecture to its fullest.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Spark的内部优化之外，我们还可以在实现方面做一些事情来使Spark更加高效。这些是用户控制的优化。如果我们了解这些挑战以及如何在现实世界的数据应用中处理它们，我们就可以充分利用Spark的分布式架构。
- en: We’ll start by looking at a very common occurrence in distributed frameworks
    called the small file problem.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从分布式框架中一个非常常见的问题——小文件问题开始讨论。
- en: Addressing the small file problem in Apache Spark
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决Apache Spark中的小文件问题
- en: The small file problem poses a significant challenge in distributed computing
    frameworks such as Apache Spark as it impacts performance and efficiency. It arises
    when data is stored in numerous small files rather than consolidated in larger
    files, leading to increased overhead and suboptimal resource utilization. In this
    section, we’ll delve into the implications of the small file problem in Spark
    and explore effective solutions to mitigate its effects.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 小文件问题在分布式计算框架（如Apache Spark）中提出了重大挑战，因为它影响了性能和效率。当数据存储在众多小文件中而不是合并成大文件时，会导致开销增加和资源利用不充分。在本节中，我们将深入探讨Spark中小文件问题的含义，并探讨缓解其影响的有效解决方案。
- en: 'The key challenges associated with the small file problem are as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 与小文件问题相关的主要挑战如下：
- en: '**Increased metadata overhead**: Storing data in numerous small files leads
    to higher metadata overhead as each file occupies a separate block and incurs
    additional I/O operations for file handling'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加的元数据开销**：将数据存储在众多小文件中会导致更高的元数据开销，因为每个文件都占用单独的块并产生额外的文件处理I/O操作。'
- en: '**Reduced throughput**: Processing numerous small files is less efficient as
    it involves a high level of overhead for opening, reading, and closing files,
    resulting in reduced throughput'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量降低**：处理众多小文件效率较低，因为它涉及打开、读取和关闭文件的高开销，导致吞吐量降低。'
- en: '**Inefficient resource utilization**: Spark’s parallelism relies on data partitioning,
    and small files can lead to inadequate partitioning, underutilizing resources,
    and hindering parallel processing'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用效率低下**：Spark的并行性依赖于数据分区，而小文件可能导致分区不足，资源利用率低，并阻碍并行处理。'
- en: 'Now that we’ve discussed the key challenges, let’s discuss some solutions to
    mitigate the small file problem:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了主要挑战，让我们讨论一些缓解小文件问题的解决方案：
- en: '**File concatenation or merging**: Consolidating small files into larger files
    can significantly alleviate the small file problem. Techniques such as file concatenation
    or merging, either manually or through automated processes, help reduce the number
    of individual files.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件连接或合并**：将小文件合并成大文件可以显著缓解小文件问题。通过手动或自动化流程进行文件连接或合并等技术有助于减少单个文件的数量。'
- en: '**File compaction or coalescing**: Tools or processes that compact or coalesce
    small files into fewer, more substantial files can streamline data storage. This
    consolidation reduces metadata overhead and enhances data access efficiency.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件压缩或合并**：将小文件压缩或合并成更少、更实质性的文件的工具或流程可以简化数据存储。这种整合减少了元数据开销并提高了数据访问效率。'
- en: '**File format optimization**: Choosing efficient file formats such as Parquet
    or ORC, which support columnar storage and compression, can reduce the impact
    of small files. These formats facilitate efficient data access and reduce storage
    space.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件格式优化**：选择高效的文件格式，如Parquet或ORC，这些格式支持列式存储和压缩，可以减少小文件的影响。这些格式便于高效的数据访问并减少存储空间。'
- en: '**Partitioning strategies**: Applying appropriate partitioning strategies during
    data ingestion or processing in Spark can mitigate the effects of the small file
    problem. It involves organizing data into larger partitions to improve parallelism.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区策略**：在Spark的数据摄入或处理过程中应用适当的分区策略可以减轻小文件问题的影响。这涉及到将数据组织成更大的分区以提高并行性。'
- en: '**Data prefetching or caching**: Prefetching or caching small files into memory
    before processing can minimize I/O overhead. Techniques such as caching or loading
    data into memory using Spark’s capabilities can improve performance.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预取或缓存**：在处理之前将小文件预取或缓存到内存中可以最小化I/O开销。使用Spark的能力进行缓存或加载数据等技术可以提高性能。'
- en: '**AQE**: Leveraging Spark’s AQE features helps optimize query plans based on
    runtime statistics. This can mitigate the impact of small files during query execution.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AQE**: 利用Spark的AQE功能可以帮助根据运行时统计信息优化查询计划。这可以在查询执行期间减轻小文件的影响。'
- en: '**Data lake architectural changes**: Reevaluating the data lake architecture
    and adopting data ingestion strategies that minimize the creation of small files
    can prevent the problem at its source.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖架构变更**: 重新评估数据湖架构并采用最小化小文件创建的数据摄取策略，可以从源头上防止问题。'
- en: 'Let’s look at the best practices for handling small files:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看处理小文件的最佳实践：
- en: '**Regular monitoring and cleanup**: Implement regular monitoring and cleanup
    processes to identify and merge small files that are generated over time'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期监控和清理**: 实施定期监控和清理流程，以识别和合并随着时间的推移生成的小文件'
- en: '**Optimize the storage layout**: Design data storage layouts that minimize
    the creation of small files while considering factors such as block size and filesystem
    settings'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化存储布局**: 设计数据存储布局，在考虑块大小和文件系统设置等因素的同时，最大限度地减少小文件的创建。'
- en: '**Automated processes**: Use automated processes or tools to consolidate and
    manage small files efficiently, reducing manual effort'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化流程**: 使用自动化流程或工具来有效地合并和管理小文件，减少人工工作量。'
- en: '**Educate data producers**: Educate data producers on the impact of small files
    and encourage practices that generate larger files or optimize file creation'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**教育数据生产者**: 教育数据生产者关于小文件的影响，并鼓励生成更大文件或优化文件创建的实践。'
- en: By adopting these strategies and best practices, organizations can effectively
    mitigate the small file problem in Apache Spark, ensuring improved performance,
    enhanced resource utilization, and efficient data processing capabilities. These
    approaches empower users to overcome the challenges posed by the small file problem
    and optimize their Spark workflows for optimal performance and scalability.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些策略和最佳实践，组织可以有效地缓解Apache Spark中的小文件问题，确保性能提升、资源利用增强以及高效的数据处理能力。这些方法赋予用户克服小文件问题带来的挑战，并优化其Spark工作流程以实现最佳性能和可扩展性。
- en: Next, we will see how data skew affects performance in Spark.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到数据倾斜如何影响Spark的性能。
- en: Tackling data skew in Apache Spark
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决Apache Spark中的数据倾斜问题
- en: '**Data skew** presents a significant challenge in distributed data processing
    frameworks such as Apache Spark, causing uneven workload distribution and hindering
    parallelism.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据倾斜**在分布式数据处理框架（如Apache Spark）中是一个重大挑战，导致工作负载分布不均并阻碍并行性。'
- en: Data skew occurs when certain keys or partitions hold significantly more data
    than others. This imbalance leads to unequal processing times for different partitions,
    causing stragglers. Skewed data distribution can result in certain worker nodes
    being overloaded while others remain underutilized, leading to inefficient resource
    allocation. Tasks that deal with skewed data partitions take longer to complete,
    causing delays in job execution and affecting overall performance.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当某些键或分区持有的数据量显著多于其他分区时，就会发生数据倾斜。这种不平衡导致不同分区的处理时间不均，造成延迟。倾斜的数据分布可能导致某些工作节点过载，而其他节点利用率不足，导致资源分配效率低下。处理倾斜数据分区的任务需要更长的时间完成，从而造成作业执行延迟并影响整体性能。
- en: 'Here are some of the solutions we can use to address data skew:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些我们可以用来解决数据倾斜的解决方案：
- en: '**Partitioning techniques**:'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区技术**:'
- en: '**Salting**: Introduce randomness by adding a salt to keys to distribute data
    more evenly across partitions. This helps prevent hotspots and balances the workload.'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Salting**: 通过向键添加盐来引入随机性，以更均匀地分布数据到分区中。这有助于防止热点并平衡工作负载。'
- en: '**Custom partitioning**: Implement custom partitioning logic to redistribute
    skewed data by grouping keys differently, ensuring a more balanced distribution
    across partitions.'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义分区**: 通过不同地分组键来实现自定义分区逻辑，以重新分配倾斜数据，确保分区之间分布更加平衡。'
- en: '**Skew-aware algorithms**: Utilize techniques such as skew join optimization,
    which handles skewed keys separately from regular joins, redistributing and processing
    them more efficiently'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倾斜感知算法**: 利用诸如倾斜连接优化等技术，这些技术将倾斜键与常规连接分开处理，更有效地重新分配和处理它们。'
- en: '**Replicate small-skewed data**: Replicate small skewed partitions across multiple
    nodes to parallelize processing and alleviate the load on individual nodes'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制小倾斜数据**: 在多个节点上复制小倾斜分区，以并行处理并减轻单个节点的负载'
- en: '**AQE**: Leverage Spark’s AQE capabilities to dynamically adjust execution
    plans based on runtime statistics, mitigating the impact of data skew'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AQE**：利用 Spark 的 AQE 功能根据运行时统计信息动态调整执行计划，减轻数据偏差的影响'
- en: '**Sampling and filtering**: Apply sampling and filtering techniques to identify
    skewed data partitions beforehand, allowing for proactive handling of skewed keys
    during processing'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样和过滤**：应用采样和过滤技术事先识别偏差数据分区，允许在处理过程中主动处理偏差键'
- en: '**Dynamic resource allocation**: Implement dynamic resource allocation to allocate
    additional resources to tasks dealing with skewed data partitions, optimizing
    resource utilization'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态资源分配**：实现动态资源分配，为处理偏差数据分区的任务分配额外资源，优化资源利用'
- en: 'Let’s discuss the best practices for handling data skew:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论处理数据偏差的最佳实践：
- en: '**Regular profiling**: Continuously profile and monitor data distribution to
    identify and address skew issues early in the processing pipeline'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期分析**：持续分析并监控数据分布，以在处理管道早期识别并解决偏差问题'
- en: '**Optimized partitioning**: Choose appropriate partitioning strategies based
    on data characteristics to prevent or mitigate data skew'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化分区**：根据数据特性选择合适的分区策略，以防止或减轻数据偏差'
- en: '**Distributed processing**: Leverage distributed processing frameworks to distribute
    skewed data across multiple nodes for parallel execution'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式处理**：利用分布式处理框架将偏差数据分布到多个节点以实现并行执行'
- en: '**Task retry mechanisms**: Implement retry mechanisms for tasks dealing with
    skewed data to accommodate potential delays and avoid job failures'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务重试机制**：为处理偏差数据的任务实现重试机制，以适应潜在的延迟并避免作业失败'
- en: '**Data preprocessing**: Apply preprocessing techniques to mitigate skew before
    data processing, ensuring a more balanced workload'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：在数据处理前应用预处理技术以减轻偏差，确保更均衡的工作负载'
- en: By employing these strategies and best practices, organizations can effectively
    combat data skew in Apache Spark, ensuring more balanced workloads, improved resource
    utilization, and enhanced overall performance in distributed data processing workflows.
    These approaches empower users to overcome the challenges posed by data skew and
    optimize Spark applications for efficient and scalable data processing.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些策略和最佳实践，组织可以有效应对 Apache Spark 中的数据偏差，确保更均衡的工作负载、改进的资源利用，并在分布式数据处理工作流程中提高整体性能。这些方法赋予用户克服数据偏差带来的挑战，并优化
    Spark 应用以实现高效和可扩展的数据处理。
- en: Addressing data skew in Apache Spark is critical for optimizing performance
    and ensuring efficient resource utilization in distributed computing environments.
    By understanding the causes and impacts of data skew and employing mitigation
    strategies users can significantly improve the efficiency and reliability of Spark
    jobs, mitigating the adverse effects of data skew.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中解决数据偏差对于优化性能和确保分布式计算环境中的资源高效利用至关重要。通过理解数据偏差的原因和影响，并采用缓解策略，用户可以显著提高
    Spark 作业的效率和可靠性，减轻数据偏差的负面影响。
- en: In the next section, we will talk about data spills in Spark and how to manage
    them.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论 Spark 中的数据溢出以及如何管理它们。
- en: Managing data spills in Apache Spark
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中管理数据溢出
- en: Data spill, something that’s often encountered in distributed processing frameworks
    such as Apache Spark, occurs when the data being processed exceeds the available
    memory capacity, leading to data being written to disk. This phenomenon can significantly
    impact performance and overall efficiency. In this section, we’ll delve into the
    implications of data spill in Spark and effective strategies to mitigate its effects
    for optimized data processing.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 数据溢出，这是在 Apache Spark 等分布式处理框架中经常遇到的问题，当正在处理的数据超过可用内存容量时发生，导致数据被写入磁盘。这种现象可能会显著影响性能和整体效率。在本节中，我们将深入探讨
    Spark 中数据溢出的影响以及缓解其影响的有效策略，以优化数据处理。
- en: Data spill occurs when Spark’s memory capacity is exceeded, resulting in excessive
    data write operations to disk, which are significantly slower than in-memory operations.
    Writing data to disk incurs high I/O overhead, leading to a substantial degradation
    in processing performance due to increased latency. Data spillage can cause resource
    contention as disk operations compete with other computing tasks, leading to inefficient
    resource utilization.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 数据溢出发生在Spark的内存容量超出时，导致过多的数据写入磁盘操作，这比内存操作慢得多。将数据写入磁盘会产生高I/O开销，由于延迟增加，导致处理性能显著下降。数据溢出可能导致资源竞争，因为磁盘操作与其他计算任务竞争，导致资源利用效率低下。
- en: 'Here are some of the solutions we can implement to address data spill:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们可以实施的解决数据溢出的一些方案：
- en: '**Memory** **management techniques**:'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存管理技术**：'
- en: '**Increase executor memory**: Allocating more memory to Spark executors can
    help reduce the likelihood of data spill by accommodating larger datasets in memory'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加执行器内存**：为Spark执行器分配更多内存可以帮助减少数据溢出的可能性，通过在内存中容纳更大的数据集来实现'
- en: '**Tune memory configuration**: Optimize Spark’s memory configurations, such
    as adjusting memory fractions for storage and execution, to better utilize available
    memory'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整内存配置**：优化Spark的内存配置，例如调整存储和执行内存的分数，以更好地利用可用内存'
- en: '**Partitioning and** **caching strategies**:'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区和缓存策略**：'
- en: '**Repartitioning**: Repartitioning data into an optimal number of partitions
    can help manage memory usage and minimize data spills by ensuring better data
    distribution across nodes'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新分区**：将数据重新分区到最佳分区数，可以帮助管理内存使用并最小化数据溢出，确保节点间更好的数据分布'
- en: '**Caching intermediate results**: Caching or persisting intermediate datasets
    in memory can prevent recomputation and reduce the chances of data spill during
    subsequent operations'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存中间结果**：在内存中缓存或持久化中间数据集可以防止重复计算，并减少后续操作中数据溢出的可能性'
- en: '**Advanced** **optimization techniques**:'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级优化技术**：'
- en: '**Shuffle tuning**: Tune shuffle operations by adjusting parameters such as
    shuffle partitions and buffer sizes to reduce the likelihood of data spill during
    shuffle phases'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shuffle调整**：通过调整如shuffle分区和缓冲区大小等参数来调整shuffle操作，以降低在shuffle阶段数据溢出的可能性'
- en: '**Data compression**: Utilize data compression techniques when storing intermediate
    data in memory or on disk to reduce the storage footprint and alleviate memory
    pressure'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据压缩**：在内存或磁盘上存储中间数据时利用数据压缩技术，以减少存储占用并缓解内存压力'
- en: '**AQE**: Leverage Spark’s AQE capabilities to dynamically adjust execution
    plans based on runtime statistics, optimizing memory usage and reducing spillage.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AQE**：利用Spark的AQE能力，根据运行时统计信息动态调整执行计划，优化内存使用并减少溢出。'
- en: '**Task and data skew handling**: Apply techniques to mitigate task and data
    skew. Skewed data can exacerbate memory pressure and increase the chances of data
    spill.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理任务和数据倾斜**：应用技术来减轻任务和数据倾斜。倾斜的数据会加剧内存压力并增加数据溢出的可能性。'
- en: 'Here are the best practices for handling data spills:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是处理数据溢出的最佳实践：
- en: '**Resource monitoring**: Regularly monitor memory usage and resource allocation
    to identify and preempt potential data spillage issues'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源监控**：定期监控内存使用和资源分配，以识别和预防潜在的数据溢出问题'
- en: '**Optimized data structures**: Utilize optimized data structures and formats
    (such as Parquet or ORC) to reduce memory overhead and storage requirements'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化数据结构**：利用优化的数据结构和格式（如Parquet或ORC）以减少内存开销和存储需求'
- en: '**Efficient caching strategies**: Strategically cache or persist intermediate
    results to minimize recomputation and reduce the probability of data spill'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的缓存策略**：策略性地缓存或持久化中间结果，以最小化重复计算并降低数据溢出的概率'
- en: '**Incremental processing**: Employ incremental processing techniques to handle
    large datasets in manageable chunks, reducing memory pressure'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增量处理**：采用增量处理技术，以可管理的块处理大数据集，减少内存压力'
- en: By adopting these strategies and best practices, organizations can effectively
    manage data spillage in Apache Spark, ensuring efficient memory utilization, optimized
    processing performance, and enhanced overall scalability in distributed data processing
    workflows. These approaches empower users to proactively address data spillage
    challenges and optimize Spark applications for improved efficiency and performance.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些策略和最佳实践，组织可以有效管理Apache Spark中的数据溢出，确保高效内存利用、优化处理性能，并在分布式数据处理工作流程中提高整体可伸缩性。这些方法使用户能够主动解决数据溢出挑战，并优化Spark应用程序以提高效率和性能。
- en: In the next section, we will talk about what data shuffle is and how to handle
    it to optimize performance.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论什么是数据洗牌以及如何处理它以优化性能。
- en: Managing data shuffle in Apache Spark
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Apache Spark中管理数据洗牌
- en: Data shuffle, a fundamental operation in distributed processing frameworks such
    as Apache Spark, involves moving data across nodes in the cluster. While shuffle
    operations are essential for various transformations, such as joins and aggregations,
    they can also introduce performance bottlenecks and resource overhead. In this
    section, we’ll explore the implications of data shuffle in Spark and effective
    strategies to optimize and mitigate its impact for efficient data processing.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 数据洗牌，在Apache Spark等分布式处理框架中的基本操作，涉及在集群节点间移动数据。虽然洗牌操作对于各种转换，如连接和聚合，是必不可少的，但它们也可能引入性能瓶颈和资源开销。在本节中，我们将探讨Spark中数据洗牌的影响以及优化和减轻其影响的有效策略，以实现高效的数据处理。
- en: Data shuffle involves extensive network and disk I/O operations, leading to
    increased latency and resource utilization. Shuffling large amounts of data across
    nodes can introduce performance bottlenecks due to excessive data movement and
    processing. Intensive shuffle operations can cause resource contention among nodes,
    impacting overall cluster performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 数据洗牌涉及大量的网络和磁盘I/O操作，导致延迟增加和资源利用率提高。在节点间移动大量数据可能会因为数据移动和处理过多而引入性能瓶颈。密集的洗牌操作可能导致节点间的资源竞争，影响整体集群性能。
- en: 'Let’s discuss the solutions for optimizing data shuffle:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论优化数据洗牌的解决方案：
- en: '**Data partitioning techniques**: Implement optimized data partitioning strategies
    to reduce shuffle overhead, ensuring a more balanced workload distribution'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分区技术**：实施优化的数据分区策略以减少洗牌开销，确保更平衡的工作负载分配'
- en: '**Skew handling**: Mitigate data skew by employing techniques such as salting
    or custom partitioning to prevent hotspots and balance data distribution'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倾斜处理**：通过采用盐分或自定义分区等技术来减轻数据倾斜，防止热点并平衡数据分布'
- en: '**Shuffle partitions adjustment**: Tune the number of shuffle partitions based
    on data characteristics and job requirements to optimize shuffle performance and
    reduce overhead'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洗牌分区调整**：根据数据特性和作业要求调整洗牌分区的数量，以优化洗牌性能并减少开销'
- en: '**Memory management**: Optimize memory allocation for shuffle operations to
    minimize spills to disk and improve overall shuffle performance'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存管理**：优化洗牌操作的内存分配，以最小化数据溢出到磁盘并提高整体洗牌性能'
- en: '**Data filtering and pruning**: Apply filtering or pruning techniques to reduce
    the amount of data shuffled across nodes, focusing only on relevant subsets of
    data'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据过滤和修剪**：应用过滤或修剪技术以减少节点间洗牌的数据量，仅关注相关数据子集'
- en: '**Join optimization**:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接优化**：'
- en: '**Broadcast joins**: Utilize broadcast joins for smaller datasets to replicate
    them across nodes, minimizing data shuffling and improving join performance'
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广播连接**：对于较小的数据集，利用广播连接在节点间复制数据，最小化数据洗牌并提高连接性能'
- en: '**Sort-merge joins**: Employ sort-merge join algorithms for large datasets
    to minimize data movement during join operations'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排序合并连接**：对于大型数据集，采用排序合并连接算法以最小化连接操作中的数据移动'
- en: '**AQE**: Leverage Spark’s AQE capabilities to dynamically optimize shuffle
    operations based on runtime statistics and data distribution'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AQE**：利用Spark的AQE功能，根据运行时统计和数据分布动态优化洗牌操作'
- en: 'The best practices for managing data shuffle are as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 管理数据洗牌的最佳实践如下：
- en: '**Profile and monitor**: Continuously profile and monitor shuffle operations
    to identify bottlenecks and optimize configurations'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析和监控**：持续分析和监控洗牌操作，以识别瓶颈并优化配置'
- en: '**Optimized partition sizes**: Determine optimal partition sizes based on data
    characteristics and adjust shuffle partitioning accordingly'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化的分区大小**: 根据数据特性确定最佳分区大小，并相应地调整shuffle分区'
- en: '**Caching and persistence**: Cache or persist intermediate shuffle results
    to reduce recomputation and mitigate shuffle overhead'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存和持久化**: 缓存或持久化中间shuffle结果以减少重复计算并减轻shuffle开销'
- en: '**Regular tuning**: Regularly tune Spark configurations related to shuffle
    operations based on workload requirements and cluster resources'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常规调优**: 根据工作负载需求和集群资源，定期调整与shuffle操作相关的Spark配置'
- en: By implementing these strategies and best practices, organizations can effectively
    optimize data shuffle operations in Apache Spark, ensuring improved performance,
    reduced resource contention, and enhanced overall efficiency in distributed data
    processing workflows. These approaches empower users to proactively manage and
    optimize shuffle operations for streamlined data processing and improved cluster
    performance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略和最佳实践，组织可以有效地优化Apache Spark中的数据shuffle操作，确保性能提升、减少资源竞争，并提高分布式数据处理工作流程的整体效率。这些方法赋予用户主动管理和优化shuffle操作的能力，以实现数据处理的简化并提高集群性能。
- en: Despite all the data-related challenges that users need to be aware of, there
    are certain types of joins that Spark has available in its internal working that
    we can utilize for better performance. We’ll take a look at these next.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管用户需要了解所有与数据相关的挑战，但Spark在其内部工作中有某些类型的join可供我们利用以获得更好的性能。我们将接下来查看这些内容。
- en: Shuffle and broadcast joins
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle和广播join
- en: 'Apache Spark offers two fundamental approaches for performing join operations:
    shuffle joins and broadcast joins. Each method has its advantages and use cases,
    and understanding when to use them is crucial for optimizing your Spark applications.
    Note that these joins are done by Spark automatically to join different datasets
    together. You can enforce some of the join types in your code but Spark takes
    care of the execution.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了两种基本方法来执行join操作：shuffle join和广播join。每种方法都有其优势和用例，了解何时使用它们对于优化Spark应用程序至关重要。请注意，这些join操作是由Spark自动执行的，以将不同的数据集连接在一起。您可以在代码中强制执行某些join类型，但Spark负责执行。
- en: Shuffle joins
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shuffle joins
- en: Shuffle joins are a common method for joining large datasets in distributed
    computing environments. These joins redistribute data across partitions, ensuring
    that matching keys end up on the same worker nodes. Spark performs shuffle joins
    efficiently thanks to its underlying execution engine.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle join是分布式计算环境中连接大型数据集的常用方法。这些join将数据重新分配到分区中，确保匹配的键最终位于同一worker节点上。Spark通过其底层执行引擎高效地执行shuffle
    join。
- en: 'Here are some of the key characteristics of shuffle joins:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是shuffle join的一些关键特性：
- en: '**Data redistribution**: Shuffle joins redistribute data to ensure that rows
    with matching keys are co-located on the same worker nodes. This process may require
    substantial network and disk I/O.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据重分布**: Shuffle join重新分配数据以确保具有匹配键的行位于同一worker节点上。此过程可能需要大量的网络和磁盘I/O。'
- en: '**Suitable for large datasets**: Shuffle joins are well-suited for joining
    large DataFrames with comparable sizes.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于大型数据集**: Shuffle join非常适合连接大小相当的大型DataFrame。'
- en: '**Replicating data**: During a shuffle join, data may be temporarily replicated
    on worker nodes to facilitate efficient joins.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据复制**: 在shuffle join过程中，数据可能会在worker节点上临时复制，以方便高效地执行join操作。'
- en: '**Costly in terms of network and disk I/O**: Shuffle joins can be resource-intensive
    due to data shuffling, making them slower compared to other join techniques for
    smaller datasets.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络和磁盘I/O成本高昂**: 由于数据shuffle，shuffle join可能非常消耗资源，这使得它们与其他join技术相比在处理较小数据集时速度较慢。'
- en: '**Examples**: Inner join, left join, right join, and full outer join are often
    implemented as shuffle joins.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**: 内连接、左连接、右连接和全外连接通常作为shuffle join实现。'
- en: Use case
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例
- en: Shuffle joins are typically used when joining two large DataFrames with no significant
    size difference.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle joins通常用于连接两个大小没有显著差异的大型DataFrame。
- en: Shuffle sort-merge joins
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shuffle sort-merge joins
- en: A shuffle sort-merge join is a type of shuffle join that leverages a combination
    of sorting and merging techniques to perform the join operation. It sorts both
    DataFrames based on the join key and then merges them efficiently.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle sort-merge join是一种shuffle join，它利用排序和合并技术的组合来执行join操作。它根据join键对两个DataFrame进行排序，然后高效地合并它们。
- en: 'Here are some of the key features of shuffle sort-merge joins:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是洗牌排序合并连接的一些关键特性：
- en: '**Data sorting**: Shuffle sort-merge joins sort the data on both sides to ensure
    efficient merging'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据排序**：洗牌排序合并连接在两边对数据进行排序，以确保有效的合并'
- en: '**Suitable for large datasets**: They are efficient for joining large DataFrames
    with skewed data distribution'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于大型数据集**：它们在连接具有倾斜数据分布的大型DataFrame时效率很高'
- en: '**Complexity**: This type of shuffle join is more complex than a simple shuffle
    join as it involves sorting operations'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：这种类型的洗牌连接比简单的洗牌连接更复杂，因为它涉及到排序操作'
- en: Use case
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**用例**'
- en: Shuffle sort-merge joins are effective for large-scale joins, especially when
    the data distribution is skewed, and a balanced distribution of data across partitions
    is essential.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌排序合并连接对于大规模连接非常有效，尤其是在数据分布不均匀时，确保数据在分区之间平衡分布至关重要。
- en: Let’s look at broadcast joins next.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看广播连接。
- en: Broadcast joins
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播连接
- en: Broadcast joins are a highly efficient technique for joining a small DataFrame
    with a larger one. In this approach, the smaller DataFrame is broadcast to all
    worker nodes, eliminating the need for shuffling data across the network. A broadcast
    join is a specific optimization technique that can be applied when one of the
    DataFrames is small enough to fit in memory. In this case, the small DataFrame
    is broadcast to all worker nodes, avoiding costly shuffling.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 广播连接是将小DataFrame与较大DataFrame连接的一种高度有效技术。在这种方法中，较小的DataFrame被广播到所有工作节点，消除了在网络中洗牌数据的需求。广播连接是一种特定的优化技术，可以在其中一个DataFrame足够小，可以放入内存时应用。在这种情况下，小DataFrame被广播到所有工作节点，避免了昂贵的洗牌。
- en: 'Let’s look at some of the key characteristics of broadcast joins:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看广播连接的一些关键特性：
- en: '**Small DataFrame broadcast**: The smaller DataFrame is broadcast to all worker
    nodes, ensuring that it is available locally'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小DataFrame广播**：较小的DataFrame被广播到所有工作节点，确保它可以在本地使用'
- en: '**Reduced network overhead**: Broadcast joins significantly reduce network
    and disk I/O because they avoid data shuffling'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少网络开销**：广播连接显著减少了网络和磁盘I/O，因为它们避免了数据洗牌'
- en: '**Ideal for dimension tables**: Broadcast joins are commonly used when joining
    a fact table with smaller dimension tables, such as in data warehousing scenarios'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于维度表**：广播连接在将事实表与较小的维度表连接时常用，例如在数据仓库场景中'
- en: '**Efficient for small-to-large joins**: They are efficient for joins where
    one DataFrame is significantly smaller than the other'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于小到大型连接**：它们在其中一个DataFrame远小于另一个DataFrame的连接操作中效率很高'
- en: Use case
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**用例**'
- en: Broadcast joins are useful when you’re joining a large DataFrame with a much
    smaller one, such as joining a fact table with dimension tables in a data warehouse.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将一个大型DataFrame与一个远小于它的DataFrame连接时，广播连接非常有用，例如在数据仓库中将事实表与维度表连接。
- en: Broadcast hash joins
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播哈希连接
- en: A specific type of broadcast join is the broadcast hash join. In this variant,
    the smaller DataFrame is broadcast as a hash table to all worker nodes, which
    allows for efficient lookups in the larger DataFrame.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 广播哈希连接是一种特定的广播连接。在这种情况下，较小的DataFrame被广播为一个哈希表到所有工作节点，这允许在大DataFrame中进行有效的查找。
- en: Use case
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**用例**'
- en: Broadcast hash joins are suitable for scenarios where one DataFrame is small
    enough to be broadcast, and you need to perform equality-based joins.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**广播哈希连接适用于以下场景**：其中一个DataFrame足够小，可以广播，并且需要执行基于等式的连接。'
- en: In this section, we discussed two fundamental join techniques in Spark – shuffle
    joins and broadcast joins – including specific variants, such as the broadcast
    hash join and the shuffle sort-merge join. Choosing the right join method depends
    on the size of your DataFrames, data distribution, and network considerations,
    and it’s essential to make informed decisions to optimize your Spark applications.
    In the next section, we will cover different types of transformations that exist
    in Spark.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了Spark中的两种基本连接技术——洗牌连接和广播连接——包括特定的变体，如广播哈希连接和洗牌排序合并连接。选择正确的连接方法取决于你的DataFrame的大小、数据分布和网络考虑因素，并且做出明智的决定对于优化你的Spark应用程序至关重要。在下一节中，我们将介绍Spark中存在的不同类型的转换。
- en: Narrow and wide transformations in Apache Spark
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的窄和宽转换
- en: 'As discussed in [*Chapter 3*](B19176_03.xhtml#_idTextAnchor053), transformations
    are the core operations for processing data. Transformations are categorized into
    two main types: narrow transformations and wide transformations. Understanding
    the distinction between these two types of transformations is essential for optimizing
    the performance of your Spark applications.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第3章*](B19176_03.xhtml#_idTextAnchor053)所述，变换是处理数据的核心操作。变换分为两大类：窄变换和宽变换。理解这两种变换之间的区别对于优化Spark应用程序的性能至关重要。
- en: Narrow transformations
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窄变换
- en: Narrow transformations are operations that do not require data shuffling or
    extensive data movement across partitions. They can be executed on a single partition
    without the need to communicate with other partitions. This inherent locality
    makes narrow transformations highly efficient and faster to execute.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 窄变换是一种不需要数据洗牌或跨分区进行大量数据移动的操作。它们可以在单个分区上执行，无需与其他分区通信。这种固有的局部性使得窄变换非常高效且执行速度快。
- en: 'The following are some of the key characteristics of narrow transformations:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些窄变换的关键特性：
- en: '**Single-partition processing**: Narrow transformations operate on a single
    partition of the data independently, which minimizes communication overhead.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单分区处理**：窄变换独立地对数据的单个分区进行操作，这最小化了通信开销。'
- en: '**Speed and efficiency**: Due to their partition-wise nature, narrow transformations
    are fast and efficient.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度和效率**：由于它们的分区特性，窄变换既快又高效。'
- en: '`map()`, `filter()`, `union()`, and `groupBy()` are typical examples of narrow
    transformations.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`、`filter()`、`union()` 和 `groupBy()` 是窄变换的典型例子。'
- en: Wide transformations
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宽变换
- en: Wide transformations, in contrast, involve data shuffling, which necessitates
    the exchange of data between partitions. These transformations require communication
    between multiple partitions and can be resource-intensive. As a result, they tend
    to be slower and more costly in terms of computation.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，宽变换涉及数据洗牌，这需要分区之间的数据交换。这些变换需要多个分区之间的通信，并且可能非常资源密集。因此，它们通常执行速度较慢，在计算方面成本更高。
- en: 'Here are a few of the key characteristics of wide transformations:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是宽变换的一些关键特性：
- en: '**Data shuffling**: Wide transformations involve the reorganization of data
    across partitions, requiring data exchange between different workers.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据洗牌**：宽变换涉及跨分区重新组织数据，需要不同工作者之间的数据交换。'
- en: '**Slower execution**: Due to the need for shuffling, wide transformations are
    relatively slower and resource-intensive compared to narrow transformations.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行速度较慢**：由于需要洗牌，宽变换相对于窄变换来说，执行速度较慢且资源密集。'
- en: '`groupByKey()`, `reduceByKey()`, and `join()` are common examples of wide transformations.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey()`、`reduceByKey()` 和 `join()` 是宽变换的常见例子。'
- en: Let’s discuss which transformation works best, depending on the operation.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论根据操作选择哪种变换效果最好。
- en: Choosing between narrow and wide transformations
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择窄变换和宽变换
- en: 'Selecting the appropriate type of transformation depends on the specific use
    case and the data at hand. Here are some considerations for choosing between narrow
    and wide transformations:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的变换类型取决于具体用例和现有数据。以下是选择窄变换和宽变换时的一些考虑因素：
- en: '**Data size**: If your data is small enough to fit comfortably within a single
    partition, it’s preferable to use narrow transformations. This minimizes the overhead
    associated with shuffling.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小**：如果你的数据足够小，可以舒适地放入单个分区，那么使用窄变换是首选。这可以最小化与洗牌相关的开销。'
- en: '**Data distribution**: If your data is distributed unevenly across partitions,
    wide transformations might be necessary to reorganize and balance the data.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：如果你的数据在分区之间分布不均匀，可能需要宽变换来重新组织和平衡数据。'
- en: '**Performance**: Narrow transformations are typically faster and more efficient,
    so if performance is a critical concern, they are preferred.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：窄变换通常更快、更高效，因此如果性能是关键关注点，则更倾向于选择它们。'
- en: '**Complex operations**: Some operations, such as joining large DataFrames,
    often require wide transformations. In such cases, the performance trade-off is
    inevitable.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂操作**：一些操作，例如连接大型DataFrame，通常需要宽变换。在这种情况下，性能权衡是不可避免的。'
- en: '**Cluster resources**: Consider the available cluster resources. Resource-intensive
    wide transformations may lead to resource contention in a shared cluster.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群资源**：考虑可用的集群资源。资源密集型的广泛转换可能导致共享集群中的资源争用。'
- en: Next, we’ll learn how to optimize wide transformations in cases where it is
    necessary to implement them.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何在必要时优化广泛转换。
- en: Optimizing wide transformations
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化广泛转换
- en: 'While wide transformations are necessary for certain operations, it’s crucial
    to optimize them to reduce their impact on performance. Here are some strategies
    for optimizing wide transformations:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于某些操作来说，广泛的转换是必要的，但优化它们以减少对性能的影响至关重要。以下是一些优化广泛转换的策略：
- en: '**Minimize data shuffling**: Whenever possible, use techniques to minimize
    data shuffling. For example, consider using broadcast joins for small DataFrames.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化数据洗牌**：尽可能使用技术来最小化数据洗牌。例如，考虑使用广播连接来处理小的数据框。'
- en: '**Partitioning**: Carefully choose the number of partitions and partitioning
    keys to ensure even data distribution, reducing the need for extensive shuffling.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区**：仔细选择分区数量和分区键以确保数据均匀分布，减少大量洗牌的需求。'
- en: '**Caching and persistence**: Caching frequently used DataFrames can help reduce
    the need for recomputation and shuffling in subsequent stages.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存和持久化**：缓存频繁使用的 DataFrame 可以帮助减少后续阶段中重新计算和洗牌的需求。'
- en: '**Tuning cluster resources**: Adjust cluster configurations, such as the number
    of executors and memory allocation, to meet the demands of wide transformations.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整集群资源**：调整集群配置，例如执行器和内存分配的数量，以满足广泛转换的需求。'
- en: '**Profiling and monitoring**: Regularly profile and monitor your Spark applications
    to identify performance bottlenecks, especially in the case of wide transformations.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析和监控**：定期分析和监控您的 Spark 应用程序以识别性能瓶颈，尤其是在广泛转换的情况下。'
- en: In this section, we explored the concepts of narrow and wide transformations
    in Apache Spark. Understanding when and how to use these transformations is critical
    for optimizing the performance of your Spark applications, especially when dealing
    with large datasets and complex operations.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 Apache Spark 中窄转换和广泛转换的概念。理解何时以及如何使用这些转换对于优化 Spark 应用程序的性能至关重要，尤其是在处理大型数据集和复杂操作时。
- en: In the next section, we will cover the persist and cache operations in Spark.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍 Spark 中的持久化和缓存操作。
- en: Persisting and caching in Apache Spark
  id: totrans-434
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 中的持久化和缓存
- en: In Apache Spark, optimizing the performance of your data processing operations
    is essential, especially when working with large datasets and complex workflows.
    Caching and persistence are techniques that allow you to store intermediate or
    frequently used data in memory or on disk, reducing the need for recomputation
    and enhancing overall performance. This section explores the concepts of persisting
    and caching in Spark.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中，优化数据处理操作的性能至关重要，尤其是在处理大型数据集和复杂工作流时。缓存和持久化是允许您在内存或磁盘上存储中间或频繁使用数据的技术，从而减少重新计算的需求并提高整体性能。本节探讨了
    Spark 中持久化和缓存的概念。
- en: Understanding data persistence
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据持久化
- en: Data persistence is the process of storing the intermediate or final results
    of Spark transformations in memory or on disk. By persisting data, you reduce
    the need to recompute it from the source data, thereby improving query performance.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 数据持久化是将 Spark 转换的中间或最终结果存储在内存或磁盘上的过程。通过持久化数据，您可以减少从源数据重新计算的需求，从而提高查询性能。
- en: 'The following key concepts are related to data persistence:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 以下关键概念与数据持久化相关：
- en: '**Storage levels**: Spark offers multiple storage levels for data, ranging
    from memory-only to disk, depending on your needs. Each storage level comes with
    its trade-offs in terms of speed and durability.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储级别**：Spark 提供多种数据存储级别，从仅内存到磁盘，根据您的需求而定。每个存储级别都带来了速度和持久性方面的权衡。'
- en: '**Lazy evaluation**: Spark follows a lazy evaluation model, meaning transformations
    are not executed until an action is called. Data persistence ensures that the
    intermediate results are available for reuse without recomputation.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟评估**：Spark 遵循延迟评估模型，这意味着转换只有在调用操作时才会执行。数据持久化确保中间结果可用于重用，而无需重新计算。'
- en: '**Caching versus persistence**: Caching is a specific form of data persistence
    that stores data in memory, while persistence encompasses both in-memory and on-disk
    storage.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存与持久化**：缓存是一种特定形式的数据持久化，它将数据存储在内存中，而持久化则包括内存和磁盘上的存储。'
- en: Caching data
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存数据
- en: Caching is a form of data persistence that stores DataFrames, RDDs, or datasets
    in memory for fast access. It is an essential optimization technique that improves
    the performance of Spark applications, particularly when dealing with iterative
    algorithms or repeated computations.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是一种数据持久化形式，它将DataFrame、RDD或数据集存储在内存中以实现快速访问。它是一种重要的优化技术，可以提高Spark应用程序的性能，尤其是在处理迭代算法或重复计算时。
- en: 'To cache a DataFrame or an RDD, you can use the `.cache()` or `.persist()`
    method while specifying the storage level:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 要缓存DataFrame或RDD，您可以在指定存储级别时使用`.cache()`或`.persist()`方法：
- en: '`.cache()` or `.persist(StorageLevel.MEMORY_ONLY)`.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.cache()`或`.persist(StorageLevel.MEMORY_ONLY)`.'
- en: '`.persist(StorageLevel.MEMORY_ONLY_SER)`.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.persist(StorageLevel.MEMORY_ONLY_SER)`.'
- en: '`.persist(StorageLevel.MEMORY_AND_DISK)`.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.persist(StorageLevel.MEMORY_AND_DISK)`.'
- en: '`.persist(StorageLevel.DISK_ONLY)`.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.persist(StorageLevel.DISK_ONLY)`.'
- en: 'Caching is particularly beneficial in the following scenarios:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存在以下场景中特别有益：
- en: '**Iterative algorithms**: Caching is vital for iterative algorithms such as
    machine learning, graph processing, and optimization problems, where the same
    data is used repeatedly'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代算法**：缓存对于迭代算法至关重要，如机器学习、图处理和优化问题，在这些算法中，相同的数据被反复使用'
- en: '**Multiple actions**: When a DataFrame is used for multiple actions, caching
    it after the first action can improve performance'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个操作**：当DataFrame用于多个操作时，在第一次操作后缓存它可以提高性能'
- en: '**Avoiding recomputation**: Caching helps avoid recomputing the same data when
    multiple transformations depend on it'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免重复计算**：缓存有助于避免在多个转换依赖于它时重复计算相同的数据'
- en: '**Interactive queries**: In interactive data exploration or querying, caching
    frequently used intermediate results can speed up ad hoc analysis'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式查询**：在交互式数据探索或查询中，缓存常用的中间结果可以加快临时分析'
- en: Unpersisting data
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 取消持久化数据
- en: Caching consumes memory, and in a cluster environment, it’s essential to manage
    memory efficiently. You can release cached data from memory using the `.unpersist()`
    method. This method allows you to specify whether to release the data immediately
    or only when it is no longer needed.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存会消耗内存，在集群环境中，高效管理内存至关重要。您可以使用`.unpersist()`方法从内存中释放缓存数据。此方法允许您指定是立即释放数据还是仅在不再需要时释放。
- en: 'Here’s an example of unpersisting data:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个取消持久化数据的示例：
- en: '[PRE38]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Best practices
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践
- en: 'To use caching and persistence effectively in your Spark applications, consider
    the following best practices:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的Spark应用程序中有效地使用缓存和持久化，请考虑以下最佳实践：
- en: '**Cache only what’s necessary**: Caching consumes memory, so cache only the
    data that is frequently used or costly to compute'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅缓存必要的数据**：缓存会消耗内存，因此仅缓存频繁使用或计算成本高的数据'
- en: '**Monitor memory usage**: Regularly monitor memory usage to avoid running out
    of memory or excessive disk spills'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控内存使用**：定期监控内存使用情况，以避免内存耗尽或磁盘溢出'
- en: '**Automate unpersistence**: If you have limited memory resources, automate
    the unpersistence of less frequently used data to free up memory for more critical
    operations'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化取消持久化**：如果您有有限的内存资源，请自动化较少使用的数据的取消持久化，以释放内存用于更关键的操作'
- en: '**Consider serialization**: Depending on your use case, consider using serialized
    storage levels to reduce memory overhead'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑序列化**：根据您的用例，考虑使用序列化存储级别以减少内存开销'
- en: In this section, we explored the concepts of persistence and caching in Apache
    Spark. Caching and persistence are powerful techniques for optimizing performance
    in Spark applications, particularly when dealing with iterative algorithms or
    scenarios where the same data is used repeatedly. Understanding when and how to
    use these techniques can significantly improve the efficiency of your data processing
    workflows.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Apache Spark中持久化和缓存的概念。缓存和持久化是优化Spark应用程序性能的强大技术，尤其是在处理迭代算法或重复使用相同数据的场景中。了解何时以及如何使用这些技术可以显著提高您数据处理工作流程的效率。
- en: In the next section, we’ll learn how repartition and coalesce work in Spark.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何在Spark中实现分区和合并。
- en: Repartitioning and coalescing in Apache Spark
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的分区和合并
- en: Efficient data partitioning plays a crucial role in optimizing data processing
    workflows in Apache Spark. Repartitioning and coalescing are operations that allow
    you to control the distribution of data across partitions. In this section, we’ll
    explore the concepts of repartitioning and coalescing and their significance in
    Spark applications.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark中优化数据处理工作流程时，高效的数据分区起着至关重要的作用。重新分区和合并是允许您控制数据在分区间分布的操作。在本节中，我们将探讨重新分区和合并的概念及其在Spark应用程序中的重要性。
- en: Understanding data partitioning
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据分区
- en: Data partitioning in Apache Spark involves dividing a dataset into smaller,
    manageable units called partitions. Each partition contains a subset of the data
    and is processed independently by different worker nodes in a distributed cluster.
    Proper data partitioning can significantly impact the efficiency and performance
    of Spark applications.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark中的数据分区涉及将数据集划分为更小、更易于管理的单元，称为分区。每个分区包含数据的一个子集，并由分布式集群中不同的工作节点独立处理。适当的数据分区可以显著影响Spark应用程序的效率和性能。
- en: Repartitioning data
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新分区数据
- en: Repartitioning is the process of redistributing data across a different number
    of partitions. This operation can help balance data distribution, improve parallelism,
    and optimize data processing. You can use the `.repartition()` method to specify
    the number of desired partitions.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 重新分区是将数据重新分配到不同数量的分区中的过程。这个操作可以帮助平衡数据分布，提高并行性，并优化数据处理。您可以使用`.repartition()`方法来指定所需的分区数。
- en: 'Here are some key points related to repartitioning data:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是关于重新分区数据的一些关键点：
- en: '**Increasing or decreasing partitions**: Repartitioning allows you to increase
    or decrease the number of partitions to suit your processing needs.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加或减少分区**：重新分区允许您根据处理需求增加或减少分区数。'
- en: '**Data shuffling**: Repartitioning often involves data shuffling, which can
    be resource-intensive. Therefore, it should be used judiciously.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据洗牌**：重新分区通常涉及数据洗牌，这可能非常消耗资源。因此，应谨慎使用。'
- en: '**Even data distribution**: Repartitioning is useful when the original data
    is unevenly distributed across partitions, causing skewed workloads.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均匀数据分布**：当原始数据在分区中分布不均匀，导致工作负载倾斜时，重新分区很有用。'
- en: '**Optimizing for joins**: Repartitioning can be beneficial when performing
    joins to minimize data shuffling.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化连接操作**：在执行连接操作时，重新分区可以有益于最小化数据洗牌。'
- en: 'Here’s an example of repartitioning data:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是重新分区数据的示例：
- en: '[PRE39]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Coalescing data
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并数据
- en: Coalescing is the process of reducing the number of partitions while preserving
    data locality. It is a more efficient operation than repartitioning because it
    avoids unnecessary data shuffling whenever possible. You can use the `.coalesce()`
    method to specify the target number of partitions.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 合并是在保持数据局部性的同时减少分区数量的过程。它比重新分区更高效，因为它尽可能避免不必要的洗牌操作。您可以使用`.coalesce()`方法来指定目标分区数。
- en: 'Here are some key points related to coalescing data:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是关于合并数据的一些关键点：
- en: '**Decreasing partitions**: Coalescing is used when you want to decrease the
    number of partitions to optimize data processing'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少分区**：当您想减少分区数量以优化数据处理时，使用合并。'
- en: '**Minimizing data movement**: Unlike repartitioning, coalescing minimizes data
    shuffling by merging partitions locally whenever possible'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化数据移动**：与重新分区不同，合并通过尽可能在本地合并分区来最小化洗牌操作。'
- en: '**Efficient for data reduction**: Coalescing is efficient when you need to
    reduce the number of partitions without incurring the full cost of data shuffling'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的数据减少**：当您需要减少分区数量而不承担数据洗牌的全部成本时，合并操作是高效的。'
- en: 'Here’s an example of coalescing data:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是合并数据的示例：
- en: '[PRE40]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Use cases for repartitioning and coalescing
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新分区和合并的使用案例
- en: Understanding when to repartition and coalesce is critical for optimizing your
    Spark applications.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 理解何时重新分区和合并对于优化您的Spark应用程序至关重要。
- en: 'The following are some use cases for repartitioning:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些重新分区的用例：
- en: '**Data skew**: When data is skewed across partitions, repartitioning can balance
    the workload'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据倾斜**：当数据在分区中倾斜时，重新分区可以帮助平衡工作负载。'
- en: '**Join optimization**: For optimizing join operations by ensuring that the
    joining keys are collocated'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接优化**：通过确保连接键是本地化的来优化连接操作。'
- en: '**Parallelism control**: Adjusting the level of parallelism to optimize resource
    utilization'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行度控制**：调整并行度以优化资源利用率。'
- en: 'Now, let’s look at some use cases for coalescing:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看合并的一些用例：
- en: '**Reducing data**: When you need to reduce the number of partitions to save
    memory and reduce overhead'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少数据**: 当你需要减少分区数量以节省内存和降低开销时'
- en: '**Minimizing shuffling**: To avoid unnecessary data shuffling and minimize
    network communication'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化洗牌**: 为了避免不必要的洗牌并最小化网络通信'
- en: '**Post-filtering**: After applying a filter or transformation that significantly
    reduces the dataset size'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后过滤**: 在应用过滤器或转换显著减少数据集大小之后'
- en: Best practices
  id: totrans-497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践
- en: 'To repartition and coalesce effectively in your Spark applications, consider
    these best practices:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Spark应用程序中有效地重新分区和合并，请考虑以下最佳实践：
- en: '**Profile and monitor**: Profile your application to identify performance bottlenecks
    related to data partitioning. Use Spark’s UI and monitoring tools to track data
    shuffling.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析和监控**: 分析你的应用程序以识别与数据分区相关的性能瓶颈。使用Spark的UI和监控工具跟踪数据洗牌。'
- en: '**Consider data size**: Consider the size of your dataset and the available
    cluster resources when deciding on the number of partitions.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑数据大小**: 在决定分区数量时，考虑你的数据集大小和可用的集群资源。'
- en: '**Balance workloads**: Aim for a balanced workload distribution across partitions
    to optimize parallelism.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡工作负载**: 旨在使分区之间的工作负载分布平衡，以优化并行性。'
- en: '**Coalesce where possible**: When reducing the number of partitions, prefer
    coalescing over repartitioning to minimize data shuffling.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尽可能合并**: 在减少分区数量时，优先选择合并而不是重新分区，以最小化数据洗牌。'
- en: '**Plan for joins**: When performing joins, plan for the optimal number of partitions
    to minimize shuffle overhead.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划连接**: 在执行连接操作时，规划最佳分区数量以最小化洗牌开销。'
- en: In this section, we explored the concepts of repartitioning and coalescing in
    Apache Spark. Understanding how to efficiently control data partitioning can significantly
    impact the performance of your Spark applications, especially when you’re working
    with large datasets and complex operations.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Apache Spark中的重新分区和合并的概念。了解如何有效地控制数据分区可以显著影响Spark应用程序的性能，尤其是在处理大型数据集和复杂操作时。
- en: Summary
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into advanced data processing capabilities in Apache
    Spark, enhancing your understanding of key concepts and techniques. We explored
    the intricacies of Spark’s Catalyst optimizer, the power of different types of
    Spark joins, the importance of data persistence and caching, the significance
    of narrow and wide transformations, and the role of data partitioning using repartition
    and coalesce. Additionally, we discovered the versatility and utility of UDFs.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了Apache Spark的高级数据处理功能，增强了你对关键概念和技术理解。我们探讨了Spark的Catalyst优化器的复杂性，不同类型Spark连接的力量，数据持久化和缓存的重要性，窄转换和宽转换的意义，以及使用重新分区和合并进行数据分区的作用。此外，我们还发现了UDFs的灵活性和实用性。
- en: As you advance in your journey with Apache Spark, these advanced capabilities
    will prove invaluable for optimizing and customizing your data processing workflows.
    By harnessing the potential of the Catalyst optimizer, you can fine-tune query
    execution for improved performance. Understanding the nuances of Spark joins empowers
    you to make informed decisions on which type of join to employ for specific use
    cases. Data persistence and caching become indispensable when you seek to reduce
    recomputation and expedite iterative processes.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你使用Apache Spark的旅程不断深入，这些高级功能将证明对优化和定制数据处理工作流程非常有价值。通过利用Catalyst优化器的潜力，你可以微调查询执行以改善性能。了解Spark连接的细微差别使你能够针对特定用例做出明智的决定选择哪种类型的连接。当你寻求减少重复计算和加速迭代过程时，数据持久化和缓存变得不可或缺。
- en: Narrow and wide transformations play a pivotal role in achieving the desired
    parallelism and resource efficiency in Spark applications. Proper data partitioning
    through repartition and coalesce ensures balanced workloads and optimal data distribution.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 窄转换和宽转换在Spark应用程序中实现所需的并行性和资源效率中起着关键作用。通过重新分区和合并进行适当的数据分区确保了平衡的工作负载和最佳的数据分布。
- en: UDFs open the door to limitless possibilities, enabling you to implement custom
    data processing logic, from data cleansing and feature engineering to complex
    calculations and domain-specific operations. However, it is crucial to use UDFs
    judiciously, optimizing them for performance and adhering to best practices.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs（用户定义函数）打开了无限可能的大门，使你能够实现自定义数据处理逻辑，从数据清洗和特征工程到复杂计算和特定领域的操作。然而，明智地使用UDFs至关重要，优化它们以获得性能，并遵循最佳实践。
- en: With this chapter’s knowledge, you are better equipped to tackle complex data
    processing challenges in Apache Spark, enabling you to extract valuable insights
    from your data efficiently and effectively. These advanced capabilities empower
    you to leverage the full potential of Spark and achieve optimal performance in
    your data-driven endeavors.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的知识，你将更好地应对Apache Spark中的复杂数据处理挑战，使你能够高效且有效地从数据中提取有价值的见解。这些高级功能使你能够充分利用Spark的潜力，并在数据驱动的努力中实现最佳性能。
- en: In the next chapter, we will be introduced to SparkSQL and will learn how to
    create and manipulate SQL queries in Spark.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍SparkSQL，并学习如何在Spark中创建和操作SQL查询。
- en: Sample questions
  id: totrans-512
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样题问题
- en: '**Question 1:**'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1:**'
- en: Which of the following code blocks returns a DataFrame showing the mean of the
    `salary` column of the `df` DataFrame, grouped by the `department` column?
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个DataFrame，显示了按`department`列分组的`df` DataFrame中`salary`列的平均值？
- en: '`df.groupBy("department").agg(avg("salary"))`'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(avg("salary"))`'
- en: '`df.groupBy(col(department).avg())`'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy(col(department).avg())`'
- en: '`df.groupBy("department").avg(col("salary"))`'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").avg(col("salary"))`'
- en: '`df.groupBy("department").agg(average("salary"))`'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(average("salary"))`'
- en: '**Question 2:**'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2:**'
- en: Which of the following code blocks returns unique values across all values in
    the `state` and `department` columns in `df`?
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了`df`中`state`和`department`列的所有唯一值？
- en: '`df.select(state).join(transactionsDf.select(''department''),` `col(state)==col(''department''),
    ''outer'').show()`'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(state).join(transactionsDf.select(''department''), col(state) ==
    col(''department''), ''outer'').show()`'
- en: '`df.select(col(''state''),` `col(''department'')).agg({''*'': ''count''}).show()`'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(col(''state''), col(''department'')).agg({''*'': ''count''}).show()`'
- en: '`df.select(''state'', ''department'').distinct().show()`'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''state'', ''department'').distinct().show()`'
- en: '`df.select(''state'').union(df.select(''department'')).distinct().show()`'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''state'').union(df.select(''department'')).distinct().show()`'
- en: Answers
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: A
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
