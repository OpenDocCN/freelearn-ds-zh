<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 6. Introducing the ML Package"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Introducing the ML Package</h1></div></div></div><p>In the previous chapter, we worked with the MLlib package in Spark that operated strictly on RDDs. In this chapter, we move to the ML part of Spark that operates strictly on DataFrames. Also, according to the Spark documentation, the primary machine learning API for Spark is now the DataFrame-based set of models contained in the <code class="literal">spark.ml</code> package.</p><p>So, let's get to it!</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>In this <a id="id290" class="indexterm"/>chapter, we will reuse a portion of the dataset we played within the previous chapter. The data can be downloaded from <a class="ulink" href="http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz">http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz</a>.</p></div></div><p>In this chapter, you will learn how to do the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Prepare transformers, estimators, and pipelines</li><li class="listitem" style="list-style-type: disc">Predict the chances of infant survival using models available in the ML package</li><li class="listitem" style="list-style-type: disc">Evaluate the performance of the model</li><li class="listitem" style="list-style-type: disc">Perform parameter hyper-tuning</li><li class="listitem" style="list-style-type: disc">Use other machine-learning models available in the package</li></ul></div><div class="section" title="Overview of the package"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/>Overview of the package</h1></div></div></div><p>At the top<a id="id291" class="indexterm"/> level, the package exposes three main abstract classes: a <code class="literal">Transformer</code>, an <code class="literal">Estimator</code>, and a <code class="literal">Pipeline</code>. We will shortly explain each with some short examples. We will provide more concrete examples of some of the models in the last section of this chapter.</p><div class="section" title="Transformer"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec62"/>Transformer</h2></div></div></div><p>The <code class="literal">Transformer</code> class, like the name suggests, <span class="emphasis"><em>transforms</em></span> your data by (normally) appending a <a id="id292" class="indexterm"/>new column to your DataFrame.</p><p>At the high level, when <a id="id293" class="indexterm"/>deriving from the <code class="literal">Transformer</code> abstract class, each and every new <code class="literal">Transformer</code> needs to implement a <code class="literal">.transform(...)</code> method. The method, as a first and normally the only obligatory parameter, requires passing a DataFrame to be transformed. This, of course, varies <span class="emphasis"><em>method-by-method</em></span> in the ML package: other <span class="emphasis"><em>popular</em></span> parameters are <code class="literal">inputCol</code> and <code class="literal">outputCol</code>; these, however, frequently default to some predefined values, such as, for example, <code class="literal">'features'</code> for the <code class="literal">inputCol</code> parameter.</p><p>There are many <code class="literal">Transformers</code> offered in the <code class="literal">spark.ml.feature </code>and we will briefly describe them here (before we use some of them later in this chapter):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Binarizer</code>: Given<a id="id294" class="indexterm"/> a threshold, the method takes a continuous variable and transforms it into a binary one.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Bucketizer</code>: Similar<a id="id295" class="indexterm"/> to the <code class="literal">Binarizer</code>, this method takes a list of thresholds (the <code class="literal">splits</code> parameter) and transforms a continuous variable into a multinomial one.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ChiSqSelector</code>: For <a id="id296" class="indexterm"/>the categorical target variables (think classification models), this feature allows you to select a predefined number of features (parameterized by the <code class="literal">numTopFeatures</code> parameter) that explain the variance in the target the best. The selection is done, as the name of the method suggests, using a Chi-Square test. It is one of the two-step methods: first, you need to <code class="literal">.fit(...)</code> your data (so the method can calculate the Chi-square tests). Calling the <code class="literal">.fit(...)</code> method (you pass your DataFrame as a parameter) returns a <code class="literal">ChiSqSelectorModel</code> object that you can then use to transform your DataFrame using the <code class="literal">.transform(...)</code> method.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note48"/>Note</h3><p>More information<a id="id297" class="indexterm"/> on Chi-squares can be found here: <a class="ulink" href="http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html">http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html</a>.</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">CountVectorizer</code>: This is useful <a id="id298" class="indexterm"/>for a tokenized text (such as <code class="literal">[['Learning', 'PySpark', 'with', 'us'],['us', 'us', 'us']]</code>). It is one of two-step methods: first, you need to <code class="literal">.fit(...)</code>, that is, learn the patterns from your dataset, before you can <code class="literal">.transform(...) </code>with the <code class="literal">CountVectorizerModel</code> returned by the <code class="literal">.fit(...) </code>method. The output from this transformer, for the tokenized text presented <a id="id299" class="indexterm"/>previously, would look similar to this: <code class="literal">[(4, [0, 1, 2, 3], [1.0, 1.0, 1.0, 1.0]),(4, [3], [3.0])]</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">DCT</code>: The Discrete Cosine Transform takes a vector of real values and returns a vector of the same length, but with the sum of cosine functions oscillating at different frequencies. Such transformations are useful to extract some underlying <a id="id300" class="indexterm"/>frequencies in your data or in data <a id="id301" class="indexterm"/>compression.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ElementwiseProduct</code>: A method that returns a vector with elements that are products <a id="id302" class="indexterm"/>of the vector passed to the method, and a vector passed as the <code class="literal">scalingVec</code> parameter. For example, if you had a <code class="literal">[10.0, 3.0, 15.0]</code> vector and your <code class="literal">scalingVec</code> was <code class="literal">[0.99, 3.30, 0.66]</code>, then the vector you would get would look as follows: <code class="literal">[9.9, 9.9, 9.9]</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HashingTF</code>: A <a id="id303" class="indexterm"/>hashing trick transformer that takes a list of tokenized text and returns a vector (of predefined length) with counts. From PySpark's documentation:<div class="blockquote"><blockquote class="blockquote"><p>"Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns."</p></blockquote></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">IDF</code>: This <a id="id304" class="indexterm"/>method computes an <span class="strong"><strong>Inverse Document Frequency</strong></span> for a list of documents. Note that the documents need to already <a id="id305" class="indexterm"/>be represented as a vector (for example, using either the <code class="literal">HashingTF </code>or <code class="literal">CountVectorizer</code>).</li><li class="listitem" style="list-style-type: disc"><code class="literal">IndexToString</code>: A complement to the <code class="literal">StringIndexer</code> method. It uses the encoding<a id="id306" class="indexterm"/> from the <code class="literal">StringIndexerModel</code> object to reverse the string index to original values. As an aside, please note that this sometimes does not work and you need to specify the values from the <code class="literal">StringIndexer</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">MaxAbsScaler</code>: Rescales the data to be within the <code class="literal">[-1.0, 1.0]</code> range (thus, it does<a id="id307" class="indexterm"/> not shift the center of the data).</li><li class="listitem" style="list-style-type: disc"><code class="literal">MinMaxScaler</code>: This is similar to the <code class="literal">MaxAbsScaler</code> with the difference that it scales <a id="id308" class="indexterm"/>the data to be in the <code class="literal">[0.0, 1.0]</code> range.</li><li class="listitem" style="list-style-type: disc"><code class="literal">NGram</code>: This <a id="id309" class="indexterm"/>method takes a list of tokenized text and returns <span class="emphasis"><em>n-grams</em></span>: pairs, triples, or <span class="emphasis"><em>n-mores</em></span> of subsequent words. For example, if you had a <code class="literal">['good', 'morning', 'Robin', 'Williams']</code> vector you would get the following output: <code class="literal">['good morning', 'morning Robin', 'Robin Williams']</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Normalizer</code>: This<a id="id310" class="indexterm"/> method scales the data to be of unit norm using the p-norm value (by default, it is L2).</li><li class="listitem" style="list-style-type: disc"><code class="literal">OneHotEncoder</code>: This <a id="id311" class="indexterm"/>method encodes a categorical column to a column of binary vectors.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PCA</code>: Performs<a id="id312" class="indexterm"/> the data reduction using principal component analysis.</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">PolynomialExpansion</code>: Performs a polynomial expansion of a vector. For example, if<a id="id313" class="indexterm"/> you had a vector symbolically written as <code class="literal">[x, y, z]</code>, the method would produce the following expansion: <code class="literal">[x, x*x, y, x*y, y*y, z, x*z, y*z, z*z]</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">QuantileDiscretizer</code>: Similar to the <code class="literal">Bucketizer</code> method, but instead of passing the <a id="id314" class="indexterm"/>splits parameter, you pass the <code class="literal">numBuckets</code> one. The method then decides, by calculating approximate quantiles <a id="id315" class="indexterm"/>over your data, what the splits should be.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RegexTokenizer</code>: This is a string <a id="id316" class="indexterm"/>tokenizer using regular expressions.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RFormula</code>: For<a id="id317" class="indexterm"/> those of you who are avid R users, you can pass a formula such as <code class="literal">vec ~ alpha * 3 + beta</code> (assuming your <code class="literal">DataFrame</code> has the <code class="literal">alpha</code> and <code class="literal">beta</code> columns) and it will produce the <code class="literal">vec</code> column given the expression.</li><li class="listitem" style="list-style-type: disc"><code class="literal">SQLTransformer</code>: Similar<a id="id318" class="indexterm"/> to the previous, but instead of R-like formulas, you can use SQL syntax.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip25"/>Tip</h3><p>The <code class="literal">FROM</code> statement should be selecting from <code class="literal">__THIS__</code>, indicating you are accessing the DataFrame. For example: <code class="literal">SELECT alpha * 3 + beta AS vec FROM __THIS__</code>.</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">StandardScaler</code>: Standardizes <a id="id319" class="indexterm"/>the column to have a 0 mean and standard deviation equal to 1.</li><li class="listitem" style="list-style-type: disc"><code class="literal">StopWordsRemover</code>: Removes <a id="id320" class="indexterm"/>stop words (such as <code class="literal">'the'</code> or <code class="literal">'a'</code>) from a tokenized text.</li><li class="listitem" style="list-style-type: disc"><code class="literal">StringIndexer</code>: Given <a id="id321" class="indexterm"/>a list of all the words in a column, this will produce a vector of indices.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Tokenizer</code>: This is the default<a id="id322" class="indexterm"/> tokenizer that converts the string to lower case and then splits on space(s).</li><li class="listitem" style="list-style-type: disc"><code class="literal">VectorAssembler</code>: This is a<a id="id323" class="indexterm"/> highly useful transformer that collates multiple numeric (vectors included) columns into a single column with a vector representation. For<a id="id324" class="indexterm"/> example, if you had three columns in your DataFrame:<div class="informalexample"><pre class="programlisting">df = spark.createDataFrame(
    [(12, 10, 3), (1, 4, 2)], 
    ['a', 'b', 'c']) </pre></div><p>The output of calling:</p><div class="informalexample"><pre class="programlisting">ft.VectorAssembler(inputCols=['a', 'b', 'c'], 
        outputCol='features')\
    .transform(df) \
    .select('features')\
    .collect() </pre></div><p>It would look as follows:</p><div class="informalexample"><pre class="programlisting">[Row(features=DenseVector([12.0, 10.0, 3.0])), 
 Row(features=DenseVector([1.0, 4.0, 2.0]))]</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">VectorIndexer</code>: This is a method for indexing categorical columns into a vector of indices. It <a id="id325" class="indexterm"/>works in a <span class="emphasis"><em>column-by-column</em></span> fashion, selecting distinct values from the column, sorting and returning an index of the value from the map instead of the original value.</li><li class="listitem" style="list-style-type: disc"><code class="literal">VectorSlicer</code>: Works on a feature vector, either dense or sparse: given a list of indices,<a id="id326" class="indexterm"/> it extracts the values from the feature vector.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Word2Vec</code>: This<a id="id327" class="indexterm"/> method takes a sentence (string) as an input and transforms it into a map of <code class="literal">{string, vector}</code> format, a representation that is useful in natural language processing.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note49"/>Note</h3><p>Note that there are many methods in the ML package that have an E letter next to it; this means the method is currently in beta (or Experimental) and it sometimes might fail or produce erroneous results. Beware.</p></div></div></li></ul></div></div><div class="section" title="Estimators"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec63"/>Estimators</h2></div></div></div><p>Estimators<a id="id328" class="indexterm"/> can be thought of as statistical models that need to be estimated to make predictions or classify your observations.</p><p>If deriving from the abstract <code class="literal">Estimator</code> class, the new model has to implement the <code class="literal">.fit(...)</code> method <a id="id329" class="indexterm"/>that fits the model given the data found in a DataFrame and some default or user-specified parameters.</p><p>There are a lot of estimators available in PySpark and we will now shortly describe the models available in Spark 2.0.</p><div class="section" title="Classification"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec01"/>Classification</h3></div></div></div><p>The ML package <a id="id330" class="indexterm"/>provides a data scientist with seven classification models to<a id="id331" class="indexterm"/> choose from. These range from the simplest ones (such as logistic regression) to more sophisticated ones. We will provide short descriptions of each of them in the following section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">LogisticRegression</code>: The<a id="id332" class="indexterm"/> benchmark model for classification. The logistic regression uses a logit function to calculate the probability of an observation belonging to a particular class. At the time of writing, the PySpark ML supports only binary classification problems.</li><li class="listitem" style="list-style-type: disc"><code class="literal">DecisionTreeClassifier</code>: A classifier that builds a decision tree to predict a class <a id="id333" class="indexterm"/>for an observation. Specifying the <code class="literal">maxDepth</code> parameter limits the depth the tree grows, the <code class="literal">minInstancePerNode</code> determines the minimum number of observations in the tree node required to further split, the <code class="literal">maxBins</code> parameter specifies the maximum <a id="id334" class="indexterm"/>number of bins the continuous variables will be split into, and the <code class="literal">impurity</code> specifies the metric to measure and calculate the information gain from the split.</li><li class="listitem" style="list-style-type: disc"><code class="literal">GBTClassifier</code>: A <span class="strong"><strong>Gradient Boosted Trees</strong></span> model for classification. The model belongs<a id="id335" class="indexterm"/> to the family of ensemble models: models<a id="id336" class="indexterm"/> that combine multiple weak predictive models to form a strong one. At the moment, the <code class="literal">GBTClassifier</code> model supports binary labels, and continuous and categorical features.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RandomForestClassifier</code>: This model produces multiple decision trees (hence the name—forest) and uses the <code class="literal">mode</code> output of those decision trees to classify<a id="id337" class="indexterm"/> observations. The <code class="literal">RandomForestClassifier</code> supports both binary and multinomial labels.</li><li class="listitem" style="list-style-type: disc"><code class="literal">NaiveBayes</code>: Based<a id="id338" class="indexterm"/> on the Bayes' theorem, this model <a id="id339" class="indexterm"/>uses conditional probability theory to classify observations. The <code class="literal">NaiveBayes</code> model in PySpark ML supports both binary and multinomial labels.</li><li class="listitem" style="list-style-type: disc"><code class="literal">MultilayerPerceptronClassifier</code>: A classifier that mimics the nature of a human<a id="id340" class="indexterm"/> brain. Deeply rooted in the Artificial Neural Networks theory, the model is a black-box, that is, it is not easy to interpret the internal parameters of the model. The model consists, at a minimum, of three, fully connected <code class="literal">layers</code> (a parameter that needs to be specified when creating the model object) of artificial neurons: the input layer (that needs to be equal to the number of features in your dataset), a number of hidden layers (at least one), and an output layer with the number of neurons equal to the number of categories in your label. All the neurons in the input and hidden layers have a sigmoid activation function, whereas the activation function of the neurons in the output layer is softmax.</li><li class="listitem" style="list-style-type: disc"><code class="literal">OneVsRest</code>: A <a id="id341" class="indexterm"/>reduction of a multiclass classification to a binary one. For example, in the case of a multinomial label, the model can train multiple binary logistic regression models. For example, if <code class="literal">label == 2</code>, the model will build a logistic regression where it will convert the <code class="literal">label == 2</code> to <code class="literal">1</code> (all remaining label values would be set to <code class="literal">0</code>) and then train a binary model. All the models are then scored and the model with the highest probability wins.</li></ul></div></div><div class="section" title="Regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec02"/>Regression</h3></div></div></div><p>There are <a id="id342" class="indexterm"/>seven models available for regression tasks in the PySpark ML package. As <a id="id343" class="indexterm"/>with classification, these range from some basic ones (such as the obligatory linear regression) to more complex ones:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">AFTSurvivalRegression</code>: Fits an Accelerated Failure Time regression model. It is <a id="id344" class="indexterm"/>a parametric model that assumes that a marginal effect of one of the features accelerates or decelerates a life expectancy (or process failure). It is highly applicable for the processes with well-defined stages.</li><li class="listitem" style="list-style-type: disc"><code class="literal">DecisionTreeRegressor</code>: Similar to the model for classification with an obvious <a id="id345" class="indexterm"/>distinction that the label is continuous instead of binary (or multinomial).</li><li class="listitem" style="list-style-type: disc"><code class="literal">GBTRegressor</code>: As with the <code class="literal">DecisionTreeRegressor</code>, the difference is the data type <a id="id346" class="indexterm"/>of the label.</li><li class="listitem" style="list-style-type: disc"><code class="literal">GeneralizedLinearRegression</code>: A family of linear models with differing kernel functions (link functions). In contrast to the linear regression that assumes<a id="id347" class="indexterm"/> normality of error terms, the GLM allows the <a id="id348" class="indexterm"/>label to have different error term distributions: the <code class="literal">GeneralizedLinearRegression</code> model from the PySpark ML package supports <code class="literal">gaussian</code>, <code class="literal">binomial</code>, <code class="literal">gamma</code>, and <code class="literal">poisson</code> families of error distributions with a host of different link functions.</li><li class="listitem" style="list-style-type: disc"><code class="literal">IsotonicRegression</code>: A type of regression that fits a free-form, non-decreasing<a id="id349" class="indexterm"/> line to your data. It is useful to fit the datasets with ordered and increasing observations.</li><li class="listitem" style="list-style-type: disc"><code class="literal">LinearRegression</code>: The most simple of regression models, it assumes a linear relationship<a id="id350" class="indexterm"/> between features and a continuous label, and normality of error terms.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RandomForestRegressor</code>: Similar to either <code class="literal">DecisionTreeRegressor</code> or <code class="literal">GBTRegressor</code>, the <code class="literal">RandomForestRegressor</code> fits a continuous label instead <a id="id351" class="indexterm"/>of a discrete one.</li></ul></div></div><div class="section" title="Clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec03"/>Clustering</h3></div></div></div><p>Clustering is <a id="id352" class="indexterm"/>a family of unsupervised models that are used to find <a id="id353" class="indexterm"/>underlying patterns in your data. The PySpark ML package provides the four most popular models at the moment:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">BisectingKMeans</code>: A combination of the k-means clustering method and hierarchical<a id="id354" class="indexterm"/> clustering. The algorithm begins with all observations in a single cluster and iteratively splits the data into <code class="literal">k</code> clusters.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note50"/>Note</h3><p>Check out this <a id="id355" class="indexterm"/>website for more information on pseudo-algorithms: <a class="ulink" href="http://minethedata.blogspot.com/2012/08/bisecting-k-means.html">http://minethedata.blogspot.com/2012/08/bisecting-k-means.html</a>.</p></div></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">KMeans</code>: This is the famous k-mean algorithm that separates data into <code class="literal">k</code> clusters, iteratively searching for<a id="id356" class="indexterm"/> centroids that minimize the sum of square distances between each observation and the centroid of the cluster it belongs to.</li><li class="listitem" style="list-style-type: disc"><code class="literal">GaussianMixture</code>: This method uses <code class="literal">k</code> Gaussian distributions with unknown<a id="id357" class="indexterm"/> parameters to dissect the dataset. Using the Expectation-Maximization algorithm, the parameters for the Gaussians are found by maximizing the log-likelihood function.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip26"/>Tip</h3><p>Beware that for datasets with many features this model might perform poorly due to the curse of dimensionality and numerical issues with Gaussian distributions.</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">LDA</code>: This model is <a id="id358" class="indexterm"/>used for topic modeling in natural language processing applications.</li></ul></div><p>There is<a id="id359" class="indexterm"/> also one recommendation model available in PySpark ML, but we will refrain from describing it here.</p></div></div><div class="section" title="Pipeline"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec64"/>Pipeline</h2></div></div></div><p>A <code class="literal">Pipeline</code> in <a id="id360" class="indexterm"/>PySpark ML is a concept of an <span class="emphasis"><em>end-to-end</em></span> transformation-estimation process (with distinct stages) that ingests some raw data (in a DataFrame form), performs<a id="id361" class="indexterm"/> the necessary data carpentry (transformations), and finally estimates a statistical model (estimator).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip27"/>Tip</h3><p>A <code class="literal">Pipeline</code> can be purely transformative, that is, consisting of <code class="literal">Transformer</code>s only.</p></div></div><p>A <code class="literal">Pipeline</code> can be<a id="id362" class="indexterm"/> thought of as a chain of multiple discrete stages. When<a id="id363" class="indexterm"/> a <code class="literal">.fit(...)</code> method is executed on a <code class="literal">Pipeline</code> object, all the stages are executed in the order they were specified in the <code class="literal">stages</code> parameter; the <code class="literal">stages </code>parameter is a list of <code class="literal">Transformer</code> and <code class="literal">Estimator</code> objects. The <code class="literal">.fit(...)</code> method of the <code class="literal">Pipeline</code> object executes the <code class="literal">.transform(...)</code> method for the <code class="literal">Transformer</code>s and the <code class="literal">.fit(...)</code> method for the <code class="literal">Estimators</code>.</p><p>Normally, the output of a preceding stage becomes the input for the following stage: when deriving from either the <code class="literal">Transformer</code> or <code class="literal">Estimator</code> abstract classes, one needs to implement the <code class="literal">.getOutputCol()</code> method that returns the value of the <code class="literal">outputCol</code> parameter specified when creating an object.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Predicting the chances of infant survival with ML"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Predicting the chances of infant survival with ML</h1></div></div></div><p>In this section, we<a id="id364" class="indexterm"/> will use the portion of the dataset from the previous chapter to present the ideas of PySpark ML.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note51"/>Note</h3><p>If you have<a id="id365" class="indexterm"/> not yet downloaded the data while reading the previous chapter, it can be accessed here: <a class="ulink" href="http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz">http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz</a>.</p></div></div><p>In this section, we will, once again, attempt to predict the chances of the survival of an infant.</p><div class="section" title="Loading the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec65"/>Loading the data</h2></div></div></div><p>First, we load<a id="id366" class="indexterm"/> the data with the help of the following code:</p><div class="informalexample"><pre class="programlisting">import pyspark.sql.types as typ
labels = [
    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),
    ('BIRTH_PLACE', typ.StringType()),
    ('MOTHER_AGE_YEARS', typ.IntegerType()),
    ('FATHER_COMBINED_AGE', typ.IntegerType()),
    ('CIG_BEFORE', typ.IntegerType()),
    ('CIG_1_TRI', typ.IntegerType()),
    ('CIG_2_TRI', typ.IntegerType()),
    ('CIG_3_TRI', typ.IntegerType()),
    ('MOTHER_HEIGHT_IN', typ.IntegerType()),
    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),
    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),
    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),
    ('DIABETES_PRE', typ.IntegerType()),
    ('DIABETES_GEST', typ.IntegerType()),
    ('HYP_TENS_PRE', typ.IntegerType()),
    ('HYP_TENS_GEST', typ.IntegerType()),
    ('PREV_BIRTH_PRETERM', typ.IntegerType())
]
schema = typ.StructType([
    typ.StructField(e[0], e[1], False) for e in labels
])
births = spark.read.csv('births_transformed.csv.gz', 
                        header=True, 
                        schema=schema)</pre></div><p>We specify<a id="id367" class="indexterm"/> the schema of the DataFrame; our severely limited dataset now only has 17 columns.</p></div><div class="section" title="Creating transformers"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec66"/>Creating transformers</h2></div></div></div><p>Before we <a id="id368" class="indexterm"/>can use the dataset to estimate a model, we need to do some transformations. Since statistical models can only operate on numeric data, we will have to encode the <code class="literal">BIRTH_PLACE</code> variable.</p><p>Before we do any of this, since we will use a number of different feature transformations later in this chapter, let's import them all:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.feature as ft</pre></div><p>To encode the <code class="literal">BIRTH_PLACE</code> column, we will use the <code class="literal">OneHotEncoder</code> method. However, the method cannot accept <code class="literal">StringType</code> columns; it can only deal with numeric types so first we will cast the column to an <code class="literal">IntegerType</code>:</p><div class="informalexample"><pre class="programlisting">births = births \
    .withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'] \
    .cast(typ.IntegerType()))</pre></div><p>Having done this, we can now create our first <code class="literal">Transformer</code>:</p><div class="informalexample"><pre class="programlisting">encoder = ft.OneHotEncoder(
    inputCol='BIRTH_PLACE_INT', 
    outputCol='BIRTH_PLACE_VEC')</pre></div><p>Let's now<a id="id369" class="indexterm"/> create a single column with all the features collated together. We will use the <code class="literal">VectorAssembler</code> method:</p><div class="informalexample"><pre class="programlisting">featuresCreator = ft.VectorAssembler(
    inputCols=[
        col[0] 
        for col 
        in labels[2:]] + \
    [encoder.getOutputCol()], 
    outputCol='features'
)</pre></div><p>The <code class="literal">inputCols</code> parameter passed to the <code class="literal">VectorAssembler</code> object is a list of all the columns to be combined together to form the <code class="literal">outputCol</code>—the <code class="literal">'features'</code>. Note that we use the output of the encoder object (by calling the <code class="literal">.getOutputCol()</code> method), so we do not have to remember to change this parameter's value should we change the name of the output column in the encoder object at any point.</p><p>It's now time to create our first estimator.</p></div><div class="section" title="Creating an estimator"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec67"/>Creating an estimator</h2></div></div></div><p>In this <a id="id370" class="indexterm"/>example, we will (once again) use the logistic regression model. However, later in the chapter, we will showcase some more complex models from the <code class="literal">.classification</code> set of PySpark ML models, so we load the whole section:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.classification as cl</pre></div><p>Once loaded, let's create the model by using the following code:</p><div class="informalexample"><pre class="programlisting">logistic = cl.LogisticRegression(
    maxIter=10, 
    regParam=0.01, 
    labelCol='INFANT_ALIVE_AT_REPORT')</pre></div><p>We would not have to specify the <code class="literal">labelCol</code> parameter if our target column had the name <code class="literal">'label'</code>. Also, if the output of our <code class="literal">featuresCreator</code> was not called <code class="literal">'features',</code> we would <a id="id371" class="indexterm"/>have to specify the <code class="literal">featuresCol</code> by (most conveniently) calling the <code class="literal">getOutputCol()</code> method on the <code class="literal">featuresCreator</code> object.</p></div><div class="section" title="Creating a pipeline"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec68"/>Creating a pipeline</h2></div></div></div><p>All that is<a id="id372" class="indexterm"/> left now is to create a <code class="literal">Pipeline</code> and fit the model. First, let's load the <code class="literal">Pipeline</code> from the ML package:</p><div class="informalexample"><pre class="programlisting">from pyspark.ml import Pipeline</pre></div><p>Creating a <code class="literal">Pipeline</code> is really easy. Here's how our pipeline should look like conceptually:</p><div class="mediaobject"><img src="images/B05793_06_01.jpg" alt="Creating a pipeline"/></div><p>Converting this structure into a <code class="literal">Pipeline</code> is a <span class="emphasis"><em>walk in the park</em></span>:</p><div class="informalexample"><pre class="programlisting">pipeline = Pipeline(stages=[
        encoder, 
        featuresCreator, 
        logistic
    ])</pre></div><p>That's it! Our <code class="literal">pipeline</code> is now created so we can (finally!) estimate the model.</p></div><div class="section" title="Fitting the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec69"/>Fitting the model</h2></div></div></div><p>Before<a id="id373" class="indexterm"/> you fit the model, we need to split our dataset into training and testing datasets. Conveniently, the DataFrame API has the <code class="literal">.randomSplit(...)</code> method:</p><div class="informalexample"><pre class="programlisting">births_train, births_test = births \
    .randomSplit([0.7, 0.3], seed=666)</pre></div><p>The first parameter is a list of dataset proportions that should end up in, respectively, <code class="literal">births_train</code> and <code class="literal">births_test</code> subsets. The <code class="literal">seed</code> parameter provides a seed to the randomizer.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note52"/>Note</h3><p>You can also split the dataset into more than two subsets as long as the elements of the list sum up to 1, and you unpack the output into as many subsets.</p><p>For example, we could split the births dataset into three subsets like this:</p><div class="informalexample"><pre class="programlisting">train, test, val = births.\
    randomSplit([0.7, 0.2, 0.1], seed=666)</pre></div><p>The preceding code would put a random 70% of the births dataset into the <code class="literal">train</code> object, 20% would go to the <code class="literal">test</code>, and the <code class="literal">val</code> DataFrame would hold the remaining 10%.</p></div></div><p>Now it is <a id="id374" class="indexterm"/>about time to finally run our pipeline and estimate our model:</p><div class="informalexample"><pre class="programlisting">model = pipeline.fit(births_train)
test_model = model.transform(births_test)</pre></div><p>The <code class="literal">.fit(...)</code> method of the pipeline object takes our training dataset as an input. Under the hood, the <code class="literal">births_train</code> dataset is passed first to the <code class="literal">encoder</code> object. The DataFrame that is created at the <code class="literal">encoder</code> stage then gets passed to the <code class="literal">featuresCreator</code> that creates the <code class="literal">'features'</code> column. Finally, the output from this stage is passed to the <code class="literal">logistic</code> object that estimates the final model.</p><p>The <code class="literal">.fit(...) </code>method returns the <code class="literal">PipelineModel</code> object (the <code class="literal">model</code> object in the preceding snippet) that can then be used for prediction; we attain this by calling the <code class="literal">.transform(...)</code> method and passing the testing dataset created earlier. Here's what the <code class="literal">test_model</code> looks like in the following command:</p><div class="informalexample"><pre class="programlisting">test_model.take(1)</pre></div><p>It generates the following output:</p><div class="mediaobject"><img src="images/B05793_06_02.jpg" alt="Fitting the model"/></div><p>As you can see, we get all the columns from the <code class="literal">Transfomers</code> and <code class="literal">Estimators</code>. The logistic regression model outputs several columns: the <code class="literal">rawPrediction</code> is the value of the linear <a id="id375" class="indexterm"/>combination of features and the β coefficients, the <code class="literal">probability</code> is the calculated probability for each of the classes, and finally, the <code class="literal">prediction</code> is our final class assignment.</p></div><div class="section" title="Evaluating the performance of the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec70"/>Evaluating the performance of the model</h2></div></div></div><p>Obviously, we <a id="id376" class="indexterm"/>would like to now test how well our model did. PySpark exposes a number of evaluation methods for classification and regression in the <code class="literal">.evaluation</code> section of the package:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.evaluation as ev</pre></div><p>We will use the <code class="literal">BinaryClassficationEvaluator</code> to test how well our model performed:</p><div class="informalexample"><pre class="programlisting">evaluator = ev.BinaryClassificationEvaluator(
    rawPredictionCol='probability', 
    labelCol='INFANT_ALIVE_AT_REPORT')</pre></div><p>The <code class="literal">rawPredictionCol</code> can either be the <code class="literal">rawPrediction</code> column produced by the estimator or the <code class="literal">probability</code>.</p><p>Let's see how well our model performed:</p><div class="informalexample"><pre class="programlisting">print(evaluator.evaluate(test_model, 
    {evaluator.metricName: 'areaUnderROC'}))
print(evaluator.evaluate(test_model, 
   {evaluator.metricName: 'areaUnderPR'}))</pre></div><p>The preceding code produces the following result:</p><div class="mediaobject"><img src="images/B05793_06_03.jpg" alt="Evaluating the performance of the model"/></div><p>The area under the ROC of 74% and area under PR of 71% shows a well-defined model, but nothing out of extraordinary; if we had other features, we could drive this up, but this is not the purpose of this chapter (nor the book, for that matter).</p></div><div class="section" title="Saving the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec71"/>Saving the model</h2></div></div></div><p>PySpark allows <a id="id377" class="indexterm"/>you to save the <code class="literal">Pipeline</code> definition for later use. It not only saves the pipeline structure, but also all the definitions of all the <code class="literal">Transformers</code> and <code class="literal">Estimators</code>:</p><div class="informalexample"><pre class="programlisting">pipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'
pipeline.write().overwrite().save(pipelinePath)</pre></div><p>So, you can load it up later and use it straight away to <code class="literal">.fit(...)</code> and predict:</p><div class="informalexample"><pre class="programlisting">loadedPipeline = Pipeline.load(pipelinePath)
loadedPipeline \
    .fit(births_train)\
    .transform(births_test)\
    .take(1)</pre></div><p>The preceding code produces the same result (as expected):</p><div class="mediaobject"><img src="images/B05793_06_04.jpg" alt="Saving the model"/></div><p>If you, however, want to save the estimated model, you can also do that; instead of saving the <code class="literal">Pipeline</code>, you need to save the <code class="literal">PipelineModel</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip28"/>Tip</h3><p>Note, that not only the <code class="literal">PipelineModel</code> can be saved: virtually all the models that are returned by calling the <code class="literal">.fit(...)</code> method on an <code class="literal">Estimator</code> or <code class="literal">Transformer</code> can be saved and loaded back to be reused.</p></div></div><p>To save your model, see the following the example:</p><div class="informalexample"><pre class="programlisting">from pyspark.ml import PipelineModel

modelPath = './infant_oneHotEncoder_Logistic_PipelineModel'
model.write().overwrite().save(modelPath)

loadedPipelineModel = PipelineModel.load(modelPath)
test_reloadedModel = loadedPipelineModel.transform(births_test)</pre></div><p>The<a id="id378" class="indexterm"/> preceding script uses the <code class="literal">.load(...)</code> method, a class method of the <code class="literal">PipelineModel</code> class, to reload the estimated model. You can compare the result of <code class="literal">test_reloadedModel.take(1)</code> with the output of <code class="literal">test_model.take(1)</code> we presented earlier.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Parameter hyper-tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Parameter hyper-tuning</h1></div></div></div><p>Rarely, our first model would be the best we can do. By simply looking at our metrics and accepting the model because it passed our pre-conceived performance thresholds is <a id="id379" class="indexterm"/>hardly a scientific method for finding the best model.</p><p>A concept of parameter hyper-tuning is to find the best parameters of the model: for example, the maximum number of iterations needed to properly estimate the logistic regression model or maximum depth of a decision tree.</p><p>In this section, we will explore two concepts that allow us to find the best parameters for our models: grid search and train-validation splitting.</p><div class="section" title="Grid search"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec72"/>Grid search</h2></div></div></div><p>Grid search is <a id="id380" class="indexterm"/>an exhaustive algorithm that loops through the list <a id="id381" class="indexterm"/>of defined parameter values, estimates separate models, and chooses the best one given some evaluation metric.</p><p>A note of caution should be stated here: if you define too many parameters you want to optimize over, or too many values of these parameters, it might take a lot of time to select the best model as the number of models to estimate would grow very quickly as the number of parameters and parameter values grow.</p><p>For example, if you want to fine-tune two parameters with two parameter values, you would have to fit four models. Adding one more parameter with two values would require estimating eight models, whereas adding one more additional value to our two parameters (bringing it to three values for each) would require estimating nine models. As you can see, this can quickly get out of hand if you are not careful. See the following chart to inspect this visually:</p><div class="mediaobject"><img src="images/B05793_06_05.jpg" alt="Grid search"/></div><p>After this<a id="id382" class="indexterm"/> cautionary tale, let's get to fine-tuning our <a id="id383" class="indexterm"/>parameters space. First, we load the <code class="literal">.tuning</code> part of the package:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.tuning as tune</pre></div><p>Next, let's specify our model and the list of parameters we want to loop through:</p><div class="informalexample"><pre class="programlisting">logistic = cl.LogisticRegression(
    labelCol='INFANT_ALIVE_AT_REPORT')
grid = tune.ParamGridBuilder() \
    .addGrid(logistic.maxIter,  
             [2, 10, 50]) \
    .addGrid(logistic.regParam, 
             [0.01, 0.05, 0.3]) \
    .build()</pre></div><p>First, we specify the model we want to optimize the parameters of. Next, we decide which parameters we will be optimizing, and what values for those parameters to test. We use the <code class="literal">ParamGridBuilder()</code> object from the <code class="literal">.tuning</code> subpackage, and keep adding the parameters to<a id="id384" class="indexterm"/> the grid with the <code class="literal">.addGrid(...)</code> method: the <a id="id385" class="indexterm"/>first parameter is the parameter object of the model we want to optimize (in our case, these are <code class="literal">logistic.maxIter</code> and <code class="literal">logistic.regParam</code>), and the second parameter is a list of values we want to loop through. Calling the <code class="literal">.build()</code> method on the <code class="literal">.ParamGridBuilder</code> builds the grid.</p><p>Next, we need some way of comparing the models:</p><div class="informalexample"><pre class="programlisting">evaluator = ev.BinaryClassificationEvaluator(
    rawPredictionCol='probability', 
    labelCol='INFANT_ALIVE_AT_REPORT')</pre></div><p>So, once again, we'll use the <code class="literal">BinaryClassificationEvaluator</code>. It is time now to create the logic that will do the validation work for us:</p><div class="informalexample"><pre class="programlisting">cv = tune.CrossValidator(
    estimator=logistic, 
    estimatorParamMaps=grid, 
    evaluator=evaluator
)</pre></div><p>The <code class="literal">CrossValidator</code> needs the <code class="literal">estimator</code>, the <code class="literal">estimatorParamMaps</code>, and the <code class="literal">evaluator</code> to do its job. The model loops through the grid of values, estimates the models, and compares their performance using the <code class="literal">evaluator</code>.</p><p>We cannot use the data straight away (as the <code class="literal">births_train</code> and <code class="literal">births_test</code> still have the <code class="literal">BIRTHS_PLACE</code> column not encoded) so we create a purely transforming <code class="literal">Pipeline</code>:</p><div class="informalexample"><pre class="programlisting">pipeline = Pipeline(stages=[encoder ,featuresCreator])
data_transformer = pipeline.fit(births_train)</pre></div><p>Having done this, we are ready to find the optimal combination of parameters for our model:</p><div class="informalexample"><pre class="programlisting">cvModel = cv.fit(data_transformer.transform(births_train))</pre></div><p>The <code class="literal">cvModel</code> will return the best model estimated. We can now use it to see if it performed better than our previous model:</p><div class="informalexample"><pre class="programlisting">data_train = data_transformer \
    .transform(births_test)
results = cvModel.transform(data_train)
print(evaluator.evaluate(results, 
     {evaluator.metricName: 'areaUnderROC'}))
print(evaluator.evaluate(results, 
     {evaluator.metricName: 'areaUnderPR'}))</pre></div><p>The preceding code will produce the following result:</p><div class="mediaobject"><img src="images/B05793_06_06.jpg" alt="Grid search"/></div><p>As you can<a id="id386" class="indexterm"/> see, we got a slightly better result. What<a id="id387" class="indexterm"/> parameters does the best model have? The answer is a little bit convoluted, but here's how you can extract it:</p><div class="informalexample"><pre class="programlisting">results = [
    (
        [
            {key.name: paramValue} 
            for key, paramValue 
            in zip(
                params.keys(), 
                params.values())
        ], metric
    ) 
    for params, metric 
    in zip(
        cvModel.getEstimatorParamMaps(), 
        cvModel.avgMetrics
    )
]
sorted(results, 
       key=lambda el: el[1], 
       reverse=True)[0]</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_06_07.jpg" alt="Grid search"/></div></div><div class="section" title="Train-validation splitting"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec73"/>Train-validation splitting</h2></div></div></div><p>The <code class="literal">TrainValidationSplit</code> model, to select the best model, performs a random split of the input <a id="id388" class="indexterm"/>dataset (the training dataset) into<a id="id389" class="indexterm"/> two subsets: smaller training and validation subsets. The split is only performed once.</p><p>In this example, we will also use the <code class="literal">ChiSqSelector</code> to select only the top five features, thus limiting the complexity of our model:</p><div class="informalexample"><pre class="programlisting">selector = ft.ChiSqSelector(
    numTopFeatures=5, 
    featuresCol=featuresCreator.getOutputCol(), 
    outputCol='selectedFeatures',
    labelCol='INFANT_ALIVE_AT_REPORT'
)</pre></div><p>The <code class="literal">numTopFeatures</code> specifies the number of features to return. We will put the selector after the <code class="literal">featuresCreator</code>, so we call the <code class="literal">.getOutputCol()</code> on the <code class="literal">featuresCreator</code>.</p><p>We covered creating the <code class="literal">LogisticRegression</code> and <code class="literal">Pipeline</code> earlier, so we will not explain how these are created again here:</p><div class="informalexample"><pre class="programlisting">logistic = cl.LogisticRegression(
    labelCol='INFANT_ALIVE_AT_REPORT',
    featuresCol='selectedFeatures'
)
pipeline = Pipeline(stages=[encoder, featuresCreator, selector])
data_transformer = pipeline.fit(births_train)</pre></div><p>The <code class="literal">TrainValidationSplit</code> object gets created in the same fashion as the <code class="literal">CrossValidator</code> model:</p><div class="informalexample"><pre class="programlisting">tvs = tune.TrainValidationSplit(
    estimator=logistic, 
    estimatorParamMaps=grid, 
    evaluator=evaluator
)</pre></div><p>As before, we fit our data to the model, and calculate the results:</p><div class="informalexample"><pre class="programlisting">tvsModel = tvs.fit(
    data_transformer \
        .transform(births_train)
)
data_train = data_transformer \
    .transform(births_test)
results = tvsModel.transform(data_train)
print(evaluator.evaluate(results, 
     {evaluator.metricName: 'areaUnderROC'}))
print(evaluator.evaluate(results, 
     {evaluator.metricName: 'areaUnderPR'}))</pre></div><p>The preceding code prints out the following output:</p><div class="mediaobject"><img src="images/B05793_06_08.jpg" alt="Train-validation splitting"/></div><p>Well, the <a id="id390" class="indexterm"/>model with less features certainly<a id="id391" class="indexterm"/> performed worse than the full model, but the difference was not that great. Ultimately, it is a performance trade-off between a more complex model and the less sophisticated one.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Other features of PySpark ML in action"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Other features of PySpark ML in action</h1></div></div></div><p>At the beginning <a id="id392" class="indexterm"/>of this chapter, we described most of the features of the PySpark ML library. In this section, we will provide examples of how to use some of the <code class="literal">Transformers</code> and <code class="literal">Estimators</code>.</p><div class="section" title="Feature extraction"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec74"/>Feature extraction</h2></div></div></div><p>We have<a id="id393" class="indexterm"/> used quite a few models from this submodule of PySpark. In<a id="id394" class="indexterm"/> this section, we'll show you how to use the most useful ones (in our opinion).</p><div class="section" title="NLP - related feature extractors"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec04"/>NLP - related feature extractors</h3></div></div></div><p>As described <a id="id395" class="indexterm"/>earlier, the <code class="literal">NGram</code> model takes<a id="id396" class="indexterm"/> a list of tokenized text and produces pairs (or n-grams) of words.</p><p>In this example, we will take an excerpt from PySpark's documentation and present how to clean up the text before passing it to the <code class="literal">NGram</code> model. Here's how our dataset looks like (abbreviated for brevity):</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip29"/>Tip</h3><p>For the full view of how the following snippet looks like, please download the code from our GitHub repository: <a class="ulink" href="https://github.com/drabastomek/learningPySpark">https://github.com/drabastomek/learningPySpark</a>.</p><p>We copied these<a id="id397" class="indexterm"/> four paragraphs from the description of the DataFrame usage in <code class="literal">Pipelines</code>: <a class="ulink" href="http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe">http://spark.apache.org/docs/latest/ml-pipeline.html#dataframe</a>.</p></div></div><div class="informalexample"><pre class="programlisting">text_data = spark.createDataFrame([
    ['''Machine learning can be applied to a wide variety 
        of data types, such as vectors, text, images, and 
        structured data. This API adopts the DataFrame from 
        Spark SQL in order to support a variety of data
        types.'''],
    (...)
    ['''Columns in a DataFrame are named. The code examples 
        below use names such as "text," "features," and 
        "label."''']
], ['input'])</pre></div><p>Each row in<a id="id398" class="indexterm"/> our single-column DataFrame is <a id="id399" class="indexterm"/>just a bunch of text. First, we need to tokenize this text. To do so we will use the <code class="literal">RegexTokenizer</code> instead of just the <code class="literal">Tokenizer</code> as we can specify the pattern(s) we want the text to be broken at:</p><div class="informalexample"><pre class="programlisting">tokenizer = ft.RegexTokenizer(
    inputCol='input', 
    outputCol='input_arr', 
    pattern='\s+|[,.\"]')</pre></div><p>The pattern here splits the text on any number of spaces, but also removes commas, full stops, backslashes, and quotation marks. A single row from the output of the <code class="literal">tokenizer </code>looks similar to this:</p><div class="mediaobject"><img src="images/B05793_06_09.jpg" alt="NLP - related feature extractors"/></div><p>As you can see, the <code class="literal">RegexTokenizer</code> not only splits the sentences in to words, but also normalizes the text so each word is in small-caps.</p><p>However, there is still plenty of junk in our text: words such as <code class="literal">be</code>, <code class="literal">a</code>, or <code class="literal">to</code> normally provide us<a id="id400" class="indexterm"/> with nothing useful when <a id="id401" class="indexterm"/>analyzing a text. Thus, we will remove these so called <code class="literal">stopwords</code> using nothing else other than the <code class="literal">StopWordsRemover(...)</code>:</p><div class="informalexample"><pre class="programlisting">stopwords = ft.StopWordsRemover(
    inputCol=tokenizer.getOutputCol(), 
    outputCol='input_stop')</pre></div><p>The output of the method looks as follows:</p><div class="mediaobject"><img src="images/B05793_06_10.jpg" alt="NLP - related feature extractors"/></div><p>Now we only have the useful words. So, let's build our <code class="literal">NGram</code> model and the <code class="literal">Pipeline</code>:</p><div class="informalexample"><pre class="programlisting">ngram = ft.NGram(n=2, 
    inputCol=stopwords.getOutputCol(), 
    outputCol="nGrams")
pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])</pre></div><p>Now that we have the <code class="literal">pipeline</code>, we follow in a very similar fashion as before:</p><div class="informalexample"><pre class="programlisting">data_ngram = pipeline \
    .fit(text_data) \
    .transform(text_data)
data_ngram.select('nGrams').take(1)</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_06_11.jpg" alt="NLP - related feature extractors"/></div><p>That's it. We<a id="id402" class="indexterm"/> have got our n-grams and <a id="id403" class="indexterm"/>we can now use them in further NLP processing.</p></div><div class="section" title="Discretizing continuous variables"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec05"/>Discretizing continuous variables</h3></div></div></div><p>Ever so often, we deal with a continuous feature that is highly non-linear and really hard to fit in <a id="id404" class="indexterm"/>our model with only one coefficient.</p><p>In such a <a id="id405" class="indexterm"/>situation, it might be hard to explain the relationship between such a feature and the target with just one coefficient. Sometimes, it is useful to band the values into discrete buckets.</p><p>First, let's create some fake data with the help of the following code:</p><div class="informalexample"><pre class="programlisting">import numpy as np
x = np.arange(0, 100)
x = x / 100.0 * np.pi * 4
y = x * np.sin(x / 1.764) + 20.1234</pre></div><p>Now, we can create a DataFrame by using the following code:</p><div class="informalexample"><pre class="programlisting">schema = typ.StructType([
    typ.StructField('continuous_var', 
                    typ.DoubleType(), 
                    False
   )
])
data = spark.createDataFrame(
    [[float(e), ] for e in y], 
    schema=schema)</pre></div><div class="mediaobject"><img src="images/B05793_06_12.jpg" alt="Discretizing continuous variables"/></div><p>Next, we <a id="id406" class="indexterm"/>will use the <code class="literal">QuantileDiscretizer</code> model<a id="id407" class="indexterm"/> to split our continuous variable into five buckets (the <code class="literal">numBuckets</code> parameter):</p><div class="informalexample"><pre class="programlisting">discretizer = ft.QuantileDiscretizer(
    numBuckets=5, 
    inputCol='continuous_var', 
    outputCol='discretized')</pre></div><p>Let's see what we have got:</p><div class="informalexample"><pre class="programlisting">data_discretized = discretizer.fit(data).transform(data)</pre></div><p>Our function now looks as follows:</p><div class="mediaobject"><img src="images/B05793_06_13.jpg" alt="Discretizing continuous variables"/></div><p>We can <a id="id408" class="indexterm"/>now treat this variable as <a id="id409" class="indexterm"/>categorical and use the <code class="literal">OneHotEncoder</code> to encode it for future use.</p></div><div class="section" title="Standardizing continuous variables"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec06"/>Standardizing continuous variables</h3></div></div></div><p>Standardizing continuous variables helps not only in better understanding the relationships<a id="id410" class="indexterm"/> between the features (as interpreting the coefficients becomes easier), but it also aids computational efficiency and<a id="id411" class="indexterm"/> protects from running into some numerical traps. Here's how you do it with PySpark ML.</p><p>First, we need to create a vector representation of our continuous variable (as it is only a single float):</p><div class="informalexample"><pre class="programlisting">vectorizer = ft.VectorAssembler(
    inputCols=['continuous_var'], 
    outputCol= 'continuous_vec')</pre></div><p>Next, we<a id="id412" class="indexterm"/> build our <code class="literal">normalizer</code> and the <code class="literal">pipeline</code>. By setting the <code class="literal">withMean</code> and <code class="literal">withStd</code> to <code class="literal">True</code>, the method will<a id="id413" class="indexterm"/> remove the mean and scale the variance to be of unit length:</p><div class="informalexample"><pre class="programlisting">normalizer = ft.StandardScaler(
    inputCol=vectorizer.getOutputCol(), 
    outputCol='normalized', 
    withMean=True,
    withStd=True
)
pipeline = Pipeline(stages=[vectorizer, normalizer])
data_standardized = pipeline.fit(data).transform(data)</pre></div><p>Here's what the transformed data would look like:</p><div class="mediaobject"><img src="images/B05793_06_14.jpg" alt="Standardizing continuous variables"/></div><p>As you can see, the data now oscillates around 0 with the unit variance (the green line).</p></div></div><div class="section" title="Classification"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec75"/>Classification</h2></div></div></div><p>So far we<a id="id414" class="indexterm"/> have only used the <code class="literal">LogisticRegression</code> model from PySpark ML. In this section, we will use the <code class="literal">RandomForestClassfier</code> to, once again, model<a id="id415" class="indexterm"/> the chances of survival for an infant.</p><p>Before we can do that, though, we need to cast the <code class="literal">label</code> feature to <code class="literal">DoubleType</code>:</p><div class="informalexample"><pre class="programlisting">import pyspark.sql.functions as func
births = births.withColumn(
    'INFANT_ALIVE_AT_REPORT', 
    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())
)
births_train, births_test = births \
    .randomSplit([0.7, 0.3], seed=666)</pre></div><p>Now that we have the label converted to double, we are ready to build our model. We progress in a similar fashion as before with the distinction that we will reuse the <code class="literal">encoder</code> and <code class="literal">featureCreator</code> from earlier in the chapter. The <code class="literal">numTrees</code> parameter specifies how many decision trees should be in our random forest, and the <code class="literal">maxDepth</code> parameter limits the depth of the trees:</p><div class="informalexample"><pre class="programlisting">classifier = cl.RandomForestClassifier(
    numTrees=5, 
    maxDepth=5, 
    labelCol='INFANT_ALIVE_AT_REPORT')
pipeline = Pipeline(
    stages=[
        encoder,
        featuresCreator, 
        classifier])
model = pipeline.fit(births_train)
test = model.transform(births_test)</pre></div><p>Let's now see how the <code class="literal">RandomForestClassifier</code> model performs compared to the <code class="literal">LogisticRegression</code>:</p><div class="informalexample"><pre class="programlisting">evaluator = ev.BinaryClassificationEvaluator(
    labelCol='INFANT_ALIVE_AT_REPORT')
print(evaluator.evaluate(test, 
    {evaluator.metricName: "areaUnderROC"}))
print(evaluator.evaluate(test, 
    {evaluator.metricName: "areaUnderPR"}))</pre></div><p>We get the following results:</p><div class="mediaobject"><img src="images/B05793_06_15.jpg" alt="Classification"/></div><p>Well, as you can <a id="id416" class="indexterm"/>see, the results are better than the logistic regression model by <a id="id417" class="indexterm"/>roughly 3 percentage points. Let's test how well would a model with one tree do:</p><div class="informalexample"><pre class="programlisting">classifier = cl.DecisionTreeClassifier(
    maxDepth=5, 
    labelCol='INFANT_ALIVE_AT_REPORT')
pipeline = Pipeline(stages=[
    encoder,
    featuresCreator, 
    classifier])
model = pipeline.fit(births_train)
test = model.transform(births_test)
evaluator = ev.BinaryClassificationEvaluator(
    labelCol='INFANT_ALIVE_AT_REPORT')
print(evaluator.evaluate(test, 
    {evaluator.metricName: "areaUnderROC"}))
print(evaluator.evaluate(test, 
    {evaluator.metricName: "areaUnderPR"}))</pre></div><p>The preceding code gives us the following:</p><div class="mediaobject"><img src="images/B05793_06_16.jpg" alt="Classification"/></div><p>Not bad at all! It actually performed better than the random forest model in terms of the precision-recall relationship and only slightly worse in terms of the area under the ROC. We just might have found a winner!</p></div><div class="section" title="Clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec76"/>Clustering</h2></div></div></div><p>Clustering is <a id="id418" class="indexterm"/>another big part of machine learning: quite often, in the<a id="id419" class="indexterm"/> real world, we do not have the luxury of having the target feature, so we need to revert to an unsupervised learning paradigm, where we try to uncover patterns in the data.</p><div class="section" title="Finding clusters in the births dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec07"/>Finding clusters in the births dataset</h3></div></div></div><p>In this <a id="id420" class="indexterm"/>example, we will use the <code class="literal">k-means</code> model to find similarities in the births data:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.clustering as clus
kmeans = clus.KMeans(k = 5, 
    featuresCol='features')
pipeline = Pipeline(stages=[
        assembler,
        featuresCreator, 
        kmeans]
)
model = pipeline.fit(births_train)</pre></div><p>Having estimated the model, let's see if we can find some differences between clusters:</p><div class="informalexample"><pre class="programlisting">test = model.transform(births_test)
test \
    .groupBy('prediction') \
    .agg({
        '*': 'count', 
        'MOTHER_HEIGHT_IN': 'avg'
    }).collect()</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_06_17.jpg" alt="Finding clusters in the births dataset"/></div><p>Well, the <code class="literal">MOTHER_HEIGHT_IN</code> is significantly different in cluster 2. Going through the results (which we will not do here for obvious reasons) would most likely uncover more differences<a id="id421" class="indexterm"/> and allow us to understand the data better.</p></div><div class="section" title="Topic mining"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec08"/>Topic mining</h3></div></div></div><p>Clustering <a id="id422" class="indexterm"/>models are not limited to numeric data only. In the field of NLP, problems such as topic extraction rely on clustering to detect documents <a id="id423" class="indexterm"/>with similar topics. We will go through such an example.</p><p>First, let's create our dataset. The data is formed from randomly selected paragraphs found on the Internet: three of them deal with topics of nature and national parks, the remaining three cover technology.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip30"/>Tip</h3><p>The code snippet is abbreviated again, for obvious reasons. Refer to the source file on GitHub for full representation.</p></div></div><div class="informalexample"><pre class="programlisting">text_data = spark.createDataFrame([
    ['''To make a computer do anything, you have to write a 
    computer program. To write a computer program, you have 
    to tell the computer, step by step, exactly what you want 
    it to do. The computer then "executes" the program, 
    following each step mechanically, to accomplish the end 
    goal. When you are telling the computer what to do, you 
    also get to choose how it's going to do it. That's where 
    computer algorithms come in. The algorithm is the basic 
    technique used to get the job done. Let's follow an 
    example to help get an understanding of the algorithm 
    concept.'''],
    (...),
    ['''Australia has over 500 national parks. Over 28 
    million hectares of land is designated as national 
    parkland, accounting for almost four per cent of 
    Australia's land areas. In addition, a further six per 
    cent of Australia is protected and includes state 
    forests, nature parks and conservation reserves.National 
    parks are usually large areas of land that are protected 
    because they have unspoilt landscapes and a diverse 
    number of native plants and animals. This means that 
    commercial activities such as farming are prohibited and 
    human activity is strictly monitored.''']
], ['documents'])</pre></div><p>First, we will <a id="id424" class="indexterm"/>once again use the <code class="literal">RegexTokenizer</code> and the <code class="literal">StopWordsRemover</code> models:</p><div class="informalexample"><pre class="programlisting">tokenizer = ft.RegexTokenizer(
    inputCol='documents', 
    outputCol='input_arr', 
    pattern='\s+|[,.\"]')
stopwords = ft.StopWordsRemover(
    inputCol=tokenizer.getOutputCol(), 
    outputCol='input_stop')</pre></div><p>Next in our <a id="id425" class="indexterm"/>pipeline is the <code class="literal">CountVectorizer</code>: a model that counts words in a document and returns a vector of counts. The length of the vector is equal to the total number of distinct words in all the documents, which can be seen in the following snippet:</p><div class="informalexample"><pre class="programlisting">stringIndexer = ft.CountVectorizer(
    inputCol=stopwords.getOutputCol(), 
    outputCol="input_indexed")
tokenized = stopwords \
    .transform(
        tokenizer\
            .transform(text_data)
    )
    
stringIndexer \
    .fit(tokenized)\
    .transform(tokenized)\
    .select('input_indexed')\
    .take(2)</pre></div><p>The preceding code will produce the following output:</p><div class="mediaobject"><img src="images/B05793_06_18.jpg" alt="Topic mining"/></div><p>As you can <a id="id426" class="indexterm"/>see, there are 262 distinct words in the text, and each <a id="id427" class="indexterm"/>document is now represented by a count of each word occurrence.</p><p>It's now time to<a id="id428" class="indexterm"/> start predicting the topics. For that purpose we will use the <code class="literal">LDA</code> model—the <span class="strong"><strong>Latent Dirichlet Allocation</strong></span> model:</p><div class="informalexample"><pre class="programlisting">clustering = clus.LDA(k=2, 
    optimizer='online', 
    featuresCol=stringIndexer.getOutputCol())</pre></div><p>The <code class="literal">k</code> parameter specifies how many topics we expect to see, the <code class="literal">optimizer</code> parameter can be either <code class="literal">'online'</code> or <code class="literal">'em' </code>(the latter standing for the Expectation Maximization algorithm).</p><p>Putting these puzzles together results in, so far, the longest of our pipelines:</p><div class="informalexample"><pre class="programlisting">pipeline = ml.Pipeline(stages=[
        tokenizer, 
        stopwords,
        stringIndexer, 
        clustering]
)</pre></div><p>Have we properly uncovered the topics? Well, let's see:</p><div class="informalexample"><pre class="programlisting">topics = pipeline \
    .fit(text_data) \
    .transform(text_data)
topics.select('topicDistribution').collect()</pre></div><p>Here's what we get:</p><div class="mediaobject"><img src="images/B05793_06_19.jpg" alt="Topic mining"/></div><p>Looks like<a id="id429" class="indexterm"/> our method discovered all the topics properly! Do not get used to <a id="id430" class="indexterm"/>seeing such good results though: sadly, real world data is seldom that kind.</p></div></div><div class="section" title="Regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec77"/>Regression</h2></div></div></div><p>We could not<a id="id431" class="indexterm"/> finish a chapter on a machine learning library without<a id="id432" class="indexterm"/> building a regression model.</p><p>In this section, we will try to predict the <code class="literal">MOTHER_WEIGHT_GAIN</code> given some of the features described here; these are contained in the features listed here:</p><div class="informalexample"><pre class="programlisting">features = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',
            'MOTHER_PRE_WEIGHT','DIABETES_PRE',
            'DIABETES_GEST','HYP_TENS_PRE', 
            'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',
            'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI', 
            'CIG_3_TRI'
           ]</pre></div><p>First, since all the features are numeric, we will collate them together and use the <code class="literal">ChiSqSelector</code> to select only the top six most important features:</p><div class="informalexample"><pre class="programlisting">featuresCreator = ft.VectorAssembler(
    inputCols=[col for col in features[1:]], 
    outputCol='features'
)
selector = ft.ChiSqSelector(
    numTopFeatures=6, 
    outputCol="selectedFeatures", 
    labelCol='MOTHER_WEIGHT_GAIN'
)</pre></div><p>In order to predict the weight gain, we will use the gradient boosted trees regressor:</p><div class="informalexample"><pre class="programlisting">import pyspark.ml.regression as reg
regressor = reg.GBTRegressor(
    maxIter=15, 
    maxDepth=3,
    labelCol='MOTHER_WEIGHT_GAIN')</pre></div><p>Finally, again, we put it all together into a <code class="literal">Pipeline</code>:</p><div class="informalexample"><pre class="programlisting">pipeline = Pipeline(stages=[
        featuresCreator, 
        selector,
        regressor])
weightGain = pipeline.fit(births_train)</pre></div><p>Having created the <code class="literal">weightGain</code> model, let's see if it performs well on our testing data:</p><div class="informalexample"><pre class="programlisting">evaluator = ev.RegressionEvaluator(
    predictionCol="prediction", 
    labelCol='MOTHER_WEIGHT_GAIN')
print(evaluator.evaluate(
     weightGain.transform(births_test), 
    {evaluator.metricName: 'r2'}))</pre></div><p>We get the following output:</p><div class="mediaobject"><img src="images/B05793_06_20.jpg" alt="Regression"/></div><p>Sadly, the<a id="id433" class="indexterm"/> model is no better than a flip of a coin. It looks that without<a id="id434" class="indexterm"/> additional independent features that are better correlated with the <code class="literal">MOTHER_WEIGHT_GAIN</code> label, we will not be able to explain its variance sufficiently.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we went into details of how to use PySpark ML: the official main machine learning library for PySpark. We explained what the <code class="literal">Transformer</code> and <code class="literal">Estimator</code> are, and showed their role in another concept introduced in the ML library: the <code class="literal">Pipeline</code>. Subsequently, we also presented how to use some of the methods to fine-tune the hyper parameters of models. Finally, we gave some examples of how to use some of the feature extractors and models from the library.</p><p>In the next chapter, we will delve into graph theory and GraphFrames that help in tackling machine learning problems better represented as graphs.</p></div></div>
</body></html>