- en: Chapter 10. R and Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come to the final chapter of this book where we will go to the very
    limits of large-scale data processing. The term *Big Data* has been used to describe
    the ever growing volume, velocity, and variety of data being generated on the
    Internet in connected devices and many other places. Many organizations now have
    massive datasets that measure in petabytes (one petabyte is 1,048,576 gigabytes),
    more than ever before. Processing and analyzing Big Data is extremely challenging
    for traditional data processing tools and database architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In 2005, Doug Cutting and Mike Cafarella at Yahoo! developed Hadoop, based on
    earlier work by Google, to address these challenges. They set out to develop a
    new data platform to process, index, and query billions of web pages efficiently.
    With Hadoop, the work which would have previously required very expensive supercomputers
    can now be done on large clusters of inexpensive standard servers. As the volume
    of data grows, more servers can simply be added to a Hadoop cluster to increase
    the storage capacity and computing power. Since then, Hadoop and its ecosystem
    of tools has become one of the most popular suites of tools to collect, store,
    process and analyze large datasets. In this chapter, we will learn how to tap
    into the power of Hadoop from R.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Hadoop on Amazon Web Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing large datasets in batches using RHadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we learn how to use Hadoop (for more information refer to [http://hadoop.apache.org/](http://hadoop.apache.org/))
    and related tools in R, we need to understand the basics of Hadoop. For our purposes,
    it suffices to know that Hadoop comprises two key components: the **Hadoop Distributed
    File System (HDFS)** and the **MapReduce** framework to execute data processing
    tasks. Hadoop includes many other components for task scheduling, job management,
    and others, but we shall not concern ourselves with those in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: HDFS, as the name suggests, is a virtual filesystem that is distributed across
    a cluster of servers. HDFS stores files in blocks, with a default block size of
    128 MB. For example, a 1 GB file is split into eight blocks of 128 MB, which are
    distributed to different servers in the cluster. Furthermore, to prevent data
    loss due to server failure, the blocks are replicated. By default, they are replicated
    three times—there are three copies of each block of data in the cluster, and each
    copy is stored on a different server. That way, even if a few servers in the cluster
    fail, the data is not lost and can be re-replicated to ensure high availability.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce is the framework to process the data stored in HDFS in a data parallel
    way. Notice how the distributed nature of data storage makes Hadoop a good fit
    for data parallel algorithms that we learned about in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*—the chunks of data stored on each worker node are processed simultaneously
    in parallel, and then the results from each node are combined to produce the final
    results. MapReduce works very similarly to the data parallel algorithms in [Chapter
    8](ch08.html "Chapter 8. Multiplying Performance with Parallel Computing"), *Multiplying
    Performance with Parallel Computing*, except that the data already resides in
    the worker nodes; it does not have to be distributed every time a task is run
    as was the case with a cluster of servers that run R. **Map** refers to the step
    of performing computations on the data in each worker node, or mapping data to
    their corresponding output. **Reduce** refers to the process of combining, or
    reducing the results of the worker nodes into the final results.
  prefs: []
  type: TYPE_NORMAL
- en: Data in MapReduce is represented as key-value pairs. Every MapReduce operation
    is essentially a transformation from one set of key-value pairs to another set
    of key-value pairs. A **mapper** might, for example, read a single customer record
    from a database and produce a key-value pair such as `("Alice` `", 32)` that contains
    the name of a customer (`"Alice"`) as the key and the reward points she or he
    collected in a given week (`32`) as the corresponding value. After the map step,
    all the key-value pairs are sorted by the key, and the pairs with the same key
    are given to individual **reducers**. So, for example, there would be one reducer
    for all pairs with the key `"Alice"`, another reducer for the key `"Bob"`, and
    another for `"Charlie"`. A reducer takes all the key-value pairs it is given,
    performs computations on them, and returns the results as another key-value pair.
  prefs: []
  type: TYPE_NORMAL
- en: The reducers in our simple example could compute the mean of weekly reward points
    collected by all customers. The MapReduce system then collects the results of
    all the reducers as the final output, which could be something like `[("Alice",
    26.5), ("Bob", 42.3), ("Charlie", 35.6), ...]`.
  prefs: []
  type: TYPE_NORMAL
- en: While HDFS and MapReduce are the foundation of Hadoop, they are not suited for
    all data processing tasks. One key reason is that data stored in HDFS resides
    on the hard drives of the servers. Each time a MapReduce task is performed, the
    data has to be read from the disk, and the results of the computations are written
    back to the disk. Thus, HDFS and MapReduce perform reasonably well for sizeable
    batch processing tasks where the time to complete the computational tasks exceeds
    the overheads of reading/writing data and other overheads of running a Hadoop
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Hadoop on Amazon Web Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to set up a Hadoop cluster. We can install Hadoop on a single
    server in pseudo-distributed mode to simulate a cluster, or on an actual cluster
    of servers, or virtual machines in fully distributed mode. There are also several
    distributions of Hadoop available from the vanilla open source version provided
    by the Apache Foundation to commercial distributions such as Cloudera, Hortonworks,
    and MapR. Covering all the different ways of setting up Hadoop is beyond the scope
    of this book. We instead provide instructions for one way to set up Hadoop and
    other relevant tools for the purpose of the examples in this chapter. If you are
    using an existing Hadoop cluster or setting up one in a different way, you might
    have to modify some of the steps.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because Hadoop and its associated tools are mostly developed for Linux/Unix
    based operating systems, the code in this chapter will probably not work on Windows.
    If you are a Windows user, follow the instructions in this chapter to set up Hadoop,
    R, and the required packages on Amazon Web Services.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) has a service called **Elastic MapReduce**
    (**EMR**) that allows us to rent and run a Hadoop cluster on an hourly basis.
    Creating a Hadoop cluster is as simple as specifying the number of servers in
    the cluster, the size of each server, and the instructions to set up the required
    tools on each server. To set up an account with AWS, follow the instructions in
    *Preface*. Running the examples in this chapter on AWS will cost some money for
    as long as the EMR cluster is running. Check this link out [http://aws.amazon.com/elasticmapreduce/pricing/](http://aws.amazon.com/elasticmapreduce/pricing/)
    for the latest EMR prices.'
  prefs: []
  type: TYPE_NORMAL
- en: We also need a script that sets up the required tools on each server. Save the
    following script as `emr-bootstrap.sh`. This scripts installs the R packages needed
    for this chapter, including `rhdfs`, `rmr2`, and `R.utils` on every server in
    the Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload `emr-bootstrap.sh` into the AWS Simple Storage Service (S3) so that
    the EMR servers can pick it up during the first run. To do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the AWS console, and click on **S3**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new bucket to store the script in by clicking on **Create Bucket**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the bucket that was just created and click on **Upload** to upload
    the script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, follow these steps to create the Hadoop cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the AWS Console and click on **EMR**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create cluster**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Software Configuration**, select the Amazon Hadoop distribution (the
    examples in this chapter were tested with Amazon Machine Image (AMI) version 3.2.1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove Hive and Pig from the applications list, as they are not needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Hardware Configuration**, select the instance types for the Hadoop servers.
    The instance types for both the master and core nodes should have at least 15
    GB of RAM, such as the `m1.xlarge` or `m3.xlarge` instance types. Enter the number
    of nodes you would like in the cluster. Given the default HDFS replication factor
    of three, there should be at least three core nodes. Task nodes are optional.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Security and Access**, select the EC2 key pair to log in to the cluster
    with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Bootstrap Actions**, select **Custom action,** then click on **Configure
    and add**. In the dialog box that appears under **S3 location**, enter or browse
    for the S3 location where `emr-bootstrap.sh` was uploaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Optional) Enable logging under **Cluster Configuration** to have all Hadoop
    logs automatically stored in the S3 bucket. To use this option, first create an
    S3 bucket to store the logs in, and enter the name of the bucket in the **Log
    folder S3 location** field. While optional, storing Hadoop logs is useful for
    tracing errors and debugging, which can be challenging without the logs, as an
    executed program gets spawned across multiple processes and computer nodes in
    Hadoop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create cluster** and wait a few minutes while the cluster is being
    set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the EMR cluster is ready, get the Master Public DNS from the cluster details
    page, and log in to the master server from the command line using your AWS EC2
    security key (replace `hadoop.pem` with the name of your key):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you are logged in, run R, which comes preinstalled with the EMR cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Processing large datasets in batches using Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch processing is the most basic type of task that HDFS and MapReduce can
    perform. Similar to the data parallel algorithms in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*, the master node sends a set of instructions to the worker nodes, which
    execute the instructions on the blocks of data stored on them. The results are
    then written to the disk in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: When an aggregate result is required, both the map and reduce steps are performed
    on the data. For example, in order to compute the mean of a distributed dataset,
    the mappers on the worker nodes first compute the sum and number of elements in
    each local chunk of data. The reducers then add up all these results to compute
    the global mean.
  prefs: []
  type: TYPE_NORMAL
- en: At other times, only the map step is performed when aggregation is not required.
    This is common in data transformation or cleaning operations where the data is
    simply being transformed form one format to another. One example of this is extracting
    email addresses from a set of documents. In this case, the results of the mappers
    on the worker nodes are stored as new datasets in HDFS, and reducers are not needed.
  prefs: []
  type: TYPE_NORMAL
- en: The R community has developed several packages to perform MapReduce tasks from
    R. One of these is the RHadoop family of packages developed by Revolution Analytics
    (for more information refer to [https://github.com/RevolutionAnalytics/RHadoop](https://github.com/RevolutionAnalytics/RHadoop)).
    RHadoop includes the packages `rhdfs`, which provides functions to manipulate
    files and directories in HDFS, and `rmr2`, which exposes the functionality of
    MapReduce as R functions. These functions make it easy to use MapReduce without
    having to program with the Hadoop Java APIs. Instead, `rmr2` runs a copy of R
    on every worker node, and the mappers and reducers are written as R functions
    to be applied on each chunk of data.
  prefs: []
  type: TYPE_NORMAL
- en: If you did not use the Hadoop setup instructions in the preceding section, follow
    the installation instructions for `rhdfs` and `rmr2` at [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki).
  prefs: []
  type: TYPE_NORMAL
- en: Uploading data to HDFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to do for this is to get data into HDFS. For this chapter, we
    will use the Google Books Ngrams data (for more information refer to [http://storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)).
    Here, n-grams are consecutive words that appear in the text where *n* represents
    the number of words in a phrase—a 1-gram is simply a word (for example, "Batman"),
    a 2-gram is two consecutive words (for example, "Darth Vader"), and a 6-gram is
    six consecutive words (for example, "Humpty Dumpty sat on a wall"). We will use
    the data of 1-grams for our examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset for this chapter is large enough to test the performance of Hadoop
    on a small cluster, but it is still relatively small compared to many other real-world
    datasets. Tools such as `ffdf` ([Chapter 7](ch07.html "Chapter 7. Processing Large
    Datasets with Limited RAM"), *Processing Large Datasets with Limited RAM*) can
    probably be used to process this dataset on a single machine. But when the data
    size gets much larger, Hadoop or other Big Data tools might be the only way to
    process the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code downloads the 1-grams data and uploads them into HDFS. Google
    provides the data in separate files, with one file for each letter of the alphabet
    containing the words that start with that letter. In this code, `hdfs.init()`
    first initializes the connection to HDFS. Then, `hdfs.mkdir()` creates the directory
    `/ngrams/data` in HDFS where the data will be stored. The code in the `for` loop
    downloads each file, decompresses it, and uploads it to HDFS using `hdfs.put()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that all the files have been uploaded successfully into HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing HDFS data with RHadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the data is loaded into HDFS, we can use MapReduce to analyze the data.
    Say we want to compare the popularity of Batman versus Superman since the 1950s.
    The Google Ngrams data might provide some insight into that.
  prefs: []
  type: TYPE_NORMAL
- en: Each line of the Ngrams data is a tab-seperated list of values, starting with
    the Ngram, followed by the year, number of occurrences of that Ngram, and number
    of books that the Ngram appeared in. For example, the following command line indicates
    that in 1978, the word "mountain" appeared 1,435,642 times in 1,453 books in the
    Google Books library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To compare the popularity of Batman and Superman, we need to find the lines
    of code that represent these two words from 1950 onwards and collate the occurrence
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the data consists of tab-separated text files, we need to specify the
    input format so that the `rmr2` functions know how to read the files. This can
    be done using the `make.input.format()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For delimited text files such as comma-separated values or tab-separated values,
    `make.input.format()` accepts most of the same arguments as `read.table()`. In
    fact, `rmr2` uses `read.table()` to read each chunk of data into a data frame.
  prefs: []
  type: TYPE_NORMAL
- en: Besides delimited text files, `rmr2` can also read/write data as raw text (`format
    = "text"`), JSON (`"json"`), R's internal data serialization format (`"native"`),
    Hadoop SequenceFiles (`"sequence.typedbytes"`), HBase tables (`"hbase"`), and
    Hive or Pig (`"pig.hive"`). See the package documentation for the arguments associated
    with these data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The map step of our analysis involves filtering each line of data to find the
    relevant records. We will define a `mapper` function, as shown in the following
    code, that accepts a set of keys and a set of values as arguments. Since the Ngrams
    data does not contain keys, the `keys` argument is `NULL`. The argument `values`
    is a data frame that contains a chunk of data. The mapper function looks for rows
    of the data frame that contain the words we are interested in, for the year 1950
    or later. If any relevant rows are found, the `keyval()` function is called to
    return key-value pairs that will be passed to the reduce function. In this case,
    the keys are the words and the values are the corresponding years and occurrence
    counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with MapReduce, you might have noticed that `rmr2` allows
    the mapper to accept and emit key-value pairs as lists and data frames that represent
    a whole chunk of data instead of one record at a time, as is the case with classical
    MapReduce. This can help with R's performance; vectorized R operations can be
    used to process the whole chunk of data.
  prefs: []
  type: TYPE_NORMAL
- en: The next step occurs behind the scenes, where MapReduce collects all the data
    emitted by the mappers and groups them by key. In this example, it will find two
    groups that correspond to the `"batman"` and `"superman"` keys. MapReduce then
    calls the reducer function to process one group of data at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The job of the reducer, given the data for a particular superhero, is to sum
    the number of occurrences of this superhero''s name by year, using `tapply()`.
    This is required because the words in the Ngrams dataset are case sensitive. So,
    for example, we need to add up the number of times that "Batman", "batman", and
    "BATMAN" appear in each year. The reducer then returns the superhero''s name as
    the key, and a data frame that contains the total number of occurrences by year
    as the value. The code for the reducer is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined our mapper and reducer functions, we can execute the
    MapReduce job using `mapreduce()`. We will call this function, specify the input
    directory and data format, the output directory where the results are to be written,
    and the mapper and reducer functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When this MapReduce job executes, the resultant key-value pairs are written
    to HDFS in the `/ngrams/batmanVsuperman` folder. We can use `from.dfs()` to retrieve
    the results from HDFS into R objects. This function returns a list with two components:
    `key` and `value`. In this case, `key` is a character vector that specifies the
    superhero''s name for each row of data, and `val` is a data frame that contains
    the corresponding years and occurrence counts.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the results in order to compare how popular these two superheroes
    have been over the years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing HDFS data with RHadoop](img/9263OS_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Popularity of Batman versus Superman since the 1950s, according to Google Books
  prefs: []
  type: TYPE_NORMAL
- en: While both the superheroes popularity steadily increased over the years, there
    is an interesting spike in the number of times Superman was mentioned in books
    in the 1970s. This could be due to the release of the multi Academy Award-winning
    film, *Superman* starring Christopher Reeve in 1978\. However, this surge in popularity
    was short-lived.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time it takes to complete the MapReduce algorithm depends on the size of
    the data, the complexity of the task, and the number of nodes in the cluster.
    We tested this example using the `m1.xlarge` AWS servers, which have 4 CPUs and
    15 GB of RAM each, with cluster sizes ranging from 4 to 32 core nodes (in EMR
    terminology, these are nodes that store data and process them). The following
    figure shows how the execution time decreases as more nodes are added to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing HDFS data with RHadoop](img/9263OS_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Execution time as cluster size increases
  prefs: []
  type: TYPE_NORMAL
- en: Because `rmr2` starts an instance of R on each Hadoop node to process the data,
    the efficiency of the MapReduce task depends on that of the R code for the mapper
    and reducer functions. Many of the techniques in this book to improve the performance
    of serial R programs can be applied when you design the mapper and reducer functions
    too. Furthermore, every MapReduce job incurs overheads of starting a new job,
    reading data from the disk, and coordinating the execution of the job across the
    cluster. Where possible, combining individual tasks into larger MapReduce tasks
    that can be executed at one go will help to improve the overall performance by
    reducing these overheads.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done using the Hadoop cluster, remember to terminate the cluster
    from the AWS EMR console to prevent unexpected charges.
  prefs: []
  type: TYPE_NORMAL
- en: Other Hadoop packages for R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the scope of this book allows us to cover only a few R packages that
    interface with Hadoop, the community has developed many more packages to bring
    the power of Hadoop to R. Here are a few more packages that can be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides `rhdfs` and `rmr2`, RHadoop also provides other packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`plyrmr`: It provides functionality similar to `plyr` on MapReduce'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rhbase`: It provides functions to work with HBase data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ravro`: It provides reading/writing of data in the Avro format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another family of packages called `RHIPE` (for more information refer to [http://www.datadr.org/](http://www.datadr.org/))
    provides similar MapReduce capabilities with a slightly different syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RHIPE`: This package provides the core HDFS and MapReduce functionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datadr`: It provides data manipulation capabilities similar to `plyr`/`dplyr`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Trelliscope`: It provides visualization of large datasets in HDFS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, `RHIPE` does not support YARN or MapReduce 2.0\. An
    older version of Hadoop is required to use the `RHIPE` packages until this is
    fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Another package, `Segue` (for more information refer to [https://code.google.com/p/segue/](https://code.google.com/p/segue/))
    takes a different approach. It does not provide full MapReduce capabilities. Rather,
    it treats Amazon's EMR as an additional computational resource for computationally
    heavy R tasks. This is similar to cluster computing in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*, but using EMR as the computational cluster. The `Segue` package provides
    the `emrlapply()` function that performs a parallel `lapply` operation on an EMR
    cluster; this is analogous to `mclapply()` from the `parallel` package.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to set up a Hadoop cluster on Amazon Elastic
    MapReduce, and how to use the RHadoop family of packages in order to analyze data
    in HDFS using MapReduce. We saw how the performance of the MapReduce task improves
    dramatically as more servers are added to the Hadoop cluster, but the performance
    eventually reaches a limit due to Amdahl's law ([Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*).
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and its ecosystem of tools is rapidly evolving. Other tools are being
    actively developed to make Hadoop perform even better. For example, Apache Spark
    ([http://spark.apache.org/](http://spark.apache.org/)) provides Resilient Distributed
    Datasets (RDDs) that store data in memory across a Hadoop cluster. This allows
    data to be read from HDFS once and to be used many times in order to dramatically
    improve the performance of interactive tasks like data exploration and iterative
    algorithms like gradient descent or k-means clustering. Another example is Apache
    Storm ([http://storm.incubator.apache.org/](http://storm.incubator.apache.org/))
    that allows you to process real-time data streams. Because these tools and their
    associated R interfaces are being actively developed, they will likely change
    by the time you read this book, so we have decided not to include them here. But
    they are worth looking into if you have specific needs like in-memory analytics
    or real-time data processing.
  prefs: []
  type: TYPE_NORMAL
- en: We have come to the end of the book. It has been an exhilarating journey looking
    at a whole spectrum of techniques to improve the performance of R programs, from
    optimizing memory utilization and computational speed to multiplying computational
    power with parallel programming and cluster computing. What we have covered here
    is only the basics; there is much more to learn about writing more efficient R
    code. There are other resources that dive into specific topics in far greater
    detail than we can here. Package documentation is always useful to read, though
    sometimes cryptic; sometimes, the only way to find out what works is to try. Of
    course, there is the great community of R users in online forums, mailing lists
    and other places, who are always eager to help with answers and tips.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that you have enjoyed this book and learned from it as much as we have
    writing it. Thank you for joining us in this journey, and we wish you the very
    best in exploring the world of R high-performance computing.
  prefs: []
  type: TYPE_NORMAL
