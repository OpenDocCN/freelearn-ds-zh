- en: Chapter 4. Learning the Naïve Bayes Classification Using Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the Naïve Bayes classification algorithm to classify
    a set of documents. Classifying text documents is a little tricky because of the
    data preparation steps involved. In this chapter, we will explore the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability and the Bayes rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Naïve Bayes algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding terms used in text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Naïve Bayes algorithm in Apache Mahout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing conditional probability and the Bayes rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before learning the Naïve Bayes algorithm, you should have an understanding
    of conditional probability and the Bayes rule.
  prefs: []
  type: TYPE_NORMAL
- en: In very simple terms, conditional probability is the probability that something
    will happen, given that something else has already happened. It is expressed as
    *P(A/B)*, which can be read as probability of A given B, and it finds the probability
    of the occurrence of event A once event B has already happened.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing conditional probability and the Bayes rule](img/4959OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, if you choose a card from a standard card deck and if you were
    asked about the probability for the card to be a diamond, you would quickly say
    13/52 or 0.25, as there are 13 diamond cards in the deck. However, if you then
    look at the card and declare that it is red, then we will have narrowed the possibilities
    for the card to 26 possible cards, and the probability that the card is a diamond
    now is 13/26 = 0.5\. So, if we define A as a diamond card and B as a red card,
    then *P(A/B)* will be the probability of the card being a diamond, given it is
    red.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, for a given pair of events, conditional probability is hard to calculate,
    and Bayes' theorem helps us here by giving the relationship between two conditional
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' theorem is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing conditional probability and the Bayes rule](img/4959OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The terms in the formula are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**P(A)**: This is called prior probability or prior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P(B/A)**: This is called conditional probability or likelihood'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P(B)**: This is called marginal probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P(A/B)**: This is called posterior probability or posterior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following formula is derived only from the conditional probability formula.
    We can define *P(B/A)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing conditional probability and the Bayes rule](img/4959OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When rearranged, the formula becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing conditional probability and the Bayes rule](img/4959OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, from the preceding conditional probability formula, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing conditional probability and the Bayes rule](img/4959OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's take an example that will help us to understand how Bayes' theorem is
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'A cancer test gives a positive result with a probability of 97 percent when
    the patient is indeed affected by cancer, while it gives a negative result with
    99 percent probability when the patient is not affected by cancer. If a patient
    is drawn at random from a population where 0.2 percent of the individuals are
    affected by cancer and he or she is found to be positive, what is the probability
    that he or she is indeed affected by cancer? In probabilistic terms, what we know
    about this problem can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (positive| cancer) = 0.97*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (positive| no cancer) = 1-0.99 = 0.01*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (cancer) = 0.002*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (no cancer) = 1-0.002= 0.998*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (positive) = P (positive| cancer) P (cancer) + P (positive| no cancer) P
    (no cancer)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.97*0.002 + 0.01*0.998*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.01192*'
  prefs: []
  type: TYPE_NORMAL
- en: Now *P (cancer| positive) = (0.97*0.002)/0.01192 = 0.1628*
  prefs: []
  type: TYPE_NORMAL
- en: So even when found positive, the probability of the patient being affected by
    cancer in this example is around 16 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Naïve Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Bayes'' theorem, we have seen that the outcome is based only on one evidence,
    but in classification problems, we have multiple evidences and we have to predict
    the outcome. In Naïve Bayes, we uncouple multiple pieces of evidence and treat
    each one of them independently. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (outcome | multiple Evidence) ) = P (Evidence 1|outcome)* P (Evidence 2|outcome)*
    P (Evidence 3|outcome) …. /P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: Run this formula for each possible outcome. Since we are trying to classify,
    each outcome will be called a class. Our task is to look at the evidence (features)
    to consider how likely it is for it to be of a particular class and then assign
    it accordingly. The class that has the highest probability gets assigned to that
    combination of evidences. Let's understand this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we have data on 1,000 pieces of fruit. They happen to be bananas,
    apples, or some other fruit. We are aware of three characteristics of each fruit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Size**: They are either long or not long'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Taste**: They are either sweet or not sweet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Color**: They are either yellow or not yellow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assume that we have a dataset like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fruit type | Taste – sweet | Taste – not sweet | Color – yellow | Color –
    not yellow | Size – long | Size – not long | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Banana** | 350 | 150 | 450 | 50 | 400 | 100 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| **Apple** | 150 | 150 | 100 | 200 | 0 | 300 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| **Other** | 150 | 50 | 50 | 150 | 100 | 100 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total** | 650 | 350 | 600 | 400 | 500 | 500 | 1000 |'
  prefs: []
  type: TYPE_TB
- en: 'Now let''s look at the things we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Banana) = 500/1000 = 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Apple) = 300/1000 = 0.3*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Other) = 200/1000 = 0.2*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the probability of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Sweet) = 650/1000 = 0.65*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Yellow) = 600/1000 = 0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (long) = 500/1000 = 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (not Sweet) = 350/1000 = 0.35*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (not yellow) = 400/1000= 0.4*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (not long) = 500/1000 = 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to know what fruit we will have if it is not yellow and not long
    and sweet. The probability of it being an apple is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Apple| sweet, not long, not yellow) = P (sweet | Apple)* P (not long |
    Apple)* P (not yellow | Apple)*P (Apple)/P (sweet)* P (not long) *P (not yellow)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.5*1*0.67*0.3/P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.1005/P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of it being a banana is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (banana| sweet, not long, not yellow) = P (sweet | banana)* P (not long
    | banana)* P (not yellow | banana)*P (banana)/P (sweet)* P (not long) *P (not
    yellow)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.7*0.2*0.1*0.5/P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.007/P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of it being any other fruit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (other fruit| sweet, not long, not yellow) = P (sweet | other fruit)* P
    (not long | other fruit)* P (not yellow | other fruit) *P (other fruit)/P (sweet)*
    P (not long) *P (not yellow)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.75*0.5*0.75*0.2/P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.05625/ P (Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: So from the results, you can see that if the fruit is sweet, not long, and not
    yellow, then the highest probability is that it will be an apple. So find out
    the highest probability and assign the unknown item to that class.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes is a very good choice for text classification. Before we move on
    to text classification using Naïve Bayes in Mahout, let's understand a few terms
    that are really useful for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the terms used in text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To prepare data so that it can be used by a classifier is a complex process.
    From raw data, we can collect explanatory and target variables and encode them
    as **vectors**, which is the input of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors are ordered lists of values as defined in two-dimensional space. You
    can take a clue from coordinate geometry as well. A point (3, 4) is a point in
    the x and y planes. In Mahout, it is different. Here, a vector can have (3, 4)
    or 10,000 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mahout provides support for creating vectors. There are two types of vector
    implementations in Mahout: sparse and dense vectors. There are a few terms that
    we need to understand for text classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag of words**: This considers each document as a collection of words. This
    ignores word order, grammar, and punctuation. So, if every word is a feature,
    then calculating the feature value of the document word is represented as a token.
    It is given the value 1 if it is present or 0 if not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Term frequency**: This considers the word count in the document instead of
    0 and 1\. So the importance of a word increases with the number of times it appears
    in the document. Consider the following example sentence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple has launched iPhone and it will continue to launch such products. Other
    competitors are also planning to launch products similar to that of iPhone.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is the table that represents term frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Term | Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Apple | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Launch | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Product | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Plan | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'The following techniques are usually applied to come up with this type of table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming of words**: With this, the suffix is removed from the word so "launched",
    "launches", and "launch" are all considered as "launch".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case normalization**: With this, every term is converted to lowercase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop word removal**: There are some words that are almost present in every
    document. We call these words stop words. During an important feature extraction
    from a document, these words come into account and they will not be helpful in
    the overall calculation. Examples of these words are "is, are, the, that, and
    so on." So, while extracting, we will ignore these kind of words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverse document frequency**: This is considered as the boost a term gets
    for being rare. A term should not be too common. If a term occurs in every document,
    it is not good for classification. The fewer documents in which a term occurs,
    the more significant it is likely to be for the documents it does occur in. For
    a term t, inverse document frequency is calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*IDF (t) = 1 + log* (total number of documents/ number of documents containing
    t)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Term frequency and inverse term frequency**: This is one of the popular representations
    of the text. It is the product of term frequency and inverse document frequency,
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TFIDF (t, d) = TF (t, d) * IDF (t)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each document is a feature vector and a collection of documents is a set of
    these feature vectors and this set works as the input for the classification.
    Now that we understand the basic concepts behind the vector creation of text documents,
    let's move on to the next section where we will classify text documents using
    the Naïve Bayes algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Naïve Bayes algorithm in Apache Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a dataset of 20 newsgroups for this exercise. The 20 newsgroups
    dataset is a standard dataset commonly used for machine learning research. The
    data is obtained from transcripts of several months of postings made in 20 Usenet
    newsgroups from the early 1990s. This dataset consists of messages, one per file.
    Each file begins with header lines that specify things such as who sent the message,
    how long it is, what kind of software was used, and the subject. A blank line
    follows and then the message body follows as unformatted text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `20news-bydate.tar.gz` dataset from [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).
    The following steps are used to build the Naïve Bayes classifier using Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `20newsdata` directory and unzip the data here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see two folders under `20newsdata: 20news-bydate-test` and `20news-bydate-train`.
    Now create another directory called `20newsdataall` and merge both the training
    and test data of the 20 newsgroups.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Come out of the directory and move to the `home` directory and execute the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a directory in Hadoop and save this data in HDFS format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the raw data into a sequence file. The `seqdirectory` command will
    generate sequence files from a directory. Sequence files are used in Hadoop. A
    sequence file is a flat file that consists of binary key/value pairs. We are converting
    the files into sequence files so that it can be processed in Hadoop, which can
    be done using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command can be seen in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the Naïve Bayes algorithm in Apache Mahout](img/4959OS_04_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Convert the sequence file into a sparse vector using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The terms used in the preceding command are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`lnorm`: This is for the output vector to be log normalized'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nv`: This refers to named vectors'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wt`: This refers to the kind of weight to use; here, we use `tfidf`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the preceding command on the console is shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the Naïve Bayes algorithm in Apache Mahout](img/4959OS_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Split the set of vectors to train and test the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The terms used in the preceding command are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`randomSelectionPct`: This divides the percentage of data into testing and
    training datasets. Here, 60 percent is for testing and 40 percent for training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xm`: This refers to the execution method to use: sequential or mapreduce.
    The default is `mapreduce`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Using the Naïve Bayes algorithm in Apache Mahout](img/4959OS_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command on the console is shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the Naïve Bayes algorithm in Apache Mahout](img/4959OS_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We get the result of our Naïve Bayes classifier for the 20 newsgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the Naïve Bayes algorithm. This algorithm is a
    simplistic yet highly regarded statistical model that is widely used in both industry
    and academia, and it produces good results on many occasions. We initially discussed
    conditional probability and the Bayes rule. We then saw an example of the Naïve
    Bayes algorithm. You learned about the approaches to convert text into a vector
    format, which is an input for classifiers. Finally, we used the 20 newsgroups
    dataset to build a classifier using the Naïve Bayes algorithm in Mahout. In the
    next chapter, we will continue our journey of exploring classification algorithms
    in Mahout with the Hidden Markov model implementation.
  prefs: []
  type: TYPE_NORMAL
