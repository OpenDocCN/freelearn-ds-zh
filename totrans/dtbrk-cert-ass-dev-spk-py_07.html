<html><head></head><body>
		<div><h1 id="_idParaDest-182" class="chapter-number"><a id="_idTextAnchor183"/>7</h1>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor184"/>Structured Streaming in Spark</h1>
			<p>The world of data processing has evolved rapidly as data volume and data velocity increase every day. With that, the need to analyze and derive insights from real-time data is becoming increasingly crucial. Structured Streaming, a component of Apache Spark, has emerged as a powerful framework to process and analyze data streams in real time. This chapter delves into the realm of Structured Streaming, exploring its capabilities, features, and real-world applications.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Real-time data processing</li>
				<li>The fundamentals of streaming</li>
				<li>Streaming architectures</li>
				<li>Spark Streaming</li>
				<li>Structured Streaming</li>
				<li>Streaming sources and sinks</li>
				<li>Advanced topics in Structured Streaming</li>
				<li>Joins in Structured Streaming</li>
			</ul>
			<p>By the end of this chapter, you will understand Spark Streaming and the power of real-time data insights.</p>
			<p>We will start by looking at what real-time data processing means.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor185"/>Real-time data processing</h1>
			<p>Real-time data processing has become<a id="_idIndexMarker490"/> increasingly critical in today’s fast-paced and data-driven world. Organizations need to analyze and derive insights from data as it arrives, enabling them to make timely decisions and take immediate action. Spark Streaming, a powerful component of Apache Spark, addresses this need by providing a scalable and fault-tolerant framework to process real-time data streams.</p>
			<p>Real-time data processing has gained immense importance in various industries, ranging from finance and e-commerce to the Internet of Things (IoT) and social media. Traditional batch processing approaches, while suitable for many scenarios, fall short when immediate insights and actions are required. Real-time data processing fills this gap by enabling the analysis and processing of data as it arrives, allowing organizations to make timely decisions and respond quickly to changing conditions.</p>
			<p>Real-time data processing involves the continuous<a id="_idIndexMarker491"/> ingestion, processing, and analysis of streaming data. Unlike <strong class="bold">batch processing</strong>, which operates on static datasets, real-time data processing systems handle data that is generated and updated in real time. This data can be sourced from various channels, including sensors, logs, social media feeds, and financial transactions.</p>
			<p>The key characteristics <a id="_idIndexMarker492"/>of real-time data processing are as follows:</p>
			<ul>
				<li><strong class="bold">Low latency</strong>: Real-time data processing aims to minimize the time delay between data generation and processing. It requires fast and efficient processing capabilities to provide near-instantaneous insights and responses.</li>
				<li><strong class="bold">Scalability</strong>: Real-time data processing systems must be able to handle high-volume and high-velocity data streams. The ability to scale horizontally and distribute processing across multiple nodes is essential to accommodate the increasing data load.</li>
				<li><strong class="bold">Fault tolerance</strong>: Given the continuous nature of streaming data, real-time processing systems need to be resilient to failures. They should have mechanisms in place to recover from failures and ensure uninterrupted processing.</li>
				<li><strong class="bold">A streaming data model</strong>: Real-time data processing<a id="_idIndexMarker493"/> systems operate on <strong class="bold">streaming data</strong>, which is an unbounded sequence of events or records. Streaming data models are designed to handle the continuous flow of data and provide mechanisms<a id="_idIndexMarker494"/> for event-time and window-based computations.</li>
			</ul>
			<p>These characteristics of real-time data processing lead to several advantages, including the following:</p>
			<ul>
				<li><strong class="bold">A rapid response</strong>: Real-time processing<a id="_idIndexMarker495"/> enables organizations to respond quickly to changing conditions, events, or opportunities. It allows for timely actions, such as fraud detection, anomaly detection, real-time monitoring, and alerting.</li>
				<li><strong class="bold">Personalization</strong>: Real-time processing enables personalized experiences by analyzing and acting on user behavior in real time. It powers real-time recommendations, dynamic pricing, targeted advertising, and content personalization.</li>
				<li><strong class="bold">Operational efficiency</strong>: Real-time processing provides insights into operational processes, allowing organizations to optimize their operations, identify bottlenecks, and improve efficiency in real time. It facilitates predictive maintenance, supply chain optimization, and real-time resource allocation.</li>
				<li><strong class="bold">Situational awareness</strong>: Real-time data processing helps organizations gain situational awareness by continuously analyzing and aggregating data from various sources. It enables real-time analytics, monitoring, and decision making in domains such as cybersecurity, financial markets, and emergency response systems.</li>
			</ul>
			<p>In summary, real-time streaming involves the continuous transmission and processing of data, enabling immediate insights and rapid decision-making. It has a wide range of applications across different industries and utilizes various technologies to facilitate efficient and reliable streaming.</p>
			<p>In the next section, we will explore the basics of streaming and understand how streaming is useful for real-time operations.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor186"/>What is streaming?</h1>
			<p>Streaming refers to the continuous<a id="_idIndexMarker496"/> and real-time processing of data as it is generated or received. Unlike batch processing, where data is processed in chunks or batches at fixed intervals, streaming enables the processing of data continuously and incrementally. It allows applications to ingest, process, and analyze data in real time, enabling timely decision-making and immediate responses to events.</p>
			<p>Different types of streaming architectures are available to handle streaming data. We will look at them next.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor187"/>Streaming architectures</h1>
			<p>Streaming architectures<a id="_idIndexMarker497"/> are designed to handle the continuous and high-velocity nature of streaming data. They typically <a id="_idIndexMarker498"/>consist of three key components:</p>
			<ul>
				<li><strong class="bold">Streaming sources</strong>: These are the origins of the streaming data, such as IoT devices, sensors, logs, social media feeds, or messaging systems. Streaming sources continuously produce and emit data in real time.</li>
				<li><strong class="bold">A streaming processing engine</strong>: The streaming processing engine is responsible for ingesting, processing, and analyzing streaming data. It provides the necessary infrastructure and computational capabilities to handle the continuous and incremental nature of streaming data.</li>
				<li><strong class="bold">Streaming sinks</strong>: Streaming sinks are destinations where the processed data is stored, visualized, or acted upon. They can be databases, data warehouses, dashboards, or external systems that consume the processed data.</li>
			</ul>
			<p>There are various streaming architectures, including the following:</p>
			<ul>
				<li><strong class="bold">Event-driven architecture</strong>: In event-driven architecture, events<a id="_idIndexMarker499"/> are generated<a id="_idIndexMarker500"/> by sources and then captured and processed by the engine, leading to immediate reactions and triggering actions or updates in real time. This framework facilitates real-time event processing, supports the development of event-driven microservices, and contributes to the creation of reactive systems.<p class="list-inset">Event-driven architecture’s advantages<a id="_idIndexMarker501"/> lie in its ability to provide responsiveness, scalability, and flexibility. This allows for prompt<a id="_idIndexMarker502"/> reactions to events as they unfold, fostering agility in system responses.</p></li>
				<li><strong class="bold">Lambda architecture</strong>: The lambda architecture seamlessly integrates<a id="_idIndexMarker503"/> batch and stream<a id="_idIndexMarker504"/> processing to effectively manage both historical and real-time data. This involves parallel processing of data streams to enable real-time analysis, coupled with offline batch processing for in-depth and comprehensive analytics.<p class="list-inset">This approach is particularly well-suited for applications that require a balance of real-time insights and thorough historical analysis. The lambda architecture’s strengths lie in its provision of fault tolerance, scalability, and capacity to handle substantial data volumes. This is achieved by harnessing the combined power of both the batch and stream processing techniques.</p></li>
				<li><strong class="bold">Unified streaming architecture</strong>: Unified streaming architectures, such<a id="_idIndexMarker505"/> as Apache Spark’s Structured<a id="_idIndexMarker506"/> Streaming, aim to provide a unified API and processing model for both batch and stream processing. They simplify the development and deployment of real-time applications by abstracting away the complexities of managing separate batch and stream processing systems.<p class="list-inset">This architecture abstracts complexities by offering a simplified approach to developing and deploying real-time applications. This is ideal for scenarios where simplicity and ease of development are crucial, allowing developers to focus more on business logic than intricate technicalities.</p><p class="list-inset">The advantages are that it simplifies development, reduces operational overhead, and ensures consistency across batch and stream processing.</p></li>
			</ul>
			<p>These architectures cater to different needs based on the specific requirements of a given application. Event-driven is ideal for real-time reactions, lambda for a balance between real-time and historical data, and unified streaming for a streamlined, unified approach to both batch and stream processing. Each approach has its strengths and trade-offs, making them suitable<a id="_idIndexMarker507"/> for various scenarios based on the specific needs of a system.</p>
			<p>In the following sections, we will delve into the specifics of Structured Streaming, its key concepts, and how it compares to Spark Streaming. We will also explore stateless and stateful streaming, streaming sources, and sinks, providing code examples and practical illustrations to enhance understanding.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor188"/>Introducing Spark Streaming</h1>
			<p>As you’ve seen so far, Spark Streaming<a id="_idIndexMarker508"/> is a powerful real-time data processing framework built on Apache Spark. It extends the capabilities of the Spark engine to support high-throughput, fault-tolerant, and scalable stream processing. Spark Streaming enables developers to process real-time data streams using the same programming model as batch processing, making it easy to transition from batch to streaming workloads.</p>
			<p>At its core, Spark Streaming divides the real-time data stream into small batches or micro-batches, which are then processed using Spark’s distributed<a id="_idIndexMarker509"/> computing capabilities. Each micro-batch is treated as a <strong class="bold">Resilient Distributed Dataset</strong> (<strong class="bold">RDD</strong>), Spark’s fundamental abstraction for distributed data processing. This approach allows developers to leverage Spark’s extensive ecosystem of libraries, such as Spark SQL, MLlib, and GraphX, for real-time analytics and machine learning tasks.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Exploring the architecture of Spark Streaming</h2>
			<p>Spark Streaming follows a <strong class="bold">master-worker architecture</strong>, where the driver program<a id="_idIndexMarker510"/> serves as the master<a id="_idIndexMarker511"/> and the worker nodes process the data. The high-level architecture consists<a id="_idIndexMarker512"/> of the following components:</p>
			<ul>
				<li><strong class="bold">The driver program</strong>: The driver program runs the main application and manages the overall execution of the Spark Streaming application. It divides the data stream into batches, schedules tasks on the worker nodes, and coordinates the processing.</li>
				<li><strong class="bold">Receivers</strong>: Receivers are responsible for connecting to the streaming data sources and receiving the data. They run on worker nodes and pull the data from sources such as Kafka, Flume, or TCP sockets. The received data is then stored in the memory of the worker nodes.</li>
				<li><strong class="bold">Discretized Stream (DStream)</strong>: DStream is the basic abstraction in Spark Streaming. It represents a continuous stream of data divided into small, discrete RDDs. DStream provides a high-level API to perform transformations and actions on the streaming data.</li>
				<li><code>map</code>, <code>filter</code>, and <code>reduceByKey</code>, are applied to each RDD in the DStream. Actions, such as <code>count</code>, <code>saveAsTextFiles</code>, and <code>foreachRDD</code>, trigger the execution of the streaming computation and produce results.</li>
				<li><strong class="bold">Output operations</strong>: Output operations allow the processed<a id="_idIndexMarker513"/> data to be written to external systems or storage. Spark Streaming supports various output operations, such as writing to files, databases, or sending to dashboards for visualization.</li>
			</ul>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Key concepts</h2>
			<p>To effectively use Spark Streaming, it is important<a id="_idIndexMarker514"/> to understand some key concepts:</p>
			<ul>
				<li><strong class="bold">DStreams</strong>: As mentioned earlier, DStreams represent the continuous stream of data in Spark Streaming. They are a sequence of RDDs, where each RDD contains data from a specific time interval. DStreams support various transformations and actions, enabling complex computations on the stream.</li>
				<li><strong class="bold">Window operations</strong>: Window operations allow you to apply transformations on a sliding window of data in the stream. It enables computations over a fixed window size or based on time durations, enabling tasks such as windowed aggregations or time-based joins.</li>
				<li><strong class="bold">Stateful operations</strong>: Spark Streaming gives you the ability to maintain stateful information across batches. It enables operations that require maintaining and updating state, such as cumulative counts.</li>
				<li><strong class="bold">Checkpointing</strong>: Checkpointing is a crucial mechanism<a id="_idIndexMarker515"/> in Spark Streaming to ensure fault tolerance and recovery. It periodically saves the metadata about the streaming application, including<a id="_idIndexMarker516"/> the configuration, DStream operations, and the processed data. It enables the recovery of the application if there are failures.</li>
			</ul>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor191"/>Advantages</h2>
			<p>Now, we will see the different advantages<a id="_idIndexMarker517"/> of using Spark Streaming in real-time operations:</p>
			<ul>
				<li><strong class="bold">A unified processing model</strong>: One of the significant advantages of Spark Streaming is its integration with the larger Spark ecosystem. It leverages the same programming model as batch processing, allowing users to seamlessly transition between batch and real-time processing. This unified processing model simplifies development and reduces the learning curve for users familiar with Spark.</li>
				<li><strong class="bold">High-level abstractions</strong>: Spark Streaming provides high-level abstractions such as DStreams to represent streaming data. DStreams are designed to handle continuous data streams and enable easy integration with existing Spark APIs, libraries, and data sources. These abstractions provide a familiar and expressive programming interface to process real-time data.</li>
				<li><strong class="bold">Fault tolerance and scalability</strong>: Spark Streaming offers fault-tolerant processing by leveraging Spark’s RDD abstraction. It automatically recovers from failures by recomputing lost data, ensuring the processing pipeline remains resilient and robust. Additionally, Spark Streaming can scale horizontally by distributing a workload across a cluster of machines, allowing it to handle large-scale data streams effectively.</li>
				<li><strong class="bold">Windowed computations</strong>: Spark Streaming supports windowed computations, which enable time-based analysis over sliding or tumbling windows of data. Window operations provide flexibility in performing aggregations, time-series analysis, and window-level transformations. This capability is particularly useful when analyzing streaming data based on temporal characteristics or patterns.</li>
				<li><strong class="bold">A wide range of data sources</strong>: Spark Streaming seamlessly integrates with various data sources, including Kafka, Flume, Hadoop Distributed File System (HDFS), and Amazon S3. This broad range of data sources allows users to ingest data from multiple streams and integrate it with existing data pipelines. Spark Streaming also supports custom data sources, enabling integration with proprietary or specialized<a id="_idIndexMarker518"/> streaming platforms.</li>
			</ul>
			<p>While Spark Streaming offers powerful real-time data processing capabilities, there are certain challenges to consider.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Challenges</h2>
			<p>Building streaming architectures<a id="_idIndexMarker519"/> for applications comes with its set of challenges, such as the following:</p>
			<ul>
				<li><strong class="bold">End-to-end latency</strong>: Latency is introduced when Spark Streaming processes data in micro-batches. The end-to-end latency can vary based on factors such as the batch interval, the data source, and the complexity of the computations.</li>
				<li><strong class="bold">Fault tolerance</strong>: Spark Streaming provides fault tolerance through RDD lineage and checkpointing. However, failures in receivers or the driver program can still disrupt the stream processing. Handling and recovering from failures is an important consideration to ensure the reliability of Spark Streaming applications.</li>
				<li><strong class="bold">Scalability</strong>: Scaling Spark Streaming applications to handle large volumes of data and meet high throughput requirements can be a challenge. Proper resource allocation, tuning, and cluster management are crucial to achieve scalability.</li>
				<li><strong class="bold">Data ordering</strong>: Spark Streaming processes data in parallel across multiple worker nodes, which can affect the order of events. Ensuring the correctness of the order of events becomes important in certain use cases, and developers need to consider this when designing their applications.</li>
			</ul>
			<p>In summary, Spark Streaming brings the power of Apache Spark to real-time data processing. Its integration with the Spark ecosystem, high-level abstractions, fault tolerance, scalability, and support for windowed computations make it a compelling choice for processing streaming data. By harnessing the advantages of Spark Streaming, organizations can unlock valuable insights and make informed decisions in real time.</p>
			<p>In the next section, we will explore<a id="_idIndexMarker520"/> Structured Streaming, a newer and more expressive streaming API in Apache Spark that overcomes some of the limitations and challenges of Spark Streaming. We will discuss its core concepts, the differences from Spark Streaming, and its benefits for real-time data processing.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor193"/>Introducing Structured Streaming</h1>
			<p>Structured Streaming is a revolutionary<a id="_idIndexMarker521"/> addition to Apache Spark that brings a new paradigm for real-time data processing. It introduces a high-level API that seamlessly integrates batch and streaming processing, providing a unified programming model. Structured Streaming treats streaming data as an unbounded table or DataFrame, enabling developers to express complex computations using familiar SQL-like queries and transformations.</p>
			<p>Unlike the micro-batch processing model of Spark Streaming, Structured Streaming follows a continuous processing model. It processes data incrementally as it arrives, providing low-latency and near-real-time results. This shift toward continuous processing opens up new possibilities for interactive analytics, dynamic visualizations, and real-time decision-making.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>Key features and advantages</h2>
			<p>Structured Streaming offers several key features and advantages <a id="_idIndexMarker522"/>over traditional stream processing frameworks:</p>
			<ul>
				<li><strong class="bold">An expressive API</strong>: Structured Streaming provides a declarative API that allows developers to express complex streaming computations using SQL queries, DataFrame operations, and Spark SQL functions. This enables developers with SQL or DataFrame expertise to easily transition to real-time data processing.</li>
				<li><strong class="bold">Fault tolerance and exactly-once semantics</strong>: Structured Streaming guarantees end-to-end fault tolerance and exactly-once semantics by maintaining the necessary metadata and state information. It handles failures gracefully and ensures that data is processed exactly once, even in the presence of failures or retries.</li>
				<li><strong class="bold">Scalability</strong>: Structured Streaming leverages the scalability of the Spark engine, enabling horizontal scaling by adding more worker nodes to the cluster. It can handle high-throughput data streams and scale seamlessly as the data volume increases.</li>
				<li><strong class="bold">Unified batch and streaming</strong>: With Structured Streaming, developers can use the same API and programming model for both batch and streaming processing. This unification simplifies the development and maintenance of applications, as there is no need to manage separate batch and stream processing code bases.</li>
				<li><strong class="bold">Ecosystem integration</strong>: Structured Streaming seamlessly integrates with the broader Spark ecosystem, enabling the use of libraries such as Spark SQL, MLlib, and GraphX for real-time analytics, machine learning, and graph processing on streaming data.</li>
			</ul>
			<p>Now, let’s take a look at some of the differences between Structured Streaming and Spark Streaming.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor195"/>Structured Streaming versus Spark Streaming</h2>
			<p>Structured Streaming differs<a id="_idIndexMarker523"/> from Spark Streaming in several fundamental ways:</p>
			<ul>
				<li><strong class="bold">The processing model</strong>: Spark Streaming processes data in micro-batches, where each batch is treated as a discrete RDD. In contrast, Structured Streaming processes data incrementally in a continuous manner, treating the stream as an unbounded table or DataFrame.</li>
				<li><strong class="bold">The API and query language</strong>: Spark Streaming primarily offers a low-level API based on RDD transformations and actions. Structured Streaming, on the other hand, provides a higher-level API with SQL-like queries, DataFrame operations, and Spark SQL functions. This makes it easier to express complex computations and leverage the power of SQL for real-time analytics.</li>
				<li><strong class="bold">Fault tolerance</strong>: Both Spark Streaming and Structured Streaming provide fault tolerance. However, Structured Streaming’s fault tolerance is achieved by maintaining the necessary metadata and state information, whereas Spark Streaming relies on RDD lineage and checkpointing for fault recovery.</li>
				<li><strong class="bold">Data processing guarantees</strong>: Spark Streaming provides at-least-once processing guarantees by default, where some duplicates may be processed if there are failures. Structured Streaming, on the other hand, provides exactly-once processing semantics, ensuring that each event is processed exactly once, even in the presence of failures or retries.</li>
			</ul>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/>Limitations and considerations</h2>
			<p>While Structured Streaming offers significant<a id="_idIndexMarker524"/> advantages, there are certain limitations and considerations to keep in mind:</p>
			<ul>
				<li><strong class="bold">Event time handling</strong>: Proper handling of event time, such as timestamp extraction, watermarking, and late data handling, is essential in Structured Streaming. Care should be taken to ensure the correct processing and handling of out-of-order events.</li>
				<li><strong class="bold">State management</strong>: Structured Streaming allows you to maintain stateful information across batches, which can introduce challenges related to state management and scalability. Monitoring memory usage and configuring appropriate state retention policies are crucial for optimal performance.</li>
				<li><strong class="bold">Ecosystem compatibility</strong>: While Structured Streaming integrates well with the Spark ecosystem, certain libraries and features might not be fully compatible with real-time streaming use cases. It is important to evaluate the compatibility of specific libraries and functionalities before using them in a Structured Streaming application.</li>
				<li><strong class="bold">Performance considerations</strong>: Structured Streaming’s continuous processing model introduces different performance considerations compared to micro-batch processing. Factors such as the event rate, processing time, and resource allocation need to be carefully<a id="_idIndexMarker525"/> monitored and optimized for efficient real-time data processing.</li>
			</ul>
			<p>In the next section, we will delve deeper into the concepts of stateless and stateful streaming, exploring their differences and use cases in the context of Structured Streaming.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor197"/>Streaming fundamentals</h1>
			<p>Let’s start by looking at some of the fundamental concepts in streaming that will help us get familiarized with different paradigms in streaming.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/>Stateless streaming – processing one event at a time</h2>
			<p>Stateless streaming refers<a id="_idIndexMarker526"/> to the processing of each event in isolation, without considering any context or history from previous events. In this approach, each event is treated independently, and the processing logic does not rely on any accumulated state or information from past events.</p>
			<p>Stateless streaming is well-suited for scenarios where each event can be processed independently, and the output is solely determined by the content of the event itself. This approach is often used for simple filtering, transformation, or enrichment operations that do not require <a id="_idIndexMarker527"/>you to maintain any contextual information across events.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor199"/>Stateful streaming – maintaining stateful information</h2>
			<p>Stateful streaming involves maintaining<a id="_idIndexMarker528"/> and utilizing contextual information or state across multiple events during processing. The processing logic considers the history of events and uses accumulated information to make decisions or perform computations. Stateful streaming enables more sophisticated analysis and complex computations that rely on context or accumulated knowledge.</p>
			<p>Stateful streaming requires you to maintain and update state information as new events arrive. The state can be as simple as a running count or more complex, involving aggregations, windowed computations, or maintaining session information. Proper management of state is essential to ensure correctness, scalability, and fault tolerance in stateful streaming applications.</p>
			<p>Let’s understand the differences between stateless and stateful streaming.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/>The differences between stateless and stateful streaming</h2>
			<p>The main differences between stateless <a id="_idIndexMarker529"/>and stateful streaming can be summarized as follows:</p>
			<ul>
				<li>Stateless streaming processes events independently, while stateful streaming maintains and uses accumulated state information across events.</li>
				<li>Stateless streaming is suitable for simple operations that don’t rely on past events, while stateful streaming enables complex computations that require context or accumulated knowledge.</li>
				<li>Stateless streaming is generally simpler to implement and reason about, while stateful streaming introduces additional challenges in managing state, fault tolerance, and scalability.</li>
				<li>Stateless streaming is often used for real-time filtering, transformation, or basic aggregations, while stateful streaming is necessary for windowed computations, sessionization, and stateful joins.</li>
			</ul>
			<p>Understanding the distinction <a id="_idIndexMarker530"/>between stateless and stateful streaming is crucial when designing real-time data processing systems, as it helps determine the appropriate processing model and requirements for a given use case.</p>
			<p>Now, let’s take a look at some of the fundamentals of Structured Streaming.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor201"/>Structured Streaming concepts</h1>
			<p>To understand Structured Streaming, it’s important<a id="_idIndexMarker531"/> for us to understand the different operations that take place in a near-real-time scenario when data arrives. We will understand them in the following section.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/>Event time and processing time</h2>
			<p>In Structured Streaming, there are two important notions of time – event time and processing time:</p>
			<ul>
				<li><strong class="bold">Event time</strong>: Event time refers to the time <a id="_idIndexMarker532"/>when an event occurred<a id="_idIndexMarker533"/> or was generated. It is typically embedded within the data itself, representing the timestamp or a field indicating when the event occurred in the real world. Event time is crucial for analyzing data based on its temporal order or performing window-based computations.</li>
				<li><strong class="bold">Processing time</strong>: Processing time, on the other hand, refers to the time<a id="_idIndexMarker534"/> when an event<a id="_idIndexMarker535"/> is processed by the streaming application. It is determined by the system clock or the time at which the event is ingested by the processing engine. Processing time is useful for tasks that require low latency or an immediate response but may not accurately reflect the actual event order.</li>
			</ul>
			<p>Based on these different time concepts, we can determine which one works best for a given use case. It’s important to understand the difference between the two. Based on that, the strategy for data processing can be determined.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor203"/>Watermarking and late data handling</h2>
			<p>Now, we will discuss how to handle data that doesn’t arrive at the defined time in real-time applications. There are different ways to handle that situation. Structured Streaming has a built-in mechanism to handle this type of data. These mechanisms include:</p>
			<ul>
				<li><strong class="bold">Watermarking</strong>: Watermarking is a mechanism<a id="_idIndexMarker536"/> in Structured Streaming<a id="_idIndexMarker537"/> used to deal with event time and handle delayed or late-arriving data. A watermark is a threshold timestamp that indicates the maximum event time seen by a system up to a certain point. It allows the system to track the progress of event time and determine when it is safe to emit results for a specific window.</li>
				<li><strong class="bold">Late data handling</strong>: Late-arriving data refers to events<a id="_idIndexMarker538"/> that have timestamps<a id="_idIndexMarker539"/> beyond the watermark threshold. Structured Streaming provides options to handle late data, such as discarding it, updating existing results, or storing it separately for further analysis.</li>
			</ul>
			<p>These built-in mechanisms save users a lot of time and efficiently handle late-arriving data.</p>
			<p>Next, we will see, once the data arrives, how we start the operations on it in streaming.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Triggers and output modes</h2>
			<p>Triggers determine when a streaming application should emit results or trigger the execution of the computation. Structured Streaming supports different types of triggers:</p>
			<ul>
				<li><strong class="bold">Event time triggers</strong>: Event time triggers operate<a id="_idIndexMarker540"/> based on the arrival <a id="_idIndexMarker541"/>of new events or when a watermark advances beyond a certain threshold. They enable more accurate and efficient processing, based on event time semantics.</li>
				<li><strong class="bold">Processing time triggers</strong>: These triggers operate<a id="_idIndexMarker542"/> based on processing<a id="_idIndexMarker543"/> time, allowing you to specify time intervals or durations at which the computation should be executed.</li>
			</ul>
			<p>Structured Streaming also offers different<a id="_idIndexMarker544"/> output modes. The output modes determine how data is updated in the sink. A sink is where we would write the output after the streaming operation:</p>
			<ul>
				<li><strong class="bold">Complete mode</strong>: In this mode, the entire updated<a id="_idIndexMarker545"/> result, including all the rows in the output, is written to the sink. This mode provides the most comprehensive view of data but can be memory-intensive for large result sets.</li>
				<li><strong class="bold">Append mode</strong>: In append mode, only the new<a id="_idIndexMarker546"/> rows appended to the result table since the last trigger are written to the sink. This mode is suitable for cases where the result is an append-only stream.</li>
				<li><strong class="bold">Update mode</strong>: Update mode only writes the changed<a id="_idIndexMarker547"/> rows to the sink, preserving the existing rows that haven’t changed since the last trigger. This mode is useful for cases where the result table is updated incrementally.</li>
			</ul>
			<p>Now, let’s take a look at the different types of aggregate operations we can do on streaming data.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/>Windowing operations</h2>
			<p>Windowing operations<a id="_idIndexMarker548"/> in Structured Streaming allow you to group and aggregate data over specific time windows. Windows for these operations can be defined based on either event time or processing time, and they provide a way to perform computations over a subset of events within a given time range.</p>
			<p>The common types of windowing operations include the following:</p>
			<ul>
				<li><strong class="bold">Tumbling windows</strong>: Tumbling windows divide a stream<a id="_idIndexMarker549"/> into non-overlapping fixed-size windows. Each event falls into exactly one window, and computations are performed independently for each window.</li>
				<li><strong class="bold">Sliding windows</strong>: Sliding windows create overlapping <a id="_idIndexMarker550"/>windows that slide or move over a stream at regular intervals. Each event can contribute to multiple windows, and computations can be performed on the overlapping parts.</li>
				<li><strong class="bold">Session windows</strong>: Session windows group<a id="_idIndexMarker551"/> events that are close in time or belong to the same session, based on a specified session timeout. A session is defined as a series of events<a id="_idIndexMarker552"/> within a certain time threshold of each other.</li>
			</ul>
			<p>The next operation<a id="_idIndexMarker553"/> that we frequently use in streaming is the join operation. Now, we will see how we can use joins with streaming data.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/>Joins and aggregations</h2>
			<p>Structured Streaming supports joins and aggregations on streaming data, enabling complex analytics and data transformations:</p>
			<ul>
				<li><strong class="bold">Joins</strong>: Streaming joins allow<a id="_idIndexMarker554"/> you to combine two or more<a id="_idIndexMarker555"/> streams or a stream with static/reference data, based on a common key or condition. The join operation can be performed using event time or processing time, and it supports different join types such as inner join, outer join, and left/right join.</li>
				<li><code>count</code>, <code>sum</code>, <code>average</code>, <code>min</code>, and <code>max</code>.</li>
			</ul>
			<p>Structured Streaming’s flexible and expressive API for handling event time, triggers, output modes, windowing operations, joins, and aggregations allows developers to perform comprehensive real-time analytics and computations on streaming data. By understanding these concepts, developers can build sophisticated streaming applications with ease and precision.</p>
			<p>In the next section, we will explore how to read and write data with streaming sources and sinks.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor207"/>Streaming sources and sinks</h1>
			<p>Streaming sources and sinks<a id="_idIndexMarker558"/> are essential components<a id="_idIndexMarker559"/> in a streaming system that enable the ingestion of data from external systems and the output of processed data to external destinations. They form the connectors between the streaming application and the data sources or sinks.</p>
			<p>Streaming sources retrieve data from various input systems, such as message queues, filesystems, databases, or external APIs, and make it available for processing in a streaming application. On the other hand, streaming sinks receive processed data from the application and write it to external storage, databases, filesystems, or other systems for further analysis or consumption.</p>
			<p>There are different types of streaming sources and sinks. We will explore some of them next.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Built-in streaming sources</h2>
			<p>Structured Streaming provides<a id="_idIndexMarker560"/> built-in support for a variety of streaming<a id="_idIndexMarker561"/> sources, making it easy to integrate with popular data systems. Some of the commonly used built-in streaming sources include the following:</p>
			<ul>
				<li><strong class="bold">The file source</strong>: The file source allows you to read data from files in a directory or a file stream from a file-based system, such as HDFS or Amazon S3.</li>
				<li><strong class="bold">KafkaSource</strong>: KafkaSource enables the consumption of data from Apache Kafka, a distributed streaming platform. It provides fault-tolerant, scalable, and high-throughput ingestion of data streams.</li>
				<li><strong class="bold">The socket source</strong>: The socket source allows a streaming application<a id="_idIndexMarker562"/> to read data from a <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) socket. It is useful for scenarios where data is sent through network connections, such as log streaming or data sent by external systems.</li>
				<li><strong class="bold">The Structured Streaming source</strong>: The Structured Streaming source allows developers to define their own streaming sources by extending the built-in source interfaces. It provides the flexibility to integrate with custom<a id="_idIndexMarker563"/> or proprietary<a id="_idIndexMarker564"/> data sources.</li>
			</ul>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor209"/>Custom streaming sources</h2>
			<p>In addition to the <a id="_idIndexMarker565"/>built-in streaming<a id="_idIndexMarker566"/> sources, Structured Streaming allows developers to create custom streaming sources to ingest data from any system that can be accessed programmatically. Custom streaming sources can be implemented by extending the <code>Source</code> interface provided by the Structured Streaming API.</p>
			<p>When implementing a custom streaming source, developers need to consider aspects such as data ingestion, event-time management, fault tolerance, and scalability. They must define how data is fetched, how it is partitioned and distributed among workers, and how to handle late-arriving data and schema evolution.</p>
			<p>Similar to different streaming sources, we have streaming sinks as well. Let’s explore them next.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor210"/>Built-in streaming sinks</h2>
			<p>Structured Streaming provides <a id="_idIndexMarker567"/>built-in support for various<a id="_idIndexMarker568"/> streaming sinks, enabling the output of processed data to different systems. Some of the commonly used built-in streaming sinks include the following:</p>
			<ul>
				<li><strong class="bold">The console sink</strong>: The console sink writes the output data to the console or standard output. It is useful for debugging and quick prototyping but not suitable for production use.</li>
				<li><strong class="bold">The file sink</strong>: The file sink writes the output data to files in a directory or a file-based system such as HDFS or Amazon S3. It allows data to be stored and consumed later for batch processing or archival purposes.</li>
				<li><strong class="bold">The Kafka sink</strong>: The Kafka sink enables you to write data to Apache Kafka topics. It provides fault-tolerant, scalable, and high-throughput output to Kafka for consumption by other systems.</li>
				<li><code>ForeachWriter</code> interface. It provides the flexibility to write<a id="_idIndexMarker569"/> data to external systems<a id="_idIndexMarker570"/> or perform custom operations on the output data.</li>
			</ul>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Custom streaming sinks</h2>
			<p>Similar to custom streaming<a id="_idIndexMarker571"/> sources, developers can implement custom<a id="_idIndexMarker572"/> streaming sinks in Structured Streaming by extending the <code>Sink</code> interface. There are instances when you would need to write the data back to a system that might not support streaming. It could be a database or a file-based storage system. Custom streaming sinks enable integration with external systems or databases that are not supported by the built-in sinks.</p>
			<p>When implementing a custom streaming sink, developers need to define how output data is written or processed by the external system. This may involve establishing connections, handling batching or buffering, and ensuring fault tolerance and exactly-once semantics.</p>
			<p>In the next section, we will talk about advanced techniques in Structured Streaming.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor212"/>Advanced techniques in Structured Streaming</h1>
			<p>There are certain built-in capabilities of Structured Streaming that makes it the default choice for even some batch operations. Instead of architecting things yourself, Structured Streaming handles these properties for you. Some of them are as follows.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Handling fault tolerance</h2>
			<p>Fault tolerance is crucial<a id="_idIndexMarker573"/> in streaming systems<a id="_idIndexMarker574"/> to ensure data integrity and reliability. Structured Streaming provides built-in fault tolerance mechanisms to handle failures in both streaming sources and sinks:</p>
			<ul>
				<li><strong class="bold">Source fault tolerance</strong>: Structured Streaming ensures<a id="_idIndexMarker575"/> end-to-end fault tolerance in sources, by tracking the progress of event time using watermarks and checkpointing the metadata related to the stream. If there are failures, the system can recover and resume processing from the last consistent state.</li>
				<li><strong class="bold">Sink fault tolerance</strong>: Fault tolerance in sinks<a id="_idIndexMarker576"/> depends on the guarantees provided by the specific sink implementation. Some sinks may inherently provide exactly-once semantics, while others may rely on idempotent writes or deduplication techniques to achieve at-least-once semantics. Sink implementations should be carefully chosen to ensure data consistency and reliability.</li>
			</ul>
			<p>Developers should consider the fault tolerance characteristics of the streaming sources and sinks they use and configure appropriate checkpointing intervals, retention policies, and recovery mechanisms to ensure the reliability of their streaming applications.</p>
			<p>Structured Streaming<a id="_idIndexMarker577"/> has built-in support for schema<a id="_idIndexMarker578"/> evolution as well. Let’s explore that in the next section.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Handling schema evolution</h2>
			<p>Structured Streaming provides<a id="_idIndexMarker579"/> support for handling schema evolution in streaming data sources. Schema evolution refers to changes<a id="_idIndexMarker580"/> in the structure or schema of incoming data over time.</p>
			<p>Structured Streaming can handle schema evolution by applying the concept of schema inference or schema merging. When reading from streaming sources, the initial schema is inferred from the incoming data. As the data evolves, subsequent DataFrames are merged with the initial schema, accommodating any new or changed fields.</p>
			<p>The following code snippet demonstrates handling schema evolution in Structured Streaming:</p>
			<pre class="source-code">
stream = spark.readStream \
  .format("csv") \
  .option("header", "true") \
  .schema(initialSchema) \
  .load("data/input")
mergedStream = stream \
  .selectExpr("col1", "col2", "new_col AS col3")</pre>			<p>In this example, the initial schema is provided explicitly using the <code>schema</code> method. As new data arrives with additional fields, such as <code>new_col</code>, it can be selected and merged into the stream using the <code>selectExpr</code> method.</p>
			<p>Handling schema evolution<a id="_idIndexMarker581"/> is crucial to ensure<a id="_idIndexMarker582"/> compatibility and flexibility in streaming applications where the data schema may change or evolve over time.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor215"/>Different joins in Structured Streaming</h1>
			<p>One of the key features of Structured Streaming is its ability to join different types of data streams together in one sink.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor216"/>Stream-stream joins</h2>
			<p>Stream-stream<a id="_idIndexMarker583"/> joins, also known as <strong class="bold">stream-stream co-grouping</strong> or <strong class="bold">stream-stream correlation</strong>, involve joining two or more <a id="_idIndexMarker584"/>streaming data sources<a id="_idIndexMarker585"/> based on a common key or condition. In this type of join, each incoming event from the streams is matched with events from other streams that share the same key or satisfy the specified condition.</p>
			<p>Stream-stream joins enable real-time data correlation and enrichment, making it possible to combine multiple streams of data to gain deeper insights and perform complex analytics. However, stream-stream joins present unique challenges compared to batch or stream-static joins, due to the unbounded nature of streaming data and potential event-time skew.</p>
			<p>One common approach to stream-stream joins is the use of windowing operations. By defining overlapping or tumbling windows on the streams, events within the same window can be joined based on their keys. Careful consideration of window size, watermarking, and event time characteristics is necessary to ensure accurate and meaningful joins.</p>
			<p>Here’s an example of a stream-stream join using Structured Streaming:</p>
			<pre class="source-code">
stream1 = spark.readStream.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092") .option("subscribe", "topic1") .load()
stream2 = spark.readStream.format("kafka").option("kafka.bootstrap.servers","localhost:9092") .option("subscribe", "topic2") .load()
 joinedStream =stream1.join(stream2, "common_key")</pre>			<p>In this example, two Kafka streams, <code>stream1</code> and <code>stream2</code>, are read from different topics. The <code>join</code> method is then applied to perform the join operation, based on the <code>common_key</code> field shared<a id="_idIndexMarker586"/> by both streams.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor217"/>Stream-static joins</h2>
			<p>Stream-static joins, also known as <strong class="bold">stream-batch joins</strong>, involve joining a streaming data source to a static<a id="_idIndexMarker587"/> or reference dataset. The static<a id="_idIndexMarker588"/> dataset typically represents reference data, such as configuration data or dimension tables, that remains constant over time.</p>
			<p>Stream-static joins are useful for enriching streaming data with additional information or attributes from the static dataset. For example, you might join a stream of user activity events with a static user profile table to enrich each event with user-related details.</p>
			<p>To perform a stream-static join in Structured Streaming, you can load the static dataset as a static DataFrame and then use the join method to perform the join with the streaming DataFrame. Since the static dataset does not change, the join operation can be performed using the default “right outer join” mode.</p>
			<p>Here’s an example of a stream-static join in Structured Streaming:</p>
			<pre class="source-code">
stream =spark.readStream.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092") .option("subscribe", "topic") .load()
  staticData = spark.read.format("csv") .option("header", "true") .load("data/static_data.csv")
 enrichedStream = stream.join(staticData,"common_key")</pre>			<p>In this example, the streaming data is read from a Kafka source, and the static dataset is loaded from a CSV file. The join method is then used to perform the stream-static join based on the “<code>common_key</code>” field.</p>
			<p>Both stream-stream and stream-static joins<a id="_idIndexMarker589"/> provide powerful capabilities for real-time data analysis and enrichment. When using these join operations, it is essential to carefully manage event time characteristics, windowing options, and data consistency to ensure accurate and reliable results. Additionally, performance considerations should be considered to handle large volumes of data and meet low-latency requirements in real-time streaming applications.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor218"/>Final thoughts and future developments</h1>
			<p>Structured Streaming has emerged<a id="_idIndexMarker590"/> as a powerful framework for real-time data processing in Apache Spark. Its unified programming model, fault tolerance, and seamless integration with the Spark ecosystem make it an attractive choice for building scalable and robust streaming applications.</p>
			<p>As Structured Streaming continues to evolve, there are several areas that hold promise for future developments. These include the following:</p>
			<ul>
				<li><strong class="bold">Enhanced support for streaming sources and sinks</strong>: Providing more built-in connectors for popular streaming systems and databases, as well as improving the integration and compatibility with custom sources and sinks.</li>
				<li><strong class="bold">Advanced event time handling</strong>: Introducing more advanced features for event time handling, including support for event-time skew detection and handling, event deduplication, and watermark optimizations.</li>
				<li><strong class="bold">Performance optimization</strong>: Continuously improving the performance of Structured Streaming, especially in scenarios with high data volumes and complex computations. This could involve optimizations in memory management, query planning, and query optimization techniques.</li>
				<li><strong class="bold">Integration with AI and machine learning</strong>: Further integrating Structured Streaming with AI and machine learning libraries in Spark, such as MLlib and TensorFlow, to enable real-time machine learning and predictive analytics on streaming data.</li>
				<li><strong class="bold">Seamless integration with streaming data warehouses</strong>: Providing better integration with streaming data warehouses or data lakes, such as Apache Iceberg or Delta Lake, to enable scalable and efficient storage and the querying of streaming data.</li>
			</ul>
			<p>In conclusion, Structured Streaming<a id="_idIndexMarker591"/> offers a modern and expressive approach to real-time data processing in Apache Spark. Its ease of use, scalability, fault tolerance, and integration with the Spark ecosystem make it a valuable tool for building robust and scalable streaming applications. By leveraging the concepts and techniques covered in this chapter, developers can unlock the full potential of real-time data processing with Structured Streaming.</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor219"/>Summary</h1>
			<p>Throughout this chapter, we have explored the fundamental concepts and advanced techniques in Structured Streaming.</p>
			<p>We started by understanding the fundamentals of Structured Streaming, its advantages, and the core concepts that underpin its operation. Then, we talked about Spark Streaming and what it has to offer.</p>
			<p>After that, we dived into the core functionalities of Structured Streaming. Then, we further delved into advanced topics, such as windowed operations in Structured Streaming. We explored sliding and tumbling windows, which enable us to perform aggregations and computations over a specified time window, allowing for time-based analysis of the streaming data. Additionally, we explored stateful streaming processing, which involves maintaining and updating state in streaming applications and integrating external libraries and APIs to enhance the capabilities of Structured Streaming.</p>
			<p>Finally, we explored emerging trends in real-time data processing and concluded the chapter by summarizing the key takeaways and insights gained.</p>
			<p>In the next chapter, we will look into machine learning techniques and how to use Spark with machine learning.</p>
		</div>
	</body></html>