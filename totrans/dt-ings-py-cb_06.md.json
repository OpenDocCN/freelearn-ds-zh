["```py\n$ jupyter Notebook\n```", "```py\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n      .master(\"local[1]\") \\\n      .appName(\"chapter6_schemas\") \\\n      .config(\"spark.executor.memory\", '3g') \\\n      .config(\"spark.executor.cores\", '1') \\\n      .config(\"spark.cores.max\", '1') \\\n      .getOrCreate()\n```", "```py\n$ pyspark --version\n```", "```py\n$ jupyter --version\n```", "```py\n    my_data = [(\"3456\",\"Cristian\",\"Rayner\",30,\"M\"),\n                (\"3567\",\"Guto\",\"Flower\",35,\"M\"),\n                (\"9867\",\"Yasmin\",\"Novak\",23,\"F\"),\n                (\"3342\",\"Tayla\",\"Mejia\",45,\"F\"),\n                (\"8890\",\"Barbara\",\"Kumar\",20,\"F\")\n                ]\n    ```", "```py\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n    schema = StructType([ \\\n        StructField(\"id\",StringType(),True), \\\n        StructField(\"name\",StringType(),True), \\\n        StructField(\"lastname\",StringType(),True), \\\n        StructField(\"age\", IntegerType(), True), \\\n        StructField(\"gender\", StringType(), True), \\\n      ])\n    ```", "```py\n    df = spark.createDataFrame(data=my_data,schema=schema)\n    ```", "```py\nschema = StructType([ \\\n    StructField(\"id\",StringType(),True), \\\n    StructField(\"name\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"age\", IntegerType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n  ])\n```", "```py\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType\n    ```", "```py\n    schema = StructType([ \\\n        StructField(\"id\",IntegerType(),True), \\\n        StructField(\"name\",StringType(),True), \\\n        StructField(\"host_id\",IntegerType(),True), \\\n        StructField(\"host_name\",StringType(),True), \\\n        StructField(\"neighbourhood_group\",StringType(),True), \\\n        StructField(\"neighbourhood\",StringType(),True), \\\n        StructField(\"latitude\",DoubleType(),True), \\\n        StructField(\"longitude\",DoubleType(),True), \\\n        StructField(\"room_type\",StringType(),True), \\\n        StructField(\"price\",FloatType(),True), \\\n        StructField(\"minimum_nights\",IntegerType(),True), \\\n        StructField(\"number_of_reviews\",IntegerType(),True), \\\n        StructField(\"last_review\",DateType(),True), \\\n        StructField(\"reviews_per_month\",FloatType(),True), \\\n          StructField(\"calculated_host_listings_count\",IntegerType(),True), \\\n        StructField(\"availability_365\",IntegerType(),True), \\\n        StructField(\"number_of_reviews_ltm\",IntegerType(),True), \\\n        StructField(\"license\",StringType(),True)\n      ])\n    ```", "```py\n    df = spark.read.options(header=True, sep=',',\n                              multiLine=True, escape='\"')\\\n                    .schema(schema) \\\n                    .csv('listings.csv')\n    ```", "```py\n    df.printSchema()\n    ```", "```py\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType\n```", "```py\ndf_2 = spark.read.options(header=True, sep=',',\n                          multiLine=True, escape='\"',\n                         inferSchema=True) \\\n                .csv('listings.csv')\n```", "```py\n    df_json = spark.read.option(\"multiline\",\"true\") \\\n                        .json('holiday_brazil.json')\n    ```", "```py\n    df_json.printSchema()\n    ```", "```py\n    df_json.toPandas()\n    ```", "```py\n    from pyspark.sql.types import StructType, ArrayType, StructField, StringType, IntegerType, MapType\n    ```", "```py\n    schema = StructType([ \\\n            StructField('status', StringType(), True),\n            StructField('holidays', ArrayType(\n                StructType([\n                    StructField('name', StringType(), True),\n                    StructField('date', DateType(), True),\n                    StructField('observed', StringType(), True),\n                    StructField('public', StringType(), True),\n                    StructField('country', StringType(), True),\n                    StructField('uuid', StringType(), True),\n                    StructField('weekday', MapType(StringType(), MapType(StringType(),StringType(),True),True))\n                ])\n            ))\n        ])\n    ```", "```py\n    df_json = spark.read.option(\"multiline\",\"true\") \\\n                        .schema(schema) \\\n                        .json('holiday_brazil.json')\n    ```", "```py\n    from pyspark.sql.functions import explode\n    exploded_json = df_json.select('status', explode(\"holidays\").alias(\"holidaysExplode\"))\\\n            .select(\"status\", \"holidaysExplode.*\")\n    ```", "```py\n    exploded_json2 = exploded_json.select(\"*\", explode('weekday').alias('weekday_type', 'weekday_objects'))\n    ```", "```py\n    exploded_json2.toPandas()\n    ```", "```py\nholiday : [{},{}...]\n```", "```py\nStructField('holidays', ArrayType(\n            StructType([\n  ...\n])\n))\n```", "```py\nStructField('weekday', MapType(\nStringType(),MapType(\nStringType(),StringType(),True)\n,True))\n```", "```py\nweekday : {\n day : {{}, {}},\n observed : {{}, {}}\n}\n```", "```py\nfrom pyspark.sql.functions import explode\n```", "```py\nexploded_json = df_json.select('status', explode(\"holidays\").alias(\"holidaysExplode\"))\\\n        .select(\"holidaysExplode.*\")\n```", "```py\n    spark.sparkContext.setLogLevel(\"INFO\")\n    ```", "```py\n    Logger= spark._jvm.org.apache.log4j.Logger\n    syslogger = Logger.getLogger(__name__)\n    ```", "```py\n    syslogger.error(\"Error message sample\")\n    syslogger.info(\"Info message sample\")\n    ```", "```py\n    try:\n        df = spark.read.options(header=True, sep=',',\n                                  multiLine=True, escape='\"',\n                                 inferSchema=True) \\\n                        .csv('listings.csv')\n    except Exception as e:\n        syslogger.error(f\"Error message: {e}\")\n    ```", "```py\nspark.sparkContext.setLogLevel(\"ERROR\")\n```", "```py\nLogger= spark._jvm.org.apache.log4j.Logger\nsyslogger = Logger.getLogger(__name__)\n```", "```py\nsyslogger.error(\"Error message sample\")\nsyslogger.info(\"Info message sample\")\n```", "```py\ntry:\n    df = spark.read.options(header=True, sep=',',\n                              multiLine=True, escape='\"',\n                             inferSchema=True) \\\n                    .csv('listings.cs') # Error here\nexcept Exception as e:\n    syslogger.error(f\"Error message: {e}\")\n```"]