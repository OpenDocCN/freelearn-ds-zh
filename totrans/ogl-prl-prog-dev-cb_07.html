<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Developing the Matrix Multiplication with OpenCL</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding matrix multiplication</li><li class="listitem" style="list-style-type: disc">OpenCL implementation of the matrix multiplication</li><li class="listitem" style="list-style-type: disc">Faster OpenCL implementation of the matrix multiplication by thread coarsening</li><li class="listitem" style="list-style-type: disc">Faster OpenCL implementation of the matrix multiplication through register tiling</li><li class="listitem" style="list-style-type: disc">Reducing global memory via shared memory data prefetching in matrix multiplication</li></ul></div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Introduction</h1></div></div></div><p>In this chapter, we are going to take a look at the problem of multiplying two matrices to produce another matrix. This problem is also known as the matrix multiplication and its applications range from mathematics, finance, physics, and it is a popular system for solving linear equations. For illustration purposes, we present a typical use case for solving linear equations:</p><div><img src="img/4520OT_07_01.jpg" alt="Introduction"/></div><p>These equations can be modeled as <img src="img/4520OT_07_02.jpg" alt="Introduction"/>, where the L.H.S of the equation consists of a 2 x 2 matrix which is multiplied by a 2 x 1 matrix (often called a vector, and they can be row vectors or column vectors) which is equal to the vector on the R.H.S. Considering the fact that matrices can have any order of rows and columns, mathematicians invented the notation, <img src="img/4520OT_07_03.jpg" alt="Introduction"/> where to solve this, we have to determine <img src="img/4520OT_07_04.jpg" alt="Introduction"/>.Here, as we can see that the inverse of the matrix needs to be known. At this point, that's all we like to say about the wonderful world of matrices, lest we fall into the rabbit hole!</p><div><div><h3 class="title"><a id="note33"/>Note</h3><p>You should be aware that only square matrices have inverses, and even among such matrices the inverses are not guaranteed to be present. We won't be covering computing inverses in this chapter or book.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Understanding matrix multiplication</h1></div></div></div><p>The product C of <a id="id500" class="indexterm"/>two matrices A and B is defined as <img src="img/4520OT_07_05.jpg" alt="Understanding matrix multiplication"/>, where j is the sum of all possible values of i and k. There is an implied summation over the indices i, j, and k. The dimensions of the matrix C is: <img src="img/4520OT_07_06.jpg" alt="Understanding matrix multiplication"/>, where <img src="img/4520OT_07_07.jpg" alt="Understanding matrix multiplication"/> denotes a matrix with <img src="img/4520OT_07_08.jpg" alt="Understanding matrix multiplication"/> rows and <img src="img/4520OT_07_09.jpg" alt="Understanding matrix multiplication"/> columns and when we write out the product explicitly, it looks as follows:</p><div><img src="img/4520OT_07_10.jpg" alt="Understanding matrix multiplication"/></div><div><img src="img/4520OT_07_11.jpg" alt="Understanding matrix multiplication"/></div><div><img src="img/4520OT_07_12.jpg" alt="Understanding matrix multiplication"/></div><div><img src="img/4520OT_07_13.jpg" alt="Understanding matrix multiplication"/></div><div><img src="img/4520OT_07_14.jpg" alt="Understanding matrix multiplication"/></div><div><img src="img/4520OT_07_15.jpg" alt="Understanding matrix multiplication"/></div><p>Another property of <a id="id501" class="indexterm"/>matrix multiplication is that multiplication is associative and distributive over addition, but they are however not commutative.</p><div><div><h3 class="title"><a id="note34"/>Note</h3><p>Two matrices A and B are considered commutative if they are diagonal matrices and are of the same dimension.</p></div></div><p>Knowing these properties will help us in formulating our initial algorithm stemming from this formula: <em>c<sub>ik</sub></em> = <em>a<sub>ij</sub></em><em>b<sub>jk</sub></em>. The commutative property<a id="id502" class="indexterm"/> basically informs us that the order of multiplication between matrices A and B matters, while the associative property allows us the flexibility to explore what happens when two matrices A and B are too huge to fit into available memory on the OpenCL device and we need to partition the matrix data across multiple devices. The following diagram illustrates what happens when a row of matrix A and a column of matrix B is read and its aggregated result is written into the appropriate location in the output matrix, C:</p><div><img src="img/4520OT_07_17.jpg" alt="Understanding matrix multiplication"/></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec118"/>Getting ready</h2></div></div></div><p>At this point, we are <a id="id503" class="indexterm"/>in pretty good shape to take a stab at matrix multiplication. As before, we begin with an implementation in C/C++, which is a direct translation of the formula and from there we will develop a better intuition on how to import it to OpenCL and apply suitable optimizations.</p><p>For the rest of this chapter, we are going to craft our algorithm so that it runs on the GPU on your desktop/laptop. The reason for this is because the GPU has more computation units than a CPU, and GPUs are often equipped with other hardware components that allows the OpenCL to take advantage of that hardware (including local data stores, out of order execution units, shared data store, and so on), which often allows an enormous number of threads to execute in. Current CPU processors don't implement OpenCL shared memory, so using GPUs is probably the best option!</p><div><div><h3 class="title"><a id="tip21"/>Tip</h3><p>Get a GPU that supports OpenCL 1.1 and the preceding information is good enough for these experiments.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec119"/>How to do it...</h2></div></div></div><p>By now, you <a id="id504" class="indexterm"/>should be familiar with creating the necessary data structures to represent our three matrices in question (let's call them A, B, and C). Coincidentally, they happen to be square matrices, but this does not affect our understanding in any way.</p><p>When we examine this problem from the previous section, we understand that we want to basically iterate through both matrices in the following fashion:</p><div><ol class="orderedlist arabic"><li class="listitem">Pick a row from matrix A.</li><li class="listitem">Pick a column from matrix B.</li><li class="listitem">Multiply each element from the picked row with the corresponding element from the picked column.</li></ol></div><p>From this description, we can begin to think of various implementation methods and one such method could be as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Create two in-memory data structures for A and B, say <code class="literal">TmpA</code> and <code class="literal">TmpB</code>.</li><li class="listitem">Loop through A and pick a row for which each element to deposit into its corresponding position in <code class="literal">TmpA</code>, do the same for a picked column and deposit into <code class="literal">TmpB</code>:<div><pre class="programlisting">  loop until i &lt; number_of_rowsA:
    TmpA[i] = A[i]
  endloop
  loop until i &lt; number_of_colsB:
    TmpB[i] = B[i]
  endloop</pre></div></li><li class="listitem">Loop through <code class="literal">TmpA</code> and <code class="literal">TmpB</code> and perform the matrix multiplication.</li><li class="listitem">In pseudo code, it looks something like this:<div><pre class="programlisting">loop until (i,j) &lt; (rowA * colB):
  loop through A[i][_] deposit values into TmpA
  loop through B[_][j] deposit values into TmpB
  foreach value in TmpA and TmpB:
    C[a] = TmpA[x] * TmpB[y]
endloop</pre></div></li></ol></div><p>Another implementation is very similar to this one with the exception that we use standard C/C++ array indexing techniques to reference the respective row(s) and column(s) and we present an implementation in the following sections.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec120"/>How it works…</h2></div></div></div><p>There are various ways of<a id="id505" class="indexterm"/> implementing matrix multiplication algorithm in C/C++ as we've discussed previously. And it seems that there isn't a best design to adopt. Personally, I've always favored a readable design versus a convoluted design. However, it's necessary to write high performance code from time to time, so that you can squeeze all the power that the programming language or hardware can provide.</p><div><div><h3 class="title"><a id="tip22"/>Tip</h3><p>At this point, you may or may not have developed the necessary intuition to design your algorithms, but one way is to continuously practice using different techniques and measure each implementation with some benchmarks, and never clump all the optimizations in one algorithm unless you're confident.</p></div></div><p>Now that we have some inkling as to what is meant by matrix multiplication, it is definitely time for us to start exploring what the algorithm looks like after being translated into its sequential form. The following is an example of the matrix multiplication program in sequential form (the code is executed by only one thread):</p><div><pre class="programlisting">Void matrixMul(float *C, 
               const float *A, 
               const float *B, 
               unsigned int hA, 
               unsigned int wA, 
               unsigned int wB) {
    for (unsigned int i = 0; i &lt; hA; ++i)
        for (unsigned int j = 0; j &lt; wB; ++j){   
            float sum = 0;
            for (unsigned int k = 0; k &lt; wA; ++k) {   
                double a = A[i * wA + k]; // statement 1
                double b = B[k * wB + j]; // statement 2
                sum += a * b;
            }   

            C[i * wB + j] = (float)sum; // statement 3
        }   
}</pre></div><p>When you examine this code, you will notice that there are three loop structures and we use regular C/C++ array indexing techniques to reference each subsequent element from their respective rows and columns. Take some time now to convince that we are actually computing the matrix multiplication.</p><p>As before, we put on our parallel developer hat and try to see how we can provide a parallel OpenCL form of the equivalent program. Again, I'm naturally drawn to the loop structures and we have three of them!</p><p>We noticed <a id="id506" class="indexterm"/>that as we iterate through the matrices A and B, the innermost loop is the code block that is performing all the heavy lifting for <code class="literal">statement 1</code>, <code class="literal">statement 2</code>, and <code class="literal">statement 3</code>. These statements will represent the core of our OpenCL kernel and let's go and take a look at how we can map it to OpenCL.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec53"/>OpenCL implementation of the matrix multiplication</h1></div></div></div><p>We have spent a good <a id="id507" class="indexterm"/>amount of time understanding how<a id="id508" class="indexterm"/> matrix multiplication works and we've looked at how it looks in its sequential form. Now we're going to attempt to map this to OpenCL in the most direct way.</p><p>The implementation technique here makes use of the fact that we create 2D thread blocks where each thread/work item in each dimension will access their respective elements in the row/column dimension.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec121"/>Getting ready</h2></div></div></div><p>In this recipe, we are going to use two matrices of dimensions 1024 x 1024 (we call A and B), and we'll multiply these two matrices together to produce a third matrix of 1024 x 1024, we call C.</p><div><div><h3 class="title"><a id="note35"/>Note</h3><p>You may wish to refresh your basic matrix theory at this point to convince yourself that this is the case.</p></div></div><p>We construct the familiar data structures in our host code and fill them with random values. The host code in <code class="literal">Ch7/matrix_multiplication_01/MatrixMultiplication.c</code> looks as follows:</p><div><pre class="programlisting">matrixA = (cl_int*)malloc(widthA * heightA * sizeof(cl_int));
matrixB = (cl_int*)malloc(widthB * heightB * sizeof(cl_int));
matrixC = (cl_int*)malloc(widthB * heightA * sizeof(cl_int));

memset(matrixA, 0, widthA * heightA * sizeof(cl_int));
memset(matrixB, 0, widthB * heightB * sizeof(cl_int));
memset(matrixC, 0, widthB * heightA * sizeof(cl_int));
            
fillRandom(matrixA, widthA, heightA, 643);
fillRandom(matrixB, widthB, heightB, 991);</pre></div><p>Next, we set up the OpenCL command queue to enable profiling because we want to keep looking at the effects of the subsequent optimizations that we are going to apply. It's definitely very important to establish a reference point to which your measurements can be compared against.</p><div><div><h3 class="title"><a id="note36"/>Note</h3><p>Recall that OpenCL command queues can be created such that commands are executed out-of-order. In this book, all command queues are created in-order so that they execute in program order also known as program reading order.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec122"/>How to do it…</h2></div></div></div><p>We present our<a id="id509" class="indexterm"/> first attempt to provide you an OpenCL <a id="id510" class="indexterm"/>version of the sequential matrix multiplication algorithm. The kernel can be found in <code class="literal">Ch7/matrix_multiplication_01/simple_mm_mult.cl</code>:</p><div><pre class="programlisting">__kernel void mmmult(int widthB, 
                     int heightA, 
                      __global int* A, 
                      __global int* B, 
                      __global int* C) {

    int i = get_global_id(0);
    int j = get_global_id(1);
    int tmp = 0;

    if ((i &lt; heightA) &amp;&amp; (j &lt; widthB)) {
        tmp = 0;
        for(int k = 0; k &lt; widthB; ++k) {
            tmp += A[i*heightA + k] * B[k*widthB + j];
        }
        C[i*heightA + j] = tmp;
    }
}</pre></div><p>Given the preceding OpenCL kernel code, we need to build an executable so that it can execute on your platform. As before, the compilation will look familiar to you. On my setup with an Intel Core i7 CPU &amp; AMD HD6870x2 GPU running Ubuntu 12.04 LTS, the compilation looks like this and it'll create an executable called <code class="literal">MatrixMultiplication</code> into the directory:</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o MatrixMultiplication -framework OpenCL</strong>
</pre></div><p>At this point, you should have an executable deposited in that directory and all you need to do now is to run the program, simply execute the <code class="literal">MatrixMultiplication</code> program in the directory and you should have noticed an output as follows:</p><div><pre class="programlisting">
<strong>Passed!</strong>
<strong>Execution of matrix-matrix multiplication took X.Xs</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec123"/>How it works…</h2></div></div></div><p>We discussed<a id="id511" class="indexterm"/> how the matrices were initialized and <a id="id512" class="indexterm"/>the next thing is to realize the execution model where each work item in each dimension would work on each element. And to accomplish this, we have to ensure that the invocation to execute the OpenCL kernel code doesn't dictate the size of the thread block:</p><div><pre class="programlisting">size_t globalThreads[] = {widthB, heightA};

cl_event exeEvt; 
cl_ulong executionStart, executionEnd;
error = clEnqueueNDRangeKernel(queue,
                               kernel,
                               2,
                               NULL,
                               globalThreads,
                               NULL, 
                               0,
                               NULL,
                               &amp;exeEvt);
clWaitForEvents(1, &amp;exeEvt);</pre></div><p>We achieve this by passing in the <code class="literal">NULL</code> value to the placeholder meant for dictating work group size in the <code class="literal">clEnqueueNDRangeKernel</code> API. Next, we set the values of the global work items to be equivalent to that of width of matrix B and height of A represented by the <code class="literal">widthB</code> and <code class="literal">heightA</code> variables respectively.</p><p>The following diagram serves to illustrate what the execution would have looked like:</p><div><img src="img/4520OT_07_19.jpg" alt="How it works…"/></div><p>An astute <a id="id513" class="indexterm"/>reader would probably start guessing that <a id="id514" class="indexterm"/>this isn't the best way to conduct this business and you're right! We are going to take a deeper look at how we can make this work better soon.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Faster OpenCL implementation of the matrix multiplication by thread coarsening</h1></div></div></div><p>In this section, let's try to<a id="id515" class="indexterm"/> make this<a id="id516" class="indexterm"/> beast run faster by <a id="id517" class="indexterm"/>applying a technique in parallel programming: thread coarsening. This is important because when you have a work item accessing an element, and then you have large matrices you could potentially have millions of work items running! In general, that's not a good thing because many devices today cannot support millions of work items in <em>n</em> dimensions unless it's a supercomputer. But there are often clever ways to reduce the amount of work items needed.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec124"/>Getting ready</h2></div></div></div><p>The general technique here is to explore ways in which we can merge threads so that each thread now calculates multiple elements. When we reexamine the preceding code, we might wonder if we could do with fewer threads and have them compute more elements, and indeed we can.</p><p>The strategy we have adopted <a id="id518" class="indexterm"/>will basically <a id="id519" class="indexterm"/>have one work item <a id="id520" class="indexterm"/>updating an entire row in the matrix C while walking through matrices A and B. At this time, we need not even explore the use of atomic functions in OpenCL, since that's an aspect we should try to delay exploring as long as possible. The main reason for not exploring the use of atomics is simply because their execution time is too long and it isn't mature of utilizing the capabilities of the OpenCL devices.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec125"/>How to do it...</h2></div></div></div><p>This OpenCL kernel is revised based on the concept of thread coarsening and can be found in <code class="literal">Ch7/matrix_multiplication_02/mmult.cl</code>:</p><div><pre class="programlisting">__kernel void mmmult(int widthB, 
                     int heightA, 
                      __global int* A,  
                      __global int* B,  
                      __global int* C) {

    int i = get_global_id(0); 
    int tmp = 0;

    if (i &lt; heightA) {
        for(int j = 0; j &lt; widthB; ++j) {
            tmp = 0;
            for(int k = 0; k &lt; widthB; ++k) {
                tmp += A[i*heightA + k] * B[k*widthB + j]; 
            }   
            C[i*heightA + j] = tmp;
        }   
    }   
}</pre></div><p>Now that we have taken a good look at the OpenCL kernel, we need to build an executable form. As before, the compilation will look familiar to you. On my setup with an Intel Core i7 CPU &amp; AMD HD6870x2 GPU running Ubuntu 12.04 LTS the compilation looks as follows, and it'll create an executable called <code class="literal">MatrixMultiplication</code> into the directory:</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o MatrixMultiplication -framework OpenCL</strong>
</pre></div><p>At this point, an<a id="id521" class="indexterm"/> executable <a id="id522" class="indexterm"/>should have been <a id="id523" class="indexterm"/>deposited in the directory and to execute it, simply execute the program <code class="literal">MatrixMultiplication</code> in the directory and you should have noticed an output as follows:</p><div><pre class="programlisting">
<strong>Passed!</strong>
<strong>Execution of matrix-matrix multiplication took X.Xs</strong>
</pre></div><p>Now if you were to compare the results with the previous one you would notice that it is running faster!</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec126"/>How it works…</h2></div></div></div><p>The hard part of this is being able to recognize when redundant work is being applied. But in our case, it won't take too much effort to recognize that we are actually using too many threads. How so you may ask? The clue lies in the fact that the original matrix multiplication algorithm ran with one executing thread, so the fact that we are using more than one work item does imply that there's more we can do to improve it.</p><p>Hence when we look back at the algorithm, we discover a way to make them run faster by getting more creative in the way we obtain those values using one work item. At this point, you should convince yourself that the OpenCL kernel we just looked at is indeed referencing the data values from the matrices A and B as expected.</p><p>To achieve what we did, we made some changes to the code in <code class="literal">Ch7/matrix_multiplication_02/MatrixMultiplication.c</code> as follows:</p><div><pre class="programlisting">size_t globalThreads[] = {heightA};
size_t localThreads[] = {256};
cl_event exeEvt; 
cl_ulong executionStart, executionEnd;
error = clEnqueueNDRangeKernel(queue,                                                                               
                               kernel,
                               1,  
                               NULL,
                               globalThreads,
                               localThreads,
                               0,  
                               NULL,
                               &amp;exeEvt);
clWaitForEvents(1, &amp;exeEvt);</pre></div><p>The problem <a id="id524" class="indexterm"/>size is known<a id="id525" class="indexterm"/> to us, which is to perform matrix<a id="id526" class="indexterm"/> multiplication for matrices of dimensions 1024 x 1024 and the reason why I chose the work group size to be 256 is because my GPU has four compute units and you can discover this by passing <code class="literal">CL_DEVICE_MAX_COMPUTE_UNITS</code> to <code class="literal">clGetDeviceInfo</code>. The following diagram illustrates what it is like with thread coarsening:</p><div><img src="img/4520OT_07_20.jpg" alt="How it works…"/></div><p>When you are able to reduce redundant work through thread coarsening, the kernel would now execute faster and scale better because now more processors can execute. It may seem counter intuitive because it defies common sense, since more threads executing the kernel means that it should execute faster. Well, that's the simple picture.</p><p>What happens under the hood is more complicated and it starts from the fact that each GPU has a number of processors and each of those processors would execute the kernel. For a GPU to be able to execute at full capacity, naturally its processors must be filled with data in the data cache and instructions should be ready to be fired and execute the OpenCL kernel.</p><p>However due<a id="id527" class="indexterm"/> to poor data <a id="id528" class="indexterm"/>spatial and temporal locality, the <a id="id529" class="indexterm"/>data caches perform suboptimal and that causes stalls in the instruction pipeline, which translates to delayed execution. Another problem is also related to the fact that memory access patterns could be erratic or non-coalesced which translates to cache misses and possibly memory ejection. This finally causes more delays.</p><p>Coming back to the problem, there is another solution for optimizing the kernel and that's by reusing the hardware registers of the work items.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Faster OpenCL implementation of the matrix multiplication through register tiling</h1></div></div></div><p>Register tiling <a id="id530" class="indexterm"/>is another technique we can apply to our matrix multiplication algorithm. What it basically means is to explore opportunities to reuse the hardware registers. In our case, what it means is that we need to examine the kernel code and find opportunities to reuse registers.</p><p>Now we need to put on our hardcore C developer hat (this person needs to think on the level of the processor core, how data moves on buses, memory loads and stores, and so on). And once your mind is sensitive enough to this level, then things become better.</p><p>Recall the kernel code <a id="id531" class="indexterm"/>in the <a id="id532" class="indexterm"/>previous section and <a id="id533" class="indexterm"/>we would notice after careful scrutiny that the <code class="literal">A[i * heightA + k]</code> statement is always executed in the loop structure, and this causes a lot of memory traffic to transpire because data needs to be loaded from device memory into the registers of the device.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec127"/>Getting ready</h2></div></div></div><p>To reduce the global memory traffic caused by the <code class="literal">A[i * heightA + k]</code> statement, we can pull that statement out of the loop structure and create a thread local memory structure that is visible only to the work item executing thread, and then we can reuse that prefetched data in the subsequent computations.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec128"/>How to do it</h2></div></div></div><p>This OpenCL kernel code is found in <code class="literal">Ch7/matrix_multiplication_03/mmult.cl</code>:</p><div><pre class="programlisting">__kernel void mmmult(int, 
                     int widthB heightA, 
                      __global int* A,                      __global int* B, 
                      __global int* C) {

    int i = get_global_id(0); 

    int tmp = 0;

    int tmpData[1024];

    if (i &lt; heightA) {
        for(int k = 0; k &lt; widthB; ++k )
            tmpData[k] = A[i*heightA + k];

        for(int j = 0; j &lt; widthB; ++j) {
            tmp = 0;
            for(int k = 0; k &lt; widthB; ++k) {
                tmp += tmpData[k] * B[k*widthB + j];
            }
            C[i*heightA + j] = tmp;
        }
    }
}</pre></div><p>Now that we<a id="id534" class="indexterm"/> have taken a good look at the<a id="id535" class="indexterm"/> OpenCL<a id="id536" class="indexterm"/> kernel, we need to build an executable form, where we can execute. As before, the compilation will look familiar to you. On my setup with an Intel Core i7 CPU &amp; AMD HD6870x2 GPU running Ubuntu 12.04 LTS, the compilation looks like this and it'll create an executable called <code class="literal">MatrixMultiplication</code> into the directory:</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o MatrixMultiplication -framework OpenCL</strong>
</pre></div><p>At this point, the executable should be available to you in the directory. To run the program, simply execute the program in the <code class="literal">MatrixMultiplication</code> directory and you should notice an output as follows:</p><div><pre class="programlisting">
<strong>Passed!</strong>
<strong>Execution of matrix-matrix multiplication took X.Xs</strong>
</pre></div><p>Now if you were to compare the results with the previous one you would notice that it is running faster.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec129"/>How it works…</h2></div></div></div><p>The idea<a id="id537" class="indexterm"/> originated from a technique<a id="id538" class="indexterm"/> found in high performance <a id="id539" class="indexterm"/>computing and some folks like to call it scalar replacement. This is the form we have applied in this section. Let's take some time to understand this with a simple algorithm.</p><p>Let's say we have the following algorithm:</p><div><pre class="programlisting">for i1 = 1 to 6
  for i2 = 1 to 6
    A[i1,i2] = A[i1 – 1, i2] + A[i1,i2 -2]</pre></div><p>Now we unroll the loop so that it looks like this:</p><div><pre class="programlisting">for i1 = 1 to 6 step-by-2
  for i2 = 1 to 6 step-by-2
    A[i1,i2] = A[i1 –1, i2] + A[i1,i2 -2]    //statement 1
    A[i1 +1,i2] = A[i1,i2] + A[i1+1,i2 -1]    //statement 2
    A[i1,i2 +1] = A[i1 –1, i2+1] + A[i1,i2]   //statement 3
    A[i1+1,i2+1] = A[i1, i2 +1] + A[i1+1,i2]</pre></div><p>When we will carefully observe this code, we will notice that the <code class="literal">statement 1</code>, <code class="literal">statement 2</code>, and <code class="literal">statement 3</code> have something in common and that is this code, <code class="literal">A[i1,i2]</code>. In computer science terms, we noticed that there is one store to memory and two loads from memory to registers. In scalar replacement, we replace <code class="literal">A[i1,i2]</code> with a variable, which we call <code class="literal">X</code> for now. The code now looks as follows after scalar replacement:</p><div><pre class="programlisting">for i1 = 1 to 6 step-by-2
  X = A[i1,0]
  for i2 = 1 to 6 step-by-2
    X          = A[i1 –1, i2] + X
    A[i1 +1,i2] = X + A[i1+1,i2 -1]    
    A[i1,i2 +1] = A[i1 –1, i2+1] + X   
    A[i1+1,i2+1] = A[i1, i2 +1] + A[i1+1,i2]
     A[i1,i2] = X </pre></div><p>When the replacements have been done consistently and the algorithm is still working as it should, we are good for now. Have a cup of tea!</p><p>Let's have a look at what we did. We have replaced array references (which are in fact memory references) with scalars, and how it helps is that we have actually reduced memory traffic by processing those items in register memory. Considering that memory speed is significantly much slower than register read-write speed, this revised algorithm is in much better form.</p><div><div><h3 class="title"><a id="tip23"/>Tip</h3><p>Loop unrolling is often used to explode the loop, so that we can identify expressions or statements that can possibly be repeating and allowing scalar replacement to extract those expressions/statements into thread private register memory.</p></div></div><p>Scalar<a id="id540" class="indexterm"/> replacement is <a id="id541" class="indexterm"/>actually more complicated<a id="id542" class="indexterm"/> in actual practice, but the presentation here serves its purpose in illustrating the general concept.</p><p>Another thing we like to share with you is to optimize memory usage for the work items and we've caught several glimpses of it before in previous chapters.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec56"/>Reducing global memory via shared memory data prefetching in matrix multiplication</h1></div></div></div><p>Our revised matrix <a id="id543" class="indexterm"/>multiplication <a id="id544" class="indexterm"/>algorithm <a id="id545" class="indexterm"/>appears to be pretty good but it isn't quite there yet. The algorithm is still making a lot of references to matrix B over global memory and we can actually reduce this traffic by prefetching the data. You may not have noticed, but the concept of prefetching, which is to keep the cache "hot" (an idea borrowed from the CPU). A CPU typically has a good size of data and instruction caches (which are really hardware registers), so that the processor can take advantage of the spatial and temporal localities of the data. How does this concept map into other OpenCL devices, for example, the GPU?</p><p>Every GPU that is an OpenCL compliant has a small amount of memory designed for this purpose and their sizes typically are 32 KB to 64 KB. If you wish to determine the exact amount of available high speed memory, simply pass the <code class="literal">CL_DEVICE_LOCAL_MEM_SIZE</code> variable to <code class="literal">clGetDeviceInfo</code> for a device.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec130"/>Getting ready</h2></div></div></div><p>In order for us to be able to reduce references to global memory, we need to make changes in our code so that we load the data we need. Sieving through the code again, we see that there is indeed one such opportunity and it is the following statement:</p><div><pre class="programlisting">for(int j = 0; j &lt; widthB; ++j) {
    tmp = 0;
    for(int k = 0; k &lt; widthB; ++k) {
        tmp += tmpData[k] * B[k*widthB + j];
    }
//more code omitted
}</pre></div><p>Concentrating <a id="id546" class="indexterm"/>on this <a id="id547" class="indexterm"/>loop, we noticed <a id="id548" class="indexterm"/>that matrix B always gets loaded and its values are always reused by all work items executing this kernel. We could of course preload this data into shared memory. That should reduce global memory requests significantly.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec131"/>How to do it...</h2></div></div></div><p>The following OpenCL kernel can be found in <code class="literal">Ch7/matrix_multiplicatione_04/mmult.cl</code>:</p><div><pre class="programlisting">__kernel void mmmult(int widthB, 
                     int heightA, 
                      __global int* A,  
                      __global int* B,  
                      __global int* C,
                      __local  int* shared) {

    int i = get_global_id(0);
    int id = get_local_id(0);
    int size = get_local_size(0);
    int tmp = 0;

    int tmpData[1024];

    if (i &lt; heightA) {
        /*
         Pre-load the data into the work-item's register memory that is 
         Visible to the work-item only. 
         */
        for(int k = 0; k &lt; widthB; ++k ) {
            tmpData[k] = A[i*heightA + k]; 
        }   

        /*
         Data pre-fetching into shared memory allows all work-items
         To read the data off it instead of loading the data from global
         Memory for every work-item
        */
        for(int k = id; k &lt; widthB; k += size) 
            shared[k] = B[k*widthB +k];
        barrier(CLK_LOCAL_MEM_FENCE);

        for(int j = 0; j &lt; widthB; ++j) {
            tmp = 0;
            for(int k = 0; k &lt; widthB; ++k) {
                tmp += tmpData[k] * shared[k];
            }
            C[i*heightA + j] = tmp;
        }
    }
}</pre></div><p>Now that you<a id="id549" class="indexterm"/> have <a id="id550" class="indexterm"/>taken a look at the OpenCL<a id="id551" class="indexterm"/> kernel, you would want to compile the code and run it. As before the compilation will look familiar to you. On my setup with an Intel Core i7 CPU and AMD HD6870x2 GPU running Ubuntu 12.04 LTS, the compilation looks like this and it'll create an executable called <code class="literal">MatrixMultiplication</code> into the directory.</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o MatrixMultiplication -framework OpenCL</strong>
</pre></div><p>To run the program, simply execute the <code class="literal">MatrixMultiplication</code> program in the directory and you should get an output that resembles this:</p><div><pre class="programlisting">
<strong>Passed!</strong>
<strong>Execution of matrix-matrix multiplication took X.Xs</strong>
</pre></div><p>Now if you were to compare the results with the previous one, you would notice that it is running much faster!</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec132"/>How it works…</h2></div></div></div><p>The code that we have introduced might cast some doubts within yourself that because it looks sequential, it is actually executed in parallel during runtime. The parallelism is introduced by the value indicated in the <code class="literal">localThreads</code> variable, which is passed to <code class="literal">clEnqueueNDRangeKernel</code>. The memory barrier we placed into the code serves to stop all work items from executing beyond that point, until all functions before that point have been executed and the following diagram serves to illustrate this:</p><div><img src="img/4520OT_07_22.jpg" alt="How it works…"/></div><p>So far you have <a id="id552" class="indexterm"/>seen changes<a id="id553" class="indexterm"/> made to the <a id="id554" class="indexterm"/>OpenCL kernel code, and now we need to make changes to our host code so that we can actually accomplish this. The following code snippet is taken from <code class="literal">Ch7/matrix_multiplication_04/MatrixMultiplication.c</code>:</p><div><pre class="programlisting">clSetKernelArg(kernel, 0, sizeof(cl_int),(void*)&amp;widthB);
clSetKernelArg(kernel, 1, sizeof(cl_int),(void*)&amp;heightA);
clSetKernelArg(kernel, 2, sizeof(cl_mem),(void*)&amp;matrixAMemObj);
clSetKernelArg(kernel, 3, sizeof(cl_mem),(void*)&amp;matrixBMemObj);
clSetKernelArg(kernel, 4, sizeof(cl_mem),(void*)&amp;matrixCMemObj);
clSetKernelArg(kernel, 5, sizeof(cl_int)*heightA,NULL);
         
size_t globalThreads[] = {heightA};
size_t localThreads[] = {256};
cl_event exeEvt; 
cl_ulong executionStart, executionEnd;
error = clEnqueueNDRangeKernel(queue,
                               kernel,
                               1,
                               NULL,
                               globalThreads,
                               localThreads,
                               0,
                               NULL,
                               &amp;exeEvt);
clWaitForEvents(1, &amp;exeEvt);</pre></div><p>The <a id="id555" class="indexterm"/>schematics <a id="id556" class="indexterm"/>of the final<a id="id557" class="indexterm"/> algorithm have seen us tailoring the algorithm, so that it achieves an initial reasonable performance and can be conceptually represented by the following diagram:</p><div><img src="img/4520OT_07_24.jpg" alt="How it works…"/></div><div><div><h3 class="title"><a id="tip24"/>Tip</h3><p>If you want to know how much shared memory you can possibly create and pass the <code class="literal">CL_DEVICE_LOCAL_MEM_SIZE</code> parameter to <code class="literal">clGetDeviceInfo</code> for your device and the value returned will be in bytes. Typical values are between 32 KB to 64 KB.</p></div></div></div></div></body></html>