<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Assessment</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 1, Why GPU Programming?</h1>
                </header>
            
            <article>
                
<ol>
<li>The first two <kbd>for</kbd> loops iterate over every pixel, whose outputs are invariant to each other; we can thus parallelize over these two <kbd>for</kbd> loops. The third <kbd>for</kbd> loop calculates the final value of a particular pixel, which is intrinsically recursive.</li>
<li>Amdahl's Law doesn't account for the time it takes to transfer memory between the GPU and the host.</li>
<li>512 x 512 amounts to 262,144 pixels. This means that the first GPU can only calculate the outputs of half of the pixels at once, while the second GPU can calculate all of the pixels at once; this means the second GPU will be about twice as fast as the first here. The third GPU has more than sufficient cores to calculate all pixels at once, but as we saw in problem 1, the extra cores will be of no use to us here. So the second and third GPUs will be equally fast for this problem.</li>
<li>One issue with generically designating a certain segment of code as parallelizable with regards to Amdahl's Law is that this makes the assumption that the computation time for this piece of code will be close to 0 if the number of processors, <em>N</em>, is very large. As we can see from the last problem, this is not the case.</li>
<li>First, using <em>time</em> consistently can be cumbersome, and it might not zero in on the bottlenecks of your program. Second, a profiler can tell you the exact computation time of all of your code from the perspective of Python, so you can tell whether some library function or background activity of your operating system is at fault rather than your code.</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 2, Setting Up Your GPU Programming Environment</h1>
                </header>
            
            <article>
                
<ol>
<li>No, CUDA only supports Nvidia GPUs, not Intel HD or AMD Radeon</li>
<li>This book only uses Python 2.7 examples</li>
<li>Device Manager</li>
<li><kbd>lspci</kbd></li>
<li><kbd>free</kbd></li>
<li><kbd>.run</kbd></li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 3, Getting Started with PyCUDA</h1>
                </header>
            
            <article>
                
<ol>
<li>Yes.</li>
<li>Memory transfers between host/device, and compilation time.</li>
<li>You can, but this will vary depending on your GPU and CPU setup.</li>
<li>Do this using the C <kbd>?</kbd> operator for both the point-wise and reduce operations.</li>
<li>If a <kbd>gpuarray</kbd> object goes out of scope its destructor is called, which will deallocate (free) the memory it represents on the GPU automatically.</li>
<li><kbd>ReductionKernel</kbd> may perform superfluous operations, which may be necessary depending on how the underlying GPU code is structured. A <em>neutral element</em> will ensure that no values are altered as a result of these superfluous operations.</li>
<li>We should set <kbd>neutral</kbd> to the smallest possible value of a signed 32-bit integer.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 4, Kernels, Threads, Blocks, and Grids</h1>
                </header>
            
            <article>
                
<ol>
<li>Try it.</li>
<li>All of the threads don't operate on the GPU simultaneously. Much like a CPU switching between tasks in an OS, the individual cores of the GPU switch between the different threads for a kernel.</li>
<li>O( n/640 log n), that is, O(n log n).</li>
<li>Try it.</li>
</ol>
<p> </p>
<ol start="5">
<li>There is actually no internal grid-level synchronization in CUDAâ€”only block-level (with <kbd>__syncthreads).</kbd> We have to synchronize anything above a single block with the host.</li>
<li>Naive: 129 addition operations. Work-efficient: 62 addition operations.</li>
<li>Again, we can't use <kbd>__syncthreads</kbd> if we need to synchronize over a large grid of blocks. We can also launch over fewer threads on each iteration if we synchronize on the host, freeing up more resources for other operations.</li>
<li>In the case of a naive parallel sum, we will likely be working with only a small number of data points that should be equal to or less than the total number of GPU cores, which can likely fit in the maximum size of a block (1032); since a single block can be synchronized internally, we should do so. We should use the work-efficient algorithm only if the number of data points are far greater than the number of available cores on the GPU.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 5, Streams, Events, Contexts, and Concurrency</h1>
                </header>
            
            <article>
                
<ol>
<li>The performance improves for both; as we increase the number of threads, the GPU reaches peak utilization in both cases, reducing the gains made through using streams.</li>
<li>Yes, you can launch an arbitrary number of kernels asynchronously and synchronize them to with <kbd>cudaDeviceSynchronize</kbd>.</li>
<li>Open up your text editor and try it!</li>
<li>High standard deviation would mean that the GPU is being used unevenly, overwhelming the GPU at some points and under-utilizing it at others. A low standard deviation would mean that all launched operations are running generally smoothly.</li>
<li>i. The host can generally handle far fewer concurrent threads than a GPU. ii. Each thread requires its own CUDA context. The GPU can become overwhelmed with excessive contexts, since each has its own memory space and has to handle its own loaded executable code.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 6, Debugging and Profiling Your CUDA Code</h1>
                </header>
            
            <article>
                
<ol>
<li>Memory allocations are automatically synchronized in CUDA.</li>
<li>The <kbd>lockstep</kbd> property only holds in single blocks of size 32 or less. Here, the two blocks would properly diverge without any <kbd>lockstep</kbd>.</li>
<li>The same thing would happen here. This 64-thread block would actually be split into two 32-thread warps.</li>
<li>Nvprof can time individual kernel launches, GPU utilization, and stream usage; any host-side profiler would only see CUDA host functions being launched.</li>
<li>Printf is generally easier to use for small-scale projects with relatively short, inline kernels. If you write a very involved CUDA kernel with thousands of lines, then probably you would want to use the IDE to step through and debug your kernel line by line.</li>
<li>This tells CUDA which GPU we want to use.</li>
<li><kbd>cudaDeviceSynchronize</kbd> will ensure that interdependent kernel launches and mem copies are indeed synchronized, and that they won't launch before all necessary operations have finished.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 7, Using the CUDA Libraries with Scikit-CUDA</h1>
                </header>
            
            <article>
                
<ol>
<li>SBLAH starts with an S, so this function uses 32-bit real floats. ZBLEH starts with a Z, which means it works with 128-bit complex floats.</li>
<li>Hint: set <kbd>trans = cublas._CUBLAS_OP['T']</kbd></li>
<li>Hint: use the Scikit-CUDA wrapper to the dot product, <kbd>skcuda.cublas.cublasSdot</kbd></li>
<li>Hint: build upon the answer to the last problem.</li>
<li>You can put the cuBLAS operations in a CUDA stream and use event objects with this stream to precisely measure the computation times on the GPU.</li>
<li>Since the input appears as being complex to cuFFT, it will calculate all of the values as NumPy.</li>
<li>The dark edge is due to the zero-buffering around the image. This can be mitigated by <em>mirroring</em> the image on its edges rather than by using a zero-buffer.</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 8, The CUDA Device Function Libraries and Thrust</h1>
                </header>
            
            <article>
                
<ol>
<li>Try it. (It's actually more accurate than you'd think.)</li>
<li>One application: a Gaussian distribution can be used to add <kbd>white noise</kbd> to samples to augment a dataset in machine learning.</li>
<li>No, since they are from different seeds, these lists may have a strong correlation if we concatenate them together. We should use subsequences of the same seed if we plan to concatenate them together.</li>
<li>Try it.</li>
<li>Hint: remember that matrix multiplication can be thought of as a series of matrix-vector multiplications, while matrix-vector multiplication can be thought of as a series of dot products.</li>
<li><kbd>Operator()</kbd> is used to define the actual function.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 9, Implementation of a Deep Neural Network</h1>
                </header>
            
            <article>
                
<ol>
<li>One problem could be that we haven't normalized our training inputs. Another could be that the training rate was too large.</li>
<li>With a small training rate a set of weights might converge very slowly, or not at all.</li>
<li>A large training rate can lead to a set of weights being over-fit to particular batch values or this training set. Also, it can lead to numerical overflows/underflows as in the first problem.</li>
<li>Sigmoid.</li>
<li>Softmax.</li>
<li>More updates.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 10, Working with Compiled GPU Code</h1>
                </header>
            
            <article>
                
<ol>
<li>Only the EXE file will have the host functions, but both the PTX and EXE will contain the GPU code.</li>
<li><kbd>cuCtxDestory</kbd>.</li>
<li><kbd>printf</kbd> with arbitrary input parameters. (Try looking up the <kbd>printf</kbd> prototype.)</li>
<li>With a Ctypes <kbd>c_void_p</kbd> object.</li>
<li>This will allow us to link to the function with its original name from Ctypes.</li>
<li>Device memory allocations and memcopies between device/host are automatically synchronized by CUDA.</li>
</ol>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 11, Performance Optimization in CUDA</h1>
                </header>
            
            <article>
                
<ol>
<li>The fact that <kbd>atomicExch</kbd> is thread-safe doesn't guarantee that all threads will execute this function at the same time (which is not the case since different blocks in a grid can be executed at different times).</li>
<li>A block of size 100 will be executed over multiple warps, which will not be synchronized within the block unless we use <kbd>__syncthreads</kbd>. Thus, <kbd>atomicExch</kbd> may be called at multiple times.</li>
<li>Since a warp executes in lockstep by default, and blocks of size 32 or less are executed with a single warp, <kbd>__syncthreads</kbd> would be unnecessary.</li>
<li>We use a naÃ¯ve parallel sum within the warp, but otherwise, we are doing as many sums with<kbd>atomicAdd</kbd> as we would do with a serial sum. While CUDA automatically parallelizes many of these <kbd>atomicAdd</kbd> invocations, we could reduce the total number of required <kbd>atomicAdd</kbd> invocations by implementing a work-efficient parallel sum.</li>
<li>Definitely <kbd>sum_ker</kbd>. It's clear that PyCUDA's sum doesn't use the same hardware tricks as we do since ours performs better on smaller arrays, but by scaling the size to be much larger, the only explanation as to why PyCUDA's version is better is that it performs fewer addition operations.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chapter 12, Where to Go from Here</h1>
                </header>
            
            <article>
                
<ol>
<li>Two examples: DNA analysis and physics simulations.</li>
<li>Two examples: OpenACC, Numba.</li>
<li>TPUs are only used for machine learning operations and lack the components required to render graphics.</li>
<li>Ethernet.</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>