- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing Data Science Projects with Snowpark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The emergence of cloud technologies has ushered in a new era of possibilities.
    With the advent of Data Cloud, a robust platform that unifies data storage, processing,
    and analysis, data scientists have many opportunities to explore, analyze, and
    extract meaningful insights from vast datasets. In this intricate digital realm,
    the role of Snowpark, a cutting-edge data processing framework, becomes paramount.
    This chapter serves as an illuminating guide, delving deep into developing data
    science projects with Snowpark, unraveling its intricacies, and harnessing its
    potential to the fullest extent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Data science in Data Cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and preparing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training **machine learning** (**ML**) models in Snowpark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you’ll require an active Snowflake account and Python installed
    with Anaconda configured locally. You can sign up for a Snowflake Trial account
    at [https://signup.snowflake.com/](https://signup.snowflake.com/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: To configure Anaconda, follow [https://conda.io/projects/conda/en/latest/user-guide/getting-started.html](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In addition, to install and set up Python for VS Code, follow [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to operate Jupyter Notebook in VS Code, go to [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The supporting materials for this chapter are available in this book’s GitHub
    repository at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Data science in Data Cloud
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science transcends traditional boundaries in the Data Cloud ecosystem,
    offering a dynamic environment where data scientists can harness the power of
    distributed computing and advanced analytics. With the ability to seamlessly integrate
    various data sources, including structured and unstructured data, Data Cloud provides
    a data exploration and experimentation environment. We will start this section
    with a brief data science and ML refresher.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Data science and ML concepts
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data science and ML have surged to the forefront of technological and business
    innovation, becoming integral components of decision-making, strategic planning,
    and product development across virtually all industries. The journey to their
    current popularity and influence is a testament to several factors, including
    advancements in technology, the explosion of data, and the increasing computational
    power available. This section will briefly discuss data science and ML concepts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Data science
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data science is a multidisciplinary field that relies on various software tools,
    algorithms, and ML principles to extract valuable insights from extensive datasets.
    Data scientists are pivotal in collecting, transforming, and converting data into
    predictive and prescriptive insights. By employing sophisticated techniques, data
    science uncovers hidden patterns and meaningful correlations within data, enabling
    businesses to act on informed decisions based on empirical evidence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) encompasses the science and engineering
    of creating intelligent machines and brilliant computer programs capable of autonomously
    processing information and generating outcomes. AI systems are designed to solve
    intricate problems using logic and reasoning, similar to human cognitive processes.
    These systems operate autonomously, aiming to emulate human-like intelligence
    in decision-making and problem-solving tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: ML
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ML, a subset of AI, involves specialized algorithms integrated into the data
    science workflow. These algorithms are meticulously crafted software programs
    that are designed to detect patterns, identify correlations, and pinpoint anomalies
    within data. ML algorithms excel at predicting outcomes based on existing data
    and continue to learn and improve their accuracy as they encounter new data and
    situations. Unlike humans, ML algorithms can process thousands of attributes and
    features, enabling the discovery of unique combinations and correlations in vast
    datasets. This capability makes ML indispensable for extracting valuable insights
    and predictions from extensive data collections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the terminologies straightened out, we will discuss how the
    Data Cloud paradigm has helped the growth of data science and ML for organizations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The Data Cloud paradigm
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data science in the cloud represents a paradigm shift in how data-driven insights
    are derived and applied. In this innovative approach, data science processes,
    tools, and techniques are seamlessly integrated into the cloud, allowing organizations
    to leverage the power of scalable infrastructure and advanced analytics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of this paradigm lies Data Cloud, a dynamic ecosystem that transcends
    traditional data storage and processing constraints. The Data Cloud paradigm represents
    a seismic shift from conventional data silos, offering a unified platform where
    structured and unstructured data coalesce seamlessly. Through distributed computing,
    parallel processing, and robust data management, Data Cloud sets the stage for
    a data science revolution. The capabilities and tools that empower data scientists
    are seamlessly integrated and are designed to handle diverse data types and analytical
    workloads within Data Cloud. As such, Data Cloud offers various advantages for
    running data science and ML workloads.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Data Cloud for data science
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key advantages of Snowflake’s Data Cloud is the ability to store
    and process vast amounts of data without the constraints of hardware limitations.
    It offers a scalable solution to handling vast volumes of data, enabling data
    scientists to work with extensive datasets without having to worry about computing
    or storage constraints. The cloud-based interface provides a collaborative and
    flexible environment for data scientists and analysts and comes with built-in
    collaboration features, version control, and support for popular data science
    libraries and frameworks through Snowpark.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Data Cloud offers a diverse ecosystem of services and resources
    tailored for data science tasks through managed services that simplify these processes,
    from data ingestion and preparation to ML model training and deployment. For instance,
    data pipelines can be automated using serverless computing, and ML models can
    be trained on powerful GPU instances, leading to faster experimentation and iteration.
    Data security and compliance are paramount in data science, especially when dealing
    with sensitive information, and Data Cloud provides different security measures,
    including encryption, access control, and row-level policies, ensuring that data
    scientists can work with sensitive data in a secure and compliant manner, adhering
    to industry regulations and organizational policies. The Snowpark framework is
    at the center of Snowflake’s Data Cloud to support these capabilities. The following
    section will discuss why Snowpark is used for data science and ML.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Why Snowpark for data science and ML?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark offers unparalleled integration capabilities for data engineers, enabling
    seamless interaction with data stored in large volumes and diverse formats. Its
    versatile API facilitates effortless data exploration, transformation, and manipulation,
    laying a robust foundation for data science models and ML development and empowering
    data scientists to harness the full potential of their analytical workflows. Data
    science teams can now focus on their core tasks without the hassle of infrastructure
    or environment maintenance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark excels in scalability and performance, which is crucial for enterprise
    data science and ML workloads; leveraging Snowflake’s distributed architecture
    to handle massive datasets and complex computations with remarkable efficiency
    and the ability to parallelize processing tasks and distribute workloads across
    multiple nodes ensures lightning-fast execution, even when dealing with petabytes
    of data. These features, combined with Snowflake’s automatic optimization features,
    allow data scientists to focus on their analyses without being burdened by infrastructure
    limitations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark offers a rich array of advanced analytics capabilities that are indispensable
    for data science and ML tasks. From statistical analysis to predictive modeling,
    geospatial analytics, or even data mining, it provides a comprehensive toolkit
    for data scientists to explore complex patterns and extract valuable insights.
    Its support for ML libraries and algorithms further amplifies its utility, enabling
    the development of sophisticated models for classification, regression, and clustering.
    With the rich features and functionalities mentioned previously, Snowpark provides
    many benefits for data science and ML workloads. In the next section, we will
    explore the world of the Snowpark ML library and its different functionalities.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Snowpark ML
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark constitutes a compendium of libraries and runtimes within Snowflake,
    facilitating the secure deployment and processing of non-SQL code by encompassing
    languages such as Python with the code execution that occurs server-side within
    the Snowflake infrastructure, all while leveraging a virtual warehouse. The newest
    addition to the Snowpark libraries is Snowpark ML. Snowpark ML represents a groundbreaking
    fusion of Snowflake’s powerful data processing capabilities and the transformative
    potential of ML. As the frontier of data science expands, Snowpark ML emerges
    as a cutting-edge framework that’s designed to empower data professionals to harness
    the full potential of their data within Snowflake’s cloud-based environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, Snowpark ML is engineered to facilitate seamless integration between
    Snowflake’s data processing capabilities and advanced ML techniques. With Snowpark
    ML, data scientists, analysts, and engineers can leverage familiar programming
    languages and libraries to develop sophisticated ML models directly within Snowflake.
    This integration eliminates the barriers between data storage, processing, and
    modeling, streamlining the end-to-end data science workflow. Snowpark ML catalyzes
    innovation, enabling data professionals to efficiently explore, transform, and
    model data. By bridging the gap between data processing and ML, Snowpark ML empowers
    organizations to make data-driven decisions, uncover valuable insights, and drive
    business growth in the digital age. The following figure shows the Snowpark ML
    framework:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Snowpark ML](img/B19923_05_1.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Snowpark ML
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The preceding architecture consists of various components that work cohesively
    together. We will look at each of these components in more detail in the next
    section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark ML API
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to Snowpark DataFrame, which helps operate with the data, Snowpark
    ML provides APIs as a Python library called `snowflake-ml` to support every stage
    of the ML development and deployment process, allowing support for pre-processing
    data and training, managing, and deploying ML models all within Snowflake:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Snowpark ML API](img/B19923_05_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Snowpark ML API
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The Snowpark ML API consists of Snowpark ML modeling for developing and training
    the models and Snowpark ML Ops for monitoring and operating the model. The `snowflake.ml.modeling`
    module provides APIs for pre-processing, feature engineering, and model training
    based on familiar libraries, such as scikit-learn and XGBoost. The complete end-to-end
    ML experience can be done using Snowpark. We’ll cover this in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end ML with Snowpark
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The quest for seamless, end-to-end ML solutions has become paramount, and Snowpark
    offers a comprehensive ecosystem for end-to-end ML. This section delves into the
    intricate world of leveraging Snowpark to craft end-to-end ML pipelines, from
    data ingestion and preprocessing to model development, training, and deployment,
    unveiling the seamless process of developing ML within Snowflake’s robust framework.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'ML processes involve a systematic approach to solving complex problems through
    data processing. This typically includes stages such as defining the problem,
    collecting and preparing data, **exploratory data analysis** (**EDA**), feature
    engineering, model selection, training, evaluation, and deployment, with each
    operation being crucial and iterative. It allows data scientists to refine their
    approaches based on insights gained along the way. The process is often cyclical,
    with continuous iterations to improve models and predictions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – End-to-end ML flow](img/B19923_05_3.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – End-to-end ML flow
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: We can broadly classify the ML stages as preparing and transforming, training
    and building the model, and interfering with the model to obtain prediction results.
    We will discuss each of these steps next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and transforming the data
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Raw data is often messy, containing missing values, outliers, and inconsistencies.
    Getting the correct data from multiple systems usually consumes most of the data
    scientist’s time. Snowflake solves this problem by providing a governed Data Cloud
    paradigm that supports all types of data and provides a unified place to instantly
    store and consume relevant data to unlock ML models’ power. The data preparation
    and transformation process involves EDA, cleaning, and processing, and ends with
    feature engineering. This step also consists of data engineering pipelines, which
    help apply data transformations to prepare the data for the next step. For data
    pre-processing, `snowflake.ml.modeling`, preprocessing, and Snowpark functions
    can be used to transform the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: EDA
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EDA is a critical step that involves preliminary investigations to understand
    the data’s structure, patterns, and trends as it helps uncover hidden patterns
    and guide feature selection. Data scientists and analysts collaborate closely
    with business stakeholders to define the specific questions that need to be answered
    or the problems that need to be solved, which guides them in selecting the relevant
    data. Through charts, graphs, and statistical summaries, data scientists can identify
    patterns, trends, correlations, and outliers within the dataset, all of which
    provide valuable insights into the data’s distribution and help them understand
    the data better to build feature selection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning and preprocessing
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data cleaning involves handling missing data, correcting errors, and ensuring
    consistency. The data is suitable for training the model through preprocessing
    techniques such as normalization, scaling, and transformations, along with various
    sampling techniques that are applied to evaluate a subset of the data, providing
    insights into its richness and variability.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Feature engineering involves creating new features or modifying existing ones
    to enhance the performance of ML models. It requires domain expertise to identify
    relevant features that can improve predictive accuracy. Performing feature engineering
    on the centralized data in Snowflake accelerates model development, reduces costs,
    and enables the reuse of new features. Some techniques, such as creating interaction
    terms and transforming variables, extract meaningful information from raw data,
    making it more informative for modeling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Training and building the model
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the data is ready and the features have been built, the next step is to
    train and develop the model. In this stage, the data scientist trains various
    models, such as regression, classification, clustering, or deep learning, depending
    on the nature of the problem, by passing a subset of the data, or training set,
    through the modeling function to derive a predictive function. The model is developed
    using statistical methods for hypothesis testing and inferential statistics. Advanced
    techniques, such as ensemble methods, neural networks, and natural language processing,
    are also used, depending on the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Once the model has been developed, it’s tested on data that wasn’t part of the
    training set to determine its effectiveness, which is usually measured in terms
    of its predictive strength and robustness, and the model is optimized with hyperparameter
    tuning. Cross-validation techniques optimize the model’s performance, ensuring
    accurate predictions and valuable insights.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: This combination of steps enables data scientists to conduct in-depth feature
    engineering, tune hyperparameters, and iteratively create and assess ML models.
    Intuitions become accurate predictions as data scientists experiment with various
    algorithms, evaluating the performance of each model and adjusting parameters
    on their chosen model to optimize the code for their specific datasets. `snowflake.ml.modeling`
    can be used for training by utilizing the `fit()` method for an algorithm such
    as XGBoost.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the models have been trained, Snowpark ML supports their seamless deployment
    and inference. Models can be deployed for inference, enabling organizations to
    make data-driven decisions based on predictive insights. Snowpark ML has a model
    registry to manage and organize Snowpark models throughout their life cycle. The
    model registry supports versioning of the models and stores metadata information
    about the models, hyperparameters, and evaluation metrics, facilitating experimentation
    and model comparison. It also supports model monitoring and auditing and aids
    in collaboration between data scientists working on the model. The model registry
    is part of Snowpark MLOps and can be accessed through `snowpark.ml.registry`.
    The pipelines can be orchestrated using Snowflake Tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established the foundations of Snowpark ML, its place in ML,
    and how Snowpark supports data science workloads, we will dive deep into the complete
    data science scenario with Snowpark. The following section will focus on exploring
    and preparing the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: A note on data engineering
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll conduct exploration, transformation, and feature
    engineering using Snowpark Python and pandas. As we proceed to build models with
    SnowparkML, we will incorporate some of the steps discussed earlier in this section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and preparing data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first step of the ML process, we must explore and prepare the data in
    Snowflake using Snowpark to make it available for training the ML models. We will
    work with the Bike Sharing dataset from Kaggle, which offers an hourly record
    of rental data for 2 years. The primary objective is to forecast the number of
    bikes rented each hour for a specific timeframe based solely on the information
    available before the rental period. In essence, the model will harness the power
    of historical data to predict future bike rental patterns using Snowpark. More
    information about the particular dataset has been provided in the respective GitHub
    repository ([https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Data exploration allows us to dissect the data to uncover intricate details
    that might otherwise stay hidden, acting as the foundation for our entire analysis.
    We will start the process by loading the dataset into a Snowpark DataFrame:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the data has been successfully loaded, the subsequent imperative is to
    gain a comprehensive understanding of the dataset’s scale:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Fortunately, Snowpark provides functions specifically designed to facilitate
    this critical task:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Total number of columns](img/B19923_05_4.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Total number of columns
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know the scale of the data, let’s get a sense of it by looking
    at a few rows of the dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This returns the two rows from the data for analysis:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Two rows of data](img/B19923_05_5.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Two rows of data
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in the preceding figure, the `COUNT` column is a straightforward
    aggregation of `CASUAL` and `REGISTERED`. In data science, these types of variables
    are commonly referred to as “leakage variables.” When we construct our models,
    we’ll delve deeper into strategies for managing these variables. Date columns
    consistently present an intriguing and complex category to grapple with. Within
    this dataset, there is potential to create valuable new features derived from
    the `DATETIME` column, which could significantly influence our response variables.
    Before we start with data cleansing and the feature engineering process, let’s
    see the column type to understand and make more informed decisions:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will give us the schema and the data types for each field so that we can
    understand the data better:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Schema information](img/B19923_05_6.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Schema information
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are equipped with basic information about the data, let’s start
    finding the missing values in the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Missing value analysis
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Addressing missing values is a fundamental preprocessing step in ML. Incomplete
    data can disrupt model training and hinder predictive accuracy, potentially leading
    to erroneous conclusions or suboptimal performance. By systematically imputing
    or filling these gaps, we can bolster the integrity of our dataset, providing
    ML algorithms with a more comprehensive and coherent dataset for more robust and
    reliable analyses and predictions. This practice is akin to affording our models
    the necessary information to make sound, data-driven decisions. Let’s check for
    any missing values in our dataset:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code helps us find out whether any values are empty or missing:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7  – Missing value analysis](img/B19923_05_7.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Missing value analysis
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Our initial examination of missing values in the column shows no missing values
    in our dataset. However, a closer examination reveals the presence of numerous
    0s within the `WINDSPEED` column, which is indicative of potentially missing values.
    Logically, windspeed cannot equate to zero, implying that each `0` within the
    column signifies a missing value:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will print out the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Output value](img/B19923_05_8.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Output value
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that there are `1313` values in the `WINDSPEED` column. With this
    column harboring missing data, the subsequent challenge is determining an effective
    strategy for imputing these missing values. As is widely acknowledged, various
    methods exist for addressing missing data within a column. In our case, we’ll
    employ a straightforward imputation, substituting the 0s with the mean value of
    the column:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code replaces the 0s with the mean value of the column:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Pre-processed data](img/B19923_05_9.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Pre-processed data
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our preprocessing journey. Next, we’ll perform outlier analysis.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Outlier analysis
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of detecting and removing outliers is pivotal in enhancing model
    accuracy and robustness. Outliers are data points that significantly deviate from
    most datasets, often stemming from errors, anomalies, or rare events. These aberrations
    can unduly influence model training, leading to skewed predictions or reduced
    generalization capabilities. By identifying and eliminating outliers, we can improve
    the quality and reliability of our models and ensure that they are better equipped
    to discern meaningful patterns within the data. This practice fosters more accurate
    predictions and a higher level of resilience, ultimately contributing to the overall
    success of ML endeavors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be transforming the DataFrame into a pandas DataFrame so that we can
    conduct insightful analyses, including constructing visualizations to extract
    meaningful patterns. Our initial focus is on the `COUNT` column as the response
    variable. Before model development, it is imperative to ascertain whether the
    `COUNT` column contains any outlier values:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code generates a plot using the `seaborn` and the `matplotlib`
    library to help us find the outliers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Outlier plot](img/B19923_05_10.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Outlier plot
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the `COUNT` column exhibits outlier data points that can potentially
    negatively impact model performance if they’re not adequately addressed. Mitigating
    outliers is a critical preprocessing step. One widely adopted approach involves
    removing data points that lie beyond a predefined threshold or permissible range,
    as outlined here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code uses the Snowpark library to analyze a dataset stored in
    `df_table`. It calculates the mean (average) and standard deviation (a measure
    of data spread) of the `''count''` column in the dataset. Then, it identifies
    and removes outliers from the dataset. Outliers are data points that significantly
    differ from the average. In this case, it defines outliers as data points more
    than three times the standard deviation away from the mean. After identifying
    these outliers, it displays the dataset without the outlier values, using `df_without_outlier.show()`
    to help with further analysis:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Outliers removed](img/B19923_05_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Outliers removed
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have taken care of the outliers, we can perform correlation analysis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Correlation analysis
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying correlations among variables is of paramount importance for several
    vital reasons. Correlations provide valuable insights into how different features
    in the dataset relate to each other. By understanding these relationships, ML
    models can make more informed predictions as they leverage the strength and direction
    of correlations to uncover patterns and dependencies. Moreover, identifying and
    quantifying correlations aids feature selection, where irrelevant or highly correlated
    features can be excluded to enhance model efficiency and interpretability. It
    also helps identify potential multicollinearity issues, where two or more features
    are highly correlated, leading to unstable model coefficients. Recognizing and
    harnessing correlations empowers ML models to make better predictions and yield
    more robust results, making it a fundamental aspect of modeling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code generates the correlation matrix as a heatmap visualization:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Correlation matrix heatmap](img/B19923_05_12_V.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Correlation matrix heatmap
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: This heatmap visualization reveals a substantial correlation between the `TEMP`
    and `ATEMP` variables, signifying a condition known as multicollinearity. Multicollinearity
    occurs when two or more predictors in a model are highly correlated, distorting
    the model’s interpretability and stability. To mitigate this issue and ensure
    the reliability of our analysis, we have opted to retain the `TEMP` variable while
    removing `ATEMP` from consideration in our subsequent modeling endeavors. This
    strategic decision is made to maintain model robustness and effectively capture
    the essence of the data without the confounding effects of multicollinearity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Leakage variables
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Leakage variables** in data science inadvertently include information that
    would not be available during prediction or decision-making in a real-world scenario.
    Eliminating them is crucial because using leakage variables can lead to overly
    optimistic model performance and unreliable results. It’s essential to detect
    and exclude these variables during data preprocessing to ensure that our models
    make predictions based on the same information that would be accessible. By doing
    so, we prevent the risk of building models that work well on historical data but
    fail to perform in real-world situations, which is a crucial goal in data science.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the `CASUAL`, `REGISTERED`, and `COUNT` columns exhibit
    high collinearity, with `COUNT` being an explicit summation of `CASUAL` and `REGISTERED`.
    This redundancy renders the inclusion of all three variables undesirable, resulting
    in a leakage variable situation. To preserve the integrity of our model-building
    process, we shall eliminate `CASUAL` and `REGISTERED` from our feature set, thereby
    mitigating any potential confounding effects and ensuring the model’s ability
    to make predictions based on the most relevant and non-redundant information.
    The next step is to perform feature engineering with the prepared data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Feature engineering** in ML is like crafting the perfect tool for a specific
    job. It involves selecting, transforming, or creating new features (variables)
    from the available data to make it more suitable for ML algorithms. This process
    is crucial because it helps the models better understand the patterns and relationships
    in the data, leading to improved predictions and insights. By carefully engineering
    features, we can uncover hidden information, reduce noise, and enhance the model’s
    performance, making it a vital step in building effective and accurate ML systems.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing the data shows that the `DATETIME` column is a promising candidate
    for feature engineering within this dataset. Given the dependency of the predictive
    outcome on temporal factors such as the time of day and day of the week, deriving
    time-related features assumes paramount significance. Extracting these temporal
    features is pivotal as it enhances the model’s performance and elevates the overall
    predictive accuracy by capturing essential nuances about the dataset’s material
    characteristics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code enriches a DataFrame by creating new columns that capture
    specific time and date details from the `DATETIME` column:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – DATETIME data](img/B19923_05_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – DATETIME data
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The `hour` column tells us the hour of the day, the `month` column identifies
    the month, the `date` column extracts the date itself, and the `weekday` column
    signifies the day of the week. These additional columns provide a more comprehensive
    view of the time-related information within the dataset, enhancing its potential
    for in-depth analysis and ML applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: This step concludes our data preparation and exploration journey. The following
    section will use this prepared data to build and train our model using Snowpark.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: A note on the model-building process
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In our model-building process, we won’t be incorporating all the steps we’ve
    discussed thus far. Instead, we’ll focus on two significant transformations to
    showcase Snowpark ML pipelines. Additionally, the accompanying notebook (**chapter_5.ipynb**)
    illustrates model building using Python’s scikit-learn library and how to call
    them as stored procedures. This will allow you to compare and contrast how the
    model-building process is simplified through Snowpark ML. To follow through the
    chapter, you can skip the model building process using the scikit-learn section
    and directly go to the Snowpark ML section in the notebook.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Training ML models in Snowpark
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have prepared our dataset, the pinnacle of our journey involves
    the model-building process, for which we will be leveraging the power of Snowpark
    ML. Snowpark ML emerges as a recent addition to the Snowpark arsenal, strategically
    deployed to streamline the intricacies of the model-building process. Its elegance
    becomes apparent when we engage in a comparative exploration of the model-building
    procedure through the novel ML library. We will start by developing the pipeline
    that we’ll use to train the model using the data we prepared previously:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code creates a preprocessing pipeline for the dataset by using
    various Snowpark ML functions. The `preprocessing` and `pipeline` modules are
    imported as these are essential for developing and training the model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Transformed data](img/B19923_05_14.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Transformed data
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline includes ordinal encoding for categorical columns (`SEASON` and
    `WEATHER`) and min-max scaling for numerical columns (`TEMP`). The pipeline is
    saved into the stage using the `joblib` library, which can be utilized for consistent
    preprocessing in future analyses. Now that we have the pipeline code ready, we
    will build the features that are required for the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code defines lists representing categorical columns, one-hot
    encoded categorical columns, and columns for min-max scaling. It also specifies
    a feature list, label columns, and output columns for an ML model. The `preprocessing_pipeline.joblib`
    file is loaded and assumed to contain a previously saved preprocessing pipeline.
    These elements collectively prepare the necessary data and configurations for
    subsequent ML tasks, ensuring consistent handling of categorical variables, feature
    scaling, and model predictions based on the pre-established pipeline. We will
    now split the data into training and testing sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code divides the dataset into training (70%) and testing (30%)
    sets using a random split. It applies the previously defined preprocessing pipeline
    to transform both sets, displaying the transformed training and testing datasets
    and ensuring consistent preprocessing for model training and evaluation. The output
    shows the different training and testing data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Training and testing dataset](img/B19923_05_15.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Training and testing dataset
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll train the model with the training data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `LinearRegression` class defines the model, specifying the input columns
    (categorical columns after one-hot encoding and additional features), label columns
    (the target variable – that is, `COUNT`), and output columns for predictions.
    The model is trained on the transformed training dataset using `fit`, and then
    predictions are generated for the transformed testing dataset using `predict`.
    The resulting predictions are displayed, assessing the model’s performance on
    the test data:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Predicted output](img/B19923_05_16.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Predicted output
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to calculate various performance metrics to evaluate the accuracy
    of the linear regression model’s predictions:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code calculates various performance metrics to assess the accuracy
    of the linear regression model’s predictions. Metrics such as mean squared error,
    explained variance score, mean absolute error, mean fundamental percentage error,
    d2 definitive error score, and d2 pinball score are computed based on the actual
    (`COUNT`) and predicted (`PREDICTED_COUNT`) values stored in the `result` DataFrame:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Performance metrics](img/B19923_05_17.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Performance metrics
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: These performance metrics provide a comprehensive evaluation of the model’s
    performance across different aspects of prediction accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Model results and efficiency
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The presented model metrics might need to showcase more exceptional results.
    It’s crucial to emphasize that the primary objective of this case study is to
    elucidate the model-building process and highlight the facilitative role of Snowpark
    ML. The focus of this chapter has been on illustrating the construction of a linear
    regression model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency of Snowpark ML
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In delving into the intricacies of the model-building process facilitated by
    Snowpark ML, the initial standout feature is its well-thought-out design. A notable
    departure from the conventional approach is evident as Snowpark ML closely mirrors
    the streamlined methodology found in scikit-learn. A significant advantage is
    eliminating the need to create separate **user-defined functions** (**UDFs**)
    and stored procedures, streamlining the entire model-building workflow.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to recognize that Snowpark ML seamlessly integrates with scikit-learn
    while adhering to similar conventions in the model construction process. A noteworthy
    distinction is a prerequisite in scikit-learn for data to be passed as a pandas
    DataFrame. Consequently, the Snowflake table must be converted into a pandas DataFrame
    before you can initiate the model-building phase. However, it’s imperative to
    be mindful of potential memory constraints, especially when dealing with substantial
    datasets. Converting a large table into a pandas DataFrame demands a significant
    amount of memory since the entire dataset is loaded into memory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, Snowpark ML provides a more native and memory-efficient approach
    to the model-building process. This native integration with Snowflake’s environment
    not only enhances the efficiency of the workflow but also mitigates memory-related
    challenges associated with large datasets. The utilization of Snowpark ML emerges
    as a strategic and seamless choice for executing complex model-building tasks
    within the Snowflake ecosystem.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Snowpark ML为模型构建过程提供了一个更原生且内存效率更高的方法。这种与Snowflake环境的原生集成不仅提高了工作流程的效率，还减轻了与大数据集相关的内存挑战。Snowpark
    ML的应用成为在Snowflake生态系统中执行复杂模型构建任务的战略性和无缝选择。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Snowpark ML emerges as a versatile and powerful tool for data scientists, enabling
    them to tackle complex data science tasks within Snowflake’s unified data platform.
    Its integration with popular programming languages, scalability, and real-time
    processing capabilities make it invaluable for various applications, from predictive
    modeling to real-time analytics and advanced AI tasks. With Snowpark ML, organizations
    can harness the full potential of their data, drive innovation, and gain a competitive
    edge in today’s data-driven landscape.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark ML成为数据科学家的一个多才多艺且强大的工具，使他们能够在Snowflake统一的数据库平台上处理复杂的数据科学任务。它与流行编程语言的集成、可扩展性和实时处理能力使其在从预测建模到实时分析和高级人工智能任务的各种应用中变得极其宝贵。借助Snowpark
    ML，组织可以利用其数据的全部潜力，推动创新，并在当今以数据驱动为导向的竞争环境中获得优势。
- en: In the next chapter, we will continue by deploying the model in Snowflake and
    operationalizing it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续通过在Snowflake中部署模型并实现其运营来推进。
